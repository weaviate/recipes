{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4661514",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/weaviate-features/services-research/late_chunking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ef08dfc-517d-417d-b631-cd87efb609aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ccd9da1-a393-4422-b6df-bfc264aa66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model = SentenceTransformer('jinaai/jina-embeddings-v2-base-en',trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75e4487-2355-44da-9120-26d6103b4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_tokens(input_text: str, tokenizer: callable, chunk_size: int = 512):\n",
    "    \"\"\"\n",
    "    Split the input text into chunks of approximately chunk_size tokens\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(input_text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    token_offsets = tokens['offset_mapping']\n",
    "    \n",
    "    chunks = []\n",
    "    span_annotations = []\n",
    "    \n",
    "    for i in range(0, len(token_offsets), chunk_size):\n",
    "        chunk_end = min(i + chunk_size, len(token_offsets))\n",
    "        if chunk_end - i > 0:\n",
    "            start_offset = token_offsets[i][0]\n",
    "            end_offset = token_offsets[chunk_end - 1][1]\n",
    "            chunks.append(input_text[start_offset:end_offset])\n",
    "            span_annotations.append((i, chunk_end))\n",
    "    \n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcf8dff-948f-48f9-a6fc-44f8c4735865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0deb5dc4-2e4e-4fda-af4f-2f8ed13dab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"As Weaviate celebrates its fifth anniversary, we've had the privilege of collaborating with tens of thousands of developers, gaining invaluable insights into the evolving landscape of AI projects and strategies. Our users constantly push the boundaries of what’s possible. As they continue to scale their applications in production, they guide the evolution of our product and the market itself. The need for optionality One of the main reasons developers choose Weaviate is the optionality it offers in terms of machine learning models, frameworks, and deployment. With new AI models and tools emerging daily, it's crucial to build systems that allow flexibility for tech stacks to evolve. This optionality, combined with ease of use, helps teams scale AI prototypes into production faster. Flexibility is also vital when it comes to architecture. Different use cases have different requirements. For example, we work with many software companies and those operating in regulated industries. They often require multi-tenancy to isolate data and maintain compliance. When building a Retrieval Augmented Generation (RAG) application, using account or user-specific data to contextualize results, data must remain within a dedicated tenant for its user group. Weaviate’s native, multi-tenant architecture shines for customers who need to prioritize data privacy while maintaining fast retrieval and accuracy. On the other hand, we support some very large scale single-tenant use cases that orient toward real-time data access. Many of these are in e-commerce and industries that compete on speed and customer experience. Even the slightest latency can send their users looking elsewhere. These use cases leverage our HNSW index on hot storage and vector compression to ensure low latency. The point is, there is no one-size-fits-all solution so optionality is key. I’m very proud that through learning from our customers and community, we’re building a solution that supports diverse use cases and the evolving needs of developers. Introducing hot, warm, and cold storage tiers It’s amazing to see our customers' products gain popularity, attracting more users, and in many cases, tenants. However, as multi-tenant use cases scale, infrastructure costs can quickly become prohibitive. Since multi-tenancy is a core tenet of our architecture, the next logical step for us was to build a way to help customers drive more efficient resource consumption. We’re pleased to offer tenant offloading and hot, warm, and cold storage tiers as part of our latest release. Weaviate users (Open Source and Enterprise Cloud) can now deactivate or offload tenants to less-expensive warm or cold storage and reactivate them dynamically, based on the unique patterns of their use case. Here’s what it might look like in practice: One of our customers develops an email platform with tens of thousands of users. 80% of their users are only active during a 12-hour window (US business hours). With our new storage tiers, they can offload tenants to cold storage to save on infrastructure costs when users are inactive. When a user comes online, they can quickly warm up the tenant. This way they reduce storage costs while still offering performance that meets the needs of their customers. alt The Weaviate AI Unit To adapt to this product change and the evolving AI stack, we’ve introduced a new pricing unit to our Enterprise Cloud offering. An AI Unit (AIU) is a Weaviate-specific unit that can be applied to hot, warm, and cold storage tiers and compute costs. AIUs enable customers to better monitor usage and improve budgeting. In addition to resource costs, AIUs will apply to new AI-native Apps as they are released (more on that next). Apps and tools to fuel AI-native development As we continue to listen to our community, it’s clear that developers need an AI-native framework offering not just flexibility, but also modular GUI tools to interact with their data and accelerate their use cases. We’re excited about a new line of AI-native apps and tools that will help developers and business users accelerate common use cases. Recommender App Our first app is a Recommender service, now in private beta. The Recommender is a fully managed, low-code way to build scalable recommendation systems. It offers configurable endpoints for item-to-item, item-to-user, and user-to-user recommendation scenarios across multimodal data. Sign up for the private beta here, and stay tuned for more Apps updates coming soon. alt Weaviate Cloud Tools Lastly, new Weaviate Cloud Tools give developers and non-technical users an easier way to manage, explore, and interact with their data within Weaviate Cloud. The Query and Collections tools are available now in the Weaviate Cloud Console. It’s been an exciting few months, and I’m ecstatic to continue learning from our community and empowering developers to build the future of AI-native possibilities. To dive deeper into our latest product updates, join our upcoming webinar.\"\"\"\n",
    "chunks, span_annotations = chunk_by_tokens(text, tokenizer, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e99334f-0687-49d7-8db5-501dafdba764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Chunk 0 Start ***\n",
      "As Weaviate celebrates its fifth anniversary, we've had the privilege of collaborating with tens of thousands of developers, gaining invaluable insights into the evolving landscape of AI projects and strategies. Our users constantly push the boundaries of what’s possible. As they continue to scale their applications in production, they guide the evolution of our product and the market itself. The need for optionality One of the main reasons developers choose Weaviate is the optionality it offers in terms of machine learning models, frameworks, and deployment. With new AI models and tools emerging daily, it's crucial to build systems that allow flexibility for\n",
      "*** Chunk 0 End ***\n",
      "*** Chunk 1 Start ***\n",
      "tech stacks to evolve. This optionality, combined with ease of use, helps teams scale AI prototypes into production faster. Flexibility is also vital when it comes to architecture. Different use cases have different requirements. For example, we work with many software companies and those operating in regulated industries. They often require multi-tenancy to isolate data and maintain compliance. When building a Retrieval Augmented Generation (RAG) application, using account or user-specific data to contextualize results, data must remain within a dedicated tenant for its user group. Weaviate’s native, multi-tenant architecture shines for customers who need to prioritize\n",
      "*** Chunk 1 End ***\n",
      "*** Chunk 2 Start ***\n",
      "data privacy while maintaining fast retrieval and accuracy. On the other hand, we support some very large scale single-tenant use cases that orient toward real-time data access. Many of these are in e-commerce and industries that compete on speed and customer experience. Even the slightest latency can send their users looking elsewhere. These use cases leverage our HNSW index on hot storage and vector compression to ensure low latency. The point is, there is no one-size-fits-all solution so optionality is key. I’m very proud that through learning from our customers and community, we’re building a solution that supports\n",
      "*** Chunk 2 End ***\n",
      "*** Chunk 3 Start ***\n",
      "diverse use cases and the evolving needs of developers. Introducing hot, warm, and cold storage tiers It’s amazing to see our customers' products gain popularity, attracting more users, and in many cases, tenants. However, as multi-tenant use cases scale, infrastructure costs can quickly become prohibitive. Since multi-tenancy is a core tenet of our architecture, the next logical step for us was to build a way to help customers drive more efficient resource consumption. We’re pleased to offer tenant offloading and hot, warm, and cold storage tiers as part of our latest release. Weaviate users (Open\n",
      "*** Chunk 3 End ***\n",
      "*** Chunk 4 Start ***\n",
      "Source and Enterprise Cloud) can now deactivate or offload tenants to less-expensive warm or cold storage and reactivate them dynamically, based on the unique patterns of their use case. Here’s what it might look like in practice: One of our customers develops an email platform with tens of thousands of users. 80% of their users are only active during a 12-hour window (US business hours). With our new storage tiers, they can offload tenants to cold storage to save on infrastructure costs when users are inactive. When a user comes online, they can quickly warm up the tenant. This way they reduce\n",
      "*** Chunk 4 End ***\n",
      "*** Chunk 5 Start ***\n",
      "storage costs while still offering performance that meets the needs of their customers. alt The Weaviate AI Unit To adapt to this product change and the evolving AI stack, we’ve introduced a new pricing unit to our Enterprise Cloud offering. An AI Unit (AIU) is a Weaviate-specific unit that can be applied to hot, warm, and cold storage tiers and compute costs. AIUs enable customers to better monitor usage and improve budgeting. In addition to resource costs, AIUs will apply to new AI-native Apps as they are released (more on that next). Apps and tools to fuel AI-native\n",
      "*** Chunk 5 End ***\n",
      "*** Chunk 6 Start ***\n",
      "development As we continue to listen to our community, it’s clear that developers need an AI-native framework offering not just flexibility, but also modular GUI tools to interact with their data and accelerate their use cases. We’re excited about a new line of AI-native apps and tools that will help developers and business users accelerate common use cases. Recommender App Our first app is a Recommender service, now in private beta. The Recommender is a fully managed, low-code way to build scalable recommendation systems. It offers configurable endpoints for item-to-item, item-to-user, and\n",
      "*** Chunk 6 End ***\n",
      "*** Chunk 7 Start ***\n",
      "user-to-user recommendation scenarios across multimodal data. Sign up for the private beta here, and stay tuned for more Apps updates coming soon. alt Weaviate Cloud Tools Lastly, new Weaviate Cloud Tools give developers and non-technical users an easier way to manage, explore, and interact with their data within Weaviate Cloud. The Query and Collections tools are available now in the Weaviate Cloud Console. It’s been an exciting few months, and I’m ecstatic to continue learning from our community and empowering developers to build the future of AI-native possibilities. To dive deeper into our latest\n",
      "*** Chunk 7 End ***\n",
      "*** Chunk 8 Start ***\n",
      "product updates, join our upcoming webinar.\n",
      "*** Chunk 8 End ***\n"
     ]
    }
   ],
   "source": [
    "for i,chunk in enumerate(chunks):\n",
    "    print(f\"*** Chunk {i} Start ***\")\n",
    "    print(chunk)\n",
    "    print(f\"*** Chunk {i} End ***\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3308a6c-d725-4eb2-87fe-6711107b7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = model.encode(text, output_value=\"token_embeddings\").unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7afb3370-72e6-4d67-92bd-fd22f486b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_embeddings = late_chunking(token_embeddings, [span_annotations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da490833-baeb-4149-ba54-658c7f22c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06dece34-be75-4e7b-8773-e83d86db4baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query is: what do customers need to prioritze?\n",
      "\n",
      "Top 10 results for Naive Chunking:\n",
      "Chunk 8: Cosine Similarity = 0.7555344104766846\n",
      "product updates, join our upcoming webinar.\n",
      "----\n",
      "Chunk 3: Cosine Similarity = 0.7479638457298279\n",
      "diverse use cases and the evolving needs of developers. Introducing hot, warm, and cold storage tiers It’s amazing to see our customers' products gain popularity, attracting more users, and in many cases, tenants. However, as multi-tenant use cases scale, infrastructure costs can quickly become prohibitive. Since multi-tenancy is a core tenet of our architecture, the next logical step for us was to build a way to help customers drive more efficient resource consumption. We’re pleased to offer tenant offloading and hot, warm, and cold storage tiers as part of our latest release. Weaviate users (Open\n",
      "----\n",
      "\n",
      "Top 10 results for Late Chunking:\n",
      "Chunk 2: Cosine Similarity = 0.7008509039878845\n",
      "data privacy while maintaining fast retrieval and accuracy. On the other hand, we support some very large scale single-tenant use cases that orient toward real-time data access. Many of these are in e-commerce and industries that compete on speed and customer experience. Even the slightest latency can send their users looking elsewhere. These use cases leverage our HNSW index on hot storage and vector compression to ensure low latency. The point is, there is no one-size-fits-all solution so optionality is key. I’m very proud that through learning from our customers and community, we’re building a solution that supports\n",
      "----\n",
      "Chunk 1: Cosine Similarity = 0.6894500851631165\n",
      "tech stacks to evolve. This optionality, combined with ease of use, helps teams scale AI prototypes into production faster. Flexibility is also vital when it comes to architecture. Different use cases have different requirements. For example, we work with many software companies and those operating in regulated industries. They often require multi-tenancy to isolate data and maintain compliance. When building a Retrieval Augmented Generation (RAG) application, using account or user-specific data to contextualize results, data must remain within a dedicated tenant for its user group. Weaviate’s native, multi-tenant architecture shines for customers who need to prioritize\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query_text = \"what do customers need to prioritze?\"\n",
    "query = model.encode(query_text)\n",
    "print(f\"The query is: {query_text}\")\n",
    "\n",
    "naive_results = []\n",
    "late_results = []\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    chunk_n = i\n",
    "    naive_sim = cos_sim(query, model.encode(chunks[chunk_n]))\n",
    "    late_sim = cos_sim(query, chunk_embeddings[0][chunk_n])\n",
    "    \n",
    "    naive_results.append((naive_sim, chunk_n))\n",
    "    late_results.append((late_sim, chunk_n))\n",
    "\n",
    "# Sort results in descending order of cosine similarity\n",
    "naive_results.sort(reverse=True)\n",
    "late_results.sort(reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 results for Naive Chunking:\")\n",
    "for sim, chunk_n in naive_results[:2]:\n",
    "    print(f\"Chunk {chunk_n}: Cosine Similarity = {sim}\")\n",
    "    print(chunks[chunk_n].strip())\n",
    "    print(\"----\")\n",
    "\n",
    "print(\"\\nTop 10 results for Late Chunking:\")\n",
    "for sim, chunk_n in late_results[:2]:\n",
    "    print(f\"Chunk {chunk_n}: Cosine Similarity = {sim}\")\n",
    "    print(chunks[chunk_n].strip())\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "late-chunking-experiments-rJSr897M-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
