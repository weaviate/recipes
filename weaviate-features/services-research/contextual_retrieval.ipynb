{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/weaviate-features/services-research/contextual_retrieval.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Retrieval\n",
    "\n",
    "Notebook author: Danny Williams @ Weaviate\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) is a technique to improve the accuracy of vector search by providing additional context for the chunks of a document, by inputting both the document and the chunk to an LLM and asking it to provide a succinct context for the chunk within the document.\n",
    "\n",
    "This is a way to combat the lost context problem that occurs in chunking, e.g., if a text is split into sentences, the context of later sentences as they relate to earlier sentences is lost.\n",
    "\n",
    "### Example\n",
    "\n",
    "We'll use the following example to illustrate this. Consider the following text (generated and not from a real company):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The recent SEC filing provided insights into ACME Corp's performance for Q2 2023. \n",
    "It highlighted a 3% revenue growth over the previous quarter. \n",
    "The company, which had a revenue of $314 million in the prior quarter, showed steady progress. \n",
    "They attributed this growth to strategic initiatives and operational efficiencies. \n",
    "The report emphasized the company's resilience and ability to navigate market challenges, reflecting positively on their financial health and future prospects.\n",
    "\"\"\".strip().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And splitting this into sentences, we get the following chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw chunks:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The recent SEC filing provided insights into ACME Corp's performance for Q2 2023.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "It highlighted a 3% revenue growth over the previous quarter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The company, which had a revenue of $314 million in the prior quarter, showed steady progress.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "They attributed this growth to strategic initiatives and operational efficiencies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The report emphasized the company's resilience and ability to navigate market challenges, reflecting positively on their financial health and future prospects.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chunks = text.split(\".\")\n",
    "chunks = [chunk.strip() + \".\" for chunk in chunks if len(chunk) > 0]\n",
    "\n",
    "print(\"Raw chunks:\")\n",
    "print(\"-\" * 100)\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the second sentence onwards, the sentences do not state anything about ACME Corp, even though the sentences are explicitly referring to this company. So the context is lost.\n",
    "\n",
    "### Comparison Methods\n",
    "\n",
    "Recent methods have been proposed to alleviate this problem, such as Contextual Retrieval (discussed above) and Late chunking, which you can find more information about [here](https://jina.ai/news/late-chunking-in-long-context-embedding-models/) and [here](https://weaviate.io/blog/late-chunking), and an implementation in Weaviate [here](https://github.com/weaviate/recipes/blob/main/weaviate-features/services-research/late_chunking_berlin.ipynb).\n",
    "\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install the packages relevant to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install anthropic sentence_transformers einops pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Anthropic: 0.34.2\n",
      "Using Sentence Transformers: 3.1.1\n",
      "Using Pandas: 2.2.3\n",
      "Using NumPy: 2.1.1\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"Using Anthropic: {anthropic.__version__}\")\n",
    "print(f\"Using Sentence Transformers: {sentence_transformers.__version__}\")\n",
    "print(f\"Using Pandas: {pd.__version__}\")\n",
    "print(f\"Using NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to set up the LLM API, so we can pass the chunks and document to the LLM.\n",
    "\n",
    "In this instance we are going to use Anthropic's Claude API, and Claude 3 Haiku, similar to the original outline in the web post for Contextual Retrieval.\n",
    "\n",
    "To reproduce this notebook, you will need to have an Anthropic API key. You can get one [here](https://console.anthropic.com/settings/keys), and you will need to place this in an environment variable (within `.env`) called `ANTHROPIC_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "client = anthropic.Anthropic(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be performing semantic search, so we need to set up the embedding model. In this instance, we are using the Jina Embedding v3 model, which is a new model that is specifically designed for text matching. We also use the Jina model partly because it is lightweight, and partly because it works well with late chunking, which we will be providing comparisons for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Retrieval Setup\n",
    "\n",
    "First, we specify the prompt we will use to generate the contextual chunks (same as in the original webpost), which just asks the LLM to provide a succinct context for the chunk within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextual chunks\n",
    "anthropic_prompt = \"\"\"\n",
    "<document> \n",
    "{{WHOLE_DOCUMENT}} \n",
    "</document> \n",
    "Here is the chunk we want to situate within the whole document \n",
    "<chunk> \n",
    "{{CHUNK_CONTENT}} \n",
    "</chunk> \n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. \n",
    "\"\"\".strip()\n",
    "anthropic_prompt = anthropic_prompt.replace(\"{{WHOLE_DOCUMENT}}\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a list of augmented chunks, which includes the context of the document provided by the LLM as well as the chunk itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_chunks = []\n",
    "for chunk in chunks:\n",
    "    anthropic_prompt = anthropic_prompt.replace(\"{{CHUNK_CONTENT}}\", chunk)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": anthropic_prompt}\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    contextual_chunks.append(response.content[0].text + \" \" + chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual chunks:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The document provides an overview of ACME Corp's financial performance in Q2 2023 based on their recent SEC filing. The recent SEC filing provided insights into ACME Corp's performance for Q2 2023.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The document provides an overview of ACME Corp's financial performance in Q2 2023 based on the recent SEC filing. It highlighted a 3% revenue growth over the previous quarter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The document provides an overview of ACME Corp's financial performance in Q2 2023. The company, which had a revenue of $314 million in the prior quarter, showed steady progress.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The document provides an overview of ACME Corp's financial performance for the second quarter of 2023 based on its recent SEC filing. They attributed this growth to strategic initiatives and operational efficiencies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The document provides an overview of ACME Corp's financial performance in Q2 2023 based on their recent SEC filing. The report emphasized the company's resilience and ability to navigate market challenges, reflecting positively on their financial health and future prospects.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Contextual chunks:\")\n",
    "print(\"-\" * 100)\n",
    "for chunk in contextual_chunks:\n",
    "    print(chunk)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late Chunking Setup\n",
    "\n",
    "We are going to compare against late chunking, which is a method of embedding the text at the token level and then averaging the embeddings for each chunk.\n",
    "\n",
    "First, we obtain the token embeddings for the document (which is length `num_tokens x embedding_dim`), and we will later average these to get a single embedding for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is on the CPU\n",
    "model._first_module().auto_model.to('cpu')\n",
    "\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Move inputs to CPU\n",
    "inputs = {key: value.to('cpu') for key, value in tokenized_text.items()}\n",
    "\n",
    "token_embeddings = model._first_module().auto_model(**inputs).last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to record the positions of the tokens that represent the start and end of each chunk, i.e. the start and end of each sentence denoted by a full stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenized_text.tokens()\n",
    "positions = []\n",
    "start = 1\n",
    "for i, token in enumerate(tokens):\n",
    "    if token == \".\":\n",
    "        positions.append((start, i+1))\n",
    "        start = i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to verify that we have the correct positions, we will convert the indices back to text and print them, and they should match the earlier chunks (up to some tokenization differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted texts:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The recent SEC filing provided insights into ACME Corp's performance for Q2 2023.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "It highlighted a 3% revenue growth over the previous quarter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The company, which had a revenue of $314 million in the prior quarter, showed steady progress.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "They attributed this growth to strategic initiatives and operational efficiencies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The report emphasized the company's resilience and ability to navigate market challenges, reflecting positively on their financial health and future prospects.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Convert indices back to text using tokenizer\n",
    "converted_texts = []\n",
    "for start, end in positions:\n",
    "    token_ids = tokenized_text['input_ids'][0][start:end]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    text = tokenizer.convert_tokens_to_string(tokens)\n",
    "    converted_texts.append(text)\n",
    "\n",
    "print(\"Converted texts:\")\n",
    "print(\"-\" * 100)\n",
    "for text in converted_texts:\n",
    "    print(text)\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Now that we have set up the contextually augmented chunks and the late chunked embeddings, we can compute the embeddings for each chunk by either pooling the token embeddings for each chunk (late chunking) or by encoding the contextual chunk directly (contextual chunking). Additionally, we will compute the naive chunking embeddings by encoding the chunks directly. The naive chunking embeddings are the standard way of doing embeddings, i.e. encoding the text directly without any context added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# late chunking embeddings\n",
    "late_chunk_embeddings = []\n",
    "for start, end in positions:\n",
    "    late_chunk_embeddings.append(token_embeddings.squeeze()[start:end, :].mean(0).float().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextual chunking embeddings\n",
    "contextual_chunk_embeddings = []\n",
    "for chunk in contextual_chunks:\n",
    "    contextual_chunk_embeddings.append(model.encode(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive chunking embeddings\n",
    "naive_chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    naive_chunk_embeddings.append(model.encode(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "We will be querying across these embeddings and comparing the results. We will use cosine similarity to compare the embeddings, given first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a given query, we want to see how similar the query is to each chunk using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is ACME Corp's revenue growth for Q2 2023?\"\n",
    "query_embedding = model.encode(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code calculates these cosine similarities for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_chunking_similarities = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in late_chunk_embeddings]\n",
    "contextual_chunking_similarities = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in contextual_chunk_embeddings]\n",
    "naive_chunking_similarities = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in naive_chunk_embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next cell displays these similarities (in markdown format) for each method. Since our query is based on the revenue growth of ACME Corp, we are looking out for the similarity score for the chunk that contains this information, which in this case is the second chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Chunk                                                                                                                                                           | Late Chunking Similarity   | Contextual Retrieval Similarity   | Naive Chunking Similarity   |\n",
       "|:----------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------|:----------------------------------|:----------------------------|\n",
       "| The recent SEC filing provided insights into ACME Corp's performance for Q2 2023.                                                                               | 0.8305                     | 0.8069                            | **0.8505**                  |\n",
       "| **It highlighted a 3% revenue growth over the previous quarter.**                                                                                               | **0.8516**                 | **0.8590**                        | *0.6343*                    |\n",
       "| The company, which had a revenue of $314 million in the prior quarter, showed steady progress.                                                                  | *0.8424*                   | *0.8546*                          | 0.6169                      |\n",
       "| They attributed this growth to strategic initiatives and operational efficiencies.                                                                              | 0.7997                     | 0.8234                            | 0.5191                      |\n",
       "| The report emphasized the company's resilience and ability to navigate market challenges, reflecting positively on their financial health and future prospects. | 0.8022                     | 0.8061                            | 0.6007                      |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = {\n",
    "    \"Chunk\": chunks,\n",
    "    \"Late Chunking Similarity\": late_chunking_similarities,\n",
    "    \"Contextual Retrieval Similarity\": contextual_chunking_similarities,\n",
    "    \"Naive Chunking Similarity\": naive_chunking_similarities\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Function to format the similarities column-wise\n",
    "def format_similarities_column_wise(df, column_name):\n",
    "    max_sim = df[column_name].max()\n",
    "    second_max_sim = df[column_name][df[column_name] != max_sim].max()\n",
    "    \n",
    "    df[column_name] = df[column_name].apply(lambda x: f\"**{x:.4f}**\" if x == max_sim else f\"*{x:.4f}*\" if x == second_max_sim else f\"{x:.4f}\")\n",
    "    return df\n",
    "\n",
    "# Apply the formatting function to each similarity column\n",
    "df_results = format_similarities_column_wise(df_results, \"Late Chunking Similarity\")\n",
    "df_results = format_similarities_column_wise(df_results, \"Contextual Retrieval Similarity\")\n",
    "df_results = format_similarities_column_wise(df_results, \"Naive Chunking Similarity\")\n",
    "\n",
    "# Make the second chunk bold\n",
    "df_results.iloc[1, 0] = f\"**{df_results.iloc[1, 0].strip()}**\"\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def df_to_markdown(df):\n",
    "    markdown_str = df.to_markdown(index=False)\n",
    "    return markdown_str\n",
    "\n",
    "markdown_results = df_to_markdown(df_results)\n",
    "display(Markdown(markdown_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table highlights, in bold, the chunk that contains the information that the query is looking for.\n",
    "\n",
    "The similiarities in bold are the highest similiarities for each method, and the similiarities in italics are the second highest.\n",
    "\n",
    "As expected, both contextual retrieval and late chunking have a high similarity score for the second chunk - which is correctly identifying the chunk with this information.\n",
    "\n",
    "Naive chunking, on the other hand, does not correctly identify the chunk with this information, since the context of the first sentence is lost. Instead, it matches to the first chunk since that contains more similarity with the words of the query, rather than the semantic meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
