{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/weaviate-features/services-research/contextual_document_embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Document Embeddings\n",
    "\n",
    "Notebook author: Danny Williams @ Weaviate\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Contextual Document Embeddings](https://arxiv.org/abs/2410.02525) is a new technique for embedding documents that takes into account the context of the neighbouring documents/chunks.\n",
    "\n",
    "The model is hosted on Hugging Face under [jxm/cde-small-v1](https://huggingface.co/jxm/cde-small-v1), and can be loaded using the `sentence-transformers` library, and therefore can easily be used to load locally and embed documents, and therefore can also easily be used in Weaviate.\n",
    "\n",
    "### Contextual Document Embeddings\n",
    "\n",
    "In short, the model does two things differently to standard embedding models:\n",
    "\n",
    "1. During training, similar documents are clustered into batches, and the model is trained on each batch.\n",
    "2. During inference, the model takes in the document and a collection of documents within the same dataset, and outputs an embedding that takes this context into account.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Firstly, let's install the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LOG_LEVEL\"] = \"error\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence-transformers\n",
    "!pip install weaviate-client\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "\n",
    "Let's first set up an example to show off the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# newsgroups posts specifically about baseball\n",
    "data = fetch_20newsgroups(subset='train', categories=['rec.sport.baseball'], remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maybe it's just me, but the combination of those *young* faces peeking out\n",
      "from under oversized aqua helmets screams \"Little League\" in every fibre of\n",
      "my being...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of one piece of text\n",
    "print(data[\"data\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "\n",
    "The model can be used as-is, or we can include surrounding contexts to improve the embedding towards our specific purposes.\n",
    "\n",
    "### As-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load the model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('jxm/cde-small-v1', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([768])\n",
      "Embedding: [0.2638474404811859, 0.1533348560333252, -0.2799359858036041, 0.05649382993578911]...\n"
     ]
    }
   ],
   "source": [
    "# embed a single document\n",
    "single_non_contextual_embedding = model.encode(\n",
    "    data[\"data\"][0],\n",
    "    prompt_name=\"document\",\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "print(f\"Embedding shape: {single_non_contextual_embedding.shape}\")\n",
    "print(f\"Embedding: {single_non_contextual_embedding[:4].tolist()}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset embeddings shape: torch.Size([597, 768])\n"
     ]
    }
   ],
   "source": [
    "# embed the entire dataset\n",
    "non_contextual_document_embeddings = model.encode(\n",
    "    data[\"data\"],\n",
    "    prompt_name=\"document\",\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "print(f\"Dataset embeddings shape: {non_contextual_document_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Context\n",
    "\n",
    "First, we compute embeddings _for context_, taken on our full dataset, so that the model can use this later to provide context to future document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-corpus size: 512\n"
     ]
    }
   ],
   "source": [
    "minicorpus_size = model[0].config.transductive_corpus_size\n",
    "print(f\"Mini-corpus size: {minicorpus_size}\")\n",
    "\n",
    "dataset_embeddings = model.encode(\n",
    "    data[\"data\"][:minicorpus_size],\n",
    "    prompt_name=\"document\",\n",
    "    convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these embeddings to provide context to a new document, with a simple argument to the `encode` method of `dataset_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([768])\n",
      "Embedding: [0.025975484400987625, 0.016233962029218674, 0.05705662816762924, -0.003157366067171097]...\n"
     ]
    }
   ],
   "source": [
    "single_contextual_embedding = model.encode(\n",
    "    data[\"data\"][0],\n",
    "    prompt_name=\"document\",\n",
    "    dataset_embeddings=dataset_embeddings,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "\n",
    "print(f\"Embedding shape: {single_contextual_embedding.shape}\")\n",
    "print(f\"Embedding: {single_contextual_embedding[:4].tolist()}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this embedding is different to the non-contextual embedding, but should still be semantically similar to the original document, due to the context added via the `dataset_embeddings` argument. These embeddings 'prime' the model to understand the context of the dataset, and so the embeddings for new documents are more contextually aware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset embeddings shape: torch.Size([597, 768])\n"
     ]
    }
   ],
   "source": [
    "contextual_document_embeddings = model.encode(\n",
    "    data[\"data\"],\n",
    "    prompt_name=\"document\",\n",
    "    dataset_embeddings=dataset_embeddings,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "print(f\"Dataset embeddings shape: {contextual_document_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate\n",
    "\n",
    "Let's create two collections in Weaviate to query, one with not contextually primed embeddings, and one with contextual embeddings.\n",
    "First we will create a local Weaviate embedded instance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_embedded()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weaviate allows [bring your own vectors](https://weaviate.io/developers/weaviate/starter-guides/custom-vectors), which is what we will use here. We can use the embeddings that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes as wvc\n",
    "\n",
    "# Collection 1: non-contextual embeddings\n",
    "non_contextual_collection = client.collections.create(\n",
    "    \"non_contextual_embeddings\",\n",
    "    vectorizer_config = wvc.config.Configure.Vectorizer.none()\n",
    ")\n",
    "\n",
    "# appending the objects to the list, consisting of the text and the embedding vector calculated earlier\n",
    "objs = []\n",
    "for i, d in enumerate(data[\"data\"]):\n",
    "    objs.append(wvc.data.DataObject(\n",
    "            properties={\n",
    "                \"text\": data[\"data\"][i],\n",
    "            },\n",
    "            vector = non_contextual_document_embeddings[i].tolist()\n",
    "        )\n",
    "    )\n",
    "\n",
    "non_contextual_collection.data.insert_many(objs);\n",
    "\n",
    "# Collection 2: contextual embeddings\n",
    "contextual_collection = client.collections.create(\n",
    "    \"contextual_embeddings\",\n",
    "    vectorizer_config = wvc.config.Configure.Vectorizer.none()\n",
    ")\n",
    "\n",
    "# appending the objects to the list\n",
    "objs = []\n",
    "for i, d in enumerate(data[\"data\"]):\n",
    "    objs.append(wvc.data.DataObject(\n",
    "            properties={\n",
    "                \"text\": data[\"data\"][i],\n",
    "            },\n",
    "            vector = contextual_document_embeddings[i].tolist()\n",
    "        )\n",
    "    )\n",
    "\n",
    "contextual_collection.data.insert_many(objs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the two collections.\n",
    "\n",
    "We first need to embed our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much is Rickey Henderson being paid?\"\n",
    "non_contextual_query_embedding = model.encode(\n",
    "    query,\n",
    "    prompt_name=\"query\",\n",
    "    convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And retrieve the results with a near vector query in Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_contextual_results = non_contextual_collection.query.near_vector(\n",
    "    near_vector = non_contextual_query_embedding.tolist(),\n",
    "    limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.collections.delete(\"non_contextual_embeddings\")\n",
    "client.collections.delete(\"contextual_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results (non-contextual):\n",
      "\n",
      "Result 1\n",
      "_____\n",
      "Text: Davis will be paid by three clubs this year, I think the Phils are\n",
      "responsbible for about $600,000 or so.  They didn't wait for him to clear\n",
      "waivers as three other clubs were also very interested in him.  A gamble?\n",
      "Yes.\n",
      "\n",
      "Won the CY Young, too, for that year.\n",
      "_____\n",
      "\n",
      "\n",
      "\n",
      "Result 2\n",
      "_____\n",
      "Text: : I believe that Rusty Staub was also a jewish ball-player\n",
      ": Also, Mordaci Brown back in the early 20th century.  He was a pitcher whose\n",
      ": nickname was \"3 fingers\" Brown....for obvious reasons....he had 3 fingers.\n",
      "\n",
      "0 for 2, ma_ind25.\n",
      "\n",
      "Daniel Patrick Staub is a Catholic school kid from Nawlins, Mordecai\n",
      "Brown a farm kid (probably Protestant) from somewhere in the Midwest.\n",
      "He lost those fingers in a farm machinery accident.\n",
      "\n",
      "Jim Palmer isn't Jewish himself, but Mr. Jockey Shorts's adoptive \n",
      "parents are.\n",
      "\n",
      "Also, I'm not absolutely certain that Carew actually converted.  His\n",
      "wife and children certainly are Jewish.\n",
      "\n",
      "--\n",
      "_____\n",
      "\n",
      "\n",
      "\n",
      "Result 3\n",
      "_____\n",
      "Text: \n",
      "Wasn't Ron Bloomberg, the former Yankee who got the first base hit\n",
      "by a Designated Hitter, Jewish??\n",
      "_____\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 3 results (non-contextual):\\n\")\n",
    "for i, obj in enumerate(non_contextual_results.objects):\n",
    "    print(f\"Result {i+1}\")\n",
    "    print(\"_____\")\n",
    "    print(f\"Text: {obj.properties['text']}\")\n",
    "    print(\"_____\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same for the more contextually aware embeddings. First embed the query with the context of the dataset, in the same way as we did for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_query_embedding = model.encode(\n",
    "    query,\n",
    "    prompt_name=\"query\",\n",
    "    dataset_embeddings=dataset_embeddings,\n",
    "    convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then query with the near vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_results = contextual_collection.query.near_vector(\n",
    "    near_vector = contextual_query_embedding.tolist(),\n",
    "    limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results (contextual):\n",
      "\n",
      "Result 1\n",
      "_____\n",
      "Text: \n",
      "Actually, I could care less what his salary is.  It has something to do\n",
      "with the fact that we live in America, and everyone is entitled to\n",
      "whatever he can legally obtain.  If Sandy Alderson and the Haas family\n",
      "willingly negotiate a salary of $35 million per year with Rickey, I couldn't\n",
      "care less.\n",
      "\n",
      "But what REALLY GETS MY GOAT is the bullshit he spouted in spring training,\n",
      "about `Well... sometimes I may not play as hard, or might be hurt more\n",
      "often, in a place where I'm not appreciated'.  This quote was in the Chronicle\n",
      "about the second week of camp, and strongly suggests that he was going to \n",
      "dog it all year if the ownership didn't kiss his butt and ante up some\n",
      "more money.  For God's sake, Rickey, you signed a contract 4 years ago,\n",
      "now honor it and play!  \n",
      "\n",
      "Say all you want to about Steve Garvey, and believe\n",
      "me, I hated him too, but at least when he put his signature on a piece\n",
      "of paper he shut his mouth and played hard until the contract was up.\n",
      "_____\n",
      "\n",
      "\n",
      "\n",
      "Result 2\n",
      "_____\n",
      "Text: \n",
      "Learn what?  I know that 3 million dollars is A LOT of money.  I know \n",
      "Rickey Henderson doesn't have a career out of baseball.  I know if he \n",
      "didn't have baseball, he wouldn't be making near the money he is now.\n",
      "\n",
      "I just don't understand how some athlete, who only plays a sport for a \n",
      "living for millions of dollars, say he is not being paid enough.\n",
      "\n",
      "If nobody will sign him for his asking price, he will be the one hurting.\n",
      "The A's will still win without him.\n",
      "\n",
      "Remeber, many of these athletes have NOTHING if not for their athletic \n",
      "ability.  NOTHING.  They are getting paid MUCH more than most hard working\n",
      "citizens, and they are complaining of not enough pay.\n",
      "_____\n",
      "\n",
      "\n",
      "\n",
      "Result 3\n",
      "_____\n",
      "Text: %I say buy out Henderson's contract and let him go bag groceries.  Next \n",
      "%season, you'll be able to sign him for nothing.  That goes for any bitching\n",
      "%ball player.\n",
      "_____\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 3 results (contextual):\\n\")\n",
    "for i, obj in enumerate(contextual_results.objects):\n",
    "    print(f\"Result {i+1}\")\n",
    "    print(\"_____\")\n",
    "    print(f\"Text: {obj.properties['text']}\")\n",
    "    print(\"_____\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
