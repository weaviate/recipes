{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Agentic RAG vs Reason-ModernColBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook author: Danny Williams @ Weaviate\n",
    "\n",
    "This notebook will compare an 'agentic' RAG solution to dynamically searching a database via breaking down a question with complex reasoning, to a new method for complex reasoning retrieval: [Reason-ModernColBERT](https://huggingface.co/lightonai/Reason-ModernColBERT).\n",
    "\n",
    "For an overview of Reason-ModernColBERT, I recommend you check out [this recipe](https://github.com/weaviate/recipes/blob/main/weaviate-features/multi-vector/reason_moderncolbert.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's set up the dependencies for the notebook. For our generative models we will use OpenAI (and tiktoken for counting tokens), we need PyLate and Sentence Transformers to load the Reason-ModernColBERT model, we will of course use Weaviate as the vector database search engine, and to assess the quality of the results we will use DeepEval. We will also use rich for pretty printing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install weaviate-client==4.14.4\n",
    "!pip install pylate==1.2.0\n",
    "!pip install openai==1.84.0\n",
    "!pip install tiktoken==0.9.0\n",
    "!pip install deepeval==3.0.1\n",
    "!pip install rich==13.9.4\n",
    "!pip install protobuf==6.31.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional requirements include your Weaviate instance being on version 1.29 or later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define some helper functions for later which count and reduce the number of tokens in the data so we can truncate the texts if they are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def tokenize_text(text: str):\n",
    "    return tiktoken.get_encoding(\"o200k_base\").encode(text)\n",
    "\n",
    "def reduce_tokens(text: str, max_tokens: int):\n",
    "    tokens = tokenize_text(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        return tiktoken.get_encoding(\"o200k_base\").decode(tokens[:max_tokens])\n",
    "    return text\n",
    "\n",
    "def count_tokens(text: str):\n",
    "    return len(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define the embedding function for the target multi vector, using the Reason-ModernColBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pylate import models\n",
    "\n",
    "# Load the ModernColBERT model\n",
    "model = models.ColBERT(\n",
    "    model_name_or_path=\"lightonai/Reason-ModernColBERT\",\n",
    ")\n",
    "\n",
    "def multi_vec_embed(text: str):\n",
    "    return model.encode(text, is_query=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [BioASQ dataset](https://www.bioasq.org/), as it contains a lot of domain specific knowledge, whose questions require breaking down into individual parts and complex reasoning to obtain good retrieval performance. The BioASQ dataset contains:\n",
    "- 40.2K text passages\n",
    "- 4.72K question and answer pairs with corresponding relevant passage ids\n",
    "\n",
    "For this example notebook, let us consider only a subset of 100 questions. Each question has corresponding `relevant_passage_ids`, which is a list detailing which passages are pertinent to answering the question. We will include all these relevant passage IDs in the subset dataset, as well as a sample of irrelevant passages.\n",
    "\n",
    "Let us first use pandas to load these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "questions_splits = {'train': 'question-answer-passages/train-00000-of-00001.parquet', 'test': 'question-answer-passages/test-00000-of-00001.parquet'}\n",
    "questions_df = pd.read_parquet(\"hf://datasets/enelpol/rag-mini-bioasq/\" + questions_splits[\"train\"])\n",
    "\n",
    "texts_splits = {'train': 'text-corpus/train-00000-of-00001.parquet', 'test': 'text-corpus/test-00000-of-00001.parquet'}\n",
    "texts_df = pd.read_parquet(\"hf://datasets/enelpol/rag-mini-bioasq/\" + texts_splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at a brief snapshot of the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>relevant_passage_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the implication of histone lysine meth...</td>\n",
       "      <td>Aberrant patterns of H3K4, H3K9, and H3K27 his...</td>\n",
       "      <td>1682</td>\n",
       "      <td>[23179372, 19270706, 23184418]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the role of STAG1/STAG2 proteins in di...</td>\n",
       "      <td>STAG1/STAG2 proteins are tumour suppressor pro...</td>\n",
       "      <td>3722</td>\n",
       "      <td>[26997282, 21589869, 19822671, 29867216, 15361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the association between cell phone use...</td>\n",
       "      <td>The association between cell phone use and inc...</td>\n",
       "      <td>1235</td>\n",
       "      <td>[20215713, 17851009, 22882019, 12527940, 24348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the applicability of the No Promoter L...</td>\n",
       "      <td>No Promoter Left Behind (NPLB) is an efficient...</td>\n",
       "      <td>2103</td>\n",
       "      <td>[26530723]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the Oncotype DX test work with paraffin e...</td>\n",
       "      <td>Yes, the Oncotype DX test works with paraffin ...</td>\n",
       "      <td>1713</td>\n",
       "      <td>[23074401, 17039265, 18922117, 17463177, 16361...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the implication of histone lysine meth...   \n",
       "1  What is the role of STAG1/STAG2 proteins in di...   \n",
       "2  What is the association between cell phone use...   \n",
       "3  What is the applicability of the No Promoter L...   \n",
       "4  Does the Oncotype DX test work with paraffin e...   \n",
       "\n",
       "                                              answer    id  \\\n",
       "0  Aberrant patterns of H3K4, H3K9, and H3K27 his...  1682   \n",
       "1  STAG1/STAG2 proteins are tumour suppressor pro...  3722   \n",
       "2  The association between cell phone use and inc...  1235   \n",
       "3  No Promoter Left Behind (NPLB) is an efficient...  2103   \n",
       "4  Yes, the Oncotype DX test works with paraffin ...  1713   \n",
       "\n",
       "                                relevant_passage_ids  \n",
       "0                     [23179372, 19270706, 23184418]  \n",
       "1  [26997282, 21589869, 19822671, 29867216, 15361...  \n",
       "2  [20215713, 17851009, 22882019, 12527940, 24348...  \n",
       "3                                         [26530723]  \n",
       "4  [23074401, 17039265, 18922117, 17463177, 16361...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New data on viruses isolated from patients wit...</td>\n",
       "      <td>9797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe an improved method for detecting d...</td>\n",
       "      <td>11906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We have studied the effects of curare on respo...</td>\n",
       "      <td>16083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kinetic and electrophoretic properties of 230-...</td>\n",
       "      <td>23188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male Wistar specific-pathogen-free rats aged 2...</td>\n",
       "      <td>23469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             passage     id\n",
       "0  New data on viruses isolated from patients wit...   9797\n",
       "1  We describe an improved method for detecting d...  11906\n",
       "2  We have studied the effects of curare on respo...  16083\n",
       "3  Kinetic and electrophoretic properties of 230-...  23188\n",
       "4  Male Wistar specific-pathogen-free rats aged 2...  23469"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_questions_df = questions_df.sample(frac=100/len(questions_df))\n",
    "\n",
    "relevant_passages = []\n",
    "for i, row in subset_questions_df.iterrows():\n",
    "    relevant_passages.extend(row[\"relevant_passage_ids\"])\n",
    "\n",
    "relevant_passages = list(set(relevant_passages))\n",
    "irrelevant_passages = texts_df.id.tolist()\n",
    "irrelevant_passages = [p for p in irrelevant_passages if p not in relevant_passages]\n",
    "\n",
    "# Get twice as many irrelevant passages\n",
    "import random\n",
    "random.seed(42)\n",
    "irrelevant_passages = random.sample(irrelevant_passages, len(relevant_passages)*2)\n",
    "\n",
    "subset_texts_df = texts_df[texts_df.id.isin(relevant_passages + irrelevant_passages)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant passages: 826\n",
      "Number of irrelevant passages: 1652\n",
      "Subset of 2478 passages created\n",
      "Subset of 100 questions created\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of relevant passages: {len(relevant_passages)}\")\n",
    "print(f\"Number of irrelevant passages: {len(irrelevant_passages)}\")\n",
    "print(f\"Subset of {len(subset_texts_df)} passages created\")\n",
    "print(f\"Subset of {len(subset_questions_df)} questions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Data to Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's connect to the Weaviate client. In this instance we are connecting to Weaviate cloud using API keys stored in the local environment. But you can also use [Weaviate Embedded](https://weaviate.io/developers/weaviate/installation/embedded), [Docker](https://weaviate.io/developers/weaviate/installation/docker-compose), amongst other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/aggregate.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/base.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/base_search.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/batch_delete.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/batch.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/search_get.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/generative.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/properties.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at v1/tenants.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.getenv(\"WCD_URL\"),\n",
    "    auth_credentials=Auth.api_key(os.getenv(\"WCD_API_KEY\")),\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a collection called `bioASQ_passages`, containing two named vectors - one for the single vector embeddings (using the default OpenAI vectorizer), and one with the multi-vector embeddings using the Reason-ModernColBERT model. For both we will use [scalar quantization](https://weaviate.io/developers/weaviate/concepts/vector-quantization#scalar-quantization) to reduce the memory footprint of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/weaviate/collections/classes/config.py:1963: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n"
     ]
    }
   ],
   "source": [
    "collection = client.collections.create(\n",
    "    \"bioASQ_passages\",\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.none(\n",
    "            name=\"multi_vector\",\n",
    "            vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                multi_vector=Configure.VectorIndex.MultiVector.multi_vector()\n",
    "            )\n",
    "        ),\n",
    "        Configure.NamedVectors.text2vec_openai(\n",
    "            name=\"single_vector\",\n",
    "            vector_index_config=Configure.VectorIndex.hnsw()\n",
    "        ),  \n",
    "    ],\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"text\",\n",
    "            data_type=DataType.TEXT,\n",
    "            vectorize_property_name=False  \n",
    "        ),\n",
    "        Property(\n",
    "            name=\"docid\",\n",
    "            data_type=DataType.TEXT,\n",
    "            vectorize_property_name=False  \n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import to Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2478/2478"
     ]
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "collection = client.collections.get(\"bioASQ_passages\")\n",
    "with collection.batch.fixed_size(10) as batch:\n",
    "    for iter, (_, doc) in enumerate(subset_texts_df.iterrows()):\n",
    "\n",
    "        uuid = generate_uuid5(doc.id)\n",
    "\n",
    "        text = doc.passage\n",
    "        \n",
    "        if count_tokens(text) > 6_000:\n",
    "            text = reduce_tokens(text, 6_000) # truncate long documents\n",
    "\n",
    "        batch.add_object(\n",
    "            properties={\"text\": text, \"docid\": str(doc.id)},\n",
    "            vector={\"multi_vector\": multi_vec_embed(text)},\n",
    "            uuid=uuid\n",
    "        )\n",
    "\n",
    "        print(f\"\\r{iter+1}/{len(subset_texts_df)}\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "answer_question_system_prompt = \"\"\"\n",
    "You are an expert at answering biomedical questions using a given context.\n",
    "You will be given a question, and a list of documents.\n",
    "You need to answer the question using the context.\n",
    "Do not answer the question if you do not have enough information.\n",
    "If the context does not fully answer the question, you must respond with \"I do not know\" or similar.\n",
    "Provide the answer only in a concise manner.\n",
    "Use citations to reference the context in your answer. It should be formatted as e.g. \"[1]\" at the end of each sentence that references the context.\n",
    "The contexts are marked with a number in square brackets, e.g. \"[1]\", at the beginning of each context.\n",
    "\"\"\"\n",
    "\n",
    "class AnswerQuestionOutput(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "def answer_question(question: str, context: list[str]):\n",
    "    context_str = \"\"\n",
    "    for i, c in enumerate(context):\n",
    "        context_str += f\"[{i+1}]: {c}\\n\\n\"\n",
    "\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": answer_question_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context_str}\"}\n",
    "        ],\n",
    "        response_format=AnswerQuestionOutput\n",
    "    )\n",
    "    return response.choices[0].message.parsed.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentic RAG with Single Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Agentic' RAG pipeline we will build is as follows. First, the system prompt loosely defines the dataset and gives instructions to the model to break down the question in terms of its core components. Then the model will extract these components to use in a search engine (Weaviate).\n",
    "\n",
    "The reasoning field will improve the model performance via chain-of-thought prompting, and allow the model to decide what parts of the question need to be delved into deeper to get more relevant search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/pydantic/fields.py:1058: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warn('`min_items` is deprecated and will be removed, use `min_length` instead', DeprecationWarning)\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/pydantic/fields.py:1064: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warn('`max_items` is deprecated and will be removed, use `max_length` instead', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ModelOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    search_components: list[str] = Field(min_items=3, max_items=3) # ensure the model is forced to provide 3 components (for comparison equity)\n",
    "\n",
    "extract_reasoning_system_prompt = \"\"\"\n",
    "You are an expert at breaking down biomedical questions into their fundamental components used for a retrieval service.\n",
    "You will be given a question, and you need to first explain your reasoning for breaking down the question into the components.\n",
    "Then you need to provide the components.\n",
    "Think carefully about what these components should be - it may not be outright stated in the question.\n",
    "Use deductive reasoning to think step by step to complete this task.\n",
    "Each component should be independent as they will be used separately to find relevant documents via a search engine.\n",
    "You MUST provide 3 components only. No more, no less.\n",
    "\"\"\"\n",
    "\n",
    "# Use GPT-4.1-mini for the model (cheaper option but will have lower quality)\n",
    "def extract_reasoning(question: str):\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": extract_reasoning_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "        ],\n",
    "        response_format=ModelOutput\n",
    "    )\n",
    "    reasoning = response.choices[0].message.parsed.reasoning\n",
    "    components = response.choices[0].message.parsed.search_components\n",
    "    return reasoning, components\n",
    "\n",
    "def agentic_rag_search(question: str, verbose: bool = False):\n",
    "    reasoning, components = extract_reasoning(question)\n",
    "    if verbose:\n",
    "        print(f\"Model Reasoning:\\n{reasoning}\")\n",
    "    \n",
    "    contexts = []\n",
    "    doc_ids = []\n",
    "    for search_component in components:\n",
    "        response = collection.query.hybrid(\n",
    "            query=search_component,\n",
    "            target_vector=\"single_vector\", # specify the single vector (not multi-vector)\n",
    "            limit=5\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"Search Component: '{search_component}'\")\n",
    "            print(f\"Search Results:\")\n",
    "            for i,obj in enumerate(response.objects):\n",
    "                print(f\"  {i+1}. {obj.properties['text'][:50].replace('\\n', ' ')}...\")\n",
    "        contexts.extend([obj.properties['text'] for obj in response.objects])\n",
    "        doc_ids.extend([int(obj.properties['docid']) for obj in response.objects])\n",
    "\n",
    "    answer = answer_question(question, contexts)\n",
    "    if verbose:\n",
    "        print(f\"Answer: {answer}\")\n",
    "    return answer, contexts, doc_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let's run the agentic model on the first question in the dataset to see an example in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Reasoning:\n",
      "The question asks specifically about thyroid hormone analogs that have been used in human studies. To address this, it is important to identify: 1) the focus on 'thyroid hormone analogs' which implies substances structurally or functionally similar to thyroid hormones, 2) the usage context in 'human studies' rather than animal models or theoretical compounds, and 3) the need to look for specific names or classes of these analogs that are documented in clinical or experimental human research. These components will help retrieve scientific or clinical literature discussing thyroid hormone analogs tested or used in humans.\n",
      "Search Component: 'Thyroid hormone analogs'\n",
      "Search Results:\n",
      "  1. Thyroid hormones [predominantly 3, 5, 3 -I- iodoth...\n",
      "  2. Thyromimetic agents that can treat dyslipidemia wi...\n",
      "  3. We previously reported that T3(3,3',5-triiodo-L-th...\n",
      "  4. We have recently described the proangiogenesis eff...\n",
      "  5. 3,5,3,'-Triiodothyroacetic acid (Triac) has been u...\n",
      "Search Component: 'Human studies'\n",
      "Search Results:\n",
      "  1. Conflict of interest statement: FUNDING: This stud...\n",
      "  2. BACKGROUND: Weight reduction without behavioral mo...\n",
      "  3. Studying the cognitive and immunological changes t...\n",
      "  4. Understanding of linkage disequilibrium (LD) in hu...\n",
      "  5. BACKGROUND: Besides fluctuations, therapy refracto...\n",
      "Search Component: 'Thyroid hormone analogs in clinical research'\n",
      "Search Results:\n",
      "  1. Thyroid hormones [predominantly 3, 5, 3 -I- iodoth...\n",
      "  2. Thyromimetic agents that can treat dyslipidemia wi...\n",
      "  3. We previously reported that T3(3,3',5-triiodo-L-th...\n",
      "  4. We have recently described the proangiogenesis eff...\n",
      "  5. 3,5,3,'-Triiodothyroacetic acid (Triac) has been u...\n",
      "Answer: The thyroid hormone analogs utilized in human studies include GC1, KB-2115, KB-141, Eprotirome, Sobetirome, DITPA, and Triac. These analogs have selective actions primarily targeting thyroid hormone receptor beta1 (TRŒ≤1) with potential uses in cholesterol reduction, obesity treatment, and therapy of resistance to thyroid hormone. Studies in humans with these drugs are preliminary, and their tolerance and efficacy are still being investigated [1][2][4][5].\n"
     ]
    }
   ],
   "source": [
    "agentic_rag_search(subset_questions_df.iloc[0][\"question\"], verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason Modern-ColBERT Embedding Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reason_moderncolbert_search(question: str, verbose: bool = False):\n",
    "    response = collection.query.near_vector(\n",
    "        near_vector=multi_vec_embed(question),\n",
    "        target_vector=\"multi_vector\", # specify the multi-vector (not single-vector)\n",
    "        limit=15 # 3*the results per component (which is max 3, so this search has the same number of contexts as agentic search)\n",
    "    )\n",
    "    contexts = [obj.properties['text'] for obj in response.objects]\n",
    "    doc_ids = [int(obj.properties['docid']) for obj in response.objects]\n",
    "    if verbose:\n",
    "        print(f\"Search Results:\")\n",
    "        for i, context in enumerate(contexts):\n",
    "            print(f\"  {i+1}. {context[:25].replace('\\n', ' ')}...\")\n",
    "\n",
    "    # use the same answer_question function as the agentic search\n",
    "    answer = answer_question(question, contexts)\n",
    "    if verbose:\n",
    "        print(f\"Answer: {answer}\")\n",
    "    return answer, contexts, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "  1. Thyroid hormones [predomi...\n",
      "  2. CONTEXT: Thyronamines are...\n",
      "  3. 3,5,3,'-Triiodothyroaceti...\n",
      "  4. We previously reported th...\n",
      "  5. Thyromimetic agents that ...\n",
      "  6. The endogenous thyroid ho...\n",
      "  7. We have recently describe...\n",
      "  8. BACKGROUND: Tetraiodothyr...\n",
      "  9. OBJECTIVE: The monocarbox...\n",
      "  10. The worldwide prevalence ...\n",
      "  11. A protein that binds tetr...\n",
      "  12. Diiodothyropropionic acid...\n",
      "  13. The monocarboxylate trans...\n",
      "  14. Gross clinical manifestat...\n",
      "  15. BACKGROUND: The effective...\n",
      "Answer: The thyroid hormone analogs utilized in human studies include GC1, KB-2115, KB-141, thyronamines (including 3-iodothyronamine, 3-T1AM), 3,5,3'-triiodothyroacetic acid (Triac), 3,5,3'-triiodothyropropionic acid (Triprop), eprotirome, sobetirome, diiodothyropropionic acid (DITPA), tetraiodothyroacetic acid (tetrac), and 3,5-diiodo-L-thyronine (T2). Some of these analogs are under investigation for effects on lipid metabolism, cardiac effects, angiogenesis, and treatment of thyroid hormone resistance. Triac has been used empirically and in clinical therapy, and preliminary human studies have been conducted with thyronamines and other analogs. Eprotirome and sobetirome are selective TRbeta1 agonists studied in dyslipidemia. Tetrac and DITPA have also been explored in models relevant to human disease. The tolerance and efficacy of many of these agents in humans are still under investigation or at early stages of clinical trials [1,2,3,5,6,7,8,10,12,13,14,15].\n"
     ]
    }
   ],
   "source": [
    "reason_moderncolbert_search(subset_questions_df.iloc[0][\"question\"], verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "  1. Thyroid hormones [predomi...\n",
      "  2. CONTEXT: Thyronamines are...\n",
      "  3. 3,5,3,'-Triiodothyroaceti...\n",
      "  4. We previously reported th...\n",
      "  5. Thyromimetic agents that ...\n",
      "  6. The endogenous thyroid ho...\n",
      "  7. We have recently describe...\n",
      "  8. BACKGROUND: Tetraiodothyr...\n",
      "  9. OBJECTIVE: The monocarbox...\n",
      "  10. The worldwide prevalence ...\n",
      "  11. A protein that binds tetr...\n",
      "  12. Diiodothyropropionic acid...\n",
      "  13. The monocarboxylate trans...\n",
      "  14. Gross clinical manifestat...\n",
      "  15. BACKGROUND: The effective...\n",
      "Answer: The thyroid hormone analogs utilized in human studies include GC1, KB-2115, KB-141, thyronamines (such as 3-iodothyronamine), 3,5,3'-triiodothyroacetic acid (Triac), 3,5,3'-triiodothyropropionic acid (Triprop), Eprotirome, Sobetirome, diiodothyropropionic acid (DITPA), tetrac, and 3,5-diiodo-L-thyronine (T2). Triac has been used in therapy for resistance to thyroid hormone and studied for transcriptional activation, while DITPA is in phase II clinical trials and has been evaluated for cardiac effects. Eprotirome and Sobetirome are selective TRŒ≤1 thyromimetics studied for dyslipidemia. Thyronamines have preliminary human studies. Tetrac has been investigated for cancer growth inhibition and is also discussed in the context of MCT8 mutation treatment. GC1, KB-2115, and KB-141 are selective thyromimetics with lipid-lowering effects studied mainly in animals but with preliminary human relevance. These analogs have varying receptor affinities and selectivity profiles, allowing different therapeutic applications and reduced side effects compared to T3 or T4 [1,3,5,6,9,10,12,13,15].\n"
     ]
    }
   ],
   "source": [
    "answer, contexts, doc_ids = reason_moderncolbert_search(subset_questions_df.iloc[0][\"question\"], verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [DeepEval framework](https://deepeval.com/) is a handy tool for measure certain statistics:\n",
    "- [Contextual Precision](https://deepeval.com/docs/metrics-contextual-precision) evaluates the relevance of the ranking of the retrieved contexts according to the question.\n",
    "- [Contextual Recall](https://deepeval.com/docs/metrics-contextual-recall) evaluates the quality of the retrieval context in how it aligns with the true answers.\n",
    "- [Answer Relevancy](https://deepeval.com/docs/metrics-answer-relevancy) evaluates the quality of the final answer output by the LLM.\n",
    "\n",
    "The DeepEval framework uses an LLM as a judge to determine these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/pydantic/fields.py:1089: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'lias'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warn(\n",
      "/Users/danny/Documents/Work/Other/recipes/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric, ContextualRecallMetric, AnswerRelevancyMetric\n",
    ")\n",
    "\n",
    "precision_metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    include_reason=False, # keep reasoning off for speed/compute cost\n",
    "    verbose_mode=False\n",
    ")\n",
    "\n",
    "recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    include_reason=False,\n",
    "    verbose_mode=False\n",
    ")\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    include_reason=False,\n",
    "    verbose_mode=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# num_test_cases = 1\n",
    "\n",
    "# random.seed(42)\n",
    "# random_indices = random.sample(range(len(subset_questions_df)), num_test_cases)\n",
    "\n",
    "# questions_subset = subset_questions_df[\"question\"].iloc[random_indices]\n",
    "# answers_subset = subset_questions_df[\"answer\"].iloc[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the test cases for this subset of the question/answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_rag_test_cases = []\n",
    "reason_moderncolbert_test_cases = []\n",
    "recalls = pd.DataFrame(index=subset_questions_df.question, columns=[\"agentic_rag\", \"reason_moderncolbert\"])\n",
    "for i in range(len(subset_questions_df)):\n",
    "    question = subset_questions_df.iloc[i]\n",
    "    answer = subset_questions_df.iloc[i]\n",
    "    relevant_passage_ids = subset_questions_df.iloc[i][\"relevant_passage_ids\"]\n",
    "\n",
    "    agentic_answer, agentic_contexts, agentic_doc_ids = agentic_rag_search(question)\n",
    "    reason_answer, reason_contexts, reason_doc_ids = reason_moderncolbert_search(question)\n",
    "\n",
    "    recalls.loc[question, \"agentic_rag\"] = recall(relevant_passage_ids, agentic_doc_ids)\n",
    "    recalls.loc[question, \"reason_moderncolbert\"] = recall(relevant_passage_ids, reason_doc_ids)\n",
    "\n",
    "    agentic_rag_test_case = LLMTestCase(\n",
    "        input=question, \n",
    "        actual_output=agentic_answer,\n",
    "        expected_output=answer,\n",
    "        retrieval_context=agentic_contexts\n",
    "    )\n",
    "\n",
    "    reason_moderncolbert_test_case = LLMTestCase(\n",
    "        input=question, \n",
    "        actual_output=reason_answer,\n",
    "        expected_output=answer,\n",
    "        retrieval_context=reason_contexts\n",
    "    )\n",
    "\n",
    "    agentic_rag_test_cases.append(agentic_rag_test_case)\n",
    "    reason_moderncolbert_test_cases.append(reason_moderncolbert_test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=93 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=97 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=95 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=100 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.evaluate import DisplayConfig\n",
    "eval_config = DisplayConfig(\n",
    "    verbose_mode=False,\n",
    "    print_results=False,\n",
    "    show_indicator=False\n",
    ")\n",
    "\n",
    "agentic_rag_results = evaluate(\n",
    "    test_cases=agentic_rag_test_cases, \n",
    "    metrics=[precision_metric, recall_metric, answer_relevancy_metric],\n",
    "    display_config=eval_config\n",
    ")\n",
    "\n",
    "reason_moderncolbert_results = evaluate(\n",
    "    test_cases=reason_moderncolbert_test_cases, \n",
    "    metrics=[precision_metric, recall_metric, answer_relevancy_metric],\n",
    "    display_config=eval_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = pd.DataFrame(\n",
    "    columns=[\"agentic_rag\", \"reason_moderncolbert\"],\n",
    "    index=pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (question, metric)\n",
    "            for question in subset_questions_df.question\n",
    "            for metric in [\"Contextual Precision\", \"Contextual Recall\", \"Answer Relevancy\"]\n",
    "        ],\n",
    "        names=[\"question\", \"metric\"]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test_result in enumerate(agentic_rag_results.test_results):\n",
    "    question = questions_subset.iloc[i]\n",
    "    for metrics_data in test_result.metrics_data:\n",
    "        metric = metrics_data.name\n",
    "        results_data.loc[(question, metric), \"agentic_rag\"] = metrics_data.score\n",
    "\n",
    "for i, test_result in enumerate(reason_moderncolbert_results.test_results):\n",
    "    question = questions_subset.iloc[i]\n",
    "    for metrics_data in test_result.metrics_data:\n",
    "        metric = metrics_data.name\n",
    "        results_data.loc[(question, metric), \"reason_moderncolbert\"] = metrics_data.score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can groupby the index on the `metric` index and see the average results for the DeepEval benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agentic_rag</th>\n",
       "      <th>reason_moderncolbert</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contextual Precision</th>\n",
       "      <td>0.332776</td>\n",
       "      <td>0.918366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contextual Recall</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     agentic_rag reason_moderncolbert\n",
       "metric                                               \n",
       "Answer Relevancy             1.0             0.777778\n",
       "Contextual Precision    0.332776             0.918366\n",
       "Contextual Recall            1.0                  1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_data.groupby(level=[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agentic_rag</th>\n",
       "      <th>reason_moderncolbert</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contextual Precision</th>\n",
       "      <td>0.332776</td>\n",
       "      <td>0.918366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contextual Recall</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     agentic_rag reason_moderncolbert\n",
       "metric                                               \n",
       "Answer Relevancy             1.0             0.777778\n",
       "Contextual Precision    0.332776             0.918366\n",
       "Contextual Recall            1.0                  1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_data.groupby(level=[1]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data.to_csv(\"results_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
