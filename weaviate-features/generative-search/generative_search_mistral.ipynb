{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Search with Mistral\n",
    "\n",
    "In this demo, we will use Mistral to generate the embeddings for the blog post and use Mistral's language model to generate content!\n",
    "\n",
    "What you will need for this demo:\n",
    "1. A Weaviate cluster (more info below)\n",
    "2. Mistral API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import weaviate.classes as wvc\n",
    "import weaviate.classes.config as wc\n",
    "import requests, json\n",
    "import weaviate.classes.query as wq\n",
    "from weaviate.classes.config import Property, DataType\n",
    "import os\n",
    "import re\n",
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate\n",
    "\n",
    "Only choose one option from the below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weaviate Cloud Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WCD_URL = os.environ[\"WEAVIATE_URL\"] # Replace with your Weaviate cluster URL\n",
    "WCD_AUTH_KEY = os.environ[\"WEAVIATE_AUTH\"] # Replace with your cluster auth key\n",
    "MISTRAL_KEY = os.environ[\"MISTRAL_API_KEY\"] # Replace with your Mistral key\n",
    "\n",
    "# Weaviate Cloud Deployment\n",
    "client = weaviate.connect_to_wcs(\n",
    "    cluster_url=WCD_URL,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(WCD_AUTH_KEY),\n",
    "      headers={ \"X-Mistral-Api-Key\": MISTRAL_KEY}\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedded Weaviate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISTRAL_KEY = os.environ[\"MISTRAL_API_KEY\"] # Replace with your Mistral key\n",
    "\n",
    "# client = weaviate.WeaviateClient(\n",
    "#     embedded_options=EmbeddedOptions(\n",
    "#         version=\"1.26.1\",\n",
    "#         additional_env_vars={\n",
    "#             \"ENABLE_MODULES\": \"text2vec-mistral, generative-mistral\"\n",
    "#         }),\n",
    "#         additional_headers={\n",
    "#             \"X-Mistral-Api-Key\": MISTRAL_KEY\n",
    "#         }\n",
    "# )\n",
    "\n",
    "# client.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISTRAL_KEY = os.environ[\"MISTRAL_API_KEY\"] # Replace with your Mistral key\n",
    "\n",
    "# client = weaviate.connect_to_local(\n",
    "#   headers={\n",
    "#     \"X-Mistral-Api-Key\": MISTRAL_KEY\n",
    "#   }\n",
    "# )\n",
    "# print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a collection\n",
    "Collection stores your data and vector embeddings.\n",
    "\n",
    "Full list of [generative models](https://weaviate.io/developers/weaviate/model-providers/octoai/generative#available-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: in practice, you shouldn't rerun this cell, as it deletes your data\n",
    "# in \"JeopardyQuestion\", and then you need to re-import it again.\n",
    "\n",
    "# Delete the collection if it already exists\n",
    "if (client.collections.exists(\"JeopardyQuestion\")):\n",
    "    client.collections.delete(\"JeopardyQuestion\")\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"BlogChunks\",\n",
    "\n",
    "    vectorizer_config=wc.Configure.Vectorizer.text2vec_mistral( # specify the vectorizer and model\n",
    "        model=\"mistral-embed\",\n",
    "    ),\n",
    "\n",
    "    generative_config=wc.Configure.Generative.mistral( # specify the generarive model\n",
    "        model=\"open-mistral-7b\"\n",
    "    ),\n",
    "\n",
    "    properties=[\n",
    "            Property(name=\"content\", data_type=DataType.TEXT) # We only have one property for our collection. It is the content within the blog posts\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Successfully created collection: BlogChunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk and Import Data\n",
    "\n",
    "We need to break our blog posts into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    \n",
    "    for file_path in os.listdir(\"./data\"):\n",
    "        index_file_path = os.path.join(\"./data\", file_path)\n",
    "        with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            sentences = split_into_sentences(content)\n",
    "            sentence_chunks = chunk_list(sentences, 5)\n",
    "            sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "            blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# run with:\n",
    "main_folder_path = './data'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chunk\n",
    "\n",
    "blog_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the objects (chunks) into the Weaviate cluster\n",
    "\n",
    "blogs = client.collections.get(\"BlogChunks\")\n",
    "\n",
    "for blog_chunk in blog_chunks:\n",
    "    random_uuid = get_valid_uuid(uuid4())\n",
    "    blogs.data.insert(\n",
    "        properties={\n",
    "            \"content\": blog_chunk\n",
    "        },\n",
    "        uuid=random_uuid\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search Query\n",
    "\n",
    "Hybrid search combines BM25 and vector search and weighs the two algorithms depending on the `alpha` parameter. \n",
    "\n",
    "`alpha`= 0 --> pure BM25\n",
    "\n",
    "`alpha`= 0.5 --> half BM25, half vector search\n",
    "\n",
    "`alpha`= 1 --> pure vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "blogs = client.collections.get(\"BlogChunks\")\n",
    "\n",
    "response = blogs.query.hybrid(\n",
    "    query=\"What is Ref2Vec\",\n",
    "    alpha=0.5,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(json.dumps(o.properties, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Search Query\n",
    "\n",
    "Here is what happens in the below:\n",
    "1. We will retrieve 3 relevant chunks from our vector database\n",
    "2. We will pass the 3 chunks to Mistral to generate the short paragraph about Ref2Vec\n",
    "\n",
    "The first line in the output is the generated text, and the `content` pieces below it, are what was retrieved from Weaviate and passed to Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = client.collections.get(\"BlogChunks\")\n",
    "\n",
    "\n",
    "response = blogs.generate.near_text(\n",
    "    query=\"What is Ref2Vec?\",\n",
    "    single_prompt=\"Write a short paragraph about ref2vec with this content: {content}\",\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "\n",
    "for o in response.objects:\n",
    "    print(o.generated)\n",
    "    print(json.dumps(o.properties, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
