{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Weaviate and Anthropic's Citations API\n",
    "\n",
    "Notebook author: Danny Williams @ Weaviate\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Anthropic's Citations API](https://www.anthropic.com/news/introducing-citations-api) is an integration from Anthropic which cites relevant parts of documents in the response from the LLM.\n",
    "\n",
    "It will output the text of the relevant parts of the document, as well as the positions of the response from the LLM that the citation is from.\n",
    "\n",
    "This notebook shows how to use the Citations API with Weaviate's vector database retriever and Anthropic's LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data (ML Wikipedia Articles)\n",
    "\n",
    "To run this section you will need to \n",
    "```\n",
    "pip install wikipedia\n",
    "```\n",
    "\n",
    "This section searches for a term with Wikipedia, takes the most relevant first page and then scrapes the page and all the links on the page. So a search for \"machine learning\" will return also every embedded link on the page for 'machine learning' and all the associated pages. This will create a large number of documents that will serve as our knowledge base for the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia import WikipediaPage\n",
    "\n",
    "def get_page(title):\n",
    "    try:\n",
    "        return WikipediaPage(title)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return None\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        return None\n",
    "    \n",
    "def get_page_contents(page):\n",
    "    words = page.content.split()\n",
    "    content = ' '.join(words[:4000]) # limit to 4000 words so we don't hit the vectorizer token limit\n",
    "    return {\n",
    "        \"title\": page.title.replace('/', '_'),\n",
    "        \"content\": content,\n",
    "        \"categories\": page.categories\n",
    "    }\n",
    "\n",
    "def scrape_wikipedia_page(page_title):\n",
    "    search_results = wikipedia.search(page_title, results=1)\n",
    "    page = get_page(search_results[0])\n",
    "    data = [get_page_contents(page)]\n",
    "\n",
    "    for link in page.links:\n",
    "        page = get_page(link)\n",
    "        if page is not None:\n",
    "            data.append(get_page_contents(page))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add the data to Weaviate. We will use the `text2vec-openai` vectorizer to embed the data, but the actual vectorizer is not important for the Citations API, this is just for the embeddings to search with via the vector database retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting item 182. Total missing items: 1/882\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.config import Property, DataType, Configure\n",
    "\n",
    "import os\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.getenv(\"WCD_URL\"),\n",
    "    auth_credentials=Auth.api_key(os.environ.get(\"WCD_API_KEY\")),\n",
    "    headers = {\"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")} # use an openai vectorizer - just for the embeddings (not the generative/LLM part)\n",
    ")\n",
    "\n",
    "# Create a new collection\n",
    "collection = client.collections.create(\n",
    "    \"wikipedia\",\n",
    "    properties=[\n",
    "        Property(name=\"title\", data_type=DataType.TEXT, description=\"The title of the Wikipedia page\"),\n",
    "        Property(name=\"content\", data_type=DataType.TEXT, description=\"The content of the Wikipedia page\"),\n",
    "        Property(name=\"categories\", data_type=DataType.TEXT_ARRAY, description=\"The categories of the Wikipedia page\")\n",
    "    ],\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_openai(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        dimensions=256\n",
    "    )\n",
    ")\n",
    "data = scrape_wikipedia_page(\"machine learning\")\n",
    "\n",
    "count = 0\n",
    "for i, item in enumerate(data):\n",
    "    try:\n",
    "        collection.data.insert(\n",
    "            properties=item,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        print(f\"Error inserting item {i}. Total missing items: {count}/{len(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.config import Property, DataType, Configure\n",
    "\n",
    "import os\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.getenv(\"WCD_URL\"),\n",
    "    auth_credentials=Auth.api_key(os.environ.get(\"WCD_API_KEY\")),\n",
    "    headers = {\"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")} # use an openai vectorizer - just for the embeddings (not the generative/LLM part)\n",
    ")\n",
    "client.collections.delete(\"wikipedia\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Weaviate and Anthropic\n",
    "\n",
    "These functions are used for the stages of the RAG pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the Weaviate collection we just made\n",
    "collection = client.collections.get(\"wikipedia\")\n",
    "\n",
    "# Use Weaviate's hybrid search to get the most relevant documents according to the query\n",
    "def query_wikipedia(query):\n",
    "    response = collection.query.hybrid(query, limit=5)\n",
    "    return [object.properties for object in response.objects]\n",
    "\n",
    "# Format the message for Anthropic's api to format the documents correctly, for the Citations API to work\n",
    "def create_message_with_documents(prompt, objects):\n",
    "    message = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": []\n",
    "    }]\n",
    "    for object in objects:\n",
    "        message[0][\"content\"].append({\n",
    "            \"type\": \"document\",\n",
    "            \"source\": {\n",
    "                \"type\": \"text\",\n",
    "                \"media_type\": \"text/plain\",\n",
    "                \"data\": object[\"content\"]\n",
    "            },\n",
    "            \"title\": object[\"title\"],\n",
    "            \"context\": str(object[\"categories\"]),\n",
    "            \"citations\": {\"enabled\": True}\n",
    "        })\n",
    "\n",
    "    message[0][\"content\"].append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "    })\n",
    "\n",
    "    return message\n",
    "\n",
    "# Run the query, and generate the response from Anthropic's LLM\n",
    "def generate_response(anthropic_client, prompt):\n",
    "    objects = query_wikipedia(prompt)\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=4096,\n",
    "        messages=create_message_with_documents(prompt, objects)\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the RAG pipeline\n",
    "\n",
    "For this section we will use the `anthropic` library to interact with Anthropic's LLM.\n",
    "I.e.\n",
    "```\n",
    "pip install anthropic\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "response = generate_response(anthropic_client, \"Explain all the different types of machine learning models: regression, classification, clustering, language models and image modelling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the response\n",
    "\n",
    "We will use the `textwrap3` library to format the response and the citations.\n",
    "```\n",
    "pip install textwrap3\n",
    "```\n",
    "This is just used to colour code the citations in the response, so the relevant parts of the LLM's response are highlighted, and underneath, the references are listed in the same colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mBased on the provided documents, I'll explain the different types of machine learning\n",
      "models:\n",
      "\n",
      "1. Classification Models:\n",
      " \u001b[93mClassification is performed by computers using statistical methods. Individual\n",
      "observations are analyzed using a set of quantifiable properties called explanatory\n",
      "variables or features.\u001b[0m\n",
      "\n",
      "Key aspects of classification:\n",
      "-  \u001b[93mProbabilistic classification is a common subclass that:\n",
      "- Uses statistical inference to find the best class\n",
      "- Outputs probabilities for each possible class\n",
      "- Usually selects the class with highest probability\n",
      "- Can output confidence values\n",
      "- Can abstain when confidence is too low\u001b[0m\n",
      "\n",
      "2. Types of Classification:\n",
      " \u001b[93mClassification can be:\n",
      "- Binary classification: involves only two classes\n",
      "- Multiclass classification: involves assigning an object to one of several classes\n",
      "- Many classification methods are specifically designed for binary classification and\n",
      "require combination for multiclass problems\u001b[0m\n",
      "\n",
      "3. Clustering Models:\n",
      " \u001b[94mTwo main methods in unsupervised learning are principal component and cluster\n",
      "analysis. Clustering:\n",
      "- Groups datasets with shared attributes\n",
      "- Works with unlabeled, unclassified, and uncategorized data\n",
      "- Identifies commonalities in data\n",
      "- Reacts based on presence/absence of commonalities\n",
      "- Helps detect anomalous data points\u001b[0m\n",
      "\n",
      "4. Unsupervised Learning Models:\n",
      " \u001b[94mUnsupervised learning is where algorithms learn patterns exclusively from unlabeled\n",
      "data, unlike supervised learning. There are also weak- or semi-supervision approaches\n",
      "where a small portion of data is tagged.\u001b[0m\n",
      "\n",
      "5. Predictive Models:\n",
      " \u001b[93mPredictive modeling:\n",
      "- Uses statistics to predict outcomes\n",
      "- Can be applied to future events or any unknown event\n",
      "- Often used to detect crimes and identify suspects\n",
      "- Often tries to guess probability of outcomes given input data\u001b[0m\n",
      "\n",
      "6. Deep Learning Models:\n",
      " \u001b[91mDeep generative models (DGMs) combine generative models with deep neural networks.\n",
      "They require:\n",
      "- Increased scale of neural networks\n",
      "- Increased scale of training data\n",
      "Popular types include:\n",
      "- Variational autoencoders (VAEs)\n",
      "- Generative adversarial networks (GANs)\n",
      "- Auto-regressive models\u001b[0m\n",
      "\n",
      "7. Language and Text Models:\n",
      " \u001b[91mRecent trends show very large deep generative models:\n",
      "- GPT-3 and GPT-2 are auto-regressive neural language models with billions of parameters\n",
      "- BigGAN and VQ-VAE are used for image generation with hundreds of millions of parameters\n",
      "- Jukebox is a large generative model for musical audio with billions of parameters\u001b[0m\n",
      "\n",
      "8. Model Categories Based on Statistical Approach:\n",
      " \u001b[93mThere are three broad categories:\n",
      "1. Parametric models: Make specific assumptions about population parameters and underlying\n",
      "distributions\n",
      "2. Non-parametric models: Involve fewer structural assumptions but usually have strong\n",
      "independence assumptions\n",
      "3. Semi-parametric models: Include features of both parametric and non-parametric models\n",
      "\n",
      "REFERENCES\n",
      "__________\n",
      "\n",
      "\u001b[0m(Source 1) \u001b[93mStatistical classification:\n",
      "When classification is performed by a computer, statistical methods are normally used to\n",
      "develop the algorithm. Often, the individual observations are analyzed into a set of\n",
      "quantifiable properties, known variously as explanatory variables or features.\n",
      "\n",
      "\u001b[0m(Source 2) \u001b[93mStatistical classification:\n",
      "A common subclass of classification is probabilistic classification. Algorithms of this\n",
      "nature use statistical inference to find the best class for a given instance. Unlike other\n",
      "algorithms, which simply output a \"best\" class, probabilistic algorithms output a\n",
      "probability of the instance being a member of each of the possible classes. The best class\n",
      "is normally then selected as the one with the highest probability. However, such an\n",
      "algorithm has numerous advantages over non-probabilistic classifiers: It can output a\n",
      "confidence value associated with its choice (in general, a classifier that can do this is\n",
      "known as a confidence-weighted classifier). Correspondingly, it can abstain when its\n",
      "confidence of choosing any particular output is too low.\n",
      "\n",
      "\u001b[0m(Source 3) \u001b[93mStatistical classification:\n",
      "== Binary and multiclass classification == Classification can be thought of as two\n",
      "separate problems â€“ binary classification and multiclass classification. In binary\n",
      "classification, a better understood task, only two classes are involved, whereas\n",
      "multiclass classification involves assigning an object to one of several classes. Since\n",
      "many classification methods have been developed specifically for binary classification,\n",
      "multiclass classification often requires the combined use of multiple binary classifiers.\n",
      "\n",
      "\u001b[0m(Source 4) \u001b[94mUnsupervised learning:\n",
      "== Probabilistic methods == Two of the main methods used in unsupervised learning are\n",
      "principal component and cluster analysis. Cluster analysis is used in unsupervised\n",
      "learning to group, or segment, datasets with shared attributes in order to extrapolate\n",
      "algorithmic relationships. Cluster analysis is a branch of machine learning that groups\n",
      "the data that has not been labelled, classified or categorized. Instead of responding to\n",
      "feedback, cluster analysis identifies commonalities in the data and reacts based on the\n",
      "presence or absence of such commonalities in each new piece of data. This approach helps\n",
      "detect anomalous data points that do not fit into either group.\n",
      "\n",
      "\u001b[0m(Source 5) \u001b[94mUnsupervised learning:\n",
      "Unsupervised learning is a framework in machine learning where, in contrast to supervised\n",
      "learning, algorithms learn patterns exclusively from unlabeled data. Other frameworks in\n",
      "the spectrum of supervisions include weak- or semi-supervision, where a small portion of\n",
      "the data is tagged, and self-supervision.\n",
      "\n",
      "\u001b[0m(Source 6) \u001b[93mPredictive modelling:\n",
      "Predictive modelling uses statistics to predict outcomes. Most often the event one wants\n",
      "to predict is in the future, but predictive modelling can be applied to any type of\n",
      "unknown event, regardless of when it occurred. For example, predictive models are often\n",
      "used to detect crimes and identify suspects, after the crime has taken place. In many\n",
      "cases, the model is chosen on the basis of detection theory to try to guess the\n",
      "probability of an outcome given a set amount of input data, for example given an email\n",
      "determining how likely that it is spam.\n",
      "\n",
      "\u001b[0m(Source 7) \u001b[91mGenerative model:\n",
      "== Deep generative models == With the rise of deep learning, a new family of methods,\n",
      "called deep generative models (DGMs), is formed through the combination of generative\n",
      "models and deep neural networks. An increase in the scale of the neural networks is\n",
      "typically accompanied by an increase in the scale of the training data, both of which are\n",
      "required for good performance. Popular DGMs include variational autoencoders (VAEs),\n",
      "generative adversarial networks (GANs), and auto-regressive models.\n",
      "\n",
      "\u001b[0m(Source 8) \u001b[91mGenerative model:\n",
      "Recently, there has been a trend to build very large deep generative models. For example,\n",
      "GPT-3, and its precursor GPT-2, are auto-regressive neural language models that contain\n",
      "billions of parameters, BigGAN and VQ-VAE which are used for image generation that can\n",
      "have hundreds of millions of parameters, and Jukebox is a very large generative model for\n",
      "musical audio that contains billions of parameters.\n",
      "\n",
      "\u001b[0m(Source 9) \u001b[93mPredictive modelling:\n",
      "Broadly speaking, there are two classes of predictive models: parametric and non-\n",
      "parametric. A third class, semi-parametric models, includes features of both. Parametric\n",
      "models make \"specific assumptions with regard to one or more of the population parameters\n",
      "that characterize the underlying distribution(s)\". Non-parametric models \"typically\n",
      "involve fewer assumptions of structure and distributional form [than parametric models]\n",
      "but usually contain strong assumptions about independencies\".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap3\n",
    "\n",
    "def wrap_text_preserve_breaks(text, width):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap3.fill(line, width=width) for line in lines]\n",
    "    return '\\n'.join(wrapped_lines)\n",
    "\n",
    "\n",
    "citations = []\n",
    "printed_text = \"\"\n",
    "# from rich import print\n",
    "citation_colours = [ '\\033[93m', '\\033[91m', '\\033[92m', '\\033[93m', '\\033[94m']\n",
    "for i, text in enumerate(response.content):\n",
    "    if text.citations is not None:\n",
    "\n",
    "        for citation in text.citations:\n",
    "            citations.append({\n",
    "                \"text\": citation.cited_text,\n",
    "                \"title\": citation.document_title,\n",
    "                \"colour\": citation_colours[citation.document_index % len(citation_colours)]\n",
    "            })\n",
    "\n",
    "        printed_text += f\"{citation_colours[citation.document_index % len(citation_colours)]}{text.text}\"\n",
    "    else:\n",
    "        printed_text += f\"\\033[0m{text.text} \"\n",
    "\n",
    "cited_text = \"REFERENCES\\n__________\\n\\n\"\n",
    "for i, citation in enumerate(citations):\n",
    "    cited_text += f\"\\033[0m(Source {i+1}) {citation['colour']}{citation['title']}:\\n\"\n",
    "    cited_text += citation['text'] + \"\\n\\n\"\n",
    "\n",
    "\n",
    "print(wrap_text_preserve_breaks(printed_text, width=90), end=\"\\n\\n\")\n",
    "print(wrap_text_preserve_breaks(cited_text, width=90))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
