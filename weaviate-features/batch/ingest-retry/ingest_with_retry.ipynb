{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data with Retry\n",
    "\n",
    "This recipe demonstrates how to ingest data into Weaviate with retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate Setup\n",
    "\n",
    "The sample code is for a local Weaviate deployment with Docker and Ollama running on localhost. However, the retry logic is agnostic to the deployment method and the underlying vector embedding service and model.\n",
    "\n",
    "### Steps to deploy Weaviate locally with CLIP\n",
    "\n",
    "We will use `docker-compose.yaml` and verify that ollama is running. Note that we set `ASYNC_INDEXING: 'true'` in the weaviate environment variables to speed up the ingestion process.\n",
    "\n",
    "Run:\n",
    "\n",
    "docker compose -f docker-compose.yaml up -d\n",
    "ollama serve\n",
    "ollama pull mxbai-embed-large:latest\n",
    "ollama pull llama3.2:latest\n",
    "curl http://localhost:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes.config as wc\n",
    "import weaviate.classes.query as wq\n",
    "from weaviate.classes.init import AdditionalConfig, Timeout\n",
    "from weaviate.util import generate_uuid5\n",
    "import os\n",
    "import json\n",
    "import ijson\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "WEAVIATE_URL = \"http://localhost:8080\"\n",
    "\n",
    "OLLAMA_EMBEDDING_MODEL_ID = \"mxbai-embed-large:latest\"\n",
    "OLLAMA_GENERATIVE_MODEL_ID = \"llama3.2:latest\"\n",
    "OLLAMA_URL = \"http://host.docker.internal:11434\"\n",
    "\n",
    "PRODUCT_COLLECTION_NAME = \"product\"\n",
    "client = weaviate.connect_to_local(\n",
    "    headers={},\n",
    "    additional_config=AdditionalConfig(\n",
    "        timeout=Timeout(init=30, query=60, insert=120)\n",
    "    )\n",
    ")\n",
    "assert client.is_live()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `Product` collection\n",
    "\n",
    "The collection has the following key characteristics:\n",
    "1. Name: `\"Product\"`\n",
    "2. Vectorizer: `text2vec-ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Multi2VecField, Property, DataType\n",
    "\n",
    "# Delete the collection if it exists.\n",
    "if client.collections.exists(PRODUCT_COLLECTION_NAME):\n",
    "    client.collections.delete(PRODUCT_COLLECTION_NAME)\n",
    "    \n",
    "client.collections.create(\n",
    "    name=PRODUCT_COLLECTION_NAME,\n",
    "    properties=[\n",
    "        wc.Property(name=\"category\", data_type=wc.DataType.TEXT_ARRAY, index_filterable=True, index_searchable=True),\n",
    "        wc.Property(name=\"tech1\", data_type=wc.DataType.TEXT, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"tech2\", data_type=wc.DataType.TEXT, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"description\", data_type=wc.DataType.TEXT_ARRAY, index_filterable=True, index_searchable=True),\n",
    "        wc.Property(name=\"fit\", data_type=wc.DataType.TEXT, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"title\", data_type=wc.DataType.TEXT, index_filterable=True, index_searchable=True),\n",
    "        wc.Property(name=\"also_buy\", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"image\", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"brand\", data_type=wc.DataType.TEXT, index_filterable=True, index_searchable=True),\n",
    "        wc.Property(name=\"feature\", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"rank\", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"also_view\", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True, index_filterable=False, index_searchable=False),\n",
    "        wc.Property(name=\"main_cat\", data_type=wc.DataType.TEXT, index_filterable=True, index_searchable=True),\n",
    "        wc.Property(name=\"date\", data_type=wc.DataType.TEXT, skip_vectorization=True, index_filterable=True, index_searchable=True),\n",
    "        wc.Property(name=\"price\", data_type=wc.DataType.TEXT, skip_vectorization=True, index_filterable=True, index_searchable=True),\n",
    "        wc.Property(name=\"asin\", data_type=wc.DataType.TEXT, index_filterable=True, index_searchable=True),\n",
    "    ],\n",
    "    vectorizer_config=wc.Configure.Vectorizer.text2vec_ollama(\n",
    "        api_endpoint=OLLAMA_URL,\n",
    "        model=OLLAMA_EMBEDDING_MODEL_ID,\n",
    "    ),\n",
    "    generative_config=wc.Configure.Generative.ollama(\n",
    "        api_endpoint=OLLAMA_URL,\n",
    "        model=OLLAMA_GENERATIVE_MODEL_ID\n",
    "    )\n",
    ")\n",
    "products = client.collections.get(PRODUCT_COLLECTION_NAME)\n",
    "print(\"Successfully created Product collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Product Logic\n",
    "Some of the data in the Amazon product dataset is stored inconsisntely, so we need to normalize the data before importing it into Weaviate.\n",
    "\n",
    "Note that the `import_products` will first import the data, and then enter an infinite loop to retry the objects that failed to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some fields are stored in the data as single items and sometimes as lists\n",
    "# Make sure these fields are always lists to match what the Weaviate\n",
    "# collection expectes\n",
    "def normalize_field(obj, field):\n",
    "    if not isinstance(obj[field], list):\n",
    "        obj[field] = [obj[field]]\n",
    "\n",
    "def process_product(obj):\n",
    "    product_obj = {\n",
    "        \"category\": obj[\"category\"],\n",
    "        \"tech1\": obj[\"tech1\"],\n",
    "        \"tech2\": obj[\"tech2\"],\n",
    "        \"description\": obj[\"description\"],\n",
    "        \"fit\": obj[\"fit\"],\n",
    "        \"title\": obj[\"title\"],\n",
    "        \"also_buy\": obj[\"also_buy\"],\n",
    "        \"image\": obj[\"image\"],\n",
    "        \"brand\": obj[\"brand\"],\n",
    "        \"feature\": obj[\"feature\"],\n",
    "        \"rank\": obj[\"rank\"],\n",
    "        \"also_view\": obj[\"also_view\"],\n",
    "        \"main_cat\": obj[\"main_cat\"],\n",
    "        \"date\": obj[\"date\"],\n",
    "        \"price\": obj[\"price\"],\n",
    "        \"asin\": obj[\"asin\"],\n",
    "    }\n",
    "\n",
    "    for field in [\"category\", \"description\", \"also_buy\", \"image\", \"feature\", \"also_view\"]:\n",
    "        normalize_field(product_obj, field)\n",
    "\n",
    "    # Sometimes rank is a string and sometimes it is an array\n",
    "    if isinstance(obj[\"rank\"], str):\n",
    "        product_obj[\"rank\"] = [obj[\"rank\"]]\n",
    "    elif isinstance(obj[\"rank\"], list):\n",
    "        product_obj[\"rank\"] = obj[\"rank\"]\n",
    "    else:\n",
    "        product_obj[\"rank\"] = []\n",
    "\n",
    "    return product_obj\n",
    "\n",
    "def import_products(local_json_path):\n",
    "    counter = 0\n",
    "    INTERVAL = 100\n",
    "\n",
    "    with products.batch.dynamic() as batch:\n",
    "        print(f\"Opening {local_json_path}\")\n",
    "        with open(local_json_path, \"rb\") as f:\n",
    "            objects = ijson.items(f, '', multiple_values=True)\n",
    "            for obj in objects:\n",
    "                product_obj = process_product(obj)\n",
    "                #print(json.dumps(product_obj, indent=2))\n",
    "                batch.add_object(\n",
    "                    properties=product_obj,\n",
    "                    uuid=generate_uuid5(obj[\"asin\"])\n",
    "                )\n",
    "\n",
    "                counter += 1\n",
    "                if counter % INTERVAL == 0:\n",
    "                    print(f\"{local_json_path}: Imported {counter} products...\")\n",
    "        print(f\"{local_json_path}: Flushing batch\")\n",
    "        batch.flush()\n",
    "        print(f\"{local_json_path}: Batch flushed\")\n",
    "\n",
    "    # The failed_objects are not available until after flush is called\n",
    "    old_failed_obj_count = len(products.batch.failed_objects)\n",
    "    new_failed_obj_count = 0\n",
    "    while True:\n",
    "        if len(products.batch.failed_objects) == 0:\n",
    "            print(f\"{local_json_path}: All products imported successfully\")\n",
    "            break\n",
    "\n",
    "        print(f\"{local_json_path}: Retrying {len(products.batch.failed_objects)} failed objects...\")\n",
    "        retry_counter = 0\n",
    "\n",
    "        current_failed_object_count = len(products.batch.failed_objects)\n",
    "        failed_objects = products.batch.failed_objects\n",
    "        with products.batch.dynamic() as batch:\n",
    "            print(f\"{local_json_path}: Inside retry loop are {len(failed_objects)} failed objects...\")\n",
    "\n",
    "            for failed in failed_objects:\n",
    "                try:\n",
    "                    print(f\"{local_json_path}: Failed with error \\\"{failed.message}\\\": {failed.object_.uuid}\")\n",
    "                    #print(f\"{local_json_path}: \"\n",
    "                    #    + json.dumps(failed.object_.properties, indent=2))\n",
    "                    if new_failed_obj_count == old_failed_obj_count:\n",
    "                        print(f\"{local_json_path}: Debugging stuck object: \"\n",
    "                                + json.dumps(failed.object_.properties, indent=2))\n",
    "                    batch.add_object(\n",
    "                        properties=failed.object_.properties,\n",
    "                        uuid=failed.object_.uuid\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"{local_json_path}: Exception while retrying: {e}\")\n",
    "                    print(f\"{local_json_path}: Failed Object: {failed}\")\n",
    "                    break\n",
    "\n",
    "                retry_counter += 1\n",
    "                if retry_counter % INTERVAL == 0:\n",
    "                    print(f\"{local_json_path}: Retried {retry_counter} products...\")\n",
    "            batch.flush()\n",
    "        old_failed_obj_count = current_failed_object_count\n",
    "        new_failed_obj_count = len(products.batch.failed_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Products\n",
    "\n",
    "The below code assumes you have downloaded the Amazon product dataset and split the JSON data into multiple files named `amazon_products_00.json`, `amazon_products_01.json`, etc.\n",
    "\n",
    "See the README for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_products('Amazon_Meta_CDs_Vinyl_00.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the number of objects in the Product collection\n",
    "\n",
    "There should be 10,000 objects in the Product collection after ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of objects in the Product collection\n",
    "print(products.aggregate.over_all(total_count=True).total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a vector search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_product(response_object):\n",
    "    print(\"Product Title: \" + response_object.properties[\"title\"])\n",
    "    print(\"  Artist: \" + response_object.properties[\"brand\"])\n",
    "    print(\"  ASIN: \" + response_object.properties[\"asin\"])\n",
    "    print(\"  Categories: \")\n",
    "    for c in response_object.properties[\"category\"]:\n",
    "        print(\"    \" + c)\n",
    "    print(\"  Price: \" + response_object.properties[\"price\"])\n",
    "    print(\"  Description: \")\n",
    "    for d in response_object.properties[\"description\"]:\n",
    "        print(\"    \" + d)\n",
    "\n",
    "response = products.query.near_text(\n",
    "    query=\"background music for falling asleep\",\n",
    "    limit=5,\n",
    "    return_metadata=wq.MetadataQuery(distance=True),\n",
    ")\n",
    "\n",
    "seen_asin = []\n",
    "for o in response.objects:\n",
    "    if o.properties[\"asin\"] in seen_asin:\n",
    "        continue\n",
    "    seen_asin.append(o.properties[\"asin\"])\n",
    "    print_product(o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
