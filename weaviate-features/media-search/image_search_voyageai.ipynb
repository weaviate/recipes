{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/weaviate-features/media-search/image_search_voyageai.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Search with VoyageAI\n",
    "This recipe demonstrates how build image search with VoyageAI's multimodal model ([multi2vec-voyageai](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-voyageai)).\n",
    "\n",
    "Voyage multimodal embedding models support text and content-rich images — such as figures, photos, slide decks, and document screenshots — eliminating the need for complex text extraction or ETL pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate Setup\n",
    "\n",
    "The VoyageAI model is available with Weaviate Cloud Database (WCD) and local Weaviate deployments also.\n",
    "\n",
    "You will need to run:\n",
    "1. Weaviate version: `1.25.28`, `1.26.12`, `1.27.8`, or newer versions\n",
    "2. Weaviate Python version: `>=4.10.0`\n",
    "\n",
    "[Documentation](https://weaviate.io/developers/weaviate/model-providers/voyageai/embeddings-multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "# Connect to Weaviate\n",
    "client = weaviate.connect_to_local(headers={\n",
    "    \"X-VoyageAI-Api-Key\": \"<YOUR_API_KEY>\",\n",
    "})\n",
    "\n",
    "client.is_ready() # Test the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `Animals` collection\n",
    "\n",
    "The collection has the following key characteristics:\n",
    "1. Name: `\"Animals\"`\n",
    "2. Vectorizer: `multi2vec-voyageai`\n",
    "3. Image property: `\"image\"` - Weaviate will use values in \"image\" property to generate vectors. Note, you can call it anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Multi2VecField, Property, DataType\n",
    "\n",
    "# Delete the collection if it exists.\n",
    "# Note you should skip this step if you don't want to reimport the data every time.\n",
    "if client.collections.exists(\"Animals\"):\n",
    "    client.collections.delete(\"Animals\")\n",
    "\n",
    "animals = client.collections.create(\n",
    "    name=\"Animals\",\n",
    "    vectorizer_config=Configure.Vectorizer.multi2vec_voyageai(\n",
    "        model=\"voyage-multimodal-3\",\n",
    "        output_encoding=None,\n",
    "        text_fields=[Multi2VecField(name=\"text\", weight=0.5)],\n",
    "        image_fields=[Multi2VecField(name=\"image\", weight=0.5)],\n",
    "    ),\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "        Property(name=\"image\", data_type=DataType.BLOB)\n",
    "    ]\n",
    ")\n",
    "print(\"Successfully created Animals collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Images\n",
    "For every object, we will store:\n",
    "* `name` - the file name \n",
    "* `path` - path to the file, so that we could display returned images at query time.\n",
    "* `image` - a base64 representation of the image file, Weaviate will use it to generate a vector - see `imageFields`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "\n",
    "# Helper function to convert a file to base64 representation\n",
    "def toBase64(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return base64.b64encode(file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of source images \n",
    "source = [\"cat1.jpg\", \"cat2.jpg\", \"cat3.jpg\",\n",
    "          \"dog1.jpg\", \"dog2.jpg\", \"dog3.jpg\",\n",
    "          \"meerkat1.jpg\", \"meerkat2.jpg\", \"meerkat3.jpg\"]\n",
    "\n",
    "\n",
    "with animals.batch.dynamic() as batch:\n",
    "    for name in source:\n",
    "        print(f\"Adding {name}\")\n",
    "        # Build the path to the image file\n",
    "        path = \"./source/image/\" + name\n",
    "        # Object to store in Weaviate\n",
    "        properties = {\n",
    "            \"name\": name,\n",
    "            \"path\": path,\n",
    "            \"image\": toBase64(path), # Weaviate will use the base64 representation of the file to generate a vector.\n",
    "        }\n",
    "        batch.add_object(\n",
    "            properties=properties,\n",
    "        )\n",
    "print(animals.batch.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of objects in the Animals collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of objects in the Animals collection\n",
    "animals.aggregate.over_all(total_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's get one vector\n",
    "animals.query.fetch_objects(include_vector=True, limit=1).objects[0].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to display results\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_image(path):\n",
    "    display(Image(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for images with \"dog\", \"dog with glasses\", \"dog with a sign\"\n",
    "response = animals.query.near_text(\n",
    "    query=\"dog\",\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "# Print first result\n",
    "result = response.objects[0]\n",
    "print(result)\n",
    "\n",
    "# Display the first image\n",
    "display_image(result.properties.get(\"path\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes import query\n",
    "response = animals.query.near_media(\n",
    "    media=\"./test/test-dog.jpg\",\n",
    "    media_type=query.NearMediaType.IMAGE,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "# Print results\n",
    "result = response.objects[0]\n",
    "print(result)\n",
    "\n",
    "# Display the first image\n",
    "display_image(result.properties.get(\"path\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
