{"relevant_passages": ["That [distance can be calculated in multiple ways](/blog/distance-metrics-in-vector-search), one of the simplest being \"The sum of the absolute differences between elements at position `i` in each vector\" (recall that all vectors have the same fixed length). Let's look at some numbers (no math, promise!) and illustrate with another text example:\n\nObjects (data): words including `cat`, `dog`, `apple`, `strawberry`, `building`, `car`\n\nSearch query: `fruit`\n\nA set of simplistic vector embeddings (with only 5 dimensions) for the objects and the query could look something like this:\n\n| Word               | Vector embedding                |\n|--------------------|---------------------------------|\n| `cat`              | `[1.5, -0.4, 7.2, 19.6, 20.2]`  |\n| `dog`              | `[1.7, -0.3, 6.9, 19.1, 21.1]`  |\n| `apple`            | `[-5.2, 3.1, 0.2, 8.1, 3.5]`    |\n| `strawberry`       | `[-4.9, 3.6, 0.9, 7.8, 3.6]`    |\n| `building`         | `[60.1, -60.3, 10, -12.3, 9.2]` |\n| `car`              | `[81.6, -72.1, 16, -20.2, 102]` |\n| **Query: `fruit`** | `[-5.1, 2.9, 0.8, 7.9, 3.1]`    |\n\nIf we look at each of the 5 elements of the vectors, we can see quickly that `cat` and `dog` are much closer than `dog` and `apple` (we don\u2019t even need to calculate the distances). In the same way, `fruit` is much closer to `apple` and `strawberry` than to the other words, so those will be the top results of the \u201cfruit\u201d query. But where do these numbers come from? That\u2019s where the real magic is, and where advances in modern deep learning have made a huge impact."], "query": "Which words are most similar to the search query 'fruit' based on their vector embeddings?"}
{"relevant_passages": ["## Overview\n![Overview](./img/hugging-face-module-overview.png)\n\nThe Hugging Face module is quite incredible, for many reasons. ### Public models\nYou get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case. In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). ### Private models\nIf you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate."], "query": "How many pre-trained sentence similarity models does Hugging Face offer?"}
{"relevant_passages": ["Sometimes when multiple batches contained identical objects with the same UUID, they could be added more than once to Weaviate, each time with different DocIDs. This, in turn, could cause issues within Weaviate. Luckily, we've addressed this issue without sacrificing performance (yay!\ud83e\udd73). Here's our journey that got us to the current solution. ## Our initial solutions\nIn the initial solution, we added a lock (sync.Mutex in Go), so that now only a single goroutine can hold the lock, check for duplicate UUIDs, and assign DocIDs. This lock makes sure that the race does not occur anymore, but as an unintended side-effect the import time increased by ~20% due to lock-congestion. Upon further consideration, our team concluded that while using a single lock is effective, it's also overkill."], "query": "What was the initial solution to prevent identical objects with the same UUID from being added multiple times to Weaviate, and what side-effect did it have?"}
{"relevant_passages": ["*Note. Hugging Face Inference uses [a pay-per-use pricing model](https://huggingface.co/inference-api#pricing).<br />\nMake sure to study it well before you run a big job.*\n\nTo learn more, head to the [HuggingFace Module docs page](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). ## Other improvements and bug fixes\n\n![Other improvements and bug fixes](./img/smaller-improvements.jpg)\n\nAnd, of course, there are many other improvements and bug fixes that went into this release. You can find the complete list and the relevant links in the [release notes](https://github.com/weaviate/weaviate/releases/tag/v1.15.0). ## Enjoy\nWe hope you enjoy all the new features, performance improvements, memory savings and bug fixes that made this the best Weaviate release yet!\ud83d\udd25\n\nimport ShareFeedback from '/_includes/share-feedback.md';\n\n<ShareFeedback />"], "query": "Where can I find the pricing model for Hugging Face Inference API?"}
{"relevant_passages": ["Just add one of the following properties to the `POST payload`:\n* `include` - an array class names we want to backup or restore\n* `exclude` - an array class names we don't want to backup or restore\n\nFor example, you can create a backup that includes Cats, Dogs and Meerkats. ```js\nPOST /v1/backups/gcs\n{\n  \"id\": \"first_backup\",\n  \"include\": [\"Cats\", \"Dogs\", \"Meerkats\"]\n}\n```\n\nThen restore all classes, excluding Cats:\n\n```js\nPOST /v1/backups/gcs/first_backup/restore\n{\n  \"exclude\": [\"Cats\"]\n}\n```\n\n### Other use cases\nIt might not be immediately obvious, but you can use the above workflow to migrate your data to other environments. So, if one day you find yourself with an environment that is not set up for what you need (i.e. not enough resources). Then create a backup, and restore it in the new environment. \ud83d\ude09\n\n### Follow up\nAre you ready to set up backups for your environment?"], "query": "How can I create a backup of specific classes using the POST payload in my API request?"}
{"relevant_passages": ["using a special algorithm, the database find the [closest](/blog/distance-metrics-in-vector-search) vectors to the given vector computed for the query. The quality of the search depends crucially on the quality of the model - this is the \"secret sauce\", as many models are [still closed source](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). The speed of the search depends crucially on Weaviate, which is open-source and [continuously improving its performance](/blog/weaviate-1-18-release). ## What exactly are vector embeddings? Vectors are numeric representations of data that capture certain features of the data."], "query": "What factors are crucial for the quality and speed of vector searches in databases?"}
{"relevant_passages": ["Thanks to the advances in machine learning in the past decade and the commoditization of AI-first database technologies, you can start using it in your business tomorrow. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What recent developments in AI technology can be applied to businesses today?"}
{"relevant_passages": ["[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n1. [Metadata Rankers](#metadata-rankers)\n1. [Score Rankers](#score-rankers)\n\n## Cross Encoders\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax."], "query": "What are Cross Encoders, and how can they be interfaced with Weaviate?"}
{"relevant_passages": ["This process is known as \u201cde-noising\u201d, is illustrated below and, is carried out for each image in the training set with multiple levels of random noise added. Once the diffusion model is trained in this way it becomes an expert at taking images that are less likely to be seen in the dataset (noisy images) and incrementally turning them into something that is more likely to be seen in the training set. By teaching the model to \u201cde-noise\u201d images we have developed a way to alter images to make them more like images from the training set. ![denoising gif](./img/denoise.gif)\n*[Source](https://yang-song.net/blog/2021/score/)*\n\n![denoising images](./img/denoisingimage.png)\n*[Source](https://huggingface.co/blog/annotated-diffusion)*\n\nNow if we take this trained diffusion model and just give it a random static image and run the de-noising process it will transform the static image into an image that resembles images in the training set!\n\n![noising denoising images](./img/noising_denoising.png)\n\n## How Text Prompts Control the Image Generation Process\n\nSo far we have explained the general idea behind how diffusion models can start off from static noise and incrementally alter the pixel values so that the picture all together gains meaning and follows the distribution of the training set. However, another important detail is that most diffusion models don\u2019t just spit out random images that look like training set images, they allow us to add a text prompt that can control the specific types of images are generated."], "query": "What is the purpose of the \"de-noising\" process in diffusion models, and how does it relate to generating images from text prompts?"}
{"relevant_passages": ["If you would like to follow along, the Jupyter notebook and data are available [here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain). You can use our free [Weaviate Cloud Services](https://console.weaviate.cloud) (WCS) sandbox, or set up your own Weaviate instance also. > Note: The vectorization is done by [Weaviate \u201ccore\u201d](https://github.com/weaviate/weaviate) and not at the client level. So even though we use Python examples, the principles are universally applicable. Let's get started!\n\n## Text2vec: behind the scenes\n\nWeaviate's `text2vec-*` modules transform text data into dense vectors for populating a Weaviate database."], "query": "Where can I find the Jupyter notebook and data to learn about Weaviate's text vectorization, and where is the vectorization process executed?"}
{"relevant_passages": ["This includes [improved APIs](https://github.com/weaviate/weaviate-python-client/issues/205) on the client side, new modules, for example, for [generative search](/developers/weaviate/modules/reader-generator-modules/generative-openai), and improvements to our existing modules. <br></br>\n\n### Community\n![community](./img/community.png)\n\nThe most important pillar is all of you \u2013 our community. This includes both free, open-source users that self-host their Weaviate setup, as well as paid enterprise users and anyone using our Weaviate-as-a-Service offerings. We value your feedback and love that you are part of shaping our future. Last year we introduced our [dynamic roadmap page](/developers/weaviate/roadmap) that allows you to create and upvote your favorite feature requests."], "query": "What new features were added to the Weaviate Python client as mentioned in the document?"}
{"relevant_passages": ["## Exploring the Power of Vector Databases\n\nThe year 2023 was all about dynamic experimentation at Weaviate. Vector databases became a strong and recognized foundation in building ever more effective AI applications, enabling **chatbots,** **agents,** and **advanced** **search systems**. ### Online Hackathons\n\nOur 2023 global online hackathons proved to be vibrant innovation hubs, fostering diversity and inclusion in collaborative work. We teamed up with friends from [Cohere](https://cohere.com/), [LangChain](https://www.langchain.com/), [AutoGPT](https://autogpt.net/), [lablab.ai](https://lablab.ai/), [SuperAGI](https://superagi.com/), and many others. ![hackathons](img/hackathons.png)\n\n### In-person Hackathons\nWhether you're a beginner just diving into the world of coding, a passionate AI enthusiast, or a seasoned expert in the field, in-person events create a burst of energy and creativity into everyone's personal AI journey."], "query": "What advancements did Weaviate make in the field of AI applications in 2023?"}
{"relevant_passages": ["---\ntitle: Why is Vector Search so fast? slug: why-is-vector-search-so-fast\nauthors: [laura]\ndate: 2022-09-13\ntags: ['search']\nimage: ./img/hero.png\ndescription: \"Vector Databases can run semantic queries on multi-million datasets in milliseconds. How is that possible?\"\n---\n![Why is Vector Search so fast?](./img/hero.png)\n\n<!-- truncate -->\n\n## Why is this so incredibly fast? Whenever I talk about vector search, I like to demonstrate it with an example of a semantic search. To add the wow factor, I like to run my queries on a Wikipedia dataset, which is populated with over 28 million paragraphs sourced from Wikipedia."], "query": "How can vector databases perform semantic searches on datasets as large as Wikipedia's 28 million paragraphs so quickly?"}
{"relevant_passages": ["Taken directly from the paper, \u201cOur findings indicate that cross-encoder re-rankers can efficiently be improved without additional computational burden and extra steps in the pipeline by explicitly adding the output of the first-stage ranker to the model input, and this effect is robust for different models and query types\u201d. Taking this a bit further, [Dinh et al.](https://arxiv.org/abs/2206.06565) shows that most tabular machine learning tasks can be translated to text and benefit from transfer learning of text-based models. Many of these metadata rankers may also take in something like a collaborative filtering score that is based on this user\u2019s history, as well as other users on the platform \u2014 another interesting feature to think of interfacing this way. The main point being, maybe we can just add these meta features to our [query, document] representation and keep the Zero-Shot party going. We recently had an interesting discussion about metadata ranking and future directions for ranking models broadly on our latest Weaviate podcast! \ud83d\udc49 Check it out [here](https://www.youtube.com/watch?v=aLY0q6V01G4)\n\n## Score Rankers\nScore rankers describe using either a classifier to detect things, or a regression model to score things, about our candidate documents to rank with."], "query": "How can cross-encoder re-rankers be improved without additional computational costs according to recent research findings?"}
{"relevant_passages": ["When compressing a vector we find the closest centroid per segment and return an array of bytes with the indexes of the centroids per segment. When decompressing a vector, we concatenate the centroids encoded by each byte on the array. The explanation above is a simplification of the complete algorithm, as we also need to be concerned about performance for which we need to address duplication of calculations, synchronization in multithreading and so on. However what we have covered above should be sufficient to understand what you can accomplish using the PQ feature released in v1.18. Let's see some of the results HNSW implemented with PQ in Weaviate can accomplish next! If you're interested in learning more about PQ you can refer to the documentation [here](/developers/weaviate/concepts/vector-index#hnsw-with-product-quantizationpq)."], "query": "What is the PQ feature in Weaviate v1.18 used for in vector compression?"}
{"relevant_passages": ["Take the longer start-up time for example. Adding replication caused node-level start-up time to increase in our experiment. But the end result was that a hundred percent of requests succeeded. In other words, the end user would not have noticed anything was going on. And again, what are peace of mind and avoiding the wrath of angry users during downtime worth to you??"], "query": "What was the impact on node-level start-up time and request success rate after adding replication in the experiment?"}
{"relevant_passages": ["Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time."], "query": "When will Product Quantization be available in Weaviate for vector compression?"}
{"relevant_passages": ["### Stuffing\n\n<img\n    src={require('./img/stuffing.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nStuffing takes the related documents from the database and stuffs them into the prompt. The documents are passed in as context and go into the language model (the robot). This is the simplest method since it doesn\u2019t require multiple calls to the LLM. This can be seen as a disadvantage if the documents are too long and surpass the context length. ### Map Reduce\n\n<img\n    src={require('./img/map-reduce.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nMap Reduce applies an initial prompt to each chunk of data."], "query": "What is the method called that involves inserting related documents directly into the prompt for a language model, and what is its main limitation?"}
{"relevant_passages": ["The problem is that moving vectors to disk would have higher latency costs since we would then need lots of disk reads. The proposed solution by [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) is to store large complete representations of vectors on disk and keep a compressed representation of them in memory. The compressed representation is used to sort the vectors while searching for the nearest neighbors, and the complete representation is fetched from disk every time we need to explore a new vector from the sorted list. In plain English, we start our search from our root in the graph. From there, we get a set of neighbor candidates."], "query": "What technique does DiskANN use to reduce latency when searching for nearest neighbors with vectors stored on disk?"}
{"relevant_passages": ["---\ntitle: Authentication in Weaviate (videos)\nslug: authentication-in-weaviate\nauthors: [jp]\ndate: 2023-04-25\nimage: ./img/hero.png\ntags: ['concepts']\ndescription: \"Videos on authentication: an overview, how to log in, how to set it up, and core concepts - including recommendations.\"\n\n---\n\n![Authentication in Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nimport ReactPlayer from 'react-player/lazy'\n\n## Overview\n\nAuthentication is one of those topics that we get quite a few questions about. And we can see why. It's a big, complex topic, and even within Weaviate, there are many options available which can make it seem quite confusing. The core concept of authentication is relatively simple. When a client (e.g. a Weaviate client) sends a request to a server (e.g. a Weaviate database), it includes a \"secret\" that provides some assurances to Weaviate as to who that request is coming from, so that it can operate on that information."], "query": "What are the core concepts of authentication in Weaviate, and where can I find videos explaining how to set it up and log in?"}
{"relevant_passages": ["So let's look behind the curtain, and see if we can reproduce the magic. More specifically, let's try to reproduce Weaviate's output vector for each object by using an external API. ![pulling back the curtains](./img/pulling-back-the-curtains-text2vec.png)\n\n### Matching Weaviate's vectorization\n\nWe know that the vector for each object corresponds to its text. What we don't know is how, exactly. As each object only contains the two properties, `question` and `answer`, let's try concatenating our text and comparing it to Weaviate's."], "query": "How can I reproduce Weaviate's output vector for an object using an external API?"}
{"relevant_passages": ["6**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 9,990,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res4](./img/image8.png)\n**Fig. 7**: *Average time (microseconds) to calculate distances from query vectors to all 9,990,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment."], "query": "How does varying the segment length affect the time efficiency and recall when using a Product Quantizer with 200,000 vectors for fitting and 9,990,000 vectors for encoding?"}
{"relevant_passages": ["### The issue\nFast forward, we identified two key issues. First, the Go library that we used for reading binary data (`binary.read`) isn't optimized for how we use it in Weaviate, as it makes many temporary memory allocations. Second, for every object in the aggregation, we would allocate new memory on the heap, process the read, and release the memory. <!-- TODO: add a picture with cakes -->\nThis is a bit like, if we want to eat a cake, we need to put it on a plate, eat the cake and then put the plate in the sink to wash. Now, if we want to eat a million cakes, we will be either very busy washing dishes or have a million plates in the sink (or even run out of plates).<br/>\nI am sure you would rather spend more time eating cakes than dealing with plates."], "query": "What are the two key issues with the Go library and memory allocation in Weaviate as described in the document?"}
{"relevant_passages": ["Finally, we can jump on a bike to reach our local destination. For a better understanding, consider the below graphic, which shows a graph with all the connections generated using 1000 objects in two dimensions. <img\n    src={require('./img/vamana-graph.png').default}\n    alt=\"Vamana graph with 1000 objects\"\n    style={{ maxWidth: \"50%\" }}\n/>\n\nIf we iterate over it in steps \u2013 we can analyze how Vamana navigates through the graph. <img\n    src={require('./img/vamana-graph-animated.gif').default}\n    alt=\"Vamana graph - animated in 3/6/9 steps\"\n    style={{ maxWidth: \"50%\" }}\n/>\n\nIn the **first step**, you can see that the entry point for the search is in the center, and then the long-range connections allow jumping to the edges. This means that when a query comes, it will quickly move in the appropriate direction.<br/>\nThe **second**, **third**, and **final steps** highlight the nodes reachable within **three**, **six**, and **nine** hops from the entry node."], "query": "How does the Vamana algorithm navigate through a graph with 1000 objects in steps?"}
{"relevant_passages": [":::info Glossary\n- **Node**: A single machine in a cluster. It often refers to a physical or virtual machine that runs part of an application or service. - **Pod**: A Kubernetes term for a group of one or more containers, with shared storage/network, and a specification for how to run the containers. Pods are the smallest deployable units in Kubernetes. - **Tenant**: In the context of Weaviate, an isolated environment or subsection within the system, designed to separate data and access between different end users or groups."], "query": "What is the definition of a \"Pod\" in Kubernetes?"}
{"relevant_passages": ["Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Vector embeddings with Weaviate\n\nFor this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules."], "query": "What vectorization models and service providers does Weaviate support, and can it integrate with Hugging Face, OpenAI, or Cohere?"}
{"relevant_passages": ["As we have mentioned before, this is not the final solution where we still do not move information to disk. Our final solution would use a compressed version of the vectors to guide the exploration and fetch the fully described data (vectors and graph) from disk as needed. With the final approach we will achieve better compression rate but also better recall since the uncompressed vectors will be used to correct the compression distortion. A final remark. Notice how the compression rate is better on Gist."], "query": "What is the proposed final solution for handling data that involves compression and disk storage as mentioned in the document?"}
{"relevant_passages": ["The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually. All this means that you can also use the schema to tweak Weaviate's vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level."], "query": "How can I customize the vectorization process for a class in Weaviate's schema?"}
{"relevant_passages": ["Later on you will see an example of how the partitioning bricks identified the elements in a research paper. Cleaning the data is an important step before passing it to an NLP model. The cleaning brick can \u2018sanitize\u2019 your text data by removing bullet points, extra whitespaces, and more. Staging is the last brick and it helps to prepare your data as input into downstream systems. It takes a list of document elements as input and returns a formatted dictionary as output."], "query": "What are the functions of partitioning bricks, cleaning brick, and staging in the context of preparing data for an NLP model?"}
{"relevant_passages": ["The idea behind conditioning the images on a text prompt requires another model, one that is trained on images along with their captions. Examples of this data are shown below:\n\n![MS COCO Image Caption Dataset](./img/mscoco.png)\n\nThis model learns to relate descriptions of an image in text format with the image representation itself. In doing so it gives us a way to represent our written prompts as vectors that also capture the visual meaning behind the prompt. We can then pass these prompt vectors into our diffusion model along with the noised images during the training process. This allows us to tame the image generation process of the diffusion model by specifying to the model what types of images in the training set to resemble as it alters the pixels step by step."], "query": "How are text prompts used to guide image generation in a diffusion model?"}
{"relevant_passages": ["Let us know in the comments below!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What component is included for user interaction in the comments section of a web page?"}
{"relevant_passages": ["I recommend checking out the GitHub repository to test this out yourself!\n\n## Additional Resources\n\u2022 [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar. import StayConnected from '/_includes/stay-connected.mdx'\n\n<StayConnected />"], "query": "Where can I find the LangChain guide by Paul from CommandBar?"}
{"relevant_passages": ["## Q&A style questions on your own dataset answered in milliseconds\nWeaviate now allows you to get to sub-50ms results by using transformers on your own data. You can learn more about Weaviate\u2019s speed in combination with transformers in [this article](https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154). import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can Weaviate achieve sub-50ms search results with transformers?"}
{"relevant_passages": ["This means that holding the vectors in memory requires 1,000,000 x 128 x 4 bytes = 512,000,000 bytes. Additionally, a graph representation of neighborhoods is built when indexing. The graph represents the k-nearest neighbors for each vector. To identify each neighbor we use an `int64`, meaning we need 8 bytes to store each of the k-nearest neighbors per vector. The parameter controlling the size of the graph is `maxConnections`."], "query": "How much memory is required to store the k-nearest neighbors for each vector if each neighbor is identified by an int64?"}
{"relevant_passages": ["For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo \u2013 [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\nYou can try it for yourself by following this [link](https://link.weaviate.io/3LiVxqp), which is already pre-populated with the above question. Press the play button, to see the magic happen. Here is the thing, finding the correct answer in a gigantic repository of unstructured data is not the most impressive part of this demonstration (I mean, it is very impressive), but it is the \ud83d\ude80 speed at which it all happens. It takes a fraction of a second for the UI to show the results. We are talking about a semantic search query, which **takes milliseconds** to find an answer in a dataset containing **28 million paragraphs**."], "query": "How fast can Weaviate perform a semantic search on a dataset of 28 million paragraphs for articles about urban planning in Europe?"}
{"relevant_passages": ["At Weaviate, we pride ourselves on our research acumen and on providing state-of-the-art solutions. So we took time to explore these solutions to identify and evaluate the right building blocks for Weaviate's future. Here we share some of our findings from this research. ## On the HNSW vs. Vamana comparison\nAs the first step to disk-based vector indexing, we decided to explore Vamana \u2013 the algorithm behind the DiskANN solution."], "query": "Which algorithm did Weaviate explore as a first step towards disk-based vector indexing?"}
{"relevant_passages": ["3**: *We are compressing a 128 dimensions vector into a 32 bytes compressed vector. For this, we define 32 segments meaning the first segment is composed of the first four dimensions, the second segment goes from dimension 5th to 8th and so on. Then for each segment we need a compression function that takes a four dimensional vector as an input and returns a byte representing the index of the center which best matches the input. The decompression function is straightforward, given a byte, we reconstruct the segment by returning the center at the index encoded by the input.*\n\nA straightforward encoding/compression function uses KMeans to generate the centers, each of which can be represented using an id/code and then each incoming vector segment can be assigned the id/code for the center closest to it. Putting all of this together, the final algorithm would work as follows: Given a set of N vectors, we segment each of them producing smaller dimensional vectors, then apply KMeans clustering per segment over the complete data and find 256 centroids that will be used as predefined centers."], "query": "How can a 128-dimensional vector be compressed into a 32-byte representation using segmentation and KMeans clustering?"}
{"relevant_passages": ["Then when a query arrives, Weaviate traverses the index to obtain a good approximated answer to the query in a fraction of the time that a brute-force approach would take. [HNSW](/developers/weaviate/concepts/vector-index#hnsw) is the first production-ready indexing algorithm we implemented in Weaviate. It is a robust and fast algorithm that builds a hierarchical representation of the index **in memory** that could be quickly traversed to find the k nearest neighbors of a query vector. ## Need for disk solutions\nThere are other challenges to overcome. Databases have grown so fast that even the above-described algorithms will not be enough."], "query": "What indexing algorithm does Weaviate use to quickly traverse its index in memory?"}
{"relevant_passages": ["For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf2](./img/image13.png)\n**Fig. 12**: *The chart shows Recall (vertical axis) Vs Indexing time (in minutes, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf3](./img/image14.png)\n**Fig. 13**: *The chart shows Recall (vertical axis) Vs Latency (in microseconds, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf4](./img/image15.png)\n**Fig."], "query": "How many vectors were added using the normal HNSW algorithm and the compressed method in the experiment, and what do the charts in the figures illustrate?"}
{"relevant_passages": ["Partitioning 2. Cleaning, 3. Staging. Partitioning bricks take an unstructured document and extract structured content from it. It takes the document and breaks it down into elements like `Title`, `Abstract`, and `Introduction`."], "query": "What process is used to extract structured content like `Title`, `Abstract`, and `Introduction` from an unstructured document?"}
{"relevant_passages": ["### Implementation observations\n\nWe initially implemented the Vamana algorithm as described, resulting in very good recall results. Yet the latency was not good at all. We have since realized that the performance decay was due to many set operations making the algorithm perform poorly as is. In our revised implementation, we have modified the algorithm a bit to keep a copy of visited and current nodes on a single sorted list. Also, as the parameter L grows, the search on the sets becomes more expensive, so we already decided to keep a bit-based representation of the vectors residing on the sets, which made a huge impact performance-wise."], "query": "What modifications were made to the original Vamana algorithm to improve its performance in terms of latency?"}
{"relevant_passages": ["This will allow us to package the backup and run Weaviate in the last step directly from the backup. In the environment variables, we set a CLUSTER_HOSTNAME, an arbitrary name you can set to identify a cluster. ```yaml\nenvironment:\n  TRANSFORMERS_INFERENCE_API: 'http:loadbalancer:8080'\n  QUERY_DEFAULTS_LIMIT: 25\n  AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n  PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n  DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'\n  ENABLE_MODULES: 'text2vec-transformers'\n  CLUSTER_HOSTNAME: '63e2f234026d'\n```\n\n*Docker environment setup*\n\nWe will also set the location of the volume outside Weaviate, in this case the data will be stored in the /var/weaviate folder\n\n```yaml\nvolumes:\n  - /var/weaviate:/var/lib/weaviate\n```\n\n*Volumes for backup*\n\nYou can find the complete Docker Compose file we've used here. ## Query the Data\nThe current Weaviate setup has two modules enabled: semantic search and Q&A. The modules can be used for different types of queries."], "query": "What is the value of the CLUSTER_HOSTNAME environment variable in the Weaviate Docker setup?"}
{"relevant_passages": ["Latency when retrieving the hundred approximate nearest neighbors.*\n\n## Vamana implementation details\nWe have also included a development implementation of the Vamana indexing algorithm in Weaviate. For the algorithm to perform well, such an implementation needs careful attention to the optimization of the code. The original algorithm from Microsoft rests upon the greedy search and the robust prune methods, which are described in the [DiskANN paper](https://suhasjs.github.io/files/diskann_neurips19.pdf) as follows:\n\n![Vamana algorithm](./img/vamana-algorithm.png)\n\n### In plain English\nThese pseudo-code snippets are notoriously difficult to read, so here's a plain-English explanation of how Vamana works. The greedy search algorithm is used to find the solution for a query. The main idea is to start looking for the best points iteratively from the entry point."], "query": "What algorithm does Weaviate use to optimize the retrieval of the hundred approximate nearest neighbors?"}
{"relevant_passages": ["Here are some key differences between Vamana and HNSW:\n\n### Vamana indexing - in short:\n* Build a random graph. * Optimize the graph, so it only connects vectors close to each other. * Modify the graph by removing some short connections and adding some long-range edges to speed up the traversal of the graph. ### HNSW indexing - in short:\n* Build a hierarchy of layers to speed up the traversal of the nearest neighbor graph. * In this graph, the top layers contain only long-range edges."], "query": "What are the key differences between Vamana and HNSW indexing methods?"}
{"relevant_passages": [":::note Proposed feature\nTo reduce the length of this time, there is a proposed feature to proactively start repairing those inconsistencies (i.e. perform asynchronous replication). If this is important, please [upvote the feature here](https://github.com/weaviate/weaviate/issues/2405). :::\n\nBut we think that the cost of high availability is worth these prices. We take system availability seriously, and architect Weaviate according to this philosophy. This is one of the reasons that we use [leaderless replication](/developers/weaviate/concepts/replication-architecture/cluster-architecture#leaderless-design), and why replication in the first place is so important to us - because it enables our users to have robust systems on which they can rely."], "query": "What is the proposed feature in Weaviate to handle repairing inconsistencies, and how can users show their support for it?"}
{"relevant_passages": ["If we use `maxConnections = 64` when indexing Sift1M we end up with 1,000,000 x 64 x 8 bytes = 512,000,000 bytes also for the graph. This would bring our total memory requirements to around ~1 GB in order to hold both, the vectors and the graph, for 1,000,000 vectors each with 128 dimensions. 1 GB doesn't sound too bad! Why should we go through the trouble of compressing the vectors at all then!? Sift1M is a rather small dataset. Have a look at some of the other experimental datasets listed in Table 1 below; these are much bigger and this is where the memory requirements can begin to get out of hand."], "query": "How much memory is required to hold both the vectors and the graph for the Sift1M dataset with 1,000,000 vectors each of 128 dimensions using 64 max connections, and why might one consider compressing the vectors?"}
{"relevant_passages": ["---\ntitle: HNSW+PQ - Exploring ANN algorithms Part 2.1\nslug: ann-algorithms-hnsw-pq\nauthors: [abdel]\ndate: 2023-03-14\ntags: ['research']\nimage: ./img/hero.png\ndescription: \"Implementing HNSW + Product Quantization (PQ) vector compression in Weaviate.\"\n---\n![HNSW+PQ - Exploring ANN algorithms Part 2.1](./img/hero.png)\n\n<!-- truncate -->\n\nWeaviate is already a very performant and robust [vector database](https://weaviate.io/blog/what-is-a-vector-database) and with the recent release of  v1.18 we are now bringing vector compression algorithms to Weaviate users everywhere. The main goal of this new feature is to offer similar performance at a fraction of the memory requirements and cost. In this blog we expand on the details behind this delicate balance between recall performance and memory management. In our previous blog [Vamana vs. HNSW - Exploring ANN algorithms Part 1](/blog/ann-algorithms-vamana-vs-hnsw), we explained the challenges and benefits of the Vamana and HNSW indexing algorithms."], "query": "What are the details of implementing HNSW+PQ vector compression in Weaviate as discussed in the blog post from March 14, 2023?"}
{"relevant_passages": ["In this blog post, we will show you how to ingest PDF documents with Unstructured and query in Weaviate. :::info\nTo follow along with this blog post, check out this [repository](https://github.com/weaviate-tutorials/how-to-ingest-pdfs-with-unstructured). :::\n\n## The Basics\nThe data we\u2019re using are two research papers that are publicly available. We first want to convert the PDF to text in order to load it into Weaviate. Starting with the first brick (partitioning), we need to partition the document into text."], "query": "How do you convert PDF documents to text for ingestion into Weaviate?"}
{"relevant_passages": ["Notice the first peak in memory at the beginning. This was the memory used for loading the first fifth of the data plus compressing the data. We then wait for the garbage collection cycle to claim the memory back after which we send the remaining data to the server. Note that the peaks in the middle are due to memory not being freed by the garbage collection process immediately. At the end you see the actual memory used after the garbage collection process clean everything up."], "query": "What causes the initial and subsequent peaks in memory usage during the data handling process described, and what is the state of memory after garbage collection?"}
{"relevant_passages": ["---\ntitle: Weaviate 2023 Recap\nslug: 2023-recap\nauthors: [femke]\ndate: 2023-12-26\ntags: []\nimage: ./img/hero.png\ndescription: \"A reflection on 2023 from team Weaviate!\"\n---\n![hero](img/hero.png)\n\n<!-- truncate -->\n\nIt\u2019s hard to imagine that less than a year ago, so very few people even knew about the concept of vector databases and how AI could benefit from them. Those who did still had many questions about how they worked and whether they could at all be helpful. Meanwhile, curiosity and interest in AI spiked, especially after OpenAI launched ChatGPT. Curiosity has sped up our progress and made more people aware of the opportunities AI offers, transforming our landscape. Let's all take a moment to reflect and appreciate the start of a momentous change in how we can communicate, learn, teach, and collaborate so much faster and more effectively by leveraging AI."], "query": "What are the key highlights from the Weaviate team's 2023 recap?"}
{"relevant_passages": ["Although a bigger machine (see below) is needed for importing the data, the serving is done on a 12 CPU, 100 GB RAM, 250Gb SSD Google Cloud VM with 1 x NVIDIA Tesla P4. The ML-models used are [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) and [bert-large-uncased-whole-word-masking-finetuned-squad](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad) both are available as [pre-built modules](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers#pre-built-images) in Weaviate. \ud83d\udcc4 The complete dataset and code is open-source and available [on GitHub](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate). ![Demo GIF of Weaviate using the Wikipedia dataset](./img/weaviate-using-the-Wikipedia-dataset.gif)\n*Example semantic search queries in Weaviate's GraphQL interface \u2014 GIF by Author*\n\n## Importing the Data In Two Steps\n> You can also directly import a backup into Weaviate without doing the import your self as outlined [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/tree/main#step-3-load-from-backup). To import the data we use two different methods."], "query": "What are the specifications of the Google Cloud VM used for serving in the Weaviate semantic search through Wikipedia project?"}
{"relevant_passages": ["<br/>\nIt compares the vector values dimension by dimension and returns a total count of differing values. The fewer differences, the closer the vectors. For example, the Hamming distance for the below vectors is **2**, which is the count of differing values. * A `[1, 9, 3, 4, 5]`\n* B `[1, 2, 3, 9, 5]`\n\n### Manhattan distance\nThe Manhattan distance (also known as L1 norm and Taxicab Distance) - calculates the distance between a pair of vectors, as if simulating a route for a Manhattan taxi driver driving from point A to point B - who is navigating the **streets of Manhattan** with the grid layout and one-way streets. For each difference in the compared vectors, the taxi driver needs to make a turn, thus making the ride this much longer."], "query": "What is the Hamming distance between the vectors [1, 9, 3, 4, 5] and [1, 2, 3, 9, 5]?"}
{"relevant_passages": ["What does replication get us? A big one is *availability*. With no replication, any node being down will make its data unavailable. But in a Kubernetes setup composed of say, three Weaviate nodes (three Kubernetes \u201cpods\u201d) and a replication factor of three, you can have any one of the three nodes down and still reach consensus. This reflects Weaviate\u2019s leaderless replication architecture, meaning any node can be down without affecting availability at a cluster level as long as the right data is available somewhere."], "query": "How does replication enhance data availability in a Weaviate Kubernetes cluster?"}
{"relevant_passages": ["---\ntitle: Pulling back the curtains on text2vec\nslug: pulling-back-the-curtains-on-text2vec\nauthors: [jp]\ndate: 2023-01-10\ntags: ['integrations', 'concepts']\nimage: ./img/hero.png\ndescription: Ever wonder how Weaviate turns objects into vectors, behind-the-scenes? Find out in this post!\n---\n\n<!-- truncate -->\n\n![Pulling back the curtains on text2vec](./img/hero.png)\n\nYou probably know that Weaviate converts a text corpus into a set of vectors - each object is given a vector that captures its 'meaning'. But you might not know exactly how it does that, or how to adjust that behavior. Here, we will pull back the curtains to examine those questions, by revealing some of the mechanics behind `text2vec`'s magic. First, we will reproduce Weaviate's output vector using only an external API."], "query": "How does Weaviate convert text into vectors using `text2vec`, and how can this process be adjusted?"}
{"relevant_passages": ["<!-- truncate -->\n\n\n## 1. Install the client library\n\nThe Python and TypeScript client libraries support running Weaviate embedded on Linux, and starting with versions 3.21.0 and 1.2.0 respectively, on macOS as well. <Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  ```bash\n  pip install weaviate-client  --upgrade\n  ```\n\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  ```bash\n  npm install weaviate-ts-embedded typescript ts-node jest  # also install support for TypeScript and Jest testing\n  ```\n\n  </TabItem>\n</Tabs>\n\n\n## 2. Run the code\n\n<Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  Save as `embedded.py` and run `python embedded.py`:\n  <br/>\n\n  <FilteredTextBlock\n    text={PyCode}\n    startMarker=\"# START 10lines\"\n    endMarker=\"# END 10lines\"\n    language=\"py\"\n  />\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  Save as `embedded.ts` and run `node --loader=ts-node/esm embedded.ts`:\n  <br/>\n\n  <FilteredTextBlock\n    text={TSCode}\n    startMarker=\"// START 10lines\"\n    endMarker=\"// END 10lines\"\n    language=\"js\"\n  />\n  </TabItem>\n</Tabs>\n\n\n## <i class=\"fa-solid fa-screwdriver-wrench\"></i> How does this work? Essentially, what happens behind the scenes is that the client library downloads the server binary, spawns it in a separate process, connects to it, then terminates it on exit."], "query": "What versions of the Weaviate client libraries support running embedded on macOS, and how do you install and run them?"}
{"relevant_passages": ["}\n    },\n    \"vectorizer\": \"text2vec-huggingface\",  # vectorizer for hugging face\n   ... }\n```\n\n*If you are wondering, yes, you can use a different model for each class.*\n\n### Step 2 \u2013 cook for some time \u2013 import data\nStart importing data into Weaviate. For this, you need your Hugging Face API token, which is used to authorize all calls with \ud83e\udd17. Add your token, to a Weaviate client configuration. For example in Python, you do it like this:\n\n```javascript\nclient = weaviate.Client(\n    url='http://localhost:8080',\n    additional_headers={\n        'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n    }\n)\n```\nThen import the data the same way as always."], "query": "How do you add a Hugging Face API token to a Weaviate client configuration in Python?"}
{"relevant_passages": ["To see a list of the newly spun up nodes, run:\n\n```shell\nkubectl get nodes -o wide\n```\n\nYou should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\n\n```shell\nNAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nminikube       Ready    control-plane   134m   v1.27.3   192.168.49.2   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m02   Ready    <none>          134m   v1.27.3   192.168.49.3   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m03   Ready    <none>          133m   v1.27.3   192.168.49.4   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n```\n\nNow, add the Weaviate helm repository to your local helm configuration by running:\n\n```shell\nhelm repo add weaviate https://weaviate.github.io/weaviate-helm\n```\n\nAnd save the default configuration with:\n\n```shell\nhelm show values weaviate/weaviate > values.yaml\n```\n\nEdit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it. ```yaml\n... # Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\n# to cases where no data is imported yet. Scaling down after importing data may\n# break usability."], "query": "How can I list the current nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas?"}
{"relevant_passages": ["With these changes alone, we achieved performance close to Microsoft's implementation, as shown in **(fig. 3)** and **(fig. 4)**. Note that the implementation from Microsoft is in C++, and our implementation is in Go.\n\nWe evaluated different data structures and achieved the best performance by:\n* Keeping a sorted array-based set. * Making insertions use binary search."], "query": "How did the Go implementation achieve performance close to Microsoft's C++ implementation?"}
{"relevant_passages": ["What if you have a case where you have a large-scale dataset but very few queries? What if cost-effectiveness is more of a priority than the lowest possible latencies? We believe there is a need for other index types besides the battle-tested HNSW implementation. But cost-effectiveness can never justify sacrificing the user experience. As a result, we are working on establishing a new type of vector index that combines the low operating cost of SSD-based solutions with the ease of use of existing in-memory solutions."], "query": "What new type of vector index is being developed to be cost-effective for large-scale datasets with few queries while still being user-friendly?"}
{"relevant_passages": ["* Join our community on [Slack](https://weaviate.io/slack) or [the forum](https://forum.weaviate.io/), where we can all talk about vector databases. * Reach out to us on [Twitter](https://twitter.com/weaviate_io). import Ending from '/_includes/blog-end-oss-comment.md'\n\n<Ending />"], "query": "How can I join a community to discuss vector databases and follow their updates on Twitter?"}
{"relevant_passages": ["The first approach seems more feasible and thus we will explore it next. In addition to the above mentioned problem, we also need the new functions to operate efficiently so the overall performance of the system doesn't suffer. Keep in mind that for a single query, any ANN algorithm would require distance calculations so by adding unnecessary complexity to the distance function we might severely damage the latency of querying or the overall time to index vectors. For these reasons we need to select the compression mechanism very carefully! There already exist many compression mechanisms, of which a very popular one is Product Quantization(PQ). This is the compression algorithm we chose to implement in Weaviate with v1.18."], "query": "What compression algorithm was chosen for implementation in Weaviate v1.18 to ensure efficient operation of new functions without compromising system performance?"}
{"relevant_passages": ["### Announcement\n\n\ud83c\udf89We are happy to share that all Weaviate `v1.15` binaries and distributions have been **compiled with Go 1.19** which comes with **GOMEMLIMIT**. Now, you can set your **soft memory cap** by setting the `GOMEMLIMIT` environment variable like this:\n\n```\nGOMEMLIMIT=120GiB\n```\n\nFor more information, see the [Docker Compose environment variables](/developers/weaviate/installation/docker-compose#environment-variables) in the docs. ## Faster imports for ordered data\n\n![Faster imports for ordered data](./img/ordered-imports.png)\n\nWeaviate `v1.5` introduced an **LSM store** (Log-Structured Merge Trees) to increase write-throughput. The high-level idea is that writes are batched up in logs, sorted into a **Binary Search Tree** (BST) structure, and then these batched-up trees are merged into the tree on disk. ### The Problem\n\nWhen importing objects with an inherent order, such as timestamps or row numbers that increase monotonously, the BST becomes unbalanced: New objects are always inserted at the \"greater than\" pointer / right node of the BST."], "query": "What environment variable can be set to define a soft memory cap in Weaviate v1.15?"}
{"relevant_passages": ["This lets you find the right balance between the **recall tradeoff** (the fraction of results that are the true top-k nearest neighbors), **latency**, **throughput** (queries per second) and **import time**.*<br/>\n*For a great example, check [Weaviate benchmarks](/developers/weaviate/benchmarks/ann#sift1m-1m-128d-vectors-l2-distance), to see how three parameters \u2013 [efConstruction, maxConnections and ef](/developers/weaviate/benchmarks/ann#what-is-being-measured) - affect recall, latency, throughput and import times.*\n\n### Examples of ANN algorithms\nExamples of ANN methods are:\n* **trees** \u2013 e.g. [ANNOY](https://github.com/spotify/annoy) (Figure 3),\n* **proximity** **graphs** - e.g. [HNSW](https://arxiv.org/abs/1603.09320) (Figure 4),\n* **clustering** - e.g. [FAISS](https://github.com/facebookresearch/faiss),\n* **hashing** - e.g. [LSH](https://en.wikipedia.org/wiki/Locality-sensitive_hashing),\n* **vector compression** - e.g. [PQ](https://ieeexplore.ieee.org/document/5432202) or [SCANN](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html). ![ANNOY](./img/ann-annoy.png)<br/>\n*[Figure 3 - Tree-based ANN search]*\n\nWhich algorithm works best depends on your project. Performance can be measured in terms of latency, throughput (queries per second), build time, and accuracy (recall). These four components often have a tradeoff, so it depends on the use case which method works best. So, while ANN is not some magic method that will always find the true k nearest neighbors in a dataset, it can find a pretty good approximation of the true k neighbors."], "query": "What factors should be considered when evaluating the performance of approximate nearest neighbor search algorithms?"}
{"relevant_passages": ["---\ntitle: Ranking Models for Better Search\nslug: ranking-models-for-better-search\nauthors: [connor, erika]\ndate: 2023-04-11\nimage: ./img/hero.png\ntags: ['search']\ndescription: \"Learn about the different ranking models that are used for better search.\"\n\n---\n\n![ranking models animation](./img/hero.png)\n<!-- truncate -->\n\nWhether searching to present information to a human, or a large language model, quality matters. One of the low hanging fruit strategies to improve search quality are ranking models. Broadly speaking, ranking models describe taking the query and each candidate document, one-by-one, as input to predict relevance. This is different from vector and lexical search where representations are computed offline and indexed for speed. Back in August, we [published](/blog/cross-encoders-as-reranker) our thoughts on Cross Encoder Ranking."], "query": "What are ranking models, and how do they differ from vector and lexical search in improving search quality?"}
{"relevant_passages": ["But that is content for another article.\ud83d\ude09*\n\n## Learn more\nThe Weaviate Core team is currently working on research and implementation for other ANN algorithms. We are going to publish some of our findings in the next couple of weeks. So, stay tuned for more content on the topic. Until then, you can:\n* Listen to a [podcast about ANN Benchmarks](https://youtu.be/kG3ji89AFyQ) with Connor and Etienne from Weaviate. * Check out the [Getting Started with Weaviate](/developers/weaviate/quickstart) guide and begin building amazing apps with Weaviate."], "query": "What upcoming content is the Weaviate Core team planning to publish regarding ANN algorithms?"}
{"relevant_passages": ["### The solution\n\nAt the beginning of August, the Go team released `Go 1.19`, which introduced `GOMEMLIMIT`. `GOMEMLIMIT` turned out to be a **game changer for high-memory applications**. With GOMEMLIMIT we can provide a soft memory cap, which tells Go how much memory we expect the application to need. This makes the GC more relaxed when RAM is plentiful and more aggressive when memory is scarce. To learn more about memory management, GC and GOMEMLIMIT, check out [this article](/blog/gomemlimit-a-game-changer-for-high-memory-applications), which explains it all in more depth."], "query": "What feature introduced in Go 1.19 helps manage memory for high-memory applications?"}
{"relevant_passages": ["It\u2019s just that easy. Before you rush off to switch on replication, though, stick with us to read about the trade-offs and our recommendations. \ud83d\ude09\n\n## Trade-offs & discussions\n\nWhile replication and high availability are wonderful, we won\u2019t quite pretend that it comes for free. Having additional replicas of course means that there are more tenants and objects overall. Although they are duplicated, they are just as *real* as objects as any others."], "query": "What are some trade-offs to consider before enabling replication for high availability?"}
{"relevant_passages": ["This collapses the binary tree into a linked list with `O(n)` insertion rather than the `O(log n)` promise of the BST. ### The Fix\n\nTo address that in Weaviate `v1.15`, we've extended the BST with a **self-balancing Red-black tree**. Through rotations of the tree at insert, [Red-black trees](https://www.programiz.com/dsa/red-black-tree) ensure that no path from the root to leaf is more than twice as long as any other path. This achieves O(log n) insert times for ordered inserts. ![Red-black tree demonstration](./img/red-black-tree.gif)\n*A visual representation of how the RBT works.*\n\nYou can try it yourself [here](https://www.cs.usfca.edu/~galles/visualization/RedBlack.html)."], "query": "What feature was introduced in Weaviate `v1.15` to ensure `O(log n)` insertion times for ordered inserts?"}
{"relevant_passages": ["There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n\nNext, we optimized memory allocations for the HNSW (vector) index."], "query": "How does Weaviate ensure optimal performance and memory usage during parallel imports?"}
{"relevant_passages": ["![Manhattan taxi driver](./img/manhatten-distance-cars.png)\n\nThe Manhattan distance is calculated by adding up the differences between vector values. Following our previous example:\n* A `[1, 9, 3, 4, 5]`\n* B `[1, 2, 3, 9, 5]`\n\nWe can calculate the Manhattan distance in these steps:\n1. distance = `|1-1| + |9-2| + |3-3| + |4-9| + |5-5|`\n1. distance = `0 + 7 + 0 + 5 + 0`\n1. distance = `12`\n\nFor a deeper dive into the Hamming and Manhattan distances, check out our [blog post on distance metrics](/blog/distance-metrics-in-vector-search)."], "query": "How is the Manhattan distance between the vectors `[1, 9, 3, 4, 5]` and `[1, 2, 3, 9, 5]` calculated?"}
{"relevant_passages": ["We are already working on a fully disk-based solution which we will release in late 2023. <br></br>\n\n### Improving our Client and Module Ecosystem\n![client modules](./img/client-modules.png)\n\nSo far, we have only discussed features related to Weaviate Core, the server in your setup. But the Weaviate experience is more than that. Modules allow you to integrate seamlessly with various embedding providers, and our language clients make Weaviate accessible right from your application. This year, we will further improve both."], "query": "When is the fully disk-based solution for Weaviate expected to be released?"}
{"relevant_passages": ["Within a schema, you can set different vectorizers and vectorize instructions on a class level. First, because our use case is semantic search over Wikipedia, we will be dividing the dataset into paragraphs and use Weaviate's graph schema to link them back to the articles. Therefore we need two classes; *Article* and *Paragraph*. ```javascript\n{\n  classes: [\n    {\n      class: \"Article\",\n      description: \"A wikipedia article with a title\",\n      properties: {...},\n      vectorIndexType: \"hnsw\",\n      vectorizer: \"none\"\n    },\n    {\n      class: \"Paragraph\",\n      description: \"A wiki paragraph\",\n      properties: {...},\n      vectorIndexType: \"hnsw\",\n      vectorizer: \"text2vec-transformers\"\n    },\n  ]\n}\n```\n\n*Weaviate class structure*\n\nNext, we want to make sure that the content of the paragraphs gets vectorized properly, the vector representations that the SentenceBERT transformers will generate are used for all our semantic search queries. ```javascript\n{\n  name: \"content\",\n  datatype: [\n    \"text\"\n  ],\n  description: \"The content of the paragraph\",\n  invertedIndex: false,\n  moduleConfig: {\n    text2vec-transformers: {\n      skip: false,\n      vectorizePropertyName: false\n    }\n  }\n}\n```\n\n*A single data type that gets vectorized*\n\nLast, we want to make graph relations, in the dataset from step one we will distill all the graph relations between articles that we can reference like this:\n\n```javascript\n{\n  name: \"hasParagraphs\"\n  dataType: [\n    \"Paragraph\"\n  ],\n  description: \"List of paragraphs this article has\",\n  invertedIndex: true\n}\n```\n*Paragraph cross-references*\n\nThe complete schema we import using the [Python client](/developers/weaviate/client-libraries/python) can be found [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/blob/main/step-2/import.py#L19-L120)."], "query": "How do you set up a Weaviate schema for semantic search on Wikipedia data with vectorized content and graph relations?"}
{"relevant_passages": ["---\ntitle: Support for Hugging Face Inference API in Weaviate\nslug: hugging-face-inference-api-in-weaviate\nauthors: [sebastian]\ndate: 2022-09-27\ntags: ['integrations']\nimage: ./img/hero.png\ndescription: \"Running ML Model Inference in production is hard. You can use Weaviate \u2013 a vector database \u2013 with Hugging Face Inference module to delegate the heavy lifting.\"\n---\n![Support for Hugging Face Inference API in Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nVector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data \u2013 text, image, audio, etc. \u2013 into vectors and use that for context-based queries) and many more use cases. All of these use cases require `Machine Learning model inference` \u2013 a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) \u2013 which is a compute-heavy process."], "query": "What functionalities does the integration of Hugging Face Inference API with Weaviate offer?"}
{"relevant_passages": ["## Recommendations & Wrap-up\n\nAs we mentioned before, all you need to configure to enable replication is this in the collection definition:\n\n```json\n{\n  class: 'YOUR_CLASS_NAME',\n  ... replicationConfig: {\n    factor: 3,\n  }\n}\n```\n\nBut what replication factor would we recommend? That\u2019s something of a subjective question, but our starting recommendation is 3. The reason is that odd numbers are preferred for consistency so that consensus can always be reached. Higher factors are also possible, but this is more of a measure to scale query throughput, rather than lead to more availability."], "query": "What is the recommended replication factor for enabling replication in a collection definition, and why is an odd number preferred?"}
{"relevant_passages": ["1**: *Suppose we have vectors $x$ and $y$ represented in their original space. We apply a compression function $C$ to obtain a shorter representation of $x$ ($x'$) and $y$ ($y'$) on a compressed space but would require a decompression function $C'$ from the compressed space into the original space to be able to use the original distance function. In this case we would obtain $x''$ and $y''$ from $x'$ and $y'$ respectively and apply the distance on the approximations of the original $x$ and $y$ so $d(x,y)=d(x'',y'') + \\delta$ where $\\delta$ is the distortion added to the distance calculation due of the reconstruction of the original vectors. The compression/decompression mechanisms should be such that the distortion is minimized.*\n\n![comp2](./img/image2.jpg)\n**Fig. 2**: *Suppose we have vectors $x$ and $y$ represented in their original space."], "query": "How does the compression and subsequent decompression of vectors affect the accuracy of distance calculations between them?"}
{"relevant_passages": ["These embeddings better reflect the polysemantic nature of words, which can only be disambiguated when they are considered in context. Some of the potential downsides include:\n* increased compute requirements: fine-tuning transformer models is much slower (on the order of hours vs. minutes)\n* increased memory requirements: context-sensitivity greatly increases memory requirements, which often leads to limited possible input lengths\n\nDespite these downsides, transformer models have been wildly successful. Countless text vectorizer models have proliferated over the recent past. Plus, many more vectorizer models exist for other data types such as audio, video and images, to name a few."], "query": "What are the advantages and disadvantages of using transformer models for word embeddings?"}
{"relevant_passages": ["The amount of control you have over exactly what is generated is quite fascinating, you can even engineer a paragraph length prompt to describe exactly what you want the diffusion model to create and watch as it brings multiple variations of your description to life. ## Diffusion Model Resources to Create Art\n\nHere are some diffusion models that you can use to generate images:\n\n- [Stable Diffusion on Hugging Face](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n- A [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb) that uses the same Stable Diffusion model as above if you\u2019d like to tinker with the code. - Midjourney - allows you to create images by submitting prompts as discord messages\n- A [Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) for Stable Diffusion\n\n![searching through vector spaces](./img/searching_through_multi_dimensional_vector_space.jpg)\n*\u201cSearching through a high dimensional vector space\u201d*\n## Sources and Further Reading\n\nHere I\u2019ve attempted to provide a general intuition of how diffusion models work, there are many many more details that are involved in this process. If you enjoyed this introduction to diffusion models and would like to dive in deeper to get a better understanding of the code and algorithms involved I would recommend the following blog posts and courses in the following order or increasing complexity:\n\n- [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)\n- [Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What resources can I use to generate images with diffusion models?"}
{"relevant_passages": ["And it can do this in a fraction of the time!\n\n## HNSW in Weaviate\n[Weaviate](/developers/weaviate/) is a great example of a vector database that uses ANN algorithms to offer ultra-fast queries. The first ANN algorithm introduced to Weaviate is a custom implementation of [Hierarchical Navigable Small World graphs (HNSW)](/developers/weaviate/concepts/vector-index#hnsw). <img\n  src={require('./img/ann-hnsw.png').default}\n  alt=\"HNSW\"\n  style={{ maxWidth: \"50%\" }}\n/>\n\n*[Figure 4 - Proximity graph-based ANN search]*\n\nCheck out [Weaviate ANN benchmarks](/developers/weaviate/benchmarks/ann) to see how HNSW performed on realistic large-scale datasets. You can use it to compare the tradeoffs between recall, QPS, latency, and import time.<br/>\nYou will find it interesting to see, that Weaviate can maintain very high recall rates (>95%), whilst keeping high throughput and low latency (both in milliseconds). That is exactly what you need for fast, but reliable vector search!\n\n## Summary\nA quick recap:\n* Vector databases use Machine Learning models to calculate and attach Vector embeddings to all data objects\n* Vector embeddings capture the meaning and context of data\n* Vector databases offer super fast queries thanks to ANN algorithms\n* ANN algorithms trade a small amount of accuracy for huge gains in performance\n\n*Of course, there is a lot more going on in a vector database that makes it so efficient."], "query": "What ANN algorithm does Weaviate use to enable fast vector search, and how does it perform in terms of recall rates, throughput, and latency?"}
{"relevant_passages": ["With 128 locks, we only have 1/128th of the congestion of a single lock while still only using 128 * 8B = 1KB of memory. With the **lock striping** pattern, the import time is the same as without a lock, and we fixed the race condition without any negative performance impact. ![lock striping solution](./img/lock-striping-solution.png)\n\nWe are very pleased to introduce this solution, which should eliminate the above issues that can be caused by data duplication at import. Additionally, we are also very happy to have arrived at a solution that comes with no data import performance penalty, having seen the mammoth datasets that our users often deal with. ## Update Weaviate\nThe **lock striping** pattern was introduced in Weaviate `v1.15.4`."], "query": "What technique was implemented in Weaviate v1.15.4 to reduce congestion and memory usage without impacting data import performance?"}
{"relevant_passages": ["We could reduce latency by paying some penalty in the distortion of the distances (observed in the recall drop) or we could aim for more accurate results with a higher latency penalty. ### Deep-Image\n\nNext we show the results on DeepImage96. We see a similar profile compared to Sift1M. Latency is nearly 10 times slower but this is due to the fact that we have nearly 10 times more vectors. ![res3](./img/image7.png)\n**Fig."], "query": "How does the latency compare between DeepImage96 and Sift1M datasets when aiming for accurate search results?"}
{"relevant_passages": ["Sometimes, it's enough to output relevant data really fast. In such cases we care about latency a lot more than recall. Compressing, not only saves memory but also time. As an example, consider brute force where we compress very aggressively. This would mean we do not need memory at all for indexing data and very little memory for holding the vectors."], "query": "Why might aggressive data compression be beneficial in a brute force approach?"}
{"relevant_passages": ["In this example we used two research papers; however, there is the possibility to add Powerpoint presentations or even scanned letters to your Weaviate instance. Unstructured has really simplified the process of using visual document parsing for diverse document types. We tested a few queries above, but we can take this one step further by using [LangChain](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/weaviate). Once the documents are imported into Weaviate, you can build a simple chatbot to chat with your pdfs by using LangChain\u2019s vectorstore. ```python\nfrom langchain.vectorstores.weaviate import Weaviate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ChatVectorDBChain\nimport weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\nvectorstore = Weaviate(client, \"NAME_OF_CLASS\", \"NAME_OF_PROPERTY\")\n\nMyOpenAI = OpenAI(temperature=0.2,\n    openai_api_key=\"ENTER YOUR OPENAI KEY HERE\")\n\nqa = ChatVectorDBChain.from_llm(MyOpenAI, vectorstore)\n\nchat_history = []\n\nwhile True:\n    query = input(\"\")\n    result = qa({\"question\": query, \"chat_history\": chat_history})\n    print(result[\"answer\"])\n    chat_history = [(query, result[\"answer\"])]\n```\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can you build a chatbot to interact with documents in Weaviate using LangChain?"}
{"relevant_passages": ["During the Anthropic Hackathon in London, Ken and Pascal built the v1 version of their product. At the hackathon, they developed a support copilot capable of answering any user queries about a product or API. What's impressive is that it can be easily integrated into any website with just a single line of code. Now, their copilots provide answers and execute actions for users. [Ricards Liskovskis](https://www.linkedin.com/in/ricards-liskovskis-1872781a9/) crafted [RicAI](https://lablab.ai/event/autonomous-agents-hackathon/ricai/ricai-autonomous-testing-agent), securing 2nd place in the [SuperAGI Hackathon](https://superagi.com/autonomous-agents-hackathon/)."], "query": "Who developed a support copilot at the Anthropic Hackathon in London that can be integrated into websites with a single line of code?"}
{"relevant_passages": ["Document understanding techniques use an encoder-decoder pipeline that leverages the power of both computer vision and natural language processing methods. On the Weaviate Podcast, Brian Raymond described one of the founding motivations of Unstructured as follows: \u201cHey, HuggingFace is exploding over here with 10s of thousands of models and an incredible community. What if we did something similar to the left of HuggingFace, and we made it cheap, fast, and easy for data scientists to get through that data engineering step, so they can consume more of that!\u201d Now that the stage is set, let\u2019s explore how Unstructured works. Unstructured simplifies the process of importing a PDF and converting it into text. The core abstraction of Unstructured is the 'brick.' Unstructured uses bricks for document pre-processing: 1."], "query": "What was the motivation behind creating Unstructured as described by Brian Raymond on the Weaviate Podcast?"}
{"relevant_passages": ["We can send these features to wherever our Metadata Ranker is hosted and get those scores back to Weaviate to sort our search results. This is also closely related to another category of ranking methods that use models like XGBoost to combine features, as well as say the bm25 score, vector distance, and maybe even the cross encoder score as well. This is a pretty interesting technique when you additionally factor in multiple properties. For example, we could have a bm25, vector, and cross encoders for the `title`, as well as `content` properties and use a learned model to combine these into a final ranking score. I recently came across a paper titled \u201cInjecting the BM25 Score as Text Improves BERT-Based Re-rankers\u201d published in ECIR 2023."], "query": "What is the title of the paper mentioned in the document that discusses improving BERT-based re-rankers by injecting the BM25 score?"}
{"relevant_passages": ["### The problem\n\nWorking with a garbage collector is very safe, and the resource cost of running GC cycles is a fairly small tradeoff. We just need to ensure that we have the right balance of frequency of GC cycles and the buffer of available memory (on top of what we have estimated for our application setup). Now, increasing each of those comes with a price:\n* Increasing the frequency of GC cycles will use more CPU, which we could make better use of elsewhere. * Increasing RAM costs money - and for memory demanding setups, that can be a big \ud83d\udcb0sum. And if we get that balance wrong, we might end up with an Out Of Memory crash."], "query": "What are the tradeoffs and potential risks associated with adjusting the frequency of garbage collector cycles and the amount of available memory in an application setup?"}
{"relevant_passages": ["Now, that's a lot of new models. \ud83d\ude09\n\n#### How this works\nThe way the module works, Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. You need a `Hugging Face API Token` to use the Hugging Face module. You can [request it here](https://huggingface.co/login?next=%2Fsettings%2Ftokens)."], "query": "How do I obtain a Hugging Face API Token to use with the Weaviate Hugging Face module?"}
{"relevant_passages": ["When we rerun the original test (with ten parallel aggregations), we saw the memory consumption drop to 30GB (vs 200GB). <!-- TODO: add a quote from Juraj\nBut don't take our word for it, this was from a test run by one of our community members\u2026\n -->\n\n## New distance metrics\n\n![Hamming and Manhattan distance metrics](./img/distance-metrics.png)\n\nThanks to the community contributions from [Aakash Thatte](https://github.com/sky-2002), Weaviate `v1.15` adds two new distance metrics: **Hamming** distance and **Manhattan** distance. In total, you can now choose between five various distance metrics to support your datasets. Check out the [metrics documentation page](/developers/weaviate/config-refs/distances#distance-implementations-and-optimizations) for a complete overview of all available metrics in Weaviate. ### Hamming distance\nThe Hamming distance is a metric for comparing two numerical vectors."], "query": "What new distance metrics were added in Weaviate version 1.15?"}
{"relevant_passages": ["Vector embeddings capture the meaning and context of data, usually predicted by Machine Learning models. At the time of entry/import (or any significant changes to data objects), for every new/updated data object, vector databases use Machine Learning models to predict and calculate vector embeddings, which then are stored together with the object. > Every data object in a dataset gets a vector\n\nIn a nutshell, vector embeddings are an array of numbers, which can be used as coordinates in a high-dimensional space. Although it is hard to imagine coordinates in more than 3-dimensional space (x, y, z), we can still use the vectors to compute the distance between vectors, which can be used to indicate similarity between objects. <br/>\n\nThere are many different distance metrics, like [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) and [Euclidean distance (L2 distance)](https://en.wikipedia.org/wiki/Euclidean_distance)."], "query": "How are vector embeddings used in vector databases to determine the similarity between data objects?"}
{"relevant_passages": ["We analyze Product Quantization with KMeans encoding using different segment lengths. Notice that the lengths for the segments should be an integer divisor of the total amount of dimensions. For Sift1M we have used 1, 2, 4 and 8 dimensions per segment, while for Gist and DeepImage we have used 1, 2, 3, 4 and 6 dimensions per segment. Using segment length of 1 on Sift1M would give us 128 segments, while a segment length of 8 would give us 16 segments. We also offer the flexibility of choosing the amount of centroids to use with KMeans."], "query": "How many segments are created when using a segment length of 8 for the Sift1M dataset in Product Quantization with KMeans encoding?"}
{"relevant_passages": ["Tech giants like Google, AWS, or Microsoft Azure offer their vector-search capabilities to customers willing to upload their data. But there's now an ecosystem of newer companies with AI-first specific (often open-source) solutions and vector-search capabilities that customers can run on a SaaS basis or on their own systems. ## The AI-first Database Ecosystem\nThe companies that make up this ecosystem provide specialized services that overlap to various degrees. Combined, four sub-groups make up the ecosystem. 1."], "query": "What are the four sub-groups that make up the AI-first database ecosystem with vector-search capabilities?"}
{"relevant_passages": ["PDF documents contain valuable insights and information that are key to unlocking text information for businesses. With the latest advancements in multimodal deep learning (models that process both images and text), it is now possible to extract high quality data from PDF documents and add it to your Weaviate workflow. [Optical Character Recognition](https://en.wikipedia.org/wiki/Optical_character_recognition) (OCR) describes technology that converts different types of visual documents (research papers, letters, etc.) into a machine readable format. [RVL-CDIP](https://huggingface.co/datasets/aharley/rvl_cdip) is a benchmark that tests the performance of classifying document images. New models like [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut) leverage both the text and visual information by using a multimodal transformer."], "query": "What are the latest advancements in extracting data from PDF documents using multimodal deep learning, and which models leverage both text and visual information?"}
{"relevant_passages": ["This delta refers to the error we introduce due to the compression - we are using a lossy compression process. As we mentioned before, we are lowering the accuracy of our data, meaning the distance we calculate is a bit distorted; this distortion is exactly what we represent using delta. We don't aim to calculate such an error, nor to try to correct it. We should however acknowledge it and try to keep it low. ![comp1](./img/image1.jpg)\n**Fig."], "query": "What does the term \"delta\" refer to in the context of lossy data compression?"}
{"relevant_passages": ["How, you ask? Having redundancy is key to achieving this. And such redundancy, or replication, has been available for a while in Weaviate for production use. In fact, configuring replication is actually easy and simple, and using it can lead to huge benefits. In this post, we will explain the benefits of replication and show you how to configure it."], "query": "What are the benefits of using replication in Weaviate and how is it configured?"}
{"relevant_passages": ["Look for entries closest to `fast food chains` like so:\n\n```python\nnear_text = {\"concepts\": [\"fast food chains\"]}\nwv_resp = client.query.get(\n    \"Question\",\n    [\"question\", \"answer\"]\n).with_limit(2).with_near_text(\n    near_text\n).with_additional(['distance', 'vector']).do()\n```\n\nThis yields the McDonald's `Question` object above, including the object vector and the distance. The result is a `768`-dimensional vector that is about `0.1` away from the query vector. This all makes intuitive sense - the entry related to the largest fast food chain (McDonald's) is returned from our \u201cfast food chains\u201d query. But wait, how was that vector derived? This is the 'magic' part."], "query": "How do you use vector search to find database entries related to fast food chains?"}
{"relevant_passages": ["<br/>\n\nJust like that, you\u2019ve got a multi-node Weaviate cluster. Remember that when you create a class, you must have replication enabled. You can do this by adding the `replicationConfig` parameter to the collection definition, like so:\n\n```json\n{\n  \"class\": \"ClassWithReplication\",\n  \"properties\": [\n    {\n      \"name\": \"exampleProperty\",\n      \"dataType\": [\"text\"]\n    }\n  ],\n  // highlight-start\n  \"replicationConfig\": {\n    \"factor\": 3\n  }\n  // highlight-end\n}\n```\n\nAnd when you insert objects into `ClassWithReplication`, they will be replicated across the three nodes. You can verify this by visiting the `/nodes` endpoint, which will verify that each node contains the same number of objects. </details>\n\n## Benefits of replication\n\nSo, let\u2019s cover the implications of doing this."], "query": "How do you enable replication for a class in a Weaviate cluster and verify it?"}
{"relevant_passages": ["We apply a compression function $C$ to obtain a shorter representation of $x$ ($x'$) and $y$ ($y'$) on a compressed space. This saves storage space but would require a new distance function $d'$ to operate directly on the compressed space so $d(x,y) = d'(x',y') + \\delta$ where $\\delta$ is the distortion of the distance. We also need the new distance to be such that the distortion is minimized.*\n\nThe second approach might be better in terms of performance since it does not require the decompression step. The main problem is that we might need to define a new distance function for each encoding compression function. This solution looks harder to maintain in the long run."], "query": "What are the considerations for using a compression function in relation to maintaining distance relationships in a compressed space?"}
{"relevant_passages": ["Within Weaviate the transformer module can be used to vectorize and query your data. ## Getting started with out-of-the-box transformers in Weaviate\nBy selecting the text-module in the [Weaviate configuration tool](/developers/weaviate/installation/docker-compose#configurator), you can run Weaviate with transformers in one command. You can learn more about the Weaviate transformer module [here](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers). ![Weaviate configurator \u2014 selecting the Transformers module](./img/configurator-demo.gif)\n*Weaviate configurator \u2014 selecting the Transformers module*\n\n## Custom transformer models\nYou can also use custom transformer models that are compatible with Hugging Face's `AutoModel` and `AutoTokenzier`. Learn more about using custom models in Weaviate [here](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers)."], "query": "How can I start using the transformer module in Weaviate?"}
{"relevant_passages": ["---\ntitle: How A.I. Creates Art - A Gentle Introduction to Diffusion Models\nslug: how-ai-creates-art\nauthors: [zain]\ndate: 2023-01-24\ntags: ['concepts']\nimage: ./img/hero.png\ndescription: \"Machine learning models can create beautiful and novel images. Learn how Diffusion Models work and how you could make use of them.\"\n---\n![How A.I. Creates Art](./img/hero.png)\n\n<!-- truncate -->\n\nOne of the major developments this past year were the advancements made in machine learning models that can create beautiful and novel images such as the ones below. Though machine learning models with the capability to create images have existed for a while, this previous year we saw a marked improvement in the quality and photo-realism of the images created by these models. ![Perception of the world](./img/perception_of_the_world.jpg)\n\nModels like [DALL\u00b7E 2](https://openai.com/product/dall-e-2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion) and others which are the technologies underlying many platforms such as Lensa and Midjourney are being used by millions of people and are quickly becoming main stream as people realize their potential. These models not only have the ability to dream up photo-realistic images when prompted with text input but can also modify given images to add details, replace objects or even paint in a given artists style."], "query": "What are diffusion models in AI, and can you give examples of their use in creating art?"}
{"relevant_passages": ["* We made sure to reuse the same memory where possible. In the world of cakes, we optimized our eating technique to consume more cakes, and we only need one plate to eat all the cakes. On top of that, we've introduced other minor optimizations. So if you are curious about that, drop us a message on Slack, and we can chat some more. ### Results\n\nAs a result, the filtered aggregations are to **10-20x faster** and require less memory."], "query": "How much faster have filtered aggregations become after the recent optimizations?"}
{"relevant_passages": ["And Weaviate will handle all the communication with Hugging Face. ### Step 3 \u2013 serving portions \u2013 querying data\nOnce, you imported some or all of the data, you can start running queries. (yes, you can start querying your database even during the import). Running queries also requires the same token. But you can reuse the same client, so you are good to go."], "query": "Can you start querying a database in Weaviate during the data import process?"}
{"relevant_passages": ["Thus, quality at the expense of speed, becomes more interesting. ### LLMs as Cross Encoders\n\nSo, let\u2019s dive into the LLM hype a little more, how can we use LLMs for re-ranking? There are generally 2 ways to do this. The first strategy is identical to the cross encoder, we give the LLM the [query, document] input and prompt it to output a score of how relevant the document is to the query. The tricky thing with this is bounding the score."], "query": "How are Large Language Models used as cross encoders for re-ranking, and what is the challenge associated with scoring?"}
{"relevant_passages": ["Almost all objects are unique and it is not a problem to process those concurrently. We found that what we really needed was just a lock for each unique UUID. Cleverly, this approach would ensure that only one object per UUID is handled at each point in time, so that Weaviate cannot add multiple instances of objects with the same UUID. Meanwhile, it would still allow full parallelization of import processes to maximize performance. ![Single-lock solution](./img/single-lock-solution.png)\n\nAs it often happens, implementing a lock-per-key solution created a different issue."], "query": "What concurrency control mechanism does Weaviate use to handle objects with the same UUID during import processes?"}
{"relevant_passages": ["![graf](./img/image11.png)\n**Fig. 10**: *Heap usage while loading the data into the Weaviate server. Memory does not grow smoothly. Instead there is a higher peak at the beginning before the vectors are compressed.*\n\nLets focus the comparison in three directions:\n- How much longer does it take to index compressed data? - How much does it affect the recall and latency?"], "query": "What is the impact of vector compression on heap usage, indexing time, recall, and latency in the Weaviate server?"}
{"relevant_passages": ["This leads to the simplest definition of a third database wave: A vector database that stores data indexed by machine learning models. Different types of databases (e.g., vector search engines) allow users to search through these vectorized datasets, and others (e.g., feature stores) allow users to store vectors at a large scale for later use. ## We're awash in unstructured data. We're living in a time of massive data accumulation and much, if not most of it, is unstructured: text, photos, video, audio files, as well as other things such as genetic information. Vector search is particularly good at extracting value from such data."], "query": "What is the third database wave and what types of unstructured data does vector search excel at handling?"}
{"relevant_passages": ["Let's explore how PQ works. ## Product Quantization\n![ann](./img/Ann.png)\n\nIf you already know the details behind how Product Quantization works feel free to skip this section!\n\nThe main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently."], "query": "How does Product Quantization compress vectors?"}
{"relevant_passages": [":::infoEach video is only 3-6 minutes, and you do *not* need to watch them in order. :::\n\n:::note Tell us what you think!\nWe would love to know what you think of video content. Would you like to see more of them? Are there any particular types of videos you'd like more of? Please let us know below or on YouTube, and we'll do our best to listen."], "query": "How long are the videos mentioned in the document?"}
{"relevant_passages": ["This pillar is all about performance. The first big step will be the move towards a [Native Roaring Bitmap Index](https://github.com/weaviate/weaviate/issues/2511). In the most extreme case, this new index time can speed up filtered vector search [by a factor of 1000](https://twitter.com/etiennedi/status/1621180981519458305). But it doesn\u2019t stop there; we are already thinking about the next steps. Whether you want faster aggregations or new types of specialized indexes, we will ensure you can hit all your p99 latency targets with Weaviate."], "query": "What is the expected performance improvement factor for filtered vector search with the new Native Roaring Bitmap Index in Weaviate?"}
{"relevant_passages": ["This will take the candidates from hybrid search and apply the cross encoder to rank the final results. :::warning\nThis feature is not implemented into Weaviate yet, so the below code is an example of what it will look like. :::\n\n```graphql\n{\n  Get {\n    PodClip(\n      hybrid: {\n        query: \"How can I use ref2vec to build a home feed?\"\n        alpha: 0.5\n      }\n    ){\n    content\n    _additional {\n      crossrank(\n        query: \"How can I use ref2vec to build a homefeed?\"\n        property: \"content\"\n      ){\n      score\n    }\n  }\n}\n```\n\nLet\u2019s see an example of this in action using the Weaviate Podcast Search dataset!\n\n**Query**: Are there any ways to benchmark the performance of self-ask prompting? | Ranker                | Output |\n|---------------------------|------------------------------------|\n| Cross Encoder                  | That's a great question. Honestly, you should invite Ofir to chat about that."], "query": "How does the cross encoder feature work in Weaviate for ranking search results?"}
{"relevant_passages": ["Weaviate generates vector embeddings using [modules](/developers/weaviate/modules/retriever-vectorizer-modules) (OpenAI, Cohere, Google PaLM etc.), and conveniently stores both objects and vectors in the same database. For example, vectorizing the two words above might result in:\n\n```text\ncat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]\nkitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]\n```\n\nThese two vectors have a very high similarity. In contrast, vectors for \u201cbanjo\u201d or \u201ccomedy\u201d would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words. Now that you\u2019ve seen what vectors are, and that they can represent meaning to some extent, you might have further questions."], "query": "How does Weaviate generate and store vector embeddings for words?"}
{"relevant_passages": ["One technique is to prompt it with:\n`please output a relevance score on a scale of 1 to 100.`\n\nI think the second strategy is a bit more interesting, in which we put as many documents as we can in the input and ask the LLM to rank them. The key to making this work is the emergence of LLMs to follow instructions, especially with formatting their output. By prompting this ranking with \u201cplease output the ranking as a dictionary of IDs with the key equal to the rank and the value equal to the document id\u201d. Also interesting is the question around how many documents we can rank like this and how expensive it is. For example, if we want to re-rank 100 documents, but can only fit 5 in the input at a time, we are going to need to construct some kind of tournament-style decomposition of the ranking task."], "query": "What are two strategies for evaluating documents using large language models, and what are the considerations for ranking a large number of documents?"}
{"relevant_passages": ["In this article, we will cover the need for disk-based solutions, explore Vamana (how it works and contrast it against HNSW), and present the result of Vamana implementation. > If you are new to vector databases and ANN, I recommend you to read \"[Why is Vector Search so Fast?](/blog/why-is-vector-search-so-fast)\"<br/>\n> The article explains what vector databases are and how they work. ## Need for approximation\nIn order for a vector database to efficiently search through a vast number of vectors, the database needs to be smart about it. A brute-force approach would calculate the distance between the query vector and every data vector in the queried collection, but this is computationally very expensive. For a database with millions of objects and thousands of high-dimensional vectors, this would take far too long."], "query": "What is the focus of the article that discusses Vamana and its comparison with HNSW in the context of vector databases?"}
{"relevant_passages": ["2)** objects per query. The chart shows the recall vs. latency on the same Google Cloud setup Microsoft used. In both cases, you might notice that both Vamana and HNSW implementations are performing similarly. *Keep in mind that \u2013 while Vamana is the algorithm that powers DiskANN \u2013 at this stage, we are comparing both solutions in memory.*\n\n![Recall vs."], "query": "How do Vamana and HNSW implementations compare in terms of recall and latency in an in-memory setup on Google Cloud?"}
{"relevant_passages": ["These events are not just about innovation. They provide a safe and welcoming space to collaborate, meet in person, and celebrate successes. It was fantastic bringing together the community in person and meeting so many of you!\n\n<div className=\"youtube\">\n    <iframe src=\"//www.youtube.com/embed/_whxda27bCQ\" frameBorder=\"0\" allowFullScreen></iframe>\n</div>\nVector Database & Large Language Model Hackathon, Berlin, 2023 - In collaboration with the MLOps community, aimed at fostering teamwork and knowledge sharing in machine learning. ## Building with Weaviate\nIn the realm of Generative AI, especially with the emergence of OpenAI's ChatGPT, the technology landscape has had a significant impact. Users began experimenting with ChatGPT early in the year."], "query": "What was the purpose of the Vector Database & Large Language Model Hackathon in Berlin, 2023?"}
{"relevant_passages": ["These models are reaching new heights in performance because they leverage visual information, not just text. <figure>\n\n![Donut pipeline](./img/donut.png)\n<figcaption> Pipeline of Donut from Kim, G. et al (2022) </figcaption>\n</figure>\n\n\n## About Unstructured\n[Unstructured](https://www.unstructured.io/) is an open-source company working at the cutting edge of PDF processing and more. They allow businesses to ingest their diverse data sources, whether this be a `PDF`, `JPEG`, or `PPT`, and convert it into data that can be passed to a LLM. This means that you could take private documents from your company and pass it to a LLM to chat with your PDFs.\n\nUnstructured\u2019s open-source [core library](https://github.com/Unstructured-IO/unstructured) is powered by document understanding models."], "query": "What can Unstructured's open-source technology do with private company documents?"}
{"relevant_passages": ["See below for the Mona Lisa drip painted in the style of Jackson Pollock!\n\n![Mona lisa drip painting](./img/the_mona_lisa_drip_painted.jpg)\n\nThe technology behind these images is called **diffusion models**. In this post I aim to provide a gentle introduction to diffusion models so that even someone with minimal understanding of machine learning or the underlying statistical algorithms will be able to build a general intuition of how they work. Additionally I\u2019ll also provide some external resources that you can use to access pre-trained diffusion models so you can start to generate your own art!\n\nThese are the points that we\u2019ll expand on in this article:\n\n- How diffusion models can create realistic images\n- How and why we can control and influence the images these models create using text prompts\n- Access to some resources you can use to generate your own images. ## How Diffusion Models Work\n\nDiffusion models are a type of generative model - which means that they can generate data points that are similar to the data points they\u2019ve been trained on(the training set). So when we ask Stable Diffusion to create an image it starts to dream up images that are similar to the billions of images from the internet that it was trained on - it\u2019s important to note that it doesn\u2019t simply copy an image from the training set(which would be no fun!) but rather creates a new image that is similar to the training set."], "query": "What are diffusion models in machine learning, and how can they be used to generate art?"}
{"relevant_passages": ["## The Six Pillars for 2023\n\n### Ingestion and Search Pipelines\n![Ingestion and Search Pipelines](./img/search-pipeline.png)\nWeaviate\u2019s strong and growing module ecosystem gives you plenty of flexibility. Whether you use Weaviate as a pure vector search engine or with the addition of vectorizer, reader, and generator modules, you can always configure it to your liking. In early 2023 we even saw the addition of the [`generative-openai`](/developers/weaviate/modules/reader-generator-modules/generative-openai) module (with other generative modules to come). We want to give you even more flexibility in combining these steps this year. You can control arbitrary querying steps through the [proposed Pipe API](https://github.com/weaviate/weaviate/issues/2560), such as reading, re-ranking, summarizing, generating, and others."], "query": "What new module was added to Weaviate's ecosystem in early 2023?"}
{"relevant_passages": ["And instead of the `text2vec-cohere` module, we will go straight to the Cohere API. Concatenating the text from the object:\n\n```python\nstr_in = ' '.join([i for i in properties.values()])\n```\n\nWe see:\n\n```text\n'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger McDonald\\'s'\n```\n\nThen, use the Cohere API to generate the vector like so, where `cohere_key` is the API key (keep it secret!), and `model` is the vectorizer. ```python\nimport cohere\nco = cohere.Client(cohere_key)\nco_resp = co.embed([str_in], model=\"embed-multilingual-v2.0\")\n```\n\nThen we run a `nearVector` based query to find the best matching object to this vector:\n\n```python\nclient.query.get(\n    \"Question\",\n    [\"question\", \"answer\"]\n).with_limit(2).with_near_vector(\n    {'vector': co_resp.embeddings[0]}\n).with_additional(['distance']).do()\n```\n\nInterestingly, we get a distance of `0.0181` - small, but not zero. In other words - we did something differently to Weaviate!\n\nLet's go through the differences one by one, which hopefully will help you to see Weaviate's default behavior. First, Weaviate sorts properties alphabetically (a-z) before concatenation."], "query": "Which company served its billionth burger live on \"The Art Linkletter Show\" in 1963?"}
{"relevant_passages": ["## How We Got Here\nFirst-wave database technology is often called by the acronym SQL\u2014the initials of a ubiquitous query language used to manage relational databases, which are conceptually similar to spreadsheets or tables. Throughout the 1980s, this technology was dominated by companies like Oracle and Microsoft. The second wave of databases is called \"NoSQL\". These are the domain of companies like MongoDB. They store data in different ways, for example, key-value stores, document stores, wide-column stores and graph databases, but what they all have in common is that they're not relational tables."], "query": "What type of database technology is associated with the company MongoDB?"}
{"relevant_passages": ["We need to explore each candidate one at a time. We sort the candidates using the compressed representations, stored in memory, and decide what the best next candidate to explore is. Exploring a candidate would mean fetching it from disk. Notice now that we are not reading all vectors from disk, but only the most promising candidates we wish to explore. This way we still need to host the compressed vectors in memory, but it will give us enough information to reduce disk reads significantly."], "query": "How does the use of compressed representations in memory help in reducing disk reads when exploring candidates?"}
{"relevant_passages": ["This means that a replication factor of 3 leads to the cluster essentially handling three times the load as no replication. The benefit here is of course redundancy and thus availability, but the cost is additional cost, such as increased hardware (e.g. memory) requirements. Note that the rolling update time was longer on our replication example than the non-replication example, as each pod now holds ~10,000 tenants rather than ~3,333 tenants. Another challenge is that any write request that comes in while any nodes are down will be temporarily missing on that node for a while. This will be repaired in time through a read-repair that [happens automatically in the background](/developers/weaviate/concepts/replication-architecture/consistency#repairs)."], "query": "What are the implications of using a replication factor of 3 on cluster load and update times?"}
{"relevant_passages": ["This is done with:\n\n```python\nfrom unstructured.partition.pdf import partition_pdf\n\nelements = partition_pdf(filename=\"../data/paper01.pdf\")\n```\n\nNow, if we want to see all of the elements that Unstructured found, we run:\n\n```python\ntitles = [elem for elem in elements if elem.category == \"Title\"]\n\nfor title in titles:\n    print(title.text)\n```\n\n<details>\n  <summary>Response from Unstructured</summary>\n\nA survey on Image Data Augmentation for Deep Learning\nAbstract\nIntroduction\nBackground\nImage Data Augmentation techniques\nData Augmentations based on basic image manipulations\nFlipping\nColor space\nCropping\nRotation\nTranslation\nNoise injection\nColor space transformations\nGeometric versus photometric transformations\nKernel filters\nMixing images\nRandom erasing\nA note on combining augmentations\nData Augmentations based on Deep Feature space augmentation\nData Augmentations based on Deep Learning\nFeature space augmentation\nAdversarial training\nGAN\u2011based Data Augmentation\nGenerated images\nNeural Style Transfer\nMeta learning Data Augmentations\nComparing Augmentations\nDesign considerations for image Data Augmentation\nTest-time augmentation\nCurriculum learning\nResolution impact\nFinal dataset size\nAlleviating class imbalance with Data Augmentation\nDiscussion\nFuture work\nConclusion\nAbbreviations\nAcknowledgements\nAuthors\u2019 contributions\nFunding\nReferences\nPublisher\u2019s Note\n\n</details>\n\nIf we want to store the elements along with the content, you run:\n\n```python\nimport textwrap\n\nnarrative_texts = [elem for elem in elements if elem.category == \"NarrativeText\"]\n\nfor index, elem in enumerate(narrative_texts[:5]):\n    print(f\"Narrative text {index + 1}:\")\n    print(\"\\n\".join(textwrap.wrap(elem.text, width=70)))\n    print(\"\\n\" + \"-\" * 70 + \"\\n\")\n```\n\nYou can then take this data, vectorize it and store it in Weaviate. ![PDFs to Weaviate](./img/Weaviate-ingesting-dark.png#gh-dark-mode-only)\n![PDFs to Weaviate](./img/Weaviate-ingesting-light.png#gh-light-mode-only)\n\n## End-to-End Example\nNow that we\u2019ve introduced the basics of using Unstructured, we want to provide an end-to-end example. We\u2019ll read a folder containing the two research papers, extract their abstracts and store them in Weaviate. Starting with importing the necessary libraries:\n\n```python\nfrom pathlib import Path\nimport weaviate\nfrom weaviate.embedded import EmbeddedOptions\nimport os\n```\n\nIn this example, we are using [Embedded Weaviate](/developers/weaviate/installation/embedded). You can also run it on [WCS](https://console.weaviate.cloud) or [docker](/developers/weaviate/installation/docker-compose)."], "query": "How can I extract titles from a PDF and store them in Weaviate using Python?"}
{"relevant_passages": ["There are many different ANN algorithms, each with different advantages and limitations. ### Large Scale\nWhen we talk about a vast number of objects, today we often see use cases with hundreds of millions of vectors, but it won't take long until billions, or even trillions, will be a pretty standard use case. To get vector databases to that kind of scale, we need to constantly evolve and look for more efficient solutions. A big part of this search is to explore ANN algorithms that would let us go beyond the available RAM (which is a bit of a bottleneck) without sacrificing the performance and the UX. ### What to expect\nIn this series of blog posts, we will take you on a journey with us as we research and implement new ANN algorithms in our quest to reach the 1T goal."], "query": "What is the future scalability goal for vector databases using ANN algorithms as mentioned in the blog series?"}
{"relevant_passages": ["We will look at the [ChatVectorDB chain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db), it lets you build an LLM that stores chat history and retrieves context from Weaviate to help with generation. To begin, the chat history in this chain uses the `stuff` configuration of CombineDocuments. This means we take as much of the chat history as we can fit in our context window and use it for a query reformulation prompt. The prompt is as follows:\n\n```python\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\n```\n\nWe put the chat_history and the latest user input in the curly bracket syntax."], "query": "What is the purpose of the `stuff` configuration in the ChatVectorDB chain's chat history, and how is it used in query reformulation?"}
{"relevant_passages": ["* The deeper the search traverses through the hierarchy, the shorter the distance between vectors captured in the edges. Put simply, Vamana can build a flat graph, in contrast to HNSW, which uses a hierarchical representation. And a flat graph may suffer less performance degradation from being stored on disk than a hierarchical representation might. The reason is that since the outgoing connections from each node are known, it is possible to store the information in such a way that we can calculate the exact position on the file to read when retrieving information on the neighbors of a given node. This makes the information retrieval process very efficient, and thus the lower speed imposed by disk storage becomes less of a problem."], "query": "Why might Vamana's flat graph structure perform better than HNSW's hierarchical representation when stored on disk?"}
{"relevant_passages": ["The points in the curve are obtained varying the amount of centroids. The \u201cbase\u201d curve refers to the fixed average time to calculate distances between uncompressed vectors.*\n\n### Gist\n\nFinally, we show results on the Gist database using the 1,000,000 vectors. We see a similar profile compared to Sift1M again. Latency is nearly 10 times slower but this time it is due to the fact that we have nearly 10 times more dimensions per vectors. ![res5](./img/image9.png)\n**Fig."], "query": "How does the latency of distance calculations on the Gist database compare to that of the Sift1M database, and what role do vector dimensions play?"}
{"relevant_passages": ["Then we will see how the text vectorization process can be tweaked, before wrapping up by discussing a few considerations also. ## Background\n\nI often find myself saying that Weaviate makes it fast and easy to produce a vector database from text. But it can be easy to forget just how fast and how easy it can make things. It is true that even in the \u201cold days\u201d of say, five to ten years ago, producing a database with vector capabilities was technically possible. You *simply* had to (*inhales deeply*) develop a vectorization algorithm, vectorize the data, build a vector index, build a database with the underlying data, integrate the vector index with the database, then forward results from a vector index query to the database and combine the outputs from both (*exhales*)."], "query": "What were the steps involved in creating a vector database before the advent of tools like Weaviate?"}
{"relevant_passages": ["The prompt is to summarize the abstract of the two papers in one sentence. This type of summarization is very useful when scouting out new research papers. This enables us to get a quick summary of the abstract and ask questions specific to the paper. ```python\nprompt = \"\"\"\nPlease summarize the following academic abstract in a one-liner for a layperson:\n\n{abstract}\n\"\"\"\n\nresults = (\n    client.query.get(\"Document\", \"source\").with_generate(single_prompt=prompt).do()\n)\n\ndocs = results[\"data\"][\"Get\"][\"Document\"]\n\nfor doc in docs:\n    source = doc[\"source\"]\n    abstract = doc[\"_additional\"][\"generate\"][\"singleResult\"]\n    wrapped_abstract = textwrap.fill(abstract, width=80)\n    print(f\"Source: {source}\\nSummary:\\n{wrapped_abstract}\\n\")\n```\n\n<details>\n  <summary>Output</summary>\n\n```\nSource: paper01.pdf\nSummary:\nData Augmentation is a technique that enhances the size and quality of training\ndatasets for Deep Learning models, particularly useful in domains with limited\ndata such as medical image analysis. ```\n```\nSource: paper02.pdf\nSummary:\nUsing machine learning techniques, researchers explore predicting house prices\nwith structured and unstructured data, finding that the best predictive\nperformance is achieved with term frequency-inverse document frequency (TF-IDF)\nrepresentations of house descriptions."], "query": "What are the one-sentence summaries of the abstracts for paper01.pdf and paper02.pdf as generated by the Python script?"}
{"relevant_passages": ["For example, many of our users already run Weaviate with multi-tenancy (introduced in version `1.20`) to host thousands of active tenants or even more. One side effect of scaling is that as load increases on each node, it will take longer to start up. While a fresh Weaviate instance typically starts up essentially instantaneously, a node with 1000s of tenants can take up over 1 minute. Node-level downtime is an unavoidable fact of life, since either hardware and software may necessitate restarts for maintenance and/or updates. But node-level downtime doesn\u2019t have to lead to user-level downtime and failed requests."], "query": "In which version of Weaviate was multi-tenancy introduced?"}
{"relevant_passages": ["---\ntitle: How we solved a race condition with the Lock Striping pattern\nslug: Lock-striping-pattern\nauthors: [dirk]\ndate: 2022-10-25\ntags: ['engineering']\nimage: ./img/hero.png\ndescription: \"The Lock Striping pattern is a great way to solve race conditions without sacrificing performance. Lean about Weaviate's improvements.\"\n---\n![How we solved a race condition with the Lock Striping pattern](./img/hero.png)\n\n<!-- truncate -->\n\n## Lock striping in database design\nDatabase design comes with interesting challenges. Like, dealing with race conditions when importing data in parallel streams. But for every new challenge, there is a clever solution. One of those clever solutions is Lock striping."], "query": "What is the Lock Striping pattern and how did it help solve race conditions in Weaviate's database design?"}
{"relevant_passages": ["Imagine that we are building a quiz app that allows our users to search for questions. Then, it may be preferable to import each quiz item into two classes, one for the question and one for the answer to avoid giving away the answer based on the user's query. But we may yet use the answer to compare to the user's input. On the other hand, in some cases, it may be preferable to import text data with many fields as one object. This will allow the user to search for matching meanings without as much consideration given to which field the information is contained in exactly."], "query": "What is a design consideration for structuring quiz items in a quiz app to enhance search functionality without revealing answers?"}
{"relevant_passages": ["Luckily, hybrid search comes to the rescue by combining the contextual semantics from the vector search and the keyword matching from the BM25 scoring. If the query is: `How can I use ref2vec to build a home feed?` The pairing of vector search and BM25 will return a good set of candidates. Now with the ranking model, it takes the [query, candidate document] pair as input and is able to further reason about the relevance of these results without specialized training. Let\u2019s begin with categories of ranking models. We see roughly 3 different genres of ranking models with:\n\n1."], "query": "What is hybrid search and how does it apply to building a home feed with ref2vec?"}
{"relevant_passages": ["The fact that the graph is still in memory makes it hard to see the difference between those different levels of compression. The more we compress the lower the recall we would expect. Let us discuss the lowest level of compression along with some expectations. For Sift1M we would require roughly 1277 MB to 1674 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.96811 to 0.99974 and latencies ranging from 293 to 1772 microseconds."], "query": "What is the range of memory required to index the Sift1M dataset using uncompressed HNSW, and what recall and latency can be expected?"}
{"relevant_passages": ["Know someone who made a difference in your Weaviate journey? Nominate them as a Weaviate Hero! Let's recognize the people who make our community great. \ud83d\udc9a\n\nNominate your hero [here](https://docs.google.com/forms/d/e/1FAIpQLSePtgvTPZGx2gbCzVdyjitQ0WHrq4gNNZKiaqrQhphmJc3vJA/viewform). As we keep building, learning, and collaborating, we're excited to see the innovations that 2024 will bring. \ud83d\udcab\n\n![Weaviate Hero](img/Weaviate-hero_(1).gif)\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can I nominate someone for the Weaviate Hero recognition?"}
{"relevant_passages": ["Latency when retrieving the ten approximate nearest neighbors](./img/fig-1.png)\n*Fig. 1: Recall vs. Latency when retrieving the ten approximate nearest neighbors.*\n\n\n![Recall vs. Latency when retrieving the hundred approximate nearest neighbors](./img/fig-2.png)\n*Fig. 2: Recall vs."], "query": "What is the relationship between recall and latency when retrieving the ten approximate nearest neighbors?"}
{"relevant_passages": ["Furthermore, you might be using Weaviate with your own data which might be even bigger than the datasets we report on below. If we extrapolate out, consider how big these memory requirements could grow as you add more objects or represent these objects with long vector embeddings. | DataSet     | Dimensions | Vectors   | Size in memory (MB) |\n|-------------|------------|-----------|---------------------|\n| Sift1M      | 128        | 1,000,000 | 512                 |\n| Gist        | 960        | 1,000,000 | 3840                |\n| DeepImage96 | 96         | 9,990,000 | 3836.16             |\n\n**Table 1**: *Description of datasets.*\n\nIncreasing the number of objects vs. storing longer dimensional vectors has an equivalent effect on the overall memory required to store the vectors. As an example consider the Gist dataset, which contains 1,000,000 vectors, each with 960 dimensions."], "query": "How much memory does the Gist dataset require to store its vectors?"}
{"relevant_passages": ["The reason for this is the fact that we have larger vectors. This means that the memory dedicated to vectors is larger than the memory dedicated to the graph and since we only compress vectors, then the larger the vectors the higher the compression rate. Having even larger vectors such as OpenAI embeddings (1536 dimensions) would reach even better compression rates. ![perf5](./img/image16.png)\n**Fig.15**: *The charts show heap usage. Top down it shows Sift1M, DeepImage and Gist."], "query": "Why do larger vectors result in higher compression rates in the context of memory usage?"}
{"relevant_passages": ["We would be amiss to not mention other search methods such as BM25F, or hybrid searches, both of which would be affected by these decisions. Now that you've seen exactly what happens behind the curtains, we encourage you to try applying these concepts yourself the next time you are building something with Weaviate. While the changes to the similarities were somewhat minor in our examples, in some domains and corpora their impact may be certainly larger. And tweaking the exact vectorization scheme may provide that extra boost your Weaviate instance is looking for. import StayConnected from '/_includes/stay-connected.mdx'\n\n<StayConnected />"], "query": "What search methods might be influenced by vectorization scheme adjustments in Weaviate?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.19 Release\nslug: weaviate-1-19-release\nauthors: [jp,zain,erika]\ndate: 2023-05-04\nimage: ./img/hero.png\ntags: ['release']\ndescription: \"Weaviate 1.19 introduces generative cohere module, gRPC API support, improved data types, and more.\"\n\n---\n\nimport Core119 from './_core-1-19-include.mdx' ;\n\n<Core119 />\n\nimport WhatsNext from '/_includes/what-next.mdx'\n\n<WhatsNext />\n\nimport Ending from '/_includes/blog-end-oss-comment.md' ;\n\n<Ending />"], "query": "What are the new features introduced in Weaviate 1.19?"}
{"relevant_passages": ["---\ntitle: The AI-First Database Ecosystem\nslug: the-ai-first-database-ecosystem\nauthors: [bob]\ndate: 2022-06-23\ntags: ['concepts']\nimage: ./img/hero.jpeg\n# canonical-url: https://www.forbes.com/sites/forbestechcouncil/2022/06/23/the-ai-first-database-ecosystem/\n# canonical-name: Forbes\ndescription: \"Learn about the vision of the AI-First Database Ecosystem, which drives the R&D of the databases of the future.\"\n---\n![The AI-First Database Ecosystem](./img/hero.jpeg)\n\n<!-- truncate -->\n\nA new ecosystem of smaller companies is ushering in a \"third wave\" of AI-first database technology. New search engines and databases brilliantly answer queries posed in natural language, but their machine-learning models are not limited to text searches. The same approach can also be used to search anything from images to DNA. Much of the software involved is open source, so it functions transparently and users can customize it to meet their specific needs. Clients can retain control of their data, keeping it safely behind their own firewalls."], "query": "What is the \"third wave\" of AI-first database technology about?"}
{"relevant_passages": ["The next natural question is: How do we compress vectors? ## How to Efficiently Compress Vectors\n\nThe main idea behind vector compression is to have a \u201cgood-enough\u201d representation of the vectors (as opposed to a perfect representation) so they take up less space in memory while still allowing us to calculate the distance between them in a performant and accurate way. Compression could come from different sources. We could, for example, aim to reduce redundant data to store information more efficiently. We could also sacrifice accuracy in the data in favor of space."], "query": "What is the main purpose of compressing vectors in data processing?"}
{"relevant_passages": ["He developed an Autonomous Testing Agent to enhance software testing efficiency, harnessing the power of [SuperAGI](https://www.linkedin.com/company/superagi/) and Weaviate. Meanwhile, [BYTE](https://lablab.ai/event/cohere-coral-hackathon/byte/byte-ai-based-nutrition-app), an AI-based nutrition app, clinched the top spot at the [Coral Cohere Hackathon](https://lablab.ai/event/cohere-coral-hackathon)! Ayesha and Moneebah built this project to transform and personalize nutrition advice. They used Weaviate\u2019s vector database for search and recommendation and multi-tenancy for data security. These projects offer just a glimpse of the boundless possibilities within the AI realm, pointing the way to a future where AI is more accessible, formidable, and transformative. So, what are you waiting for if you haven't already started building with Weaviate?"], "query": "What AI-based nutrition app won the top spot at the Coral Cohere Hackathon?"}
{"relevant_passages": ["For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use."], "query": "Should I enable a specific feature when using Weaviate in production at scale?"}
{"relevant_passages": ["We no longer talk about thousands of vectors but **hundred of millions** or **even billions**! Keeping all the vectors in memory and adding a graph representation of vector connections **requires a lot of RAM**. This sparked the emergence of a new set of algorithms that allow vectors to reside on disk instead of in memory whilst retaining high performance. Some prominent examples of disk-based solutions include [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) and [SPANN](https://arxiv.org/abs/2111.08566). ## The future of Weaviate\nToday, users use Weaviate in production to serve large-scale use cases with single-digit millisecond latencies and massive throughput. But not every use case requires such a high throughput that the cost of keeping all indexes in memory is justified."], "query": "What are some examples of disk-based solutions for storing vectors that allow for high performance while not residing in memory?"}
{"relevant_passages": ["Here are the results:\n\n### Queries during maintenance - without replication\n\nThe below figure shows results from our setup with no replication. The area chart at the top shows how many requests failed, and the line graph shows pod readiness. We see immediately that over the course of the update time, just about one out of nine (11.5%) requests failed. ![Monitoring stats showing failures during restart](./img/queries_without_replication.png)\n\nAnd at times, it is even worse, with the failure rate being as high as one in three when the node is down and before it starts to load tenants. :::info\nThe failure rate here is less than one in three, as Weaviate is capable of loading tech tenant\u2019s data (i.e. shard) and making them available."], "query": "What was the failure rate of requests during maintenance without replication, and how does Weaviate perform with tech tenant's data in this scenario?"}
{"relevant_passages": ["<img\n  src={require('./img/knn-linear.png').default}\n  alt=\"kNN - O(n) complexity\"\n  style={{ maxWidth: \"50%\" }}\n/>\n\n*[Figure 2 - O(n) and O(log n) complexity]*\n\nIn summary, kNN search doesn't scale well, and it is hard to image using it with a large dataset in production. ## The answer - Approximate nearest neighbors (ANN)\nInstead of comparing vectors one by one, vector databases use [Approximate Nearest Neighbor (ANN) algorithms](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods), which trade off a bit of accuracy (hence the A in the name) for a huge gain in speed. ANN algorithms may not return the true k nearest vectors, but they are very efficient. ANN algorithms maintain good performance (sublinear time, e.g. (poly)logarithmic complexity, see Figure 2) on very large-scale datasets. *Note that most vector databases allow you to configure how your ANN algorithm should behave."], "query": "Why is the k-nearest neighbors algorithm not suitable for large datasets, and what is the alternative approach to address this issue?"}
{"relevant_passages": ["So if you are a Weaviate user, we encourage you to update Weaviate to the latest release to take advantage of this improvement as well as many others. Thank you for reading, and see you next time!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Why should Weaviate users update to the latest release?"}
{"relevant_passages": ["---\ntitle: How to run an embedded vector database in 10 lines of code\nslug: embedded-local-weaviate\nauthors: [dan]\ndate: 2023-06-06\nimage: ./img/hero.png\ntags: ['how-to']\ndescription: \"The Weaviate server can be run locally directly from client code\"\n\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\nimport FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';\nimport PyCode from '!!raw-loader!/_includes/code/embedded.py';\nimport TSCode from '!!raw-loader!/_includes/code/embedded.ts';\n\nYes, 10 Python lines of code, generously formatted with whitespace. Or 14 for TypeScript. Oh, and all your data stays private locally, and we don't charge you anything. We're also going to build a useful example, illustrating a testing scenario. Here's how."], "query": "How can I run a Weaviate vector database locally with just a few lines of Python or TypeScript code?"}
{"relevant_passages": ["What I love about all this work, like self-ask, chain of thought, we're developing new querying languages. This is like us inventing SQL, except that we didn't design the database. The database came into being and we have to figure out how to interact with it. That example I mentioned about the IPython interaction, like that's a, again, it's a new querying language. And I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part."], "query": "What does the document compare the development of new querying languages to?"}
{"relevant_passages": ["First, modify the configuration file to enable a multi-node setup (e.g. 3), and add the `replicationConfig` setting to the collection definition like so:\n\n```json\n{\n  class: YOUR_CLASS_NAME,\n  ... replicationConfig: {\n    factor: 3\n  }\n}\n```\n\n:::tip Replication factor & nodes\nThe replication factor should be less than or equal to the number of nodes. :::\n\nOnce you\u2019ve modified the configuration file and set the replication factor, you should have a multi-node setup. If you are keen to try running a multi-node setup yourself, follow the optional guide here. Or you can read ahead ;)."], "query": "How do I configure a multi-node setup with a replication factor of 3?"}
{"relevant_passages": ["![vector embeddings example](./img/vector-embeddings-example.png)\n\nHow can computers mimic our understanding of language, and similarities of words or paragraphs? To tackle this problem, semantic search uses at its core a data structure called **vector embedding** (or simply, **vector** or **embedding**), which is an array of numbers. Here's how the semantic search above works, step by step:\n\n1. The [vector database](/blog/vector-library-vs-vector-database) computes a vector embedding for each data object as it is inserted or updated into the database, using a given model. 2."], "query": "What is vector embedding and how is it used in semantic search?"}
{"relevant_passages": ["### The search part\nIn a similar fashion, whenever we run a query (like: \"What is the tallest building in Berlin?\"), a vector database can also convert it to a \"query\" vector. The task of a vector database is to identify and retrieve a list of vectors that are closest to the given vector of your query, using a distance metric and a search algorithm. This is a bit like a game of boules \u2013 where the small marker (jack) is the location of our query vector, and the balls (boules) are our data vectors \u2013 and we need to find the boules that are nearest to the marker. ## k-nearest neighbors (kNN)\nOne way to find similar vectors is with a simple [k-nearest neighbors (kNN) algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm), which returns the k nearest vectors, by comparing every data vector in the database to the query vector. In our boules example (as illustraded below), with 6 boules, the kNN algorithm would measure the distance between the jack and each of the 6 boules on the ground."], "query": "How does a vector database find the closest data vectors to a given query vector?"}
{"relevant_passages": ["We aim to do the latter in this post. Once we have the data compressed, we still need to be able to calculate distances. This can be accomplished in two ways: Either we compress vectors from the original space to a compressed space to store them and we decompress the vectors back to the original space when calculating the distance, or we define the distance function directly over the compressed space. Figures 1 and 2 graphically demonstrate the first and second options respectively. Notice the delta($\\delta$) term in the explanation."], "query": "What are the two methods for calculating distances with compressed data as demonstrated in Figures 1 and 2, and what does the delta ($\\delta$) term refer to in this context?"}
{"relevant_passages": ["Jay Alamar provided a helpful [visualization](https://jalammar.github.io/illustrated-word2vec/) around this equation. Several concepts (\u201cwoman\u201d, \u201cgirl\u201d, \u201cboy\u201d etc.) are vectorized into (represented by) an array of 50 numbers generated using the [GloVe model](https://en.wikipedia.org/wiki/GloVe). In [vector terminology](https://en.wikipedia.org/wiki/Vector_(mathematics)), the 50 numbers are called dimensions. The vectors are visualized using colors and arranged next to each word:\n\n![vector embeddings visualization](./img/vector-embeddings-visualization.png)\n*Credit: Jay Alamar*\n\nWe can see that all words share a dark blue column in one of the dimensions (though we can\u2019t quite tell what that represents), and the word \u201cwater\u201d _looks_ quite different from the rest, which makes sense given that the rest are people. Also, \u201cgirl\u201d and \u201cboy\u201d look more similar to each other than to \u201cking\u201d and \u201cqueen\u201d respectively, while \u201cking\u201d and \u201cqueen\u201d look similar to each other as well."], "query": "What is an example of a visualization that explains how words like \"woman\", \"girl\", \"boy\", \"king\", \"queen\", and \"water\" are represented in vector space using the GloVe model?"}
{"relevant_passages": ["Embedding providers (e.g., Hugging Face or OpenAI)\n1. Neural framework (e.g., deepset or Jina)\n1. Feature stores (e.g., FeatureBase, FeatureForm or Tecton)\n1. Vector databases (e.g., Weaviate or Vertex)\n\nWhile the number of data that companies are collecting in their data warehouses keeps growing, the need for better, more efficient searches keeps growing too. The more data we collect, the more complex searching through it becomes."], "query": "What technologies are used to improve search capabilities in large data warehouses?"}
{"relevant_passages": ["---\ntitle: What to expect from Weaviate in 2023\nslug: what-to-expect-from-weaviate-in-2023\nauthors: [etienne]\ndate: 2023-02-14\ntags: ['engineering']\nimage: ./img/hero.png\ndescription: \"Learn about the six pillars outlining how Weaviate will evolve in 2023.\"\n---\n![What to expect from Weaviate in 2023](./img/hero.png)\n\nWithout a doubt, 2022 has been the most exciting year for Weaviate so far. The company and the product have grown tremendously, and we are incredibly excited about 2023. Weaviate\u2019s usage numbers are through the roof, and so are your feedback and requests for what you\u2019re still missing from Weaviate. <!-- truncate -->\n\nIn this blog post, I will introduce you to the six pillars outlining how Weaviate will get even better in 2023. Weaviate development is highly dynamic \u2013 we don\u2019t waterfall-plan for the entire year \u2013 but nevertheless, we want to give you the best possible overview of what to expect in the coming year."], "query": "What are the six pillars guiding the evolution of Weaviate in 2023?"}
{"relevant_passages": ["On the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:\n\n```yaml\nDEFAULT_VECTORIZER_MODULE: text2vec-huggingface\nENABLE_MODULES: text2vec-huggingface\n```\n\n## How to get started\n\n:::note\nThis article is not meant as a hands-on tutorial. For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). :::\n\nThe overall process to use a Hugging Face module with Weaviate is fairly straightforward. ![Recipe for using the Hugging Face module](./img/how-to-get-started-recipe.png)\nIf this was a cooking class and you were following a recipe."], "query": "How do you set the default vectorizer module to Hugging Face in Weaviate v1.15 or newer?"}
{"relevant_passages": ["## How are vector embeddings generated? The magic of vector search resides primarily in how the embeddings are generated for each entity and the query, and secondarily in how to efficiently search within very large datasets (see our [\u201cWhy is Vector Search so Fast\u201d](/blog/why-is-vector-search-so-fast) article for the latter). As we mentioned, vector embeddings can be generated for various media types such as text, images, audio and others. For text, vectorization techniques have evolved tremendously over the last decade, from the venerable [word2vec](https://en.wikipedia.org/wiki/Word2vec) ([2013](https://code.google.com/archive/p/word2vec/)), to the state-of-the-art transformer models era, spurred by the release of [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) in [2018](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). ### Word-level dense vector models (word2vec, GloVe, etc.)\n[word2vec](https://wiki.pathmind.com/word2vec) is a [family of model architectures](https://www.tensorflow.org/tutorials/text/word2vec) that introduced the idea of \u201cdense\u201d vectors in language processing, in which all values are non-zero."], "query": "What are some significant milestones in the development of text vectorization techniques, and which model introduced the concept of dense vectors in language processing?"}
{"relevant_passages": ["The embeddings are placed into an index, so that the database can [quickly](/blog/why-is-vector-search-so-fast) perform searches. 3. For each query,\n    1. a vector embedding is computed using the same model that was used for the data objects. 2."], "query": "How does a database perform searches using vector embeddings?"}
{"relevant_passages": ["We could use regular HNSW to start building our index. Once we have added some vectors (a fifth of the total amount, for example) we could then compress the existing vectors and from this point on, we compress the vectors once they come into the index. The separation between loading data uncompressed, then compressing afterwards is necessary. The threshold of how much data needs to be loaded in prior to compressing (a fifth of the data) is not a rule though. The decision on when to compress should be taken keeping in mind that we need to send enough data for the Product Quantizer to infer the distribution of the data before actually compressing the vectors."], "query": "At what point in the index-building process using HNSW should vector compression begin, and is this threshold strict?"}
{"relevant_passages": ["And for more cost-sensitive applications, even 2 would introduce high availability and robustness to the system. As we've mentioned before - using Weaviate Cloud Services is a convenient way to set up a cluster with replication enabled. Set the `Enable High Availability` button to `Yes` at cluster creation time, and it will spin up a multi-node cluster with replication enabled, including the appropriate replication factor for each class. So there it is. We hope we\u2019ve convinced you of the benefits of using replication, and how easy it is to use."], "query": "How do you enable high availability for a Weaviate Cloud Services cluster during its creation?"}
{"relevant_passages": ["<br></br>\n\n### Cloud Operations & Scaling\n![cloud operations scaling](./img/cloud-operations-scaling.png)\n\nWhen we introduced [Replication](/developers/weaviate/concepts/replication-architecture) to Weaviate in late 2022, we celebrated a significant milestone. It\u2019s never been easier to achieve a highly available setup, and you can even dynamically scale your setup to increase throughput. 2023 is all about improving your cloud and operations experience. We will give you more control over [how to structure your workloads](https://github.com/weaviate/weaviate/issues/2586) in a distributed setup and more [flexibility to adapt to your ever-changing needs](https://github.com/weaviate/weaviate/issues/2228). And, of course, we\u2019re constantly working on making your distributed cluster [even more resilient](https://github.com/weaviate/weaviate/issues/2405)."], "query": "What feature was introduced to Weaviate in late 2022 to enhance scalability and availability?"}
{"relevant_passages": [":::\n\n## Enabling replication in Weaviate\n\nJust the simple act of enabling replication on a Weaviate cluster will provide huge benefits. Doing so might actually be simpler than you might imagine. ### How to enable replication on Weaviate Cloud Services (WCS)\n\nEnabling replication on a Weaviate Cloud Services cluster is as simple as selecting the `Enable High Availability` button at cluster creation time. (Not available on sandboxes.)\n\n\nThis will enable a multi-node configuration in Weaviate and ensures that each class is configured with the appropriate replication factor. ### How to enable replication on self-deployed Weaviate\n\nEnabling replication in a self-deployment setup such as a Docker or Kubernetes setup involves the following two steps."], "query": "How do you enable replication in a Weaviate Cloud Services cluster?"}
{"relevant_passages": ["- Topics varied from RAG and multimodal search to scaling AI applications from prototype into production. - We partnered with the [MLOps Community](https://mlops.community/), [AICamp](https://www.aicamp.ai/), and other companies across the AI ecosystem. The Weaviate World Tour will be continuing in 2024! \u2728 Check our [Weaviate Events & Webinars Page](https://weaviate.io/community/events) for upcoming conferences, meetups, webinars, and workshops. Subscribe to the [Weaviate Newsletter](https://newsletter.weaviate.io/) and contact us if you're interested in speaking or hosting. ![worldtour](img/Group_2597.png)\n\n### Weaviate Hero Program\n\nAs 2023 wraps up, we're thrilled to celebrate our amazing Weaviate Community! Introducing the *Weaviate Hero Program* to honor those who embody our values and actively support others with their knowledge and expertise."], "query": "What program was introduced to honor active supporters in the Weaviate Community as 2023 came to an end?"}
{"relevant_passages": ["\ud83e\udd17We hope to see many more big and small contributions in the coming months and years. **#CommunityRocks**\n\n## Cloud-native backups\n\n![Cloud-native backups](./img/cloud-native-backup.png)\n\nCreating and restoring database backups in Weaviate was one of the most requested features from the Weaviate community and customers. And, of course, database backups are not just about disaster recovery. Sometimes we need to migrate our data to a different environment. Maybe because our database grew and now we need more resources, or perhaps we need to set up a new developer environment."], "query": "What was one of the most requested features from the Weaviate community and customers related to database management?"}
{"relevant_passages": ["And it's a great question. I hadn't thought of it that way. I don't know where it is in the data and it may simply be that the model is mashing up all this other data that it has in an interesting way, the same way that you can get stable diffusion to give you like people sitting around a campfire on an airplane, I think I saw go by, or like salmon swimming in a stream where it was literal pieces of salmon. That was never anywhere in the training set, but it managed to mash it up and figure out what to do with it. And I wonder if something similar is happening here."], "query": "What can generative AI models do with data to create novel outputs?"}
{"relevant_passages": ["Our new schema is below - note the commented lines:\n\n```python\nquestion_class = {\n    \"class\": \"Question\",\n    \"description\": \"Details of a Jeopardy! question\",\n    \"moduleConfig\": {\n        \"text2vec-cohere\": {  # The vectorizer name - must match the vectorizer used\n            \"vectorizeClassName\": False,  # Ignore class name\n        },\n    },\n    \"properties\": [\n        {\n            \"name\": \"answer\",\n            \"description\": \"What the host prompts the contestants with.\",\n            \"dataType\": [\"string\"],\n            \"moduleConfig\": {\n                \"text2vec-cohere\": {\n                    \"skip\": False,  # Do not skip class\n                    \"vectorizePropertyName\": False  # Ignore property name\n                }\n            }\n        },\n        {\n            \"name\": \"question\",\n            \"description\": \"What the contestant is to provide.\",\n            \"dataType\": [\"string\"],\n            \"moduleConfig\": {\n                \"text2vec-cohere\": {\n                    \"skip\": False,  # Do not skip class\n                    \"vectorizePropertyName\": True  # Do not ignore property name\n                }\n            }\n        },\n    ]\n}\nclient.schema.create_class(question_class)\n```\n\nThe schema is defined such that at least some of the options, such as `moduleConfig`/`text2vec-cohere` /`vectorizeClassName` and `properties`/`moduleConfig`/`text2vec-cohere`/`vectorizePropertyName` differ from their defaults. And as a result, a `nearVector` search with the previously-matching Cohere API vector is now at a distance of `0.00395`. To get this back down to zero, we must revise the text generation pipeline to match the schema. Once we've done that, which looks like this:\n\n```python\nstr_in = ''\nfor k in sorted(input_props.keys()):\n    v = input_props[k]\n    if type(v) == str:\n        if k == 'question':\n            str_in += k + ' '\n        str_in += v + ' '\nstr_in = str_in.lower().strip()\n```\n\nSearching with the vector generated from this input, the closest matching object in Weaviate once again has a distance of zero. We've come full circle \ud83d\ude42."], "query": "What changes were made to the `text2vec-cohere` vectorization settings in the new \"Question\" class schema, and how was the text generation pipeline adjusted to fix the `nearVector` search distance issue?"}
{"relevant_passages": ["![workshops](img/workshops_3.png)\n\n## Building a Global Community\n\nThe heart of Weaviate lies in its **community**. Our strength comes from the combined efforts of our team and users, whether they're engaging in the [Community Slack](https://weaviate.slack.com/), participating in meetups, or using Weaviate in their projects. This year, we introduced the **Weaviate Hero Program**, an initiative by [Marion](https://www.linkedin.com/in/marionnehring/), our Community Manager, to honor those significantly contributing to our community's growth and success. ### Meetups and Events\n\n**Weaviate World Tour - End of Year Edition:** To ensure everyone can benefit from the knowledge, we decided to share our knowledge and connect, collaborate, and network with community members around the globe. We introduced our Weaviate World Tour: Year-End Special Edition! bringing tech experts to community events in Amsterdam, Berlin, London, San Francisco, and New York to hundreds of developers, data scientists, and AI enthusiasts."], "query": "What is the name of the program introduced by Weaviate to honor significant contributors to its community?"}
{"relevant_passages": ["For one, what does each number represent? That depends on the machine learning model that generated the vectors, and isn\u2019t necessarily clear, at least in terms of our human conception of language and meaning. But we can sometimes gain a rough idea by correlating vectors to words with which we are familiar. Vector-based representation of meaning caused quite a [stir](https://www.ed.ac.uk/informatics/news-events/stories/2019/king-man-woman-queen-the-hidden-algebraic-struct) a few years back, with the revelation of mathematical operations between words. Perhaps *the* most famous result was that of\n\n    \u201cking \u2212 man + woman \u2248 queen\u201d\n\nIt indicated that the difference between \u201cking\u201d and \u201cman\u201d was some sort of \u201croyalty\u201d, which was analogously and mathematically applicable to \u201cqueen\u201d minus \u201cwoman\u201d."], "query": "What is the famous result demonstrating the algebraic structure of word vectors in machine learning?"}
{"relevant_passages": ["One of the primary reasons is the computational cost associated with processing and storing longer sequences of tokens. The longer the sequence, the more memory and processing power required to operate on it, which can be a significant challenge for even the most powerful computing systems. The relatively long input window of LLMs is what drives the integration with semantic search. For example, we can use this entire blog post as input in addition to questions if we want the LLM to answer questions such as \u201cWhat are LLM Chains?\u201d However, when we want to give the LLM an entire book or pages of search results, we need more clever techniques to decompose this task. This is where the `CombineDocuments` chain comes to play! Note, that one method is not better than another, and the performance is dependent solely on your application."], "query": "Why might the `CombineDocuments` chain be used in the context of large language models (LLMs)?"}
{"relevant_passages": ["* Making the copy function from Go to move full memory sections. ![Recall vs. Latency when retrieving the ten approximate nearest neighbors](./img/fig-3.png)\n*Fig. 3: Recall vs. Latency when retrieving the ten approximate nearest neighbors.*\n\n![Recall vs."], "query": "What is the relationship between recall and latency in the context of retrieving the ten approximate nearest neighbors according to Fig. 3?"}
{"relevant_passages": ["Due to the large dataset size mentioned earlier, there can be millions or even billions of objects with unique UUIDs in Weaviate, and creating a lock for each of them would require a lot of memory. We found an elegant solution that is in-between both of the solutions above - a **lock striping** pattern. ## Solving both challenges\nBased on the UUID we assign each object to one of the 128 locks. This process is deterministic so objects with an identical UUID will always use the same lock. This gives us the best of both worlds: we have a small, fixed number of locks, but it still guarantees that two objects with the same UUID are never processed concurrently."], "query": "What lock management strategy does Weaviate use to handle millions of objects with unique UUIDs efficiently?"}
{"relevant_passages": ["- How much do we save in memory requirements? ### Performance Results\n\nIn the following figures we show the performance of HNSW+PQ on the three databases used above. Notice how compressing with KMeans keeps the recall closer to the uncompressed results. Compressing too aggressively (KMeans with a few dimensions per segment) improves memory, indexing and latency performance but it rapidly destroys the recall so we recommend using it discreetly. Notice also that KMeans encoding with as many segments as dimensions ensures a 4 to 1 compression ratio."], "query": "What is the compression ratio when using KMeans encoding with as many segments as dimensions?"}
{"relevant_passages": ["This will give us two important results, the time it would take to fit the data with KMeans clustering and compress the vectors (which will be needed at some step by the indexing algorithm) and the distortion introduced by reconstructing the compressed vectors to calculate the distance. This distortion is measured in terms of a drop in recall. We considered three datasets for this study: Sift1M, Gist1M and DeepImage 96. Table 1, shown above, summarizes these datasets. To avoid overfitting, and because this is how we intend to use it in combination with the indexing algorithm, we use only 200,000 vectors to fit KMeans and the complete data to calculate the recall."], "query": "What were the results of the study on the time and distortion of KMeans clustering with Sift1M, Gist1M, and DeepImage 96 datasets?"}
{"relevant_passages": ["This is bad for three reasons:\n* Grabbing a new plate for each cake and then washing it takes time - **higher CPU use** with many GC cycles. * We would pile up many plates between each wash - **high memory consumption** between each GC cycle. * We might run out of clean plates - **OOM crash** if we **run out of RAM**. ### The solution\nTo solve the problem, we implemented two solutions:\n* We created our own library for reading binary data optimized for the Weaviate-specific needs. It makes fewer temporary memory allocations."], "query": "What problems does the custom library for reading binary data solve in the context of Weaviate?"}
{"relevant_passages": ["That \"wisdom of the crowd\" approach worked, but to improve the quality of the results it returned, Google needed RankBrain to \"understand\" the text it searched through. So, it used machine learning to vectorize (the process happening inside machine learning models such as transformers) text on sites and in links. Returning to the grocery store for a moment, the challenge comes because a grocery store is a three-dimensional space, but every significant word in unstructured text data needs to be related to hundreds of other words that it is frequently associated with. So, machine learning systems automatically classify text in hypervolumes\u2014imaginary spaces with hundreds or even thousands of dimensions. For any given item in a database, those vectors form what's known as a \"representation\" of the item."], "query": "How does Google's RankBrain use machine learning to understand and represent text?"}
{"relevant_passages": ["So we can *see* that these vector embeddings of words align with our intuitive understanding of meaning. And even more amazingly, vector embeddings are not limited to representing meanings of words. In fact, effective vector embeddings can be generated from any kind of data object. Text is the most common, followed by images, then audio (this is how Shazam recognizes songs based on a short and even noisy audio clip), but also time series data, 3D models, video, molecules etc. Embeddings are generated such that two objects with similar semantics will have vectors that are \"close\" to each other, i.e. that have a \"small\" distance between them in [vector space](https://en.wikipedia.org/wiki/Vector_space_model)."], "query": "What are vector embeddings and what types of data can they be applied to?"}
{"relevant_passages": ["It takes one input/output and then uses the output for the next step. Let\u2019s look at an example of sequential chains in a conversation between the bot and me. This visualization shows an example of a step-by-step reasoning to fact check an LLM tasked with question answering:\n\n<img\n    src={require('./img/sequential-chains.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\n<div style = {{textAlign: \"center\"}}>\n\n*All credit to [jagilley](https://github.com/jagilley/fact-checker) for creating this awesome example*\n\n</div>\n\nWhen we ask an LLM, \u201cWhat type of mammal lays the biggest eggs?\u201d, it initially answers \u201cThe biggest eggs laid by any mammal belong to the elephant.\u201d This is a clear example of hallucination, where the response is misleading and incorrect. By adding in a simple chain of asking the LLM to reason about its assumptions, we are able to fix the hallucination problem. ## CombineDocuments\nLLMs have a limited token length due to various practical and technical constraints."], "query": "How can sequential chains be used to correct hallucinations in language model responses?"}
{"relevant_passages": ["\ud83d\ude01\n:::\n\n## Videos\n\n### Introduction to authentication in Weaviate\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/5xB5cRUFe2M' controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:13 What is authentication? & Key considerations\n- 0:58 Overview of available authentication options in Weaviate (anonymous / API key authentication / OIDC authentication)\n- 1:54 General Recommendations\n\n### Authentication: A client-side perspective\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/I3o93Co1Ezg' width=\"100%\" controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:28 Getting authentication information from WCS\n- 2:10 Authenticating against Weaviate\n- 2:28 Anonymous access\n- 3:01 API key authentication\n- 3:45 OIDC (username+password) authentication\n- 4:21 Read-only key\n- 4:38 Authentication in action\n- 5:36 Wrap-up\n\n### Authentication: Key concepts\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/Ok9AcWK0R38' width=\"100%\" controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:31 Anonymous access\n- 0:46 Authentication\n- 0:58 API key authentication\n- 1:04 OIDC authentication\n- 1:36 Authentication & Authorization\n- 1:52 A comparison of options\n- 2:09 Additional complexities in OIDC\n- 2:54 summary\n\n### Authentication: A server-side perspective\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/0oxL1J0W-Hs' width=\"100%\" controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:35 Weaviate without authentication\n- 1:39 Setting up API key access\n- 2:32 Enabling authorization (tiered access)\n- 3:46 Setting up OIDC access\n- 5:30 Enabling authorization with OIDC\n- 5:54 Summary\n- 6:02 Relevant environment variables\n\n## Read more:\n\n- [How to configure authentication](/developers/weaviate/configuration/authentication)\n- [How to configure authorization](/developers/weaviate/configuration/authorization)\n- [References: Environment variables](/developers/weaviate/config-refs/env-vars)\n- [Weaviate clients](/developers/weaviate/client-libraries/)\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What are the available authentication options in Weaviate as discussed in the \"Introduction to authentication in Weaviate\" video?"}
{"relevant_passages": ["Fortunately, there are emerging technologies that help solve this limitation. <!-- truncate -->\n\n[LangChain](https://langchain.readthedocs.io/en/latest/) is one of the most exciting new tools in AI. LangChain helps overcome many limitations of LLMs such as hallucination and limited input lengths. Hallucination refers to where the LLM generates a response that is not supported by the input or context \u2013 meaning it will output text that is irrelevant, inconsistent, or misleading. As you can imagine, this is a huge problem in many applications."], "query": "What is LangChain, and what limitations of LLMs does it help to overcome?"}
{"relevant_passages": ["HNSW, on the other hand, implements the same idea a bit differently. Instead of having all information together on a flat graph, it has a hierarchical representation distributed across multiple layers. The top layers only contain long-range connections, and as you dive deeper into the layers, your query is routed to the appropriate region where you can look more locally for your answer. So your search starts making only big jumps across the top layers until it finally looks for the closest points locally in the bottom layers. ## Performance comparison\nSo, how do they perform?"], "query": "How does the HNSW algorithm structure its graph and conduct searches differently from a flat graph approach?"}
{"relevant_passages": ["Latency when retrieving the hundred approximate nearest neighbors](./img/fig-4.png)\n*Fig. 4: Recall vs. Latency when retrieving the hundred approximate nearest neighbors.*\n\n## So, when do SSDs come into play? But wait, is this the end of it? Was moving to disk, not the final goal?"], "query": "What is the relationship between recall and latency in retrieving the hundred approximate nearest neighbors using SSDs?"}
{"relevant_passages": ["LLMs have a limited input length when referring to the scale of inputting a book or pages of search results. LangChain has various techniques implemented to solve this problem. This blog post will begin by explaining some of the key concepts introduced in LangChain and end with a demo. The demo will show you how to combine LangChain and Weaviate to build a custom LLM chatbot powered with semantic search!\n\n## Sequential Chains\n[Chains](https://python.langchain.com/docs/modules/chains/) enable us to combine multiple LLM inferences together. As you can guess from the name, sequential chains execute their links in a sequential order."], "query": "What is the purpose of Sequential Chains in LangChain?"}
{"relevant_passages": ["Then, you just run the queries, as per usual:\n```javascript\nnearText = {\n    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\n    \"distance\": 0.6,\n}\n\nresult = (\n    client.query\n    .get(\"Notes\", [\n        \"name\",\n        \"comment\",\n        \"_additional {certainty distance} \"])\n    .with_near_text(nearText)\n    .do()\n)\n```\n\n## Summary\n> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\nCheck out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module."], "query": "How can I use Hugging Face modules to run queries in Weaviate without managing the Inference API myself?"}
{"relevant_passages": ["Add any ordered input, for example, 1, 2, 3, 4 and see how the tree stays balanced. ### Results\nWe've run a few local tests to paint a better picture of what you could expect. First, we saw that the RB-Tree is a factor of 20 faster than the binary tree when adding objects with sequential keys (just the tree, without anything else). With a full import test, we saw a **3x performance improvement** \ud83d\ude80. * Weaviate `1.14.1` - import time **~38 minutes**\n* Weaviate `1.15.0` - import time **~13 minutes** \ud83d\udd25\n\n## More efficient filtered aggregations\n\n![More efficient filtered aggregations](./img/filtered-aggregation.png)\n\nRecently we've been working with a customer who was running multiple filtered aggregations on a large dataset."], "query": "How much did the import time improve from Weaviate version 1.14.1 to 1.15.0?"}
{"relevant_passages": ["Then we take this new query and hit the Weaviate vector database to get context to answer the question. The ChatVectorDB chain we use has a default value of k = 4 search results, if we use longer search results we will need another CombineDocuments chain here as well! With the 4 search results, we answer the question with this final prompt:\n\n```python\nPrompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. {context}\nQuestion: {question}\nHelpful Answer:\"\"\"\n```\n\nHopefully this was a nice look under the hood of how the ChatVectorDB chain works. Let\u2019s get into how we can use this with Weaviate!\n\n### The Code\n\nIf this is your first time using Weaviate, please check out the [Quickstart tutorial](/developers/weaviate/quickstart)."], "query": "How does the ChatVectorDB chain in Weaviate use context to answer questions?"}
{"relevant_passages": ["5**: *Average time (microseconds) to calculate distances from query vectors to all 1,000,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 8 dimensions per segment. The points in the curve are obtained varying the amount of centroids. The \u201cbase\u201d curve refers to the fixed average time to calculate distances between uncompressed vectors.*\n\nNotice some important findings from the experiments. Latency could vary significantly depending on the settings we use."], "query": "What is the relationship between segment length and recall in the context of calculating distances from query vectors to a dataset of 1,000,000 vectors?"}
{"relevant_passages": ["This demo is built off of Connor Shorten\u2019s [Podcast Search](https://github.com/weaviate/weaviate-podcast-search) demo. We are connecting to our Weaviate instance and specifying what we want LangChain to see in the `vectorstore`. `PodClip` is our class and we want to use the `content` property, which contains the transcriptions of the podcasts. Next in `qa` we will specify the OpenAI model. ```python\nfrom langchain.vectorstores.weaviate import Weaviate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ChatVectorDBChain\nimport weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\nvectorstore = Weaviate(client, \"PodClip\", \"content\")\n\nMyOpenAI = OpenAI(temperature=0.2,\n    openai_api_key=\"sk-key\")\n\nqa = ChatVectorDBChain.from_llm(MyOpenAI, vectorstore)\n\nchat_history = []\n\nprint(\"Welcome to the Weaviate ChatVectorDBChain Demo!\")\nprint(\"Please enter a question or dialogue to get started!\")\n\nwhile True:\n    query = input(\"\")\n    result = qa({\"question\": query, \"chat_history\": chat_history})\n    print(result[\"answer\"])\n    chat_history = [(query, result[\"answer\"])]\n```\nAnd just like that, you have built an application using LangChain and Weaviate."], "query": "How do you create a chatbot using LangChain and Weaviate that answers questions based on podcast transcriptions?"}
{"relevant_passages": ["![Conceptual diagram of sending a request with authentication credentials](./img/auth_light.png#gh-light-mode-only)\n![Conceptual diagram of sending a request with authentication credentials](./img/auth_dark.png#gh-dark-mode-only)\n\nIn other words, the server can provide as much access as the particular user is allowed. But balancing security with usability can be a tricky line to draw, as everybody has different needs and often use different systems. So, we thought that this might be a good time to provide an overview of all things authentication in Weaviate. Also, we've recently introduced an API key-based authentication method, which we think might be a good balance of security and usability for many of you. Please check them out below."], "query": "What new authentication method has Weaviate recently introduced to balance security and usability?"}
{"relevant_passages": ["The points in the curve are obtained varying the amount of centroids. The \u201cbase\u201d curve refers to the fixed average time to calculate distances between uncompressed vectors.*\n\nAs we should expect, Product Quantization is very useful for saving memory. It comes at a cost though. The most expensive part is fitting the KMeans clustering algorithm. To remedy this we could use a different encoder based on the distribution of the data, however that is a topic we shall save for later!\n\nA final word on performance, not all applications require a high recall."], "query": "What is the trade-off of using Product Quantization for saving memory in terms of computational cost, and how does it affect recall in applications?"}
{"relevant_passages": ["However, this can be done by aggregating vectors of constituent words, which is often done by incorporating weightings such that certain words are weighted more heavily than others. However, word2vec still suffers from important limitations:\n* it doesn\u2019t address words with multiple meanings (polysemantic): \u201crun\u201d, \u201cset\u201d, \u201cgo\u201d, or \u201ctake\u201d each have [over 300 meanings](https://www.insider.com/words-with-the-most-definitions-2019-1) (!)\n* it doesn\u2019t address words with ambiguous meanings: \u201cto consult\u201d can be its own antonym, like [many other words](http://www.dailywritingtips.com/75-contronyms-words-with-contradictory-meanings/)\n\nWhich takes us to the next, state-of-the-art, models. ### Transformer models (BERT, ELMo, and others)\nThe current state-of-the-art models are based on what\u2019s called a \u201ctransformer\u201d architecture as introduced in [this paper](https://arxiv.org/abs/1706.03762). [Transformer models](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) such as BERT and its successors improve search accuracy, [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) by looking at every word\u2019s context to create full contextual embeddings (though [the exact mechanism of BERT\u2019s success is not fully understood](https://aclanthology.org/D19-1445/)). Unlike word2vec embeddings which are context-agnostic, transformer-generated embeddings take the entire input text into account\u2014each occurrence of a word has its own embedding that is modified by the surrounding text."], "query": "What are the limitations of word2vec in handling words with multiple meanings, and how do transformer models like BERT address this issue?"}
{"relevant_passages": ["Similarly, we want to give you more flexibility during ingestion time: how about [extracting PDFs](https://github.com/weaviate/weaviate/issues/2509) or applying [stemming](https://github.com/weaviate/weaviate/issues/2439) to your BM25 and hybrid search? <br></br>\n\n### Beyond Billion Scale: Large-Scale Performance\n![Billion Scale](./img/billion-scale.png)\n\nIn 2022, we published the [Sphere Demo Dataset for Weaviate](/blog/sphere-dataset-in-weaviate). This marked the first time (to our knowledge) that more than a billion objects and vectors were imported into Weaviate. Dealing with ever-growing datasets is not only about being able to handle their size. Our users run complex queries in production and often have strict latency requirements."], "query": "What are the new ingestion time flexibility features for Weaviate, and what was the significance of the Sphere Demo Dataset published in 2022?"}
{"relevant_passages": ["The compression is carried out by using predefined centers, which we will explain shortly. If we aim to compress each segment down to 8 bits (one byte) of memory, we might have 256 (total combinations with 8 bits) predefined centers per segment. When compressing a vector we would go segment-by-segment assigning a byte representing the index of the predefined center. The segmentation and compression process is demonstrated in Figure 3 below. ![pq](./img/image3.jpg)\n**Fig."], "query": "How many predefined centers are used per segment in the 8-bit compression method described?"}
{"relevant_passages": ["*import-ance* (sorry) \ud83e\udd41. Weaviate offers and strongly recommends the [batch import feature](/developers/weaviate/tutorials/import#import-setup) for adding data objects in bulk. To further speed up the import process, you can use parallelization, which lets you run multiple batches concurrently. Each object in these batches is then checked for duplicates and assigned a unique internal DocID used by Weaviate to access objects. We uncovered that there could be a race condition in this process."], "query": "What is the recommended method for bulk data import in Weaviate, and how does it handle object uniqueness and potential race conditions?"}
{"relevant_passages": ["### The elephant in the room\nRunning model inference in production is hard. * It requires expensive specialized hardware. * You need a lot more computing power during the initial data import. * Hardware tends to be underutilized once the bulk of the heavy work is done. * Sharing and prioritizing resources with other teams is hard."], "query": "What are the challenges of running model inference in production?"}
{"relevant_passages": ["However, we did not explain the underlying motivation of moving vectors to disk. In this post we explore:\n\n- What kind of information we need to move to disk\n- The challenges of moving data to disk\n- What the implications of moving information to disk are\n- Introduce and test a full first solution offered by the new HNSW+PQ feature in Weaviate v1.18\n\n## What Information to Move to disk\n\nWhen indexing, there exist two big chunks of information that utilize massive amounts of memory: vectors and neighborhood graphs. Weaviate currently supports vectors of `float32`. This means we need to allocate 4 bytes, or 32 bits, per stored vector dimension. A database like Sift1M contains 1 million vectors of 128 dimensions each."], "query": "What is the size in bytes of each vector dimension stored in Weaviate v1.18, and how many vectors of what dimension does the Sift1M database contain?"}
{"relevant_passages": ["I am proud of you all and highly excited about the future. Thank you all, and let\u2019s make 2023 the most exciting year for Weaviate users so far!\n\n<br></br>\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Who is the speaker expressing pride and excitement for in the year 2023, and what code component is being imported in the message?"}
{"relevant_passages": ["To end this article, let\u2019s discuss a little further why ranking is so exciting for the most hyped pairing of LLMs and Search: Retrieval-Augmented Generation. ## Ranking for Retrieval-Augmented Generation\nA lot of the recent successes of vector search can be attributed to their effectiveness as a tool for Large Language Models. So whereas the speed trade-off with rankers may be a major bottleneck for how humans use search, it might not be as much of a problem for how LLMs use search. Of course fast generation is preferred, but if you are paying for the result, quality may be more important than speed. Shi et al."], "query": "Why is ranking considered exciting in the context of Retrieval-Augmented Generation for Large Language Models?"}
{"relevant_passages": ["The compression rate will depend on the amount of centroids. If we use 256 centroids we only need one byte per segment. Additionally, using an amount in the range 257 to 65536 will require two bytes per segment. ### Sift\n\nFirst, we show results on Sift1M. We vary the amount of centroids starting with 256 and increase the number until it performs poorly compared to the next curve."], "query": "How many bytes per segment are needed when using 256 centroids for compression on the Sift1M dataset?"}
{"relevant_passages": ["---\ntitle: Vamana vs. HNSW - Exploring ANN algorithms Part 1\nslug: ann-algorithms-vamana-vs-hnsw\nauthors: [abdel]\ndate: 2022-10-11\ntags: ['research']\nimage: ./img/hero.png\ndescription: \"Vector search on disks: How does Vamana compare to HNSW?\"\n---\n![Vamana vs HNSW - Exploring ANN algorithms Part 1](./img/hero.png)\n\n<!-- truncate -->\n\nVector databases must be able to search through a vast number of vectors at speed. This is a huge technical challenge that is only becoming more difficult over time as the vector dimensions and dataset sizes increase. Like many others, our current prevailing solution is to use Approximate Nearest Neighbor (ANN) algorithms to power Weaviate. But the key question is - which ones to use?"], "query": "What are the differences between Vamana and HNSW algorithms in vector search on disks?"}
{"relevant_passages": ["We found that the data structures relied on dynamic allocations. So, even if we knew that an array would never be longer than 64 elements, the Go runtime could still decide to allocate an array[100] in the background when the array reaches 51 elements. To fix that, we switched to static allocations, and Weaviate instructs the Go runtime to allocate the exact number of elements. This reduced **static** memory usage even when idle. ### Results\n\n\ud83c\udf89 Between these two major updates, plus some smaller ones, we saw a **significant reduction in memory usage of 10-30%**\ud83d\ude80."], "query": "What was the impact on memory usage after switching from dynamic to static allocations in Go data structures according to the document?"}
{"relevant_passages": ["If we compress too soon, when too little data is present in uncompressed form, the centroids will be underfit and won't capture the underlying data distribution. Alternatively, compressing too late will take up unnecessary ammounts of memory prior to compression. Keep in mind that the uncompressed vectors will require more memory so if we send the entire dataset and compress only at the end we will need to host all of these vectors in memory at some point prior to compressing after which we can free that memory. Depending on the size of your vectors the compressing time could be optimally calculated but this is not a big issue either. Figure 10 shows a profile of the memory used while loading Sift1M."], "query": "What are the trade-offs of compressing data too early or too late in terms of memory usage and centroid quality?"}
{"relevant_passages": ["on Article {\n          title\n        }\n      }\n    }\n  }\n}\n```\n\n\ud83d\udca1 LIVE \u2014 [try out this query](https://console.weaviate.io/console/query#weaviate_uri=http://semantic-search-wikipedia-with-weaviate.api.vectors.network:8080&graphql_query=%23%23%0A%23%20Mixing%20scalar%20queries%20and%20semantic%20search%20queries%0A%23%23%0A%7B%0A%20%20Get%20%7B%0A%20%20%20%20Paragraph(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22What%20was%20Michael%20Brecker's%20first%20saxophone%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22content%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20where%3A%20%7B%0A%20%20%20%20%20%20%20%20operator%3A%20Equal%0A%20%20%20%20%20%20%20%20path%3A%20%5B%22inArticle%22%2C%20%22Article%22%2C%20%22title%22%5D%0A%20%20%20%20%20%20%20%20valueText%3A%20%22Michael%20Brecker%22%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20content%0A%20%20%20%20%20%20order%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inArticle%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Article%20%7B%0A%20%20%20%20%20%20%20%20%20%20title%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D)\n\n### Example 4 \u2014 mix generic concept search with graph relations\nWith Weaviate you can also use the GraphQL interface to make graph relations like -in the case of Wikipedia- links between different articles. In this overview we connect the paragraphs to the articles and show the linking articles. ```graphql\n{\n  Get {\n    Paragraph(\n      nearText: {\n        concepts: [\"jazz saxophone players\"]\n      }\n      limit: 25\n    ) {\n      content\n      order\n      title\n      inArticle {\n        ... on Article { # <== Graph connection I\n          title\n          hasParagraphs { # <== Graph connection II\n            ... on Paragraph {\n              title\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n\ud83d\udca1 LIVE \u2014 [try out this query](https://console.weaviate.io/console/query#weaviate_uri=http://semantic-search-wikipedia-with-weaviate.api.vectors.network:8080&graphql_query=%23%23%0A%23%20Using%20the%20Q%26A%20module%20I%0A%23%23%0A%7B%0A%20%20Get%20%7B%0A%20%20%20%20Paragraph(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22Where%20is%20the%20States%20General%20of%20The%20Netherlands%20located%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22content%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%20%20certainty%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20content%0A%20%20%20%20%20%20title%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D)\n\n## Implementation Strategies \u2014 Bringing Semantic Search to Production\nThe goal of Weaviate is to allow you to bring large ML-first applications to production."], "query": "How can I perform a semantic search for jazz saxophone players using GraphQL?"}
{"relevant_passages": ["We stop searching when all points in the current set are visited and return the top k results. On the other hand, the robust prune method optimizes the graph on a particular point p so that the greedy search will run faster. For that, it checks a list of visited nodes and selects on each iteration a point that minimizes the distance from p (the node for which we are currently optimizing the graph) and adds it to the out neighbors of the node p. It also removes the nodes that could be reached from the newly added node with a shorter distance than from p (amplifying the former distance by alpha to keep long-range connections). Together, these two algorithms should enable fast searches with high recall."], "query": "What are the two algorithms described for optimizing graph searches, and how does the robust prune method work?"}
{"relevant_passages": ["They discovered the influence of prompt phrasing on responses, leading to a growing interest in \"prompt engineering\" and its best practices within the AI community. The emphasis on scalable, open-source vector databases to manage data from AI models aligns perfectly with the goal of making generative AI accessible to all. ### In-house open source projects\n\nOur in-house projects, [Verba, an open-source RAG app](https://github.com/weaviate/Verba) and [Health Search](https://github.com/weaviate/healthsearch-demo) built by [Edward Schmuhl](https://www.linkedin.com/in/edwardschmuhl/), and other projects like [Leonie Monigatti's](https://www.linkedin.com/in/804250ab/) [Awesome-Moviate](https://github.com/weaviate-tutorials/awesome-moviate) and [Adam Chan](https://www.linkedin.com/in/itsajchan/)'s [Recommendation System](https://github.com/itsajchan/Create-JSON-Data-with-AI) have pushed the boundaries of what's possible with vector databases applicable to different industry use cases. ![build with weaviate](img/Group_2596_(1).png)\n\n### Community projects\n\nIncredible projects came to life through Weaviate in 2023! One standout project is [Basepilot](https://www.basepilot.com/), a project by [Ken Hendricks](https://www.linkedin.com/in/ken-hendricks-96181611a/) and [Pascal Wieler](https://www.linkedin.com/in/pascal-wieler/) that enables businesses to create their own product copilots easily. Their embedded copilot allows product users to find information and get tasks done just by chatting to it."], "query": "Who created the open-source RAG app called Verba?"}
{"relevant_passages": ["If we make one final trip to the supermarket to understand what a guava is, we could look at the things around it\u2014other fruit. Somewhat further away we might find guava juice or tinned guavas, but there's really no reason to look four aisles over for guava-flavored cat food. Using ANN allowed high-dimensional searches with near-perfect accuracy to be returned in milliseconds instead of hours. To be practical, vector databases also needed something prosaically called CRUD support. That stands for \"create, read, update and delete,\" and solving that technical challenge meant that the complex process of indexing the database could be done once, rather than being repeated from scratch whenever the database was updated."], "query": "What does CRUD stand for in the context of vector databases?"}
{"relevant_passages": ["The database backups include data objects, plus their vectors and indexes. This way, restoring a backup is a straight copy of all the required elements without the need to re-create vectors or rebuild the indexes. (Read, this is going to be fast)\n\n### Backup modules\nCloud-native backups in Weaviate are handled with the addition of the new **backup modules**:\n\n* `backup-s3` - for S3\n* `backup-gcs` - for GCS\n* `backup-fs` - for local filesystem\n\nWithout getting into too many details (see the [docs for more precise instructions](/developers/weaviate/configuration/backups)), each module requires a different set of settings. For S3 and GCS, you need your cloud bucket name, authentication details and some extra details like a project name or the cloud region. > For S3 authentication you can use access keys or IAM with role ARN's."], "query": "What are the components included in Weaviate database backups, and what modules are available for cloud-native backups?"}
{"relevant_passages": ["But like with any technology, it is not a silver bullet and success depends on your implementation. ### Scalability\nThe demo dataset runs on a Docker setup on a single machine, you can easily spin up a Kubernetes cluster if you want to use the Weaviate dataset in production. How to do this, is outlined [here](/developers/weaviate/concepts/cluster). ## Conclusion\nTo bring semantic search solutions to production, you need three things:\n\n1. Data\n1."], "query": "How can I scale the Weaviate dataset for production use?"}
{"relevant_passages": ["This makes the restarting node \u2018partly\u2019 available. :::\n\n### Queries during maintenance - with replication\n\nOn the other hand, this figure shows results from a scenario with replication configured with a factor of 3. ![Monitoring stats showing no failures during restart](./img/queries_with_replication.png)\n\nWe see that a grand total of zero queries failed here over the course of 8-9 minutes, even though individual pods did go down as they did before. In other words, the end users wouldn\u2019t even have noticed that a new version was rolled out, as node-level downtime did not lead to system-level downtime. Did we mention that the only change between the two was setting the replication factor?"], "query": "How does configuring replication with a factor of 3 affect query failures during node restarts in a maintenance scenario?"}
{"relevant_passages": ["## Step 2.2 \u2014 Import the Data\nBecause we are going to vectorize a lot of data. We will be using the same machine as mentioned in the opening but with 4 instead of 1 GPU. ![Google Cloud GPU setup with a Weaviate load balancer](./img/load-balancer.png)\n*Google Cloud GPU setup with a Weaviate load balancer*\n\nThe load balancer will redirect the traffic to available Weaviate transformer modules so that the import speed significantly increases. In the section: *Implementation Strategies \u2014 Bringing Semantic Search to Production* below you'll find more info about how you can run this in production. Most critically, we are going to set an external volume in the Docker Compose file to make sure that we store the data outside the container."], "query": "What hardware change is made to the machine setup in Step 2.2 of the data import process to increase import speed?"}
{"relevant_passages": ["Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\n\n![Reduced memory usage](./img/reduced-memory-usage.jpg)\n\nAs part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\n\nFirst, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs)."], "query": "What optimization was introduced in Weaviate to reduce memory usage during parallel imports?"}
{"relevant_passages": ["The main metric of concern in this setting is the latency with which we can obtain our replies. Product Quantization not only helps with reducing memory requirements in this case but also with cutting down on latency. The following table compares improvement in latency that can be achieved using PQ aggressively. | Dataset   | Segments | Centroids | Compression |              | Latency (ms) |\n|-----------|----------|-----------|-------------|--------------|--------------|\n| Sift      | 8        | 256       | x64         | Compressed   | 46 (x12)     |\n|           |          |           |             | Uncompressed | 547          |\n| DeepImage | 8        | 256       | x48         | Compressed   | 468 (x8.5)   |\n|           |          |           |             | Uncompressed | 3990         |\n| Gist      | 48       | 256       | x80         | Compressed   | 221 (x17.5)  |\n|           |          |           |             | Uncompressed | 3889         |\n\n**Tab. 2**: *Brute force search latency with high compression ratio.*\n\n## HNSW+PQ\n\nOur complete implementation of [FreshDiskANN](https://arxiv.org/abs/2105.09613) still requires a few key pieces, however at this point we have released the HNSW+PQ implementation with v1.18 for our users to take advantage of."], "query": "What improvements in latency can be achieved by using Product Quantization for different datasets, and in which version was the HNSW+PQ implementation released?"}
{"relevant_passages": ["[Two new distance metrics](#new-distance-metrics) - with the addition of Hamming and Manhattan distance metrics, you can choose the metric (or a combination of) to best suit your data and use case. 1. [Two new Weaviate modules](#new-weaviate-modules) - with the Summarization module, you can summarize any text on the fly, while with the HuggingFace module, you can use compatible transformers from the HuggingFace\n1. [Other improvements and bug fixes](#other-improvements-and-bug-fixes) - it goes without saying that with every Weaviate release, we strive to make Weaviate more stable - through bug fixes - and more efficient - through many optimizations. Read below to learn more about each of these points in more detail."], "query": "What new distance metrics were added in the latest Weaviate release?"}
{"relevant_passages": [":::\n\n## Implications for database maintenance\n\nIn production, this can dramatically reduce the critical downtime. Let\u2019s take an example three-pod Kubernetes setup with 10,000 tenants, and see how replication affects availability during a rolling update of Weaviate versions. Each Weaviate pod will restart one by one, as demonstrated in the example below (from `kubectl get pods`), which shows `weaviate-2` as having been recently restarted and ready, while `weaviate-1` is just restarting. ![Node statuses showing restarts](./img/node_statuses.png)\n\nWhat will the cluster availability be like during this period? We performed an experiment simulating non-trivial load with ~3,800 queries per second to approximate a real-life scenario."], "query": "How does replication affect database availability during a rolling update in a Kubernetes environment?"}
{"relevant_passages": ["The next segment amounts ensure 8, 16 and 32 to 1 compression ratio (in the case of sift which uses 2, 4 and 8 dimensions per segment). All experiments were performed adding 200,000 vectors using uncompressed behavior, then compressing and adding the rest of the data. We do not include the same charts for DeepImage but results are similar to those obtained over Sift1M. ![perf1](./img/image12.png)\n**Fig. 11**: *The chart shows Recall (vertical axis) Vs Latency (in microseconds, on the horizontal axis)."], "query": "What compression ratios were tested in the experiments with Sift1M, and how many vectors were initially added using uncompressed behavior?"}
{"relevant_passages": ["Typically this is in the context of recommendation in which we have metadata about users, as well as the documents or items. So for example, say we have features that describe a Users looking for Movies such as:\n\nUser Features - (Age, Gender, Location, Occupation, Preferences)\nMovie Features - (Release Year, Genre, Box Office, Duration). So together, the Metadata ranker takes as input something like: [Age, Gender, Location, Occupation, Preferences, Release year, Genre, Box Office, Duration] and predicts a score of how much this User will like the movie. We can fix the User features and rotate in each Document to get a score for each of the candidate movies (retrieved with something like ref2vec) to rank with. In addition to vectors, Weaviate also enables storing metadata features about objects such as `price`, or `color`."], "query": "How does a metadata ranker use user and movie features to recommend movies?"}
{"relevant_passages": ["The past for vector searching definitely was not a \u201csimpler time\u201d, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post."], "query": "How does Weaviate convert data into vectors and allow users to control the process?"}
{"relevant_passages": ["The first is to clean the data set and the second one is to import the data. ### Step 1 \u2013 Cleaning the Data\nThe first step is pretty straightforward, we will clean the data and create a [JSON Lines](https://jsonlines.org/) file to iterate over during import. You can run this process yourself or download the proceed file following [this](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate#step-1-process-the-wikipedia-dump) link. ### Step 2 \u2014 Importing the Data\nThis is where the heavy lifting happens because all paragraphs need to be vectorized we are going to use Weaviate's modular setup to use multiple GPUs that we will stuff with models, but before we do this we need to create a Weaviate schema that represents our use case. ### Step 2.1 \u2014 Create a Weaviate Schema\nWithin Weaviate we will be using a schema that determines how we want to query the data in GraphQL and which parts we want to vectorize."], "query": "What are the two main steps described for processing data in the provided document?"}
{"relevant_passages": ["These kinds of models are increasingly being used as guardrails for generative models. For example, a harmful or NSFW content detector can prevent these generations from making it through the search pipeline. An interesting idea I recently heard from Eddie Zhou on Jerry Liu\u2019s Llama Index Fireside Chat is the idea of using Natural Language Inference models to prevent hallucination by predicting the entailment or contradiction taking as the [retrieved context, generated output] as input. Because large language models are stochastic models, we can sample several candidate generations and filter them through score rankers like these. ## A Recap of the Ranking Models\n* **Cross Encoders** are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents."], "query": "What is the role of Natural Language Inference models in preventing hallucination in generative models?"}
{"relevant_passages": ["|\n\nBy re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et al. in the self-ask paper! This result was originally placed at #6 with Hybrid Search only. This is a great opportunity to preview the discussion of how LLMs use search versus humans. When humans search, we are used to scrolling through the results a bit to see the one that makes sense. In contrast, language models are constrained by input length; we can only give so many results to the input of the LLM."], "query": "What did Jonathan Frankle say about the benchmarks in the self-ask paper by Ofir Press et al. after re-ranking the search results?"}
{"relevant_passages": ["14**: *The chart shows Recall (vertical axis) Vs Indexing time (in minutes, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n### Memory compression results\n\nTo explore how the memory usage changes with the HNSW+PQ feature, we compare the two versions: uncompressed HNSW and HNSW plus compression using the KMeans encoder. We only compare KMeans using the same amount of segments as dimensions. All other settings could achieve a little bit of a higher compression rate but since we do not compress the graph, it is not significant for these datasets. Keep in mind that the whole graph built by HNSW is still hosted in memory."], "query": "How does the memory usage compare between uncompressed HNSW and HNSW with KMeans compression for the same number of segments as dimensions?"}
{"relevant_passages": ["This way, you can make sure that your voice is heard, and we can see what all of you need the most. <br></br>\n\n## Conclusion: Proud of how far we\u2019ve come, excited about the future\nIn the beginning, I mentioned that not just the product but also the company grew significantly last year. I am incredibly proud of what we have achieved \u2013 both overall and in the past year. This wouldn\u2019t have been possible without an absolutely fantastic team. Everyone working on Weaviate \u2013 whether a full-time employee or open-source contributor \u2013 is doing a fantastic job."], "query": "What is the team working on that contributed to the company's significant growth last year?"}
{"relevant_passages": ["This feature allows HNSW to work directly with compressed vectors. This means using Product Quantization to compress vectors and calculate distances. As mentioned before, we could still store the complete representation of the vectors on disk and use them to correct the distances as we explore nodes during querying. For the time being, we are implementing only HNSW+PQ which means we do no correction to the distances. In the future we will explore adding such a correction and see the implications in recall and latency since we will have more accurate distances but also much more disk reads."], "query": "What is the current state of distance correction in the HNSW+PQ implementation?"}
{"relevant_passages": ["Interestingly enough, it takes longer to render the results than it takes the vector database to find the answer. *Note, a semantic search is unlike a regular keywords search (which matches keywords like-for-like), but instead, we are searching for answers based on the semantic meaning of our query and data.*\n\nThe inevitable question that follows up this demonstration is always:\n\n> Why is this so incredibly fast? ## What is a vector search? To answer this question we need to look at how vector databases work. Vector databases index data, unlike other databases, based on data vectors (or vector embeddings)."], "query": "Why are vector databases faster at finding answers than rendering the results?"}
{"relevant_passages": ["Let's recap exactly what Weaviate does. ### Text vectorization in Weaviate\n\nimport VectorizationBehavior from '/_includes/vectorization.behavior.mdx';\n\n<VectorizationBehavior/>\n\nNow that we understand this, you might be asking - is it possible to customize the vectorization process? The answer is, yes, of course. ## Tweaking text2vec vectorization in Weaviate\n\nSome of you might have noticed that we have not done anything at all with the schema so far. This meant that the schema used is one generated by the auto-schema feature and thus the vectorizations were carried out using default options."], "query": "Can the text vectorization process in Weaviate be customized, and does it initially use a default schema for vectorization?"}
{"relevant_passages": ["We listened to your feedback, suggestions and use cases! So we made it our mission for the `1.15` release to design and implement an **elegant solution** with a great **Developer Experience (DX)**, which you will love \ud83d\ude0d to use for years to come. ### Announcement\nIntroducing **Weaviate Cloud-native backups**. \ud83c\udf89\n\nIt allows you to make full database backups (or selected classes) straight to **S3**, **GCS** or the **local filesystem** with a single API call \ud83e\udd29; and restore the data to a Weaviate instance of your choice with another API call. What is really great about this implementation is that you can create a backup without downtime on a running instance. The database stays fully operational (including receiving writes) while the backup is transferred to the remote storage."], "query": "What new backup feature was introduced in Weaviate version 1.15 that allows for operational continuity?"}
{"relevant_passages": ["This would mean we need roughly 500 MB for the graph but nearly ten times more memory for the vectors. On the other hand, a database such as DeepImage96 would have 96 dimensions but almost 10,000,000 vectors, meaning, that we would need around 10 GB to hold the vectors and the graph, ~5 GB for each graph. Our final goal is to move both vectors and graphs to disk. However, we will only explore moving vectors to disk in this post. Storing vectors on disk is not too challenging."], "query": "How much memory is required to hold the vectors and the graph for the DeepImage96 database?"}
{"relevant_passages": ["The main advantage of the hierarchical representation used in HNSW is that the traversal of the graph is accelerated. This is solved in the Vamana implementation by hosting long-range connections with a similar function. ### Traversing a graph\nTraversing a graph is a bit like planning international travel. First, we could take a long-distance flight (akin to a fast jump), taking us to a city closer to our destination. Then we could take a train (a lot better for the environment \ud83d\ude09) to get to the town of our choice."], "query": "What is the main advantage of the hierarchical representation in HNSW, and how does Vamana replicate this feature?"}
{"relevant_passages": ["---\ntitle: Achieve Zero-Downtime Upgrades with Weaviate\u2019s Multi-Node Setup\nslug: zero-downtime-upgrades\nauthors: [etienne,jp]\ndate: 2023-11-30\nimage: ./img/hero.png\ntags: ['concepts', 'engineering', 'how-to']\n# tags: ['replication']  # Added for further SEO without changing the original tags\ndescription: \"Learn about high-availability setups with Weaviate, which can allow upgrades and other maintenance with zero downtime. \"\n\n---\n\n![Image of Weaviate robots pointing at each other](./img/hero.png)\n\n## The tyranny of database downtime\n\nLike the old saying goes, a chain is only as strong as its weakest link. For tech infrastructure products, the weak link can often be its uptime. Think about how big a deal it is when social networks, web apps or databases are not available. This is why we at Weaviate really pride ourselves on having a robust, production-ready database that can scale as our users do."], "query": "How can Weaviate's multi-node setup enable zero-downtime upgrades?"}
{"relevant_passages": ["You can even run transformer models locally with [`text2vec-transformers`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers), and modules such as [`multi2vec-clip`](/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip) can convert images and text to vectors using a CLIP model. But they all perform the same core task\u2014which is to represent the \u201cmeaning\u201d of the original data as a set of numbers. And that\u2019s why semantic search works so well. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What tools are mentioned for running transformer models locally and converting images and text to vectors?"}
{"relevant_passages": ["If you are new to Weaviate, check out the [getting started guide](/developers/weaviate/quickstart). Let us know if you found this article interesting or useful. We are always happy to receive constructive feedback. \ud83d\ude00\n\nWe are always working on new articles and are looking for new topics. Let us know if there is anything that you would like us to write about."], "query": "Where can I find a getting started guide for Weaviate?"}
{"relevant_passages": ["Thus you could take any image from your training set, and step by step, add increasing levels of random noise to it and generate incrementally more noisy versions of that image as shown below. ![noising gif](./img/noise.gif)\n*[Source](https://yang-song.net/blog/2021/score/)*\n\n![noising images](./img/noisingimage.png)\n*[Source](https://huggingface.co/blog/annotated-diffusion)*\n\nThis \u201cnoising\u201d process, shown in the images above allows us to take training set images and add known quantities of noise to it until it becomes completely random noise. This process takes images from a state of having high probability of being found in the training set to having a low probability of existing in the training set. Once the \u201cnoising\u201d step is completed, then we can use these clean and noisy image combinations during the training phase of the diffusion model. In order to train a diffusion model we ask it to remove the noise from the noised images step by step until it recovers something as close as possible to the original image."], "query": "What is the process of adding noise to training set images to train a diffusion model?"}
{"relevant_passages": ["This means you could have a much faster reply, but there is no guarantee that you will actually have the closest element from your search. In the vector search space, we use [recall](https://en.wikipedia.org/wiki/Precision_and_recall) to measure the rate of the expected matches returned. The trade-off between recall and latency can be tuned by adjusting indexing parameters. Weaviate comes with reasonable defaults, but also it allows you to adjust build and query-time parameters to find the right balance. Weaviate incrementally builds up an index (graph representation of the vectors and their closest neighbors) with each incoming object."], "query": "How does Weaviate balance recall and latency in vector search?"}
{"relevant_passages": ["\ud83e\udd17\n\nimport ShareFeedback from '/_includes/share-feedback.md';\n\n<ShareFeedback />"], "query": "What is the purpose of the ShareFeedback component in the web application's codebase?"}
{"relevant_passages": ["[Better control over Garbage Collector](#better-control-over-garbage-collector) - with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 1. [Faster imports for ordered data](#faster-imports-for-ordered-data) - by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n)\n1. [More efficient filtered aggregations](#more-efficient-filtered-aggregations) - thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 1."], "query": "What feature was introduced to improve control over the garbage collector and reduce OOM kills in Weaviate setups?"}
{"relevant_passages": ["For example, in the case of text data, \u201ccat\u201d and \u201ckitty\u201d have similar meaning, even though the _words_ \u201ccat\u201d and \u201ckitty\u201d are very different if compared letter by letter. For semantic search to work effectively, representations of \u201ccat\u201d and \u201ckitty\u201d must sufficiently capture their semantic similarity. This is where vector representations are used, and why their derivation is so important. In practice, vectors are arrays of real numbers, of a fixed length (typically from hundreds to thousands of elements), generated by machine learning models. The process of generating a vector for a data object is called vectorization."], "query": "What is the process called that generates vector representations for words to capture their semantic similarity in text data?"}
{"relevant_passages": ["Here is the example above as [Google Colab notebook](https://colab.research.google.com/drive/1XAJc9OvkKhsJRmheqWZmjYU707dqEIl8?usp=sharing). ![Colab screenshot](./img/colab.png)\n\n### Use Weaviate in CI/CD pipelines\n\nYou can use Embedded Weaviate in automated tests, where you can run integration tests without having to manage a separate server instance. Here is the example above slightly modified to perform similarity search and test that the added object was found. <Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  Save as `embedded_test.py` and run `pytest`. (If you don't have pytest, run `pip install pytest`.)\n  <br/>\n\n  <FilteredTextBlock\n    text={PyCode}\n    startMarker=\"# START TestExample\"\n    endMarker=\"# END TestExample\"\n    language=\"py\"\n  />\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  Save as `embedded_test.ts` and run `npx jest`:\n  <br/>\n\n  <FilteredTextBlock\n    text={TSCode}\n    startMarker=\"// START TestExample\"\n    endMarker=\"// END TestExample\"\n    language=\"js\"\n  />\n  </TabItem>\n</Tabs>\n\n\nHave you found other use cases for embedded Weaviate?"], "query": "How can Embedded Weaviate be used in CI/CD pipelines for integration testing?"}
{"relevant_passages": ["You would need the following ingredients:\n* Raw Data\n* Hugging Face API token \u2013 which you can request from [their website](https://huggingface.co/settings/tokens)\n* A working Weaviate instance with the `text2vec-huggingface` enabled\n\nThen you would follow these steps. ### Step 1 \u2013 initial preparation \u2013 create schema and select the hf models\nOnce you have a Weaviate instance up and running. Define your schema (standard stuff \u2013 pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class. This is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\n```javascript\n{\n    \"class\": \"Notes\",\n    \"moduleConfig\": {\n        \"text2vec-huggingface\": {\n            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\n            ..."], "query": "What are the prerequisites and initial steps for integrating the `text2vec-huggingface` module into a Weaviate instance?"}
{"relevant_passages": ["It then takes the summaries generated so far to influence the next output. It repeats this process until all documents have been processed. ### Map Rerank\n\n<img\n    src={require('./img/map-rerank.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nMap Rerank involves running an initial prompt that asks the model to give a relevance score. It is then passed through the language model and assigns a score based on the certainty of the answer. The documents are then ranked and the top two are stuffed to the language model to output a single response."], "query": "What is the Map Rerank process in document processing and how does it work?"}
{"relevant_passages": ["### Patch 1.15.1 note\nWe have published a patch release v1.15.1.<br/>\nTo learn more check the [Weaviate 1.15.1 patch release](/blog/weaviate-1-15-1-release) blog. ### Community effort\n![New Contributors](./img/new-contributors.jpg)\n\n\ud83d\ude00We are extremely happy about this release, as it includes two big community contributions from [Aakash Thatte](https://github.com/sky-2002) and [Dasith Edirisinghe](https://github.com/DasithEdirisinghe). Over the last few weeks, they collaborated with our engineers to make their contributions. \ud83d\ude80**Aakash** implemented the two **new distance metrics**, while **Dasith** contributed by implementing the two **new Weaviate modules**. \ud83d\udc55We will send some Weaviate t-shirts to Aakash and Dasith soon."], "query": "Who implemented the new distance metrics in Weaviate 1.15.1 patch release?"}
{"relevant_passages": ["You can see how replication significantly improves availability. Weaviate provides further configurability and nuance for you in this area by way of a consistency guarantee setting. For example, a request made with a consistency level of QUORUM would require over half of the nodes which contain the data to be up, while a request with ONE consistency would only require one node to be up. :::note Notes\n- The replication algorithm makes sure that no node holds a tenant twice. Replication is always spread out across nodes."], "query": "How does Weaviate ensure high availability through its replication feature, and what are the different consistency levels it offers?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.15 release\nslug: weaviate-1-15-release\nauthors: [connor, erika, laura, sebastian]\ndate: 2022-09-07\ntags: ['release']\nimage: ./img/hero.png\ndescription: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\n---\n![Weaviate 1.15 release](./img/hero.png)\n\n<!-- truncate -->\n\nWe are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\n\nIf you like your content brief and to the point, here is the TL;DR of this release:\n1. [\u2601\ufe0fCloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\n1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1."], "query": "What are the new features introduced in Weaviate 1.15?"}
{"relevant_passages": ["---\ntitle: Wikipedia and Weaviate\nslug: semantic-search-with-wikipedia-and-weaviate\nauthors: [bob]\ndate: 2021-11-25\ntags: ['how-to']\nimage: ./img/hero.jpg\n# canonical-url: https://towardsdatascience.com/semantic-search-through-wikipedia-with-weaviate-graphql-sentence-bert-and-bert-q-a-3c8a5edeacf6\n# canonical-name: Towards Data Science\ndescription: \"Semantic search on Wikipedia dataset with Weaviate \u2013 vector database.\"\n---\n![Wikipedia and Weaviate](./img/hero.jpg)\n\n<!-- truncate -->\n\nTo conduct semantic search queries on a large scale, one needs a vector database to search through the large number of vector representations that represent the data. To show you how this can be done, [we have open-sourced the complete English language Wikipedia corpus](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) backup in Weaviate. In this article, I will outline how we've created the dataset, show you how you can run the dataset yourself, and present search strategies on how to implement similar vector and semantic search solutions in your own projects and how to bring them to production. The Wikipedia dataset used is the \"truthy\" version of October 9th, 2021. After processing it contains 11.348.257 articles, 27.377.159 paragraphs, and 125.447.595 graph cross-references."], "query": "How can Weaviate be used for semantic search on the Wikipedia dataset?"}
{"relevant_passages": ["---\ntitle: Ingesting PDFs into Weaviate\nslug: ingesting-pdfs-into-weaviate\nauthors: [erika, shukri]\ndate: 2023-05-23\nimage: ./img/hero.png\ntags: ['integrations','how-to']\ndescription: \"Demo on how to ingest PDFs into Weaviate using Unstructured.\"\n\n---\n\n![PDFs to Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nSince the release of ChatGPT, and the subsequent realization of pairing Vector DBs with ChatGPT, one of the most compelling applications has been chatting with your PDFs (i.e. [ChatPDF](https://www.chatpdf.com/) or [ChatDOC](https://chatdoc.com/)). Why PDFs? PDFs are fairly universal for visual documents, encompassing research papers, resumes, powerpoints, letters, and many more. In our [latest Weaviate Podcast](https://www.youtube.com/watch?v=b84Q2cJ6po8) with Unstructured Founder Brian Raymond, Brian motivates this kind of data by saying \u201cImagine you have a non-disclosure agreement in a PDF and want to train a classifier\u201d. Although PDFs are great for human understanding, they have been very hard to process with computers."], "query": "What is the title of the document that explains how to ingest PDFs into Weaviate and discusses the application of ChatGPT with Vector DBs?"}
{"relevant_passages": ["They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience."], "query": "What are Metadata Rankers and what features do they use to rank search results?"}
{"relevant_passages": ["---\ntitle: Combining LangChain and Weaviate\nslug: combining-langchain-and-weaviate\nauthors: [erika]\ndate: 2023-02-21\ntags: ['integrations']\nimage: ./img/hero.png\ndescription: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\"\n---\n![Combining LangChain and Weaviate](./img/hero.png)\n\nLarge Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general."], "query": "What are the benefits of combining LangChain with Weaviate in the context of LLMs?"}
{"relevant_passages": ["Among a long list of capabilities, first- and second-wave databases have their strengths. For example, some are very good at finding every instance of a certain value in a database, and others are very good at storing time sequences. The third wave of database technologies focuses on data that is processed by a machine learning model first, where the AI models help in processing, storing and searching through the data as opposed to traditional ways. To better understand the concept, think of a supermarket with 50,000 items. Items on display are not organized alphabetically or by price, the way you'd expect a structured, digital system to do it; they're placed in context."], "query": "What is the unique feature of third-wave database technologies compared to first- and second-wave databases?"}
{"relevant_passages": ["Generative models are not limited to just generating images; they can also generate songs, written language or any other modality of data - however to make it easier for us to understand we will only consider generative models that for image data. The core idea behind all generative models is that they try to learn and understand what the training set \u201clooks\u201d like. In other words they try to learn the underlying distribution of the training set - which just means that they want to know how likely a datapoint is to be observed in the training set. For example, if you are training a generative model on images of beautiful landscapes then, for that generative model, images of trees and mountains are going to be much more common then images of someones kitchen. Furthering this line of reasoning, for that same generative model, an image of static noise would also be quite unlikely since we don\u2019t see that in the training set."], "query": "What is the core principle behind generative models in relation to their training data, and what types of data can they generate?"}
{"relevant_passages": ["\ud83e\udd14 With this, you can get more out of your existing setups and push your Weaviate instances to do more, or you could save on the resources. ## Better control over Garbage Collector\n\n![GOMEMLIMIT](./img/gomemlimit.jpg)\n\nWeaviate is built from the ground up in Go, which allows for building very performant and memory-safe applications. Go is a garbage-collected language. > *A quick refresher:*<br/>\n> In a garbage-collected language, such as Go, C#, or Java, the programmer doesn't have to deallocate objects manually after using them. Instead, a GC cycle runs periodically to collect memory no longer needed and ensure it can be assigned again."], "query": "What advantage does Weaviate have due to being built in Go related to garbage collection?"}
{"relevant_passages": ["Let's take a look in terms of speed as well as recall. The chart below illustrates a comparison of the C++ Vamana [reference code](https://github.com/microsoft/DiskANN) provided by Microsoft and our [HNSW implementation](https://github.com/weaviate/weaviate/tree/master/adapters/repos/db/vector/hnsw) when using Sift1M. Following Microsoft\u2019s experiments, we have used sift-query.fvecs (100,000 vectors sample) for building the index and sift-query.fvecs (1,000 vectors sample) for querying. We are retrieving 10 **(fig. 1)** and 100 **(fig."], "query": "How does the performance of the C++ Vamana reference code compare to the HNSW implementation when using the Sift1M dataset for indexing and querying?"}
{"relevant_passages": ["Further these approaches are well positioned to generalize to Recommendation. In Recommendation, instead of taking a [query, document] as input to a cross-encoder, we take as input a [user description, document] pair. For example, we can ask users to describe their preferences. Further, we could combine these in trios of [user description, query, item] for LLM, or more lightweight cross-encoder, ranking. There is a bonus 3rd idea where we use the log probabilities concatenating the query with the document."], "query": "What is the novel input method for cross-encoders in recommendation systems that involves user descriptions?"}
{"relevant_passages": [">\n> For GCS you can use a Google Application Credentials json file. Alternatively, you can configure backups with the **local filesystem**. All you need here is to provide the path to the backup folder. > Note, you can have multiple storage configurations - one for each S3, GCS and the local filesystem. ### Creating backups - API\nOnce you have the backup module up and running, you can create backups with a single `POST` command:\n\n```js\nPOST /v1/backups/{storage}/\n{\n  \"id\": \"backup_id\"\n}\n```\n\nThe `storage` values are `s3`, `gcs`, and `filesystem`."], "query": "How can I create backups using an API for different storage configurations such as S3, GCS, and the local filesystem?"}
{"relevant_passages": ["Our initial concatenation had the `question` text come first, so let's reverse it to:\n\n```text\n'McDonald\\'s In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger '\n```\n\nThis lowers the distance to `0.0147`. Weaviate adds the class name to the text. So we will prepend the word `question` producing:\n\n```text\n'question McDonald\\'s In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger'\n```\n\nFurther lowering the distance to `0.0079`. Then the remaining distance can be eliminated by converting the text to lowercase like so:\n\n```python\nstr_in = ''\nfor k in sorted(properties.keys()):\n    v = properties[k]\n    if type(v) == str:\n        str_in += v + ' '\nstr_in = str_in.lower().strip()  # remove trailing whitespace\nstr_in = 'question ' + str_in\n```\n\nProducing:\n\n```text\n'question mcdonald\\'s in 1963, live on \"the art linkletter show\", this company served its billionth burger'\n```\n\nPerforming the `nearVector` search again results in zero distance (`1.788e-07` - effectively zero)!\n\nIn other words - we have manually reproduced Weaviate's default vectorization process. It's not overly complex, but knowing it can certainly be helpful."], "query": "Which company served its billionth burger live on \"The Art Linkletter Show\" in 1963?"}
{"relevant_passages": ["Charts to the left show Recall (vertical axis) Vs Heap usage (horizontal axis). Charts to the right show Heap usage (horizontal axis) Vs the different parameter sets. Parameter sets to achieve a larger graph (also producing a more accurate search) are charted from top down.*\n\nLet's sumamrize what we see in the above charts. We could index our data using high or low parameters set. Additionally, we could aim for different levels of compression."], "query": "How does heap usage affect recall and the accuracy of search results based on different parameter sets?"}
{"relevant_passages": ["Since it conveys both content and context, such a representation obviously presents a more complete and nuanced data picture. The challenge comes from searching through myriad dimensions. Initially, this was done with a brute force approach, looking at every vector associated with every entry. Needless to say, that approach didn't scale. One breakthrough that helped third-wave vector databases to scale was an approach called \"approximate nearest neighbor\" (ANN) search."], "query": "What technique enabled third-wave vector databases to scale effectively?"}
{"relevant_passages": ["<!-- TODO: update with a link to the article once it is ready -->\n*We are working on an article that will guide you on how to create your own model and upload it to Hugging Face.*\n\n### Fully automated and optimized\nWeaviate manages the whole process for you. From the perspective of writing your code \u2013 once you have your schema configuration \u2013 you can almost forget that Hugging Face is involved at all. For example, when you import data into Weaviate, Weaviate will automatically extract the relevant text fields, send them Hugging Face to vectorize, and store the data with the new vectors in the database. ### Ready to use with a minimum of fuss\nEvery new Weaviate instance created with the [Weaviate Cloud Services](/pricing) has the Hugging Face module enabled out of the box. You don't need to update any configs or anything, it is there ready and waiting."], "query": "How does Weaviate automatically handle data vectorization with Hugging Face, and is the Hugging Face module enabled by default in new Weaviate Cloud Services instances?"}
{"relevant_passages": ["For example, you can create a backup called **first_backup** and push it to **GCS**, like this:\n\n```js\nPOST /v1/backups/gcs/\n{\n  \"id\": \"first_backup\"\n}\n```\n\nThen, you can check the backup status by calling:\n\n```js\nGET /v1/backups/gcs/first_backup\n```\n\n### Restore\nTo restore a backup, you can call:\n\n```js\nPOST /v1/backups/{store}/{backup_id}/restore\n```\n\nSo, using our previous example, you can restore the **first_backup**, like this:\n\n```js\nPOST /v1/backups/gcs/first_backup/restore\n```\n\nYou can also, check the status of an ongoing restoration by calling:\n\n```js\nGET /v1/backups/gcs/first_backup/restore\n```\n\n### Cross-cloud\nHere is one interesting thing that you might not have noticed. You can use this setup to run Weaviate with one cloud provider but then store and restore backups to/from another cloud provider. So, for example, you can run Weaviate on AWS and use GCS for your backup needs. How cool is that? ### Class backups\nYou can also create backups for specific classes or select which classes you want to restore."], "query": "How do you restore a backup named \"first_backup\" from GCS using the Weaviate API?"}
{"relevant_passages": ["You find things in the supermarket by understanding how they relate to each other. So if the store gets a new product\u2014say, guavas\u2014you know to look near the apples and bananas, not near garbage bags or other things that happen to also cost $1.98/lb. A key early milestone in the third wave happened in 2015 when Google changed its search algorithm from one based on page rankings to one based on a machine learning model that it dubbed RankBrain. Before then, Google's search engine was essentially a high-powered keyword search that ranked websites by the number of other sites that linked back to them. Essentially, Google trusted rankings to the collective users of the Internet."], "query": "When did Google introduce RankBrain to its search algorithm?"}
{"relevant_passages": ["Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept."], "query": "What is the Weaviate Ref2Vec feature and how does it relate to the Cohere embedding model in the context of zero-shot AI?"}
{"relevant_passages": ["The `nearText` filter also allows for [more specific filters](https://towardsdatascience.com/semantic-search-through-wikipedia-with-weaviate-graphql-sentence-bert-and-bert-q-a-3c8a5edeacf6#:~:text=more%20specific%20filters) like `moveAwayFrom` and `MoveTo` concepts to manipulate the search through vector space. ```graphql\n{\n  Get {\n    Paragraph(\n      nearText: {\n        concepts: [\"Italian food\"]\n      }\n      limit: 50\n    ) {\n      content\n      order\n      title\n      inArticle {\n        ... on Article {\n          title\n        }\n      }\n    }\n  }\n}\n```\n\n\ud83d\udca1 LIVE \u2014 [try out this query](https://console.weaviate.io/console/query#weaviate_uri=http://semantic-search-wikipedia-with-weaviate.api.vectors.network:8080&graphql_query=%23%23%0A%23%20Generic%20question%20about%20Italian%20food%0A%23%23%0A%7B%0A%20%20Get%20%7B%0A%20%20%20%20Paragraph(%0A%20%20%20%20%20%20nearText%3A%20%7B%0A%20%20%20%20%20%20%20%20concepts%3A%20%5B%22Italian%20food%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%2050%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20content%0A%20%20%20%20%20%20order%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inArticle%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Article%20%7B%0A%20%20%20%20%20%20%20%20%20%20title%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D)\n\n### Example 3 \u2014 mix natural language questions with scalar search\nWithin Weaviate you can also mix scalar search filters with vector search filters. In the specific case, we want to conduct a semantic search query through all the paragraphs of articles about the saxophone player Michael Brecker. ```graphql\n{\n  Get {\n    Paragraph(\n      ask: {\n        question: \"What was Michael Brecker's first saxophone?\"\n        properties: [\"content\"]\n      }\n      where: {\n        operator: Equal\n        path: [\"inArticle\", \"Article\", \"title\"]\n        valueText: \"Michael Brecker\"\n      }\n      limit: 1\n    ) {\n      _additional {\n        answer {\n          result\n        }\n      }\n      content\n      order\n      title\n      inArticle {\n        ..."], "query": "What are the specific filters available in Weaviate's `nearText` search to manipulate vector space, and how can you combine natural language questions with scalar search filters in a query?"}
{"relevant_passages": ["Notice that similar recall/latency results with less segments still mean better compression rate. ![res1](./img/image5.png)\n**Fig. 4**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 1,000,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 8 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res2](./img/image6.png)\n**Fig."], "query": "How does varying the segment length affect the time to fit a Product Quantizer and encode vectors in relation to the recall achieved?"}
{"relevant_passages": ["The code is then passed through the Python REPL. Python REPL is a code executor implemented in LangChain. Once the code is executed, the output of the code is printed. The language model then sees this output and judges if the code is correct. ## ChatVectorDB\nOne of the most exciting features of LangChain is its collection of preconfigured chains."], "query": "What is the role of the Python REPL in LangChain?"}
{"relevant_passages": ["---\ntitle: Vector Embeddings Explained\nslug: vector-embeddings-explained\nauthors: [dan]\ndate: 2023-01-16\ntags: ['concepts']\nimage: ./img/hero.png\ndescription: \"Get an intuitive understanding of what exactly vector embeddings are, how they're generated, and how they're used in semantic search.\"\n---\nThe core function of Weaviate is to provide high-quality search results, going beyond simple keyword or synonym searches, and actually finding what the user _means_ by the query, or providing an actual answer to questions the user asks. <!-- truncate -->\n\nSemantic searches (as well as question answering) are essentially searches by similarity, such as by the meaning of text, or by what objects are contained in images. For example, consider a library of wine names and descriptions, one of which mentioning that the wine is \u201cgood with **fish**\u201d. A \u201cwine for **seafood**\u201d keyword search, or even a synonym search, won\u2019t find that wine. A meaning-based search should understand that \u201cfish\u201d is similar to \u201cseafood\u201d, and \u201cgood with X\u201d means the wine is \u201cfor X\u201d\u2014and should find the wine."], "query": "What are vector embeddings and how do they facilitate semantic search?"}
{"relevant_passages": ["\ud83d\ude00 <br/>\nKeep in touch and check [our blog](/blog) from time to time. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Where can I find updates or new content related to the sender of this message?"}
{"relevant_passages": ["ML-Models\n1. Vector database\n\nIn this article, we have shown how you can bring the complete Wikipedia corpus (data) using open-source ML-models (Sentence-BERT) and a vector database (Weaviate) to production. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can you use Sentence-BERT and Weaviate to bring the Wikipedia corpus to production?"}
{"relevant_passages": [":::\n\n## Conclusions\nWe've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n* How do we proceed to the natural disk solution of Weaviate?"], "query": "What indexing algorithm was implemented on DiskANN, and how does its performance compare to Weaviate's HNSW algorithm?"}
{"relevant_passages": ["* **Score Rankers** employ classifiers or regression models to score and detect content, acting as guardrails for generative models. These scores can help in filtering harmful or NSFW content and prevent hallucinations with cutting edge ideas such as Natural Language Inference filters. Each of these ranking models have particular use cases. However, the lines between these models are blurring with new trends such as translating tabular metadata features into text to facilitate transfer learning from transformers pre-trained on text. Of course, the recent successes of LLMs are causing a rethink of most AI workflows and the application of LLMs to rank and score rankers to filter generations are both exciting."], "query": "What are Score Rankers and how are they being influenced by the recent advancements in Large Language Models?"}
{"relevant_passages": ["It refers to an arrangement where locking occurs on multiple buckets or 'stripes'. Are you curious about, the challenge that we faced, which solutions we considered, and what was our final solution? Read on \ud83d\ude00. ## Background\nDatabases must be able to import data quickly and reliably while maintaining data integrity and reducing time overhead. Weaviate is no exception to this! Given that our users populate Weaviate with hundreds of millions of data objects (if not more), we appreciate that import performance is of the highest ..."], "query": "How did Weaviate improve its data import performance while maintaining data integrity?"}
{"relevant_passages": ["Share what you build with Weaviate in [Slack](https://weaviate.slack.com/), on our [Forum](https://forum.weaviate.io/), or on socials. ## Embracing Open Source and Sharing Knowledge\n\nAs AI accelerated throughout the year with ever-new innovations popping up, so did the community's curiosity to learn and share knowledge in that area. As an open-source solution, **community** is a foundational pillar of Weaviate. ### [Hacktoberfest](https://weaviate.io/blog/hacktoberfest-2023)\n\nCelebrating the spirit of **open source**, we participated in our first [Hacktoberfest](https://hacktoberfest.com/) this October, which was organized by our very own **[Leonie Monigatti](https://www.linkedin.com/in/804250ab/)**! This global event, aimed at engineers and machine learning enthusiasts, fosters collaboration and contributions to open-source technology. Participants who had four pull requests (PRs) accepted between October 1 and 31, 2023, earned a unique digital reward and some Weaviate Merch! Contributions varied in scope, ranging from minor, non-coding inputs to more substantial technical improvements."], "query": "Who organized Weaviate's first participation in Hacktoberfest 2023?"}
{"relevant_passages": ["## Discussions & wrap-up\n\nSo there it is. Throughout the above journey, we saw how exactly Weaviate creates vectors from the text data objects, which is:\n\n- Vectorize properties that use `string` or `text` data types\n- Sorts properties in alphabetical (a-z) order before concatenating values\n- Prepends the class name\n- And converts the whole string to lowercase\n\nAnd we also saw how this can be tweaked through the schema definition for each class. One implication of this is that your vectorization requirements are a very important part of considerations in the schema definition. It may determine how you break down related data objects before importing them into Weaviate, as well as which fields you choose to import. Let's consider again our quiz question corpus as a concrete example."], "query": "How does Weaviate create vectors from text data objects?"}
{"relevant_passages": ["## Tool Use\nThe last building block we will cover is tool use. [Tool use](https://python.langchain.com/docs/modules/agents/tools/) is a way to augment language models to use tools. For example, we can hook up an LLM to [vector databases](https://weaviate.io/blog/what-is-a-vector-database), calculators, or even code executors. Of course we will dive into the vector databases next, but let\u2019s start with an example of the code executor tool use. <img\n    src={require('./img/tool-use.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nThe task for the language model is to write python code for the bubble sort algorithm."], "query": "What is an example of augmenting language models with a tool to write Python code?"}
{"relevant_passages": ["There you will learn how each of the distances works in more detail, when to use each, and how they compare to other metrics. ## New Weaviate modules\n\n<!-- TODO: add an image for Weaviate modules -->\n![New Weaviate modules](./img/weaviate-modules.png)\n\nThe list of the new goodies included with Weaviate `v1.15` goes on. Courtesy of a fantastic community contribution from [Dasith Edirisinghe](https://github.com/DasithEdirisinghe), we have two new Weaviate modules for you: Summarization and Hugging Face modules. ### Summarization Module\nThe Summarization module allows you to summarize text data at query time. The module adds a `summary` filter under the `_additional` field, which lets you list the properties that should be summarized."], "query": "What new features does Weaviate version 1.15 offer, and who contributed to its development?"}
{"relevant_passages": ["It was that Ofir did such a phenomenal job of figuring out a way to measure the complexity of the knowledge that was extracted from the model. He gave us a benchmark, a ladder to climb, a way to measure whether we could retrieve certain kinds of information from models. And I think that's going to open the door to a ton more benchmarks. And you know what happens when there's a benchmark. We optimize the hell out of that benchmark and it moves science forward\u2026 [ truncated for visibility ] |\n| Hybrid Only            | Or, at least being able to ask follow up questions when it\u2019s unclear about and that\u2019s surprisingly not that difficult to do with these current systems, as long as you\u2019re halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might get."], "query": "Who created a benchmark for measuring the complexity of knowledge extracted from models?"}
{"relevant_passages": ["We are still on memory, are we not? Don't worry. This was just the first step toward our goal. We want to ensure that we have a solid implementation in memory before we move to disk, and this milestone ends here. :::note\n!Spoilers alert, as you read this article, we are evaluating our implementation on disk.<br/>\nWe will prepare a similar article to outline; how we moved everything to disk and what the price was performance-wise."], "query": "What is the current stage of the implementation mentioned in the document, and what are the future plans regarding its transition to disk?"}
{"relevant_passages": ["<details>\n  <summary>Optional: Try it yourself (with minikube)</summary>\n\nYou can try running a local, multi-node Weaviate cluster with `minikube`, which can conveniently run a local Kubernetes cluster. We note that deploying Weaviate on a cloud provider\u2019s kubernetes service follows a similar process. <br/>\n\nFirst, install `minikube` and `helm` for your system by following these guides ([minikube](https://minikube.sigs.k8s.io/docs/start), [helm](https://helm.sh/docs/intro/install)). We also recommend installing `kubectl` ([by following this guide](https://kubernetes.io/docs/tasks/tools/#kubectl)). <br/>\n\nOnce minikube is installed, start a three-node minikube cluster by running the following from the shell:\n\n```shell\nminikube start --nodes 3\n```\n\nOnce the nodes have been created, you should be able to interact with them through the `kubectl` command-line tool."], "query": "How can I set up a local multi-node Weaviate cluster using minikube?"}
{"relevant_passages": ["8**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 1,000,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res6](./img/image10.png)\n**Fig. 9**: *Average time (microseconds) to calculate distances from query vectors to all 1,000,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment."], "query": "What is the relationship between segment length and recall in the performance of a Product Quantizer when fitting and encoding vectors, as well as calculating distances?"}
{"relevant_passages": ["Unfortunately, the queries were slow, resulting in Out Of Memory kills in some cases. This was not good enough for what we expected of Weaviate. ### Investigation\n\nTo investigate the issue, we've set up a database with 1M objects and a profiler to watch memory consumption. We used that setup to run ten parallel filtered aggregations. Upon reviewing the memory consumption, we noted that some of the filtered aggregations were taking up to **200GB** of RAM (note, this was not the total allocated memory on the heap, as a big part of it was waiting to be collected by GC)."], "query": "How much RAM did some filtered aggregations consume during the investigation of slow queries in Weaviate?"}
{"relevant_passages": ["For example, if we broke down this blog post into **chapters** in Weaviate, with **title** and **content** properties. We could run a query to summarize the *\"New distance metrics\"* chapter like this:\n\n```graphql\n{\n  Get {\n    Chapter(\n      where: {\n        operator: Equal\n        path: \"title\"\n        valueText: \"New distance metrics\"\n      }\n    ) {\n      title\n      _additional{\n        summary(\n          properties: [\"content\"],\n        ) {\n          property\n          result\n        }\n      }\n    }\n  }\n}\n```\n\nWhich would return the following result:\n\n```graphql\n{\n  \"data\": {\n    \"Get\": {\n      \"Chapters\": [\n        {\n          \"_additional\": {\n            \"summary\": [\n              {\n                \"property\": \"content\",\n                \"result\": \"Weaviate 1.15 adds two new distance metrics - Hamming\n                 distance and Manhattan distance. In total, you can now choose\n                 between five various distance metrics to support your datasets. Check out the metrics documentation page, for the full overview\n                 of all the available metrics in Weaviate.\"\n              }\n            ]\n          },\n          \"title\": \"New distance metrics\"\n        }\n      ]\n    }\n  },\n  \"errors\": null\n}\n```\n\nHead to the [Summarization Module docs page](/developers/weaviate/modules/reader-generator-modules/sum-transformers) to learn more. ### Hugging Face Module\nThe Hugging Face module (`text2vec-huggingface`) opens up doors to over 600 [Hugging Face sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity), ready to be used in Weaviate as a vectorization module."], "query": "What new distance metrics were added in Weaviate 1.15?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.2 release - transformer models\nslug: weaviate-1-2-transformer-models\nauthors: [etienne]\ndate: 2021-03-30\ntags: ['release']\nimage: ./img/hero.png\n# canonical-url: https://medium.com/semi-technologies/weaviate-version-1-2-x-now-supports-transformer-models-4a12d858cce3\n# canonical-name: Medium\ndescription: \"Weaviate v1.2 introduced support for transformers (DistilBERT, BERT, RoBERTa, Sentence-BERT, etc) to vectorize and semantically search through your data.\"\n---\n![Weaviate 1.2 release - transformer models](./img/hero.png)\n\nIn the v1.0 release of Weaviate ([docs](/developers/weaviate/) \u2014 [GitHub](https://github.com/weaviate/weaviate)) we introduced the concept of [modules](/developers/weaviate/concepts/modules). Weaviate modules are used to extend the vector database with vectorizers or functionality that can be used to query your dataset. With the release of Weaviate v1.2, we have introduced the use of transformers ([DistilBERT](https://arxiv.org/abs/1910.01108), [BERT](https://github.com/google-research/bert), [RoBERTa](https://arxiv.org/abs/1907.11692), Sentence-[BERT](https://arxiv.org/abs/1908.10084), etc) to vectorize and semantically search through your data. <!-- truncate -->\n\n### Weaviate v1.2 introduction video\n\n<div className=\"youtube\">\n    <iframe src=\"//www.youtube.com/embed/S4lXPPZvGPQ\" frameBorder=\"0\" allowFullScreen></iframe>\n</div>\n\n## What are transformers? A [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) (e.g., [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))) is a deep learning model that is used for NLP tasks."], "query": "What new feature related to transformer models was introduced in Weaviate version 1.2?"}
{"relevant_passages": ["Full dynamic scalability will be added in a future release. # highlight-start\nreplicas: 3\n# highlight-end\n... ```\n\nYou can now deploy Weaviate in this configuration by running:\n\n```shell\nkubectl create namespace weaviate\n\nhelm upgrade --install \\\n  \"weaviate\" \\\n  weaviate/weaviate \\\n  --namespace \"weaviate\" \\\n  --values ./values.yaml\n```\n\nThis will deploy the Weaviate clusters. You can check the status of the deployment by running:\n\n```shell\nkubectl get pods -n weaviate\n```\n\nThis should produce an output similar to the following:\n\n```shell\nNAME         READY   STATUS    RESTARTS   AGE\nweaviate-0   1/1     Running   0          3m00s\nweaviate-1   1/1     Running   0          2m50s\nweaviate-2   1/1     Running   0          2m40s\n```\n\nNow, you need to expose the Weaviate service to the outside world - i.e. to your local machine. You can do this by running:\n\n```shell\nminikube service weaviate --namespace weaviate\n```\n\nThis should show an output similar to the following that shows the URL to access the Weaviate cluster:\n\n```shell\n|-----------|----------|-------------|------------------------|\n| NAMESPACE |   NAME   | TARGET PORT |          URL           |\n|-----------|----------|-------------|------------------------|\n| weaviate  | weaviate |             | http://127.0.0.1:54847 |\n|-----------|----------|-------------|------------------------|\n```\n\nAnd it should also open a browser window showing the list of Weaviate endpoints."], "query": "How many replicas does the current Weaviate deployment configuration specify?"}
{"relevant_passages": ["* Should it be just an implementation of DiskANN? * Or should we explore the capabilities of HNSW and adjust it to work on disk? * How can we guarantee the excellent database UX \u2013 so valuable to many Weaviate users \u2013 while reaping the benefits of a disk-based solution? Stay tuned as we explore these challenges and questions. We will share our insights as we go."], "query": "What challenges and questions are being explored regarding disk-based solutions for Weaviate users?"}
{"relevant_passages": ["This would result in 6 separate calaculations. <img\n  src={require('./img/knn-boules.png').default}\n  alt=\"kNN search in a game of Boules\"\n  style={{ maxWidth: \"50%\" }}\n/>\n\n*[Figure 1 - kNN search in a game of Boules.]*\n\n### A kNN search is computationally very expensive\nComparing a search vector with 10, 100, or 1000 data vectors in just two dimensions is an easy job. But of course, in the real world, we are more likely to deal with millions (like in the Wikipedia dataset) or even billions of data items. In addition, the number of dimensions that most ML models use in semantic search goes up to hundreds or thousands of dimensions!\n\nThe *brute* force of a **kNN search is computationally very expensive** - and depending on the size of your database, a single query could take anything from several seconds to even hours (yikes\ud83d\ude05). If you compare a vector with 300 dimensions with 10M vectors, the vector search would need to do 300 x 10M = 3B computations! The number of required calculations increases linearly with the number of data points (O(n)) (Figure 2)."], "query": "How many computations are required for a kNN search comparing a 300-dimensional vector with 10 million data vectors?"}
{"relevant_passages": ["[Weaviate](/developers/weaviate/), an open-source vector database written in Go, can serve thousands of queries per second. Running Weaviate on [Sift1M](https://www.tensorflow.org/datasets/catalog/sift1m) (a 128-dimensional representation of objects) lets you serve queries in single-digit milliseconds. But how is this possible? ![SIFT1M Benchmark example](./img/SIFT1M-benchmark.png)\n*See the [benchmark](/developers/weaviate/benchmarks/ann) page for more stats.*\n\nWeaviate does not look for the exact closest vectors in the store. Instead, it looks for approximate (close enough) elements."], "query": "How does Weaviate achieve high query performance on the Sift1M dataset?"}
{"relevant_passages": ["![animation](./img/animation.png)\n\nThe blog post included this great visual to help with the visualization of combining Bi-Encoders and Cross-Encoders. This fishing example explains the concept of coarse-grained retrieval (fishing net = vector search / bm25) and manual inspection of the fish (fishermen = ranking models). Depicted with manual inspection of fish, the main cost of ranking models is speed. In March, Bob van Luijt appeared on a Cohere panel to discuss [\u201cAI and The Future of Search\u201d](https://twitter.com/cohereai/status/1636396916157079554?s=46&t=Zzg6vgh4rwmYEkdV-3v5gg). Bob explained the effectiveness of combining zero-shot vector embedding models from providers such as Cohere, OpenAI, or HuggingFace with BM25 sparse search together in Hybrid Search."], "query": "What visual analogy does the blog post use to explain the combination of Bi-Encoders and Cross-Encoders in search technology?"}
{"relevant_passages": ["* At the **class** level, `vectorizeClassName` will determine whether the class name is used for vectorization. * At the **property** level:\n    * `skip` will determine whether the property should be skipped (i.e. ignored) in vectorization, and\n    * `vectorizePropertyName` will determine whether the property name will be used. * The property `dataType` determines whether Weaviate will ignore the property, as it will ignore everything but `string` and `text` values. > You can read more about each variable in the [schema configuration documentation](/developers/weaviate/manage-data/collections). Let's apply this to our data to set Weaviate's vectorization behavior, then we will confirm it manually using the Cohere API as we did above."], "query": "What settings determine the vectorization behavior of classes and properties in Weaviate, and which data types are eligible for vectorization?"}
{"relevant_passages": ["This is then passed through the language model to generate multiple responses. Another prompt is created to combine all of the initial outputs into one. This technique requires more than one call to the LLM. ### Refine\n\n<img\n    src={require('./img/refine.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nRefine is a unique technique because it has a local memory. An example of this is to ask the language model to summarize the documents one by one."], "query": "What is the \"Refine\" technique and how does it utilize a language model's local memory?"}
{"relevant_passages": ["Word2vec in particular uses a neural network [model](https://arxiv.org/pdf/1301.3781.pdf) to learn word associations from a large corpus of text (it was initially trained by Google with 100 billion words). It first creates a vocabulary from the corpus, then learns vector representations for the words, typically with 300 dimensions. Words found in similar contexts have vector representations that are close in vector space, but each word from the vocabulary has only one resulting word vector. Thus, the meaning of words can be quantified - \u201crun\u201d and \u201cran\u201d are recognized as being far more similar than \u201crun\u201d and \u201ccoffee\u201d, but words like \u201crun\u201d with multiple meanings have only one vector representation. As the name suggests, word2vec is a word-level model and cannot by itself produce a vector to represent longer text such as sentences, paragraphs or documents."], "query": "What are the characteristics of the Word2vec model for learning word associations from text?"}
{"relevant_passages": ["However, most of the LLM APIs don\u2019t actually give us these probabilities. Further, this is probably pretty slow. We will keep an eye on it, but it doesn\u2019t seem like the next step to take for now. ## Metadata Rankers\nWhereas I would describe Cross-Encoders as `content-based` re-ranking, I would say Metadata rankers are `context-based` re-rankers. Metadata rankers describe using symbolic features to rank relevance."], "query": "What is the difference between Cross-Encoders and Metadata rankers in the context of re-ranking?"}
{"relevant_passages": ["The data is persisted, so you can use it from future invocations, or you can [transfer it to another instance](/developers/weaviate/manage-data/read-all-objects/#restore-to-a-target-instance). You can learn more about running Weaviate locally from client code on the [Embedded Weaviate](/developers/weaviate/installation/embedded/) page. ## <i class=\"fa-solid fa-lightbulb\"></i> Use cases\n\nWhat can you do with Embedded Weaviate? Quite a few things!\n\nFirst off, you can get started very quickly with Weaviate on your local machine, without having to explicitly download, install or instantiate a server. ### Jupyter notebooks\n\nYou can also use Embedded Weaviate from Jupyter notebooks, including on Google Colaboratory."], "query": "Can Embedded Weaviate be used from Jupyter notebooks on Google Colaboratory?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.23 Release\nslug: weaviate-1-23-release\nauthors: [jp, dave]\ndate: 2023-12-19\nimage: ./img/hero.png\ntags: ['release', 'engineering']\ndescription: \"Weaviate 1.23 released with AutoPQ, flat indexing + Binary Quantization, OSS LLM support through Anyscale, and more!\"\n\n---\n\nimport Core123 from './_core-1-23-include.mdx' ;\n\n<Core123 />\n\nimport WhatsNext from '/_includes/what-next.mdx'\n\n<WhatsNext />\n\nimport Ending from '/_includes/blog-end-oss-comment.md' ;\n\n<Ending />"], "query": "What features were introduced in Weaviate 1.23 released on 2023-12-19?"}
{"relevant_passages": ["Speaking of Cloud, arguably the easiest way to spin up a new use case with Weaviate is through the [Weaviate Cloud Services](/pricing). <br></br>\n\n### New Vector Indexes\n![vector indexes](./img/vector-indexes.png)\n\nLast year we gave you a sneak peek into our [Vector Indexing Research](/blog/ann-algorithms-vamana-vs-hnsw), and this year you will be able to try out new vector indexes for yourself. Since the beginning, Weaviate has supported vector indexing with [HNSW](/developers/weaviate/concepts/vector-index), which leads to [best-in-class query times](/developers/weaviate/benchmarks/ann). But not every use case requires single-digit millisecond latencies. Instead, some prefer cost-effectiveness."], "query": "What vector indexing method has Weaviate traditionally supported for fast query times?"}
{"relevant_passages": ["This demo is also using OpenAI for vectorization; you can choose another `text2vec` module [here](/developers/weaviate/modules/retriever-vectorizer-modules). ```python\nclient = weaviate.Client(\n    embedded_options=EmbeddedOptions(\n        additional_env_vars={\"OPENAI_APIKEY\": os.environ[\"OPENAI_APIKEY\"]}\n    )\n)\n```\n\n### Configure the Schema\n\nNow we need to configure our schema. We have the `document` class along with the `abstract` property. ```python\nclient.schema.delete_all()\n\nschema = {\n    \"class\": \"Document\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"properties\": [\n        {\n            \"name\": \"source\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"abstract\",\n            \"dataType\": [\"text\"],\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\"skip\": False, \"vectorizePropertyName\": False}\n            },\n        },\n    ],\n    \"moduleConfig\": {\n        \"generative-openai\": {},\n        \"text2vec-openai\": {\"model\": \"ada\", \"modelVersion\": \"002\", \"type\": \"text\"},\n    },\n}\n\nclient.schema.create_class(schema)\n```\n\n### Read/Import the documents\n\nNow that our schema is defined, we want to build the objects that we want to store in Weaviate. We wrote a helper class,  `AbstractExtractor` to aggregate the element class."], "query": "How do you configure a Weaviate schema to use OpenAI's `text2vec-openai` vectorizer with the `ada` model version `002`?"}
{"relevant_passages": ["The algorithm keeps a result set of points, starting with the entry point. On every iteration, it checks what points are in the result set that has not been visited yet and, from them, takes the best candidate (the one closest to the query point) and explores it. Exploring in this context means adding the candidate (out neighbors) from the graph to the result set and marking it as visited. Notice the size of the result set has to stay bounded, so every time it grows too much, we only keep those L points closer to the query. The bigger the maximum size of the result set, the more accurate the results and the slower the search."], "query": "How does the algorithm ensure that the result set of points remains bounded in size during the search process?"}
{"relevant_passages": ["have published \u201cLarge Language Models are easily distracted by irrelevant context\u201d, highlighting how problematic bad precision in search can be for retrieval-augmented generation. The recent developments in LLM agent tooling such as LangChain, LlamaIndex, and recent projects such as AutoGPT or Microsoft\u2019s Semantic Kernel are paving the way towards letting LLMs run for a while to complete complex tasks. By ranking each handoff from search to prompt, we can achieve better results in each intermediate task. Thus when we leave an LLM running overnight to research the future of ranking models, we can expect a better final result in the morning!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What are the recent developments in tooling for Large Language Models that aid in complex task completion?"}
{"relevant_passages": ["The good news is, there are companies \u2013 like Hugging Face, OpenAI, and Cohere \u2013 that offer running model inference as a service. > \"Running model inference in production is hard,\nlet them do it for you.\"\n\n## Support for Hugging Face Inference API in Weaviate\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\n> Both modules are enabled by default in the [Weaviate Cloud Services](/pricing)."], "query": "Which version of Weaviate started to include support for the Hugging Face Inference API?"}
{"relevant_passages": ["Each `text2vec-*` module uses an external API (like `text2vec-openai` or `text2vec-huggingface`) or a local instance like `text2vec-transformers` to produce a vector for each object. Let's try vectorizing data with the `text2vec-cohere` module. We will be using data from `tiny_jeopardy.csv` [available here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain) containing questions from the game show Jeopardy. We'll just use a few (20) questions here, but the [full dataset on Kaggle](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) includes 200k+ questions. Load the data into a Pandas dataframe, then populate Weaviate like this:\n\n```python\nclient.batch.configure(batch_size=100)  # Configure batch\nwith client.batch as batch:\n    for i, row in df.iterrows():\n        properties = {\n            \"question\": row.Question,\n            \"answer\": row.Answer\n        }\n        batch.add_data_object(properties, \"Question\")\n```\n\nThis should add a series of `Question` objects with text properties like this:\n\n```text\n{'question': 'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger',\n 'answer': \"McDonald's\"}\n```\n\nSince we use the `text2vec-cohere` module to vectorize our data, we can query Weaviate to find data objects most similar to any input text."], "query": "How can you use the `text2vec-cohere` module to find questions similar to \"In 1963, live on 'The Art Linkletter Show', this company served its billionth burger\"?"}
{"relevant_passages": ["We will call this in order to grab the abstract element along with the content. <details>\n  <summary>AbstractExtractor</summary>\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n\nclass AbstractExtractor:\n    def __init__(self):\n        self.current_section = None  # Keep track of the current section being processed\n        self.have_extracted_abstract = (\n            False  # Keep track of whether the abstract has been extracted\n        )\n        self.in_abstract_section = (\n            False  # Keep track of whether we're inside the Abstract section\n        )\n        self.texts = []  # Keep track of the extracted abstract text\n\n    def process(self, element):\n        if element.category == \"Title\":\n            self.set_section(element.text)\n\n            if self.current_section == \"Abstract\":\n                self.in_abstract_section = True\n                return True\n\n            if self.in_abstract_section:\n                return False\n\n        if self.in_abstract_section and element.category == \"NarrativeText\":\n            self.consume_abstract_text(element.text)\n            return True\n\n        return True\n\n    def set_section(self, text):\n        self.current_section = text\n        logging.info(f\"Current section: {self.current_section}\")\n\n    def consume_abstract_text(self, text):\n        logging.info(f\"Abstract part extracted: {text}\")\n        self.texts.append(text)\n\n    def consume_elements(self, elements):\n        for element in elements:\n            should_continue = self.process(element)\n\n            if not should_continue:\n                self.have_extracted_abstract = True\n                break\n\n        if not self.have_extracted_abstract:\n            logging.warning(\"No abstract found in the given list of objects.\")\n\n    def abstract(self):\n        return \"\\n\".join(self.texts)\n```\n</details>\n\n```python\ndata_folder = \"../data\"\n\ndata_objects = []\n\nfor path in Path(data_folder).iterdir():\n    if path.suffix != \".pdf\":\n        continue\n\n    print(f\"Processing {path.name}...\")\n\n    elements = partition_pdf(filename=path)\n\n    abstract_extractor = AbstractExtractor()\n    abstract_extractor.consume_elements(elements)\n\n    data_object = {\"source\": path.name, \"abstract\": abstract_extractor.abstract()}\n\n    data_objects.append(data_object)\n```\n\nThe next step is to import the objects into Weaviate. ```python\nclient.batch.configure(batch_size=100)  # Configure batch\nwith client.batch as batch:\n    for data_object in data_objects:\n        batch.add_data_object(data_object, \"Document\")\n```\n\n### Query Time\n\nNow that we have imported our two documents, we can run some queries! Starting with a simple BM25 search. We want to find a document that discusses house prices. ```python\nclient.query.get(\"Document\", \"source\").with_bm25(\n    query=\"some paper about housing prices\"\n).with_additional(\"score\").do()\n```\n\n<details>\n  <summary>Output</summary>\n\n```\n{'data': {'Get': {'Document': [{'_additional': {'score': '0.8450042'},\n     'source': 'paper02.pdf'},\n    {'_additional': {'score': '0.26854637'}, 'source': 'paper01.pdf'}]}}}\n```\n\n</details>\n\nWe can take this one step further by using the generative search module."], "query": "How can you extract abstracts from documents and perform a BM25 search for a specific topic in Weaviate?"}
{"relevant_passages": ["```\n\n</details>\n\n## Limitations\nThere are a few limitations when it comes to a document that has two columns. For example, if a document is structured with two columns, then the text doesn\u2019t extract perfectly. The workaround for this is to set `strategy=\"ocr_only\"` or `strategy=\"fast\"` into `partition_pdf`. There is a [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/356) on fixing multi-column documents, give it a \ud83d\udc4d up!\n\n<details>\n  <summary>strategy=\"ocr_only\"</summary>\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"ocr_only\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n</details>\n\n<details>\n  <summary>strategy=\u201dfast\u201d</summary>\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"fast\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n</details>\n\n## Weaviate Brick in Unstructured\nThere is a [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/566) to add a Weaviate staging brick! The goal of this integration is to add a Weaviate section to the documentation and show how to load unstructured outputs into Weaviate. Make sure to give this issue a \ud83d\udc4d up!\n\n## Last Thought\nThis demo introduced how you can ingest PDFs into Weaviate."], "query": "What strategies are recommended for extracting text from two-column PDF documents using `partition_pdf`?"}
{"relevant_passages": ["If you want to learn how to configure Weaviate to use PQ refer to the docs [here](/developers/weaviate/config-refs/schema/vector-index#how-to-configure-hnsw). ## KMeans encoding results\n\nFirst, the PQ feature added to version 1.18 of Weaviate is assessed in the sections below. To check performance and distortion, we compared our implementation to [NanoPQ](https://github.com/matsui528/nanopq) and we observed similar results. The main idea behind running these experiments is to explore how PQ compression would affect our current indexing algorithms. The experiments consist of fitting the Product Quantizer on some datasets and then calculating the recall by applying brute force search on the compressed vectors."], "query": "How does Weaviate's PQ feature performance compare to NanoPQ?"}
{"relevant_passages": ["![Hacktober video](img/hacktober.gif)\n\n### [Weaviate Academy](/developers/academy) & [Workshops](/learn/workshops)\nWeaviate Academy and Workshops have had a fantastic year of learning and growth! We've been focusing on ensuring everyone has the chance to understand and use vector databases and get a grasp on Generative AI and data handling. Every week, [Zain](https://www.linkedin.com/in/zainhas/), [JP](https://www.linkedin.com/in/jphwang/), [Daniel](https://www.linkedin.com/in/malgamves/), and [Duda](https://www.linkedin.com/in/dudanogueira/) have been running introductory workshops on vector databases and Weaviate, which have been a hit. Plus, we're super excited about [JP Hwang](https://www.linkedin.com/in/jphwang/)'s initiative, the Weaviate Academy. It's a program that takes you from the basics all the way to production-level skills. Later in the year, we teamed up with [DeepLearningAI](https://www.deeplearning.ai/) to create a short course with [Sebastian](https://www.linkedin.com/in/sebawita/) and [Zain](https://www.linkedin.com/in/zainhas/), \"[Vector Databases: from Embeddings to Applications with Weaviate](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/).\" It\u2019s been a year packed with learning."], "query": "Who has been running introductory workshops on vector databases and Weaviate every week as part of the Weaviate Academy and Workshops?"}
{"relevant_passages": ["This seems like a weird point to make but it will be very important later on!\n\n![three images](./img/three_images.png)\n\nLearning the true underlying distribution of any set of images is not computationally feasible  because it requires you to consider every pixel of every image. However, if our model could learn the underlying distribution of the training set of images it could calculate the likelihood that any new image came from that set. It could also generate novel images that it thinks are most likely to belong to the training set. One way to do this, using the underlying distribution, would be to start off with static noise (an image with random pixel values) and then slightly alter pixel values over and over again while making sure each time you alter the pixel values it increases the likelihood of the overall image coming from the dataset - this is indeed what diffusion models do!\n\nThe question then becomes how diffusion models can learn (or even approximate) the underlying distribution of the images in your training set? The main insight behind how this happens is: if you take any image from your training set and add a small amount of random static noise to it you will create a new image that is slightly less likely - since images with random noise are unlikely to be seen in the training set."], "query": "How do diffusion models generate images that are likely to belong to a training set?"}
