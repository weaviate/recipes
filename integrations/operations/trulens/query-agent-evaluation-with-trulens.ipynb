{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/operations/trulens/query-agent-evaluation-with-trulens.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Agent Evaluation\n",
    "\n",
    "In this recipe, we extend the [E-Commerce Query Agent](../../../weaviate-services/agents/query-agent-get-started.ipynb) and show how to evaluate and improve its performance.\n",
    "\n",
    "We will use [TruLens](https://www.trulens.org/) to trace and evaluate the agent. By using TruLens we can identify opportunities to improve the agent, track our experiments, and compare across versions of the app.\n",
    "\n",
    "To evaluate the agent, we can access metadata from the intermediate steps in the trace. Then, we can use this metadata to evaluate things like the relevance of the filter used by the query agent, or the relevance of the intermediate search results.\n",
    "\n",
    "[Custom evaluations](https://www.trulens.org/component_guides/evaluation/feedback_implementations/custom_feedback_functions/) are particularly valuable here, because they allow us to easily extend existing feedbacks to unique scenarios. In this example, we show how to record a Query Agent run. We also show how to use custom instructions to customize an existing LLM judge to provide tailored feedback for our situation.\n",
    "\n",
    "By evaluating this ecommerce agent, we are able to identify opportunities for improvement when the search results include items that do not match what the customer is looking for.\n",
    "\n",
    "Follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Keys and Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trulens-core trulens-providers-openai trulens-dashboard weaviate-client weaviate-agents datasets pydantic==2.10.6 # note: pydantic < 2.11.0 is required for now due to compatibility issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"WEAVIATE_URL\"]=\"...\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"]=\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create weaviate client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.agents.query import QueryAgent\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.environ[\"WEAVIATE_URL\"],\n",
    "    auth_credentials=Auth.api_key(os.environ[\"WEAVIATE_API_KEY\"]),\n",
    "    headers=headers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "# Using `auto-schema` to infer the data schema during import\n",
    "client.collections.create(\n",
    "    \"Brands\",\n",
    "    description=\"A dataset that lists information about clothing brands, their parent companies, average rating and more.\",\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_weaviate(),\n",
    ")\n",
    "\n",
    "# Explicitly defining the data schema\n",
    "client.collections.create(\n",
    "    \"ECommerce\",\n",
    "    description=\"A dataset that lists clothing items, their brands, prices, and more.\",\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_weaviate(),\n",
    "    properties=[\n",
    "        Property(name=\"collection\", data_type=DataType.TEXT),\n",
    "        Property(\n",
    "            name=\"category\",\n",
    "            data_type=DataType.TEXT,\n",
    "            description=\"The category to which the clothing item belongs\",\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"tags\",\n",
    "            data_type=DataType.TEXT_ARRAY,\n",
    "            description=\"The tags that are assocciated with the clothing item\",\n",
    "        ),\n",
    "        Property(name=\"subcategory\", data_type=DataType.TEXT),\n",
    "        Property(name=\"name\", data_type=DataType.TEXT),\n",
    "        Property(\n",
    "            name=\"description\",\n",
    "            data_type=DataType.TEXT,\n",
    "            description=\"A detailed description of the clothing item\",\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"brand\",\n",
    "            data_type=DataType.TEXT,\n",
    "            description=\"The brand of the clothing item\",\n",
    "        ),\n",
    "        Property(name=\"product_id\", data_type=DataType.UUID),\n",
    "        Property(\n",
    "            name=\"colors\",\n",
    "            data_type=DataType.TEXT_ARRAY,\n",
    "            description=\"The colors on the clothing item\",\n",
    "        ),\n",
    "        Property(name=\"reviews\", data_type=DataType.TEXT_ARRAY),\n",
    "        Property(name=\"image_url\", data_type=DataType.TEXT),\n",
    "        Property(\n",
    "            name=\"price\",\n",
    "            data_type=DataType.NUMBER,\n",
    "            description=\"The price of the clothing item in USD\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "brands_dataset = load_dataset(\n",
    "    \"weaviate/agents\", \"query-agent-brands\", split=\"train\", streaming=True\n",
    ")\n",
    "ecommerce_dataset = load_dataset(\n",
    "    \"weaviate/agents\", \"query-agent-ecommerce\", split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "brands_collection = client.collections.get(\"Brands\")\n",
    "ecommerce_collection = client.collections.get(\"ECommerce\")\n",
    "\n",
    "with brands_collection.batch.dynamic() as batch:\n",
    "    for item in brands_dataset:\n",
    "        batch.add_object(properties=item[\"properties\"], vector=item[\"vector\"])\n",
    "\n",
    "with ecommerce_collection.batch.dynamic() as batch:\n",
    "    for item in ecommerce_dataset:\n",
    "        batch.add_object(properties=item[\"properties\"], vector=item[\"vector\"])\n",
    "\n",
    "failed_objects = brands_collection.batch.failed_objects\n",
    "if failed_objects:\n",
    "    print(f\"Number of failed imports: {len(failed_objects)}\")\n",
    "    print(f\"First failed object: {failed_objects[0]}\")\n",
    "\n",
    "print(f\"Size of the ECommerce dataset: {len(ecommerce_collection)}\")\n",
    "print(f\"Size of the Brands dataset: {len(brands_collection)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the Query Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.agents.query import QueryAgent\n",
    "from trulens.apps.app import instrument\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, client):\n",
    "        self.agent =  QueryAgent(\n",
    "            client=client,\n",
    "            collections=[\"ECommerce\", \"Brands\"],\n",
    "        )\n",
    "\n",
    "    @instrument\n",
    "    def run(self, query):\n",
    "        return self.agent.run(query)\n",
    "    \n",
    "    @instrument\n",
    "    def fetch_sources(self, agent_response): # fetch sources is unneccessary, but gives us more power to examine and evaluate the sources\n",
    "        sources = []\n",
    "        for source in agent_response.sources:\n",
    "            object_id = source.object_id\n",
    "            collection_name = source.collection\n",
    "            collection = client.collections.get(collection_name)\n",
    "            data_obj = collection.query.fetch_object_by_id(object_id)\n",
    "            sources.append(data_obj)\n",
    "        return sources\n",
    "    \n",
    "    @instrument\n",
    "    def run_and_fetch_sources(self, query):\n",
    "        agent_response = self.run(query)\n",
    "        self.fetch_sources(agent_response)\n",
    "        return agent_response\n",
    "    \n",
    "query_agent = Agent(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set the logging\n",
    "\n",
    "By default, TruLens will log traces and evaluations locally in a `sqlite` file.\n",
    "\n",
    "TruLens can also connect to any [SQL-alchemy database](https://www.trulens.org/component_guides/logging/where_to_log/) to store the logs, including [Snowflake](https://www.trulens.org/component_guides/logging/where_to_log/log_in_snowflake/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "# Set logging\n",
    "session = TruSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define evaluations\n",
    "\n",
    "For this example, we will define three feedback functions to use as evaluators.\n",
    "\n",
    "* Answer relevance will evaluate the relevance of the answer end-to-end\n",
    "* Filter relevance will assess the effectiveness of the filter produced as an intermediate step by the agent\n",
    "* Context relevance identifies the relevance of the search results produced as an intermediate step by the the agent\n",
    "\n",
    "In addition to scoring relevance, all three feedback functions will also provide chain-of-thought reasons to explain the scoring. These explanations, along with the scores will be accessible in the TruLens dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenAI-based feedback function collection class:\n",
    "fopenai = fOpenAI()\n",
    "\n",
    "# answer relevance\n",
    "f_answer_relevance = Feedback(fopenai.relevance_with_cot_reasons, name = \"Answer Relevance\").on_input().on(Select.RecordCalls.run.rets.final_answer)\n",
    "\n",
    "# filter relevance\n",
    "filter_relevance_custom_criteria = \"You are specifically gauging the relevance of the filter, described as a python list of dictionaries, to the query. The filter is a list of dictionaries, where each dictionary represents a filter condition. Each dictionary has three keys: 'operator', 'property_name', and 'value'. The 'operator' key is a string that represents the comparison operator to use for the filter condition. The 'property_name' key is a string that represents the property of the object to filter on. The 'value' key is a float that represents the value to compare the property to. The relevance score should be a float between 0 and 1, where 0 means the filter is not relevant to the query, and 1 means the filter is highly relevant to the query.\"\n",
    "\n",
    "f_filter_relevance = Feedback(fopenai.relevance_with_cot_reasons, name = \"Filter Relevance\",\n",
    "                              min_score_val=0,\n",
    "                              max_score_val=1,\n",
    "                              criteria = filter_relevance_custom_criteria,\n",
    "                              ).on_input().on(Select.RecordCalls.run.rets.searches[0][0].filters[0][0].collect())\n",
    "\n",
    "# context relevance\n",
    "f_context_relevance = (\n",
    "    Feedback(fopenai.context_relevance_with_cot_reasons, \n",
    "                 name = \"Context Relevance\",\n",
    "                 criteria=\"Evaluate the relevance of the individual SEARCH RESULT option to the query, regardless of whether the user requests multiple options. If the only issue is that the SEARCH RESULT does not provide a list of multiple options, return a RELEVANCE score of 3.\")\n",
    "                 .on_input()\n",
    "                 .on(Select.RecordCalls.fetch_sources.rets[:].properties)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Register the agent\n",
    "\n",
    "Registering the agent allows us to track our experiments. We'll give it a name, version and list the feedback functions we want to use to evaluate application runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.app import TruApp\n",
    "\n",
    "tru_agent = TruApp(\n",
    "    query_agent,\n",
    "    app_name=\"query agent\",\n",
    "    app_version=\"base\",\n",
    "    feedbacks=[f_answer_relevance, f_filter_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run and record the agent\n",
    "\n",
    "Using the `tru_agent` we just defined as a context manager, we can run our agent. This will record the traces and kick off evaluations for each time the agent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_agent as recording:\n",
    "    response = query_agent.run_and_fetch_sources(\"I like vintage clothes, can you list me some options that are less than $200?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run the dashboard\n",
    "\n",
    "TruLens ships with a Streamlit dashboard that can be launched using `run_dashboard`, and operates off the evaluation and trace logs.\n",
    "\n",
    "This is the primary way you can explore traces, view evaluation results, and compare app versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "run_dashboard(session)  # open a local streamlit app to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Identify issue using the TruLens dashboard\n",
    "\n",
    "By evaluating the query agent, we notice it occasionally returns non-clothing items even though the customer specifically asks for clothing.\n",
    "\n",
    "![trulens identify issues](images/trulens_weaviate_identify_issues.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Improve the agent\n",
    "\n",
    "Let's add additional instruction into the system prompt to help guide the agent to return only the type of result the user is looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.agents.query import QueryAgent\n",
    "from trulens.apps.app import instrument\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, client):\n",
    "        self.agent =  QueryAgent(\n",
    "            client=client,\n",
    "            collections=[\"ECommerce\", \"Brands\"],\n",
    "            system_prompt=\"You are a helpful assistant that always returns only results that match the user's query. For example, if the user asks for clothing, only return clothing.\"\n",
    "        )\n",
    "\n",
    "    @instrument\n",
    "    def run(self, query):\n",
    "        return self.agent.run(query)\n",
    "    \n",
    "    @instrument\n",
    "    def fetch_sources(self, agent_response): # fetch sources is unneccessary for running the agent, but gives us more power to examine and evaluate the sources\n",
    "        sources = []\n",
    "        for source in agent_response.sources:\n",
    "            object_id = source.object_id\n",
    "            collection_name = source.collection\n",
    "            collection = client.collections.get(collection_name)\n",
    "            data_obj = collection.query.fetch_object_by_id(object_id)\n",
    "            sources.append(data_obj)\n",
    "        return sources\n",
    "    \n",
    "    @instrument\n",
    "    def run_and_fetch_sources(self, query):\n",
    "        agent_response = self.run(query)\n",
    "        self.fetch_sources(agent_response)\n",
    "        return agent_response\n",
    "    \n",
    "query_agent = Agent(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Validate performance\n",
    "\n",
    "Last, we'll register the improved version of teh app and validate the performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.app import TruApp\n",
    "\n",
    "tru_agent = TruApp(\n",
    "    query_agent,\n",
    "    app_name=\"query agent\",\n",
    "    app_version=\"modified system prompt\",\n",
    "    feedbacks=[f_answer_relevance, f_filter_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_agent as recording:\n",
    "    response = query_agent.run_and_fetch_sources(\"I like vintage clothes, can you list me some options that are less than $200?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dashboard, we can compare application versions and their evaluation results.\n",
    "\n",
    "Comparing here, we see the context relevance improvement.\n",
    "\n",
    "![trulens validate](images/trulens_weaviate_validate.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weaviate_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
