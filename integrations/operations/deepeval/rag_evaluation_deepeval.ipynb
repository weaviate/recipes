{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/operations/deepeval/rag_evaluation_deepeval.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s49gpkvZ7q53"
      },
      "source": [
        "# RAG Evaluation with DeepEval\n",
        "\n",
        "# Tutorial Overview\n",
        "\n",
        "In this tutorial, we will use [DeepEval](https://docs.confident-ai.com/) to evaluate a RAG pipeline built with **Weaviate**. Our goal is to optimize RAG responses by selecting the appropriate hyperparameters. These hyperparameters include both:\n",
        "\n",
        "- **Generation Hyperparameters:** such as `model` and `prompt template`\n",
        "- **Retrieval Hyperparameters:** such as `top-K`, `embedding model`, and `chunk size`\n",
        "\n",
        "Evaluating will allow us to identify and pick the best hyperparameters that optimize our RAG pipeline performance.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Define **DeepEval [metrics](https://docs.confident-ai.com/docs/metrics-contextual-precision)** to measure RAG performance\n",
        "2. Build a simple RAG pipeline with Weaviate  \n",
        "3. Run evaluations on the RAG pipeline using DeepEval metrics\n",
        "4. Optimize the hyperparameters based on evaluation results  \n",
        "\n",
        "DeepEval metrics work out of the box without any additional configuration. This example demonstrates the basics of using DeepEval. For more details on advanced usage, please visit the [docs](https://docs.confident-ai.com/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7fdas5wmIqC"
      },
      "source": [
        "# 1. Install packages and dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySlCIWZvm4XU"
      },
      "source": [
        "Begin by installing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-eMnl3EoMer"
      },
      "outputs": [],
      "source": [
        "!pip install -U deepeval weaviate-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqPhCyNFqmYE"
      },
      "source": [
        "# 2. Define DeepEval RAG Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDCZ3KZAz0Zk"
      },
      "source": [
        "There are 2 types of RAG evaluation metrics:\n",
        "\n",
        "*   **Generator Metrics:** measures response generation quality\n",
        "*   **Retrieval Metrics:** measures retriever generation quality\n",
        "\n",
        "We'll be using both of these metrics in this tutorial to evaluate our RAG.\n",
        "\n",
        "DeepEval metrics are **powered by LLMs**. You can use any LLM, but for this tutorial we'll be using `gpt-4o` as the default model.\n",
        "\n",
        "Begin by setting your `OPENAI_API_KEY`. Once this environment variable is set, `gpt-4o` will automatically be used as the default model for running these metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_E9HWwi5s7bA"
      },
      "outputs": [],
      "source": [
        "# Export the API key to an environment variable\n",
        "openai_api_key = \"Your OPENAI_API_KEY\"\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V21CBHMpuEPZ"
      },
      "source": [
        "Deepeval offers **2 generator metrics** to evaluate response generations:\n",
        "\n",
        "* [Answer Relevancy](https://docs.confident-ai.com/docs/metrics-answer-relevancy): evaluates how relevant the output of your LLM application is compared to the provided input.\n",
        "* [Faithfulness](https://docs.confident-ai.com/docs/metrics-faithfulness): evaluates whether the actual output factually aligns with the contents of your retrieval_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kM9_55N8utbc"
      },
      "outputs": [],
      "source": [
        "from deepeval.metrics import (\n",
        "    AnswerRelevancyMetric,\n",
        "    FaithfulnessMetric\n",
        ")\n",
        "\n",
        "# Initialize the generator metrics\n",
        "answer_relevancy = AnswerRelevancyMetric(threshold=0.7)\n",
        "faithfulness = FaithfulnessMetric(threshold=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99lPNzQ1ib0"
      },
      "source": [
        "...and **3 RAG retriever metrics metrics** to measure retrieval:\n",
        "\n",
        "* [Contextual Precision](https://docs.confident-ai.com/metrics/contextual-precision): Ensures the most relevant information are ranked higher than the irrelevant ones.\n",
        "* [Contextual Recall](https://docs.confident-ai.com/metrics/contextual-recall): Measures how well the retrieved information aligns with the expected LLM output\n",
        "* [Contextual Relevancy](https://docs.confident-ai.com/metrics/contextual-relevancy): Checks how well the retrieved context aligns with the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Psky_BSzx1z-"
      },
      "outputs": [],
      "source": [
        "from deepeval.metrics import (\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric\n",
        ")\n",
        "\n",
        "# Initialize the retriever metrics\n",
        "contextual_precision = ContextualPrecisionMetric(threshold=0.7)\n",
        "contextual_recall = ContextualRecallMetric(threshold=0.7)\n",
        "contextual_relevancy = ContextualRelevancyMetric(threshold=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaTFHLJC-Mgi"
      },
      "source": [
        "# 3. Defining your Weaviate RAG Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fprfgM1SJs"
      },
      "source": [
        "With the metrics defined, we can start building our RAG pipeline. In this tutorial, we'll construct and evaluate a QA RAG system designed to answer questions about git. Begin by defining the Weaviate client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dvMQkiE0ls1P"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WCD_URL\"] = \"Your Weaviate URL\"\n",
        "os.environ[\"WCD_API_KEY\"] = \"Your Weaviate API Key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "K9Q1p2C9-wce"
      },
      "outputs": [],
      "source": [
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=os.environ[\"WCD_URL\"],\n",
        "    auth_credentials=Auth.api_key(os.environ[\"WCD_API_KEY\"]),\n",
        "    headers={\n",
        "        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzq2Z1wBs3M"
      },
      "source": [
        "### Download & Chunk\n",
        "\n",
        "Next, we'll populate our Weaviate client with information chunks about Git. To do this, we'll first create the chunks by downloading a chapter from the Pro Git book, cleaning the text, and then chunking it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uP_GTVRi-d96"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def download_and_chunk(src_url: str, chunk_size: int, overlap_size: int) -> List[str]:\n",
        "    import requests\n",
        "    import re\n",
        "\n",
        "    response = requests.get(src_url)  # Retrieve source text\n",
        "    source_text = re.sub(r\"\\s+\", \" \", response.text)  # Remove multiple whitespaces\n",
        "    text_words = re.split(r\"\\s\", source_text)  # Split text by single whitespace\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(text_words), chunk_size):  # Iterate through & chunk data\n",
        "        chunk = \" \".join(text_words[max(i - overlap_size, 0): i + chunk_size])  # Join a set of words into a string\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "pro_git_chapter_url = \"https://raw.githubusercontent.com/progit/progit2/main/book/01-introduction/sections/what-is-git.asc\"\n",
        "chunked_text = download_and_chunk(pro_git_chapter_url, 150, 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNGXJlZQgk9z"
      },
      "source": [
        "### Creating the Collection\n",
        "\n",
        "Once we have our chunks, we can define a collection for them. This collection will not only serve as our vector database and retrieval engine, but Weaviate also has **built-in RAG capabilities** that let you generate responses to queries automatically.\n",
        "\n",
        "\n",
        "When using RAG capabilities with Weaviate, it's important to designate your preferred generative module directly at the collection level. In the example below, the `GitBookChunk` collection is configured with `text2vec-openai` as the vectorizer and `generative-openai` as the generative module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL094odP7mZk"
      },
      "source": [
        "Let's define our collection and call it `GitBookChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "O7_rj18jz_fg"
      },
      "outputs": [],
      "source": [
        "import weaviate.classes as wvc\n",
        "\n",
        "collection_name = \"GitBookChunk\"\n",
        "\n",
        "if client.collections.exists(collection_name):  # In case we've created this collection before\n",
        "    client.collections.delete(collection_name)  # THIS WILL DELETE ALL DATA IN THE COLLECTION\n",
        "\n",
        "wvc.config.Configure.Generative.openai.model = \"gpt-3.5-turbo\"\n",
        "chunks = client.collections.create(\n",
        "    name=collection_name,\n",
        "    properties=[\n",
        "        wvc.config.Property(\n",
        "            name=\"chunk\",\n",
        "            data_type=wvc.config.DataType.TEXT\n",
        "        ),\n",
        "        wvc.config.Property(\n",
        "            name=\"chapter_title\",\n",
        "            data_type=wvc.config.DataType.TEXT\n",
        "        ),\n",
        "        wvc.config.Property(\n",
        "            name=\"chunk_index\",\n",
        "            data_type=wvc.config.DataType.INT\n",
        "        ),\n",
        "    ],\n",
        "    vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_openai(),  # Use `text2vec-openai` as the vectorizer\n",
        "    generative_config=wvc.config.Configure.Generative.openai(),  # Use `generative-openai` with default parameters\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlhLb7qm7pv6"
      },
      "source": [
        "Then, populate the collection with the text chunks we created and cleaned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaiPa86ttpbK"
      },
      "outputs": [],
      "source": [
        "chunks_list = list()\n",
        "for i, chunk in enumerate(chunked_text):\n",
        "    data_properties = {\n",
        "        \"chapter_title\": \"What is Git\",\n",
        "        \"chunk\": chunk,\n",
        "        \"chunk_index\": i\n",
        "    }\n",
        "    data_object = wvc.data.DataObject(properties=data_properties)\n",
        "    chunks_list.append(data_object)\n",
        "chunks.data.insert_many(chunks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkdykBS29FFp"
      },
      "source": [
        "Finally, run a test query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7i-nt19Bzy"
      },
      "outputs": [],
      "source": [
        "response = chunks.generate.fetch_objects(\n",
        "    limit=1,\n",
        "    grouped_task=\"What is git\"\n",
        ")\n",
        "print(response.generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhtaO3qC_Pw6"
      },
      "source": [
        "# 4. Evaluating the RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVS4rgHY_UaT"
      },
      "source": [
        "\n",
        "With the RAG collection pipeline ready, we can begin evaluating it. Evaluation consists of two main steps:\n",
        "\n",
        "1. **Test Case Preparation:**  \n",
        "   Prepare an input query along with the expected LLM response. Then, use the input to generate a response from your RAG pipeline, creating an `LLMTestCase` that contains:\n",
        "   - `input`\n",
        "   - `actual_output`\n",
        "   - `expected_output`\n",
        "   - `retrieval_context`\n",
        "\n",
        "2. **Test Case Evaluation:**  \n",
        "   Evaluate the test case using the selection of RAG metrics we previously defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etObIacvBS3x"
      },
      "source": [
        "### Test Case preparation\n",
        "\n",
        "Let's begin by defining an `input` and preparing an `expected_output` for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ShpEd9-y_Ph0"
      },
      "outputs": [],
      "source": [
        "input = \"How does Git work, and why is it so fast?\"\n",
        "expected_output = \"Git is a distributed version control system that manages project data by capturing snapshots of the entire filesystem with each commit. This snapshot-based approach, combined with the fact that nearly all operations are performed locally, enables Git to provide near-instantaneous responses even without a network connection.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuJUr5FZAhmI"
      },
      "source": [
        "Next, retrieve the `actual_output` and `retrieval_context` for this input by running a RAG query using the `chunks` collection we defined in the previous setion and create an `LLMTestCase` from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0-xp0vUhAkFy"
      },
      "outputs": [],
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Example usage\n",
        "response = chunks.generate.fetch_objects(\n",
        "    limit=1,\n",
        "    grouped_task=f\"Answer the following question using only the information contained in your chunks, in not more than 2 sentences: {input}\"\n",
        ")\n",
        "actual_output = response.generated\n",
        "retrieval_context = [o.properties['chunk'] for o in response.objects]\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=input,\n",
        "    actual_output=actual_output,\n",
        "    expected_output=expected_output,\n",
        "    retrieval_context=retrieval_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juvQN5_3upp6"
      },
      "outputs": [],
      "source": [
        "print(test_case)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n4MhhENCeXl"
      },
      "source": [
        "### Run Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9RdzKcsIiq2"
      },
      "source": [
        "To run evaluations, simply pass the test case and metrics into DeepEval's `evaluate` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWHI3aLDCeJQ"
      },
      "outputs": [],
      "source": [
        "from deepeval import evaluate\n",
        "\n",
        "evaluate(\n",
        "  [test_case],\n",
        "  [answer_relevancy, faithfulness, contextual_precision, contextual_recall, contextual_relevancy]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvHHaFpMDnju"
      },
      "source": [
        "# 6. Optimizing RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_20m8zPoJmY8"
      },
      "source": [
        "You may notice that the RAG model pipeline we created is performing well on some metrics while underperforming on others. This highlights the importance of iterating over different hyperparameters to determine which combinations yield the best scores across the board.\n",
        "\n",
        "Even though we defined several hyperparameters—such as the embedding model and prompt template—let's iterate over **top-K (limit)** values to identify the best-performing top-K option across these metrics. This can be accomplished with a simple `for` loop in DeepEval.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84OKKFkHDye-"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "for top_k in [1, 3, 5, 7]:\n",
        "    response = chunks.generate.fetch_objects(\n",
        "        limit=top_k,\n",
        "        grouped_task=f\"Answer the following question using only the information contained in your chunks, in not more than 2 sentences: {input}\"\n",
        "    )\n",
        "    actual_output = response.generated\n",
        "    retrieval_context = [o.properties['chunk'] for o in response.objects]\n",
        "\n",
        "    test_case = LLMTestCase(\n",
        "        input=input,\n",
        "        actual_output=actual_output,\n",
        "        expected_output=expected_output,\n",
        "        retrieval_context=retrieval_context\n",
        "    )\n",
        "\n",
        "    evaluate([test_case], [contextual_precision, contextual_recall, contextual_relevancy])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxag_cDh8XZQ"
      },
      "source": [
        "To optimize all hyperparameters, iterate over each one along with the metrics to find the optimal combination for your specific use case!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
