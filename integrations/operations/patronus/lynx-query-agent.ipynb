{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patronus `Lynx` Hallucination Detection\n",
    "\n",
    "In AI systems, the term **Hallucination** is used to describe cases where an AI model produces responses that are broadly defined as not factual. For example, if you ask an LLM, `What is the atomic number of oxygen?` and it returns `15`, that would be considered a **Hallucination**, as the correct atomic number of oxygen is `8`.\n",
    "\n",
    "\n",
    "**Hallucinations** are of course worrisome for all cases of AI systems, but they are especially important for systems that utilize Retreival-Augmented Generation (RAG). RAG is generally predicated upon the promise of equipping LLMs with knowledge about your particular context, and it is thus especially important to make sure this is indeed being achieved.\n",
    "\n",
    "A recent [paper from researchers at Yale University](https://arxiv.org/pdf/2504.17004) highlights that completely avoiding **Hallucinations** in LLMs is extremely difficult, if not impossible. It is very unlikely that the next generation of LLMs will magically solve this problem with increased compute, new neural network architectures, or more training data.\n",
    "\n",
    "Fortunately, we have several techniques to combat **Hallucinations** that are getting stronger and stronger, especially in the context of RAG. In this notebook, we will firstly present the `Lynx` model from Patronus AI. As explained further in their [technical report](https://arxiv.org/abs/2407.08488), `Lynx` is a state-of-the-art model for **Hallucination** detection.\n",
    "\n",
    "From the Weaviate side of the house, we have kicked off our efforts to fight **Hallucinations** with the Weaviate Query Agent by instructing the model to cite its sources with a structured output model and return a `sources` output to the user in the final response.\n",
    "\n",
    "This notebook will illustrate how to evaluate if the Weaviate Query Agent is **hallucinating** by connecting the returned `sources` with `Lynx` and Patronus AI's observability platform!\n",
    "\n",
    "> This notebook is using patronus `0.1.3`\n",
    "\n",
    "Authored by Connor Shorten and Josh Goldstein. May 6th, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Patronus AI](./images/patronus-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of Patronus AI's `Lynx` model\n",
    "\n",
    "This Python code uses the Patronus library to evaluate AI responses for hallucinations. It sets up a hallucination detector that checks if an AI's answer about `the largest animal in the world` (claiming it's \"the giant sandworm\") aligns with the provided factual context stating **blue whales** hold this title, rather than the fictional sandworms from Dune. The code demonstrates automated detection of when AI systems provide fictional information instead of factual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.01 pass_=False text_output=None metadata={'positions': [[0, 19]], 'extra': None, 'confidence_interval': None} explanation='\\'The context mentions that the blue whale is the largest known animal on the planet.\\', \\'The context also mentions that in the book Dune, Sandworms are the largest animals, but this is in a fictional context.\\', \"The answer \\'The giant sandworm\\' is not faithful to the context because it incorrectly identifies a fictional entity as the largest animal in the world, whereas the context clearly states that the blue whale is the largest known animal.\"' tags={} dataset_id=None dataset_sample_id=None evaluation_duration=datetime.timedelta(microseconds=940000) explanation_duration=datetime.timedelta(0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import patronus\n",
    "from patronus.evals import RemoteEvaluator\n",
    "\n",
    "patronus.init(\n",
    "    os.getenv(\"PATRONUS_API_KEY\")\n",
    ")\n",
    "\n",
    "patronus_evaluator = RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n",
    "# See other built-in evaluators here - https://docs.patronus.ai/docs/evaluation_api/reference_guide\n",
    "\n",
    "result = patronus_evaluator.evaluate(\n",
    "    task_input=\"What is the largest animal in the world?\",\n",
    "    task_context=[\"The blue whale is the largest known animal on the planet.\",\"In Dune by Frank Herbert, Sandworms are the largest animals - beware if you like spice!\"],\n",
    "    task_output=\"The giant sandworm.\",\n",
    "    gold_answer=\"\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Weaviate Blogs to Weaviate\n",
    "\n",
    "We will now test Hallucination Detection by answering questions about information contained in Weaviate's Blog Posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weaviate\n",
    "\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.getenv(\"WEAVIATE_URL\"),\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/collections/classes/config.py:1950: PydanticDeprecatedSince211: Accessing this attribute on the instance is deprecated, and will be removed in Pydantic V3. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported 1463 blog chunks into Weaviate.\n",
      "Upload time: 9.45 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "import weaviate\n",
    "import weaviate.collections.classes.config as wvcc\n",
    "from dotenv import load_dotenv\n",
    "from weaviate.classes.init import AdditionalConfig, Timeout\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "local_blogs = []\n",
    "\n",
    "# The blogs dataset can be found in recipes within `integrations/llm-agent-frameworks/data`\n",
    "# You can also get it from `github.com/weaviate-io/blog``\n",
    "main_folder_path = \"./blog/\"\n",
    "\n",
    "for i, folder_name in enumerate(os.listdir(main_folder_path)):\n",
    "    subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        index_file_path = os.path.join(subfolder_path, \"index.mdx\")\n",
    "        if os.path.isfile(index_file_path):\n",
    "            with open(index_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                local_blogs.append(\n",
    "                    {\n",
    "                        \"content\": content,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "if weaviate_client.collections.exists(\"Blogs\"):\n",
    "    weaviate_client.collections.delete(\"Blogs\")\n",
    "blogs = weaviate_client.collections.create(\n",
    "    name=\"Blogs\",\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_weaviate(),\n",
    "    properties=[\n",
    "        wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "    ],\n",
    ")\n",
    "\n",
    "def chunk_text(text, max_tokens=300):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = enc.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunked_blogs = []\n",
    "for blog in local_blogs:\n",
    "    chunks = chunk_text(blog[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        chunked_blogs.append({\n",
    "            \"content\": chunk\n",
    "        })\n",
    "\n",
    "start_time = time.time()\n",
    "with weaviate_client.batch.dynamic() as batch:\n",
    "    for blog_chunk in chunked_blogs:\n",
    "        batch.add_object(\n",
    "            collection=\"Blogs\",\n",
    "            properties={\n",
    "                \"content\": blog_chunk[\"content\"],\n",
    "            }\n",
    "        )\n",
    "end_time = time.time()\n",
    "upload_time = end_time - start_time\n",
    "\n",
    "print(f\"Successfully imported {len(chunked_blogs)} blog chunks into Weaviate.\")\n",
    "print(f\"Upload time: {upload_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaviate Query Agent\n",
    "\n",
    "The Weaviate Query Agent returns meta information about its execution in addition to the `final_answer`, such as `sources`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ” Original Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚                                                                                                                 â”‚\n",
       "â”‚ How does HNSW work?                                                                                             â”‚\n",
       "â”‚                                                                                                                 â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ” Original Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚                                                                                                                 â”‚\n",
       "â”‚ How does HNSW work?                                                                                             â”‚\n",
       "â”‚                                                                                                                 â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span> ğŸ“ Final Answer <span style=\"color: #008080; text-decoration-color: #008080\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ HNSW, or Hierarchical Navigable Small World, is an algorithm designed for efficient approximate nearest         â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ neighbor (ANN) search in high-dimensional spaces. HNSW builds a hierarchical, multi-layered proximity graph to  â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ organize data vectors, enabling rapid navigation and search.                                                    â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ Here's how HNSW works:                                                                                          â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ - **Hierarchy of Layers:** The index consists of multiple graph layers. Upper layers contain nodes with         â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ long-range connections, while lower layers hold more densely connected nodes with shorter-range (local) edges.  â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ - **Search Process:** When searching for the nearest neighbors of a query vector, the search starts at the top  â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ layer, where the graph allows for large jumps across the data using long-range connections. As the algorithm    â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ descends layer by layer, it focuses more locally, narrowing the search to nodes more likely to be close to the  â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ query in high-dimensional space.                                                                                â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ - **Traversal:** This hierarchical structure enables the search to quickly zero in on relevant regions of the   â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ dataset, significantly reducing the number of comparisons required compared to a flat (single-layer) or         â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ brute-force search.                                                                                             â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ - **Tradeoff:** HNSW delivers very high recall (accuracy of finding true neighbors) with much improved search   â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ speed and low latency, by returning approximate (but very close) results rather than exact nearest neighbors.   â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ Overall, HNSW is well-suited for large-scale vector search scenarios, such as those found in modern vector      â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚ databases, where performance and scalability are critical.                                                      â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m ğŸ“ Final Answer \u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m                                                                                                                 \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mHNSW, or Hierarchical Navigable Small World, is an algorithm designed for efficient approximate nearest \u001b[0m\u001b[36m       \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mneighbor (ANN) search in high-dimensional spaces. HNSW builds a hierarchical, multi-layered proximity graph to \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36morganize data vectors, enabling rapid navigation and search.\u001b[0m\u001b[36m                                                   \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36m                                                                                                               \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mHere's how HNSW works:\u001b[0m\u001b[36m                                                                                         \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36m- **Hierarchy of Layers:** The index consists of multiple graph layers. Upper layers contain nodes with \u001b[0m\u001b[36m       \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mlong-range connections, while lower layers hold more densely connected nodes with shorter-range (local) edges.\u001b[0m\u001b[36m \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36m- **Search Process:** When searching for the nearest neighbors of a query vector, the search starts at the top \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mlayer, where the graph allows for large jumps across the data using long-range connections. As the algorithm \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mdescends layer by layer, it focuses more locally, narrowing the search to nodes more likely to be close to the \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mquery in high-dimensional space.\u001b[0m\u001b[36m                                                                               \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36m- **Traversal:** This hierarchical structure enables the search to quickly zero in on relevant regions of the \u001b[0m\u001b[36m \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mdataset, significantly reducing the number of comparisons required compared to a flat (single-layer) or \u001b[0m\u001b[36m       \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mbrute-force search.\u001b[0m\u001b[36m                                                                                            \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36m- **Tradeoff:** HNSW delivers very high recall (accuracy of finding true neighbors) with much improved search \u001b[0m\u001b[36m \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mspeed and low latency, by returning approximate (but very close) results rather than exact nearest neighbors.\u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36m                                                                                                               \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mOverall, HNSW is well-suited for large-scale vector search scenarios, such as those found in modern vector \u001b[0m\u001b[36m    \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m \u001b[0m\u001b[36mdatabases, where performance and scalability are critical.\u001b[0m\u001b[36m                                                     \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m\u001b[36m                                                                                                                 \u001b[0m\u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span> ğŸ”­ Searches Executed 1/1 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryResultWithCollection</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                                                                      â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚     </span><span style=\"color: #808000; text-decoration-color: #808000\">queries</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">=</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'how Hierarchical Navigable Small World (HNSW) algorithm works'</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">]</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">,                                  â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚     </span><span style=\"color: #808000; text-decoration-color: #808000\">filters</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">=</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">[[]]</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">,                                                                                               â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚     </span><span style=\"color: #808000; text-decoration-color: #808000\">filter_operators</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'AND'</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">,                                                                                     â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚     </span><span style=\"color: #808000; text-decoration-color: #808000\">collection</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Blogs'</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                                                                          â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">)</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                                                                                               â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mâ•­â”€\u001b[0m\u001b[37mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m ğŸ”­ Searches Executed 1/1 \u001b[37mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[37mâ”€â•®\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m                                                                                                                 \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[1;35mQueryResultWithCollection\u001b[0m\u001b[1;37m(\u001b[0m\u001b[37m                                                                                     \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m    \u001b[0m\u001b[33mqueries\u001b[0m\u001b[37m=\u001b[0m\u001b[1;37m[\u001b[0m\u001b[32m'how Hierarchical Navigable Small World \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHNSW\u001b[0m\u001b[32m)\u001b[0m\u001b[32m algorithm works'\u001b[0m\u001b[1;37m]\u001b[0m\u001b[37m,\u001b[0m\u001b[37m                                 \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m    \u001b[0m\u001b[33mfilters\u001b[0m\u001b[37m=\u001b[0m\u001b[1;37m[\u001b[0m\u001b[1;37m[\u001b[0m\u001b[1;37m]\u001b[0m\u001b[1;37m]\u001b[0m\u001b[37m,\u001b[0m\u001b[37m                                                                                              \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m    \u001b[0m\u001b[33mfilter_operators\u001b[0m\u001b[37m=\u001b[0m\u001b[32m'AND'\u001b[0m\u001b[37m,\u001b[0m\u001b[37m                                                                                    \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m    \u001b[0m\u001b[33mcollection\u001b[0m\u001b[37m=\u001b[0m\u001b[32m'Blogs'\u001b[0m\u001b[37m                                                                                         \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[1;37m)\u001b[0m\u001b[37m                                                                                                              \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m                                                                                                                 \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d75f5f; text-decoration-color: #d75f5f\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #d75f5f; text-decoration-color: #d75f5f\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #d75f5f; text-decoration-color: #d75f5f\">â”‚ ğŸ“Š No Aggregations Run                                                                                          â”‚</span>\n",
       "<span style=\"color: #d75f5f; text-decoration-color: #d75f5f\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #d75f5f; text-decoration-color: #d75f5f\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;167mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[38;5;167mâ”‚\u001b[0m\u001b[38;5;167m                                                                                                                 \u001b[0m\u001b[38;5;167mâ”‚\u001b[0m\n",
       "\u001b[38;5;167mâ”‚\u001b[0m\u001b[38;5;167m \u001b[0m\u001b[38;5;167mğŸ“Š No Aggregations Run\u001b[0m\u001b[38;5;167m                                                                                         \u001b[0m\u001b[38;5;167m \u001b[0m\u001b[38;5;167mâ”‚\u001b[0m\n",
       "\u001b[38;5;167mâ”‚\u001b[0m\u001b[38;5;167m                                                                                                                 \u001b[0m\u001b[38;5;167mâ”‚\u001b[0m\n",
       "\u001b[38;5;167mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span> ğŸ“š Sources <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚  - object_id='01a7eda0-729a-466a-bdfe-3ad8e18c36f6' collection='Blogs'                                          â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚  - object_id='ba0ef4c5-a4f0-45c6-8318-fd91e739ae64' collection='Blogs'                                          â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚  - object_id='f5c5d781-75eb-4728-a037-6ee2406e4c26' collection='Blogs'                                          â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚  - object_id='58d43298-0672-42f5-a0ed-2cccf49c4d16' collection='Blogs'                                          â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚                                                                                                                 â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mâ•­â”€\u001b[0m\u001b[37mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m ğŸ“š Sources \u001b[37mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[37mâ”€â•®\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m                                                                                                                 \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m - object_id='01a7eda0-729a-466a-bdfe-3ad8e18c36f6' collection='Blogs'\u001b[0m\u001b[37m                                         \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m - object_id='ba0ef4c5-a4f0-45c6-8318-fd91e739ae64' collection='Blogs'\u001b[0m\u001b[37m                                         \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m - object_id='f5c5d781-75eb-4728-a037-6ee2406e4c26' collection='Blogs'\u001b[0m\u001b[37m                                         \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m \u001b[0m\u001b[37m - object_id='58d43298-0672-42f5-a0ed-2cccf49c4d16' collection='Blogs'\u001b[0m\u001b[37m                                         \u001b[0m\u001b[37m \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m\u001b[37m                                                                                                                 \u001b[0m\u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   ğŸ“Š Usage Statistics   </span>\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ LLM Requests:  â”‚ 3    â”‚\n",
       "â”‚ Input Tokens:  â”‚ 8859 â”‚\n",
       "â”‚ Output Tokens: â”‚ 411  â”‚\n",
       "â”‚ Total Tokens:  â”‚ 9270 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m   ğŸ“Š Usage Statistics   \u001b[0m\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ LLM Requests:  â”‚ 3    â”‚\n",
       "â”‚ Input Tokens:  â”‚ 8859 â”‚\n",
       "â”‚ Output Tokens: â”‚ 411  â”‚\n",
       "â”‚ Total Tokens:  â”‚ 9270 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Total Time Taken:</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.</span><span style=\"color: #008080; text-decoration-color: #008080\">77s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mTotal Time Taken:\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m10.\u001b[0m\u001b[36m77s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from weaviate.agents.query import QueryAgent\n",
    "from weaviate.agents.utils import print_query_agent_response\n",
    "\n",
    "qa = QueryAgent(\n",
    "    client=weaviate_client, collections=[\"Blogs\"]\n",
    ")\n",
    "\n",
    "response = qa.run(\"How does HNSW work?\")\n",
    "print_query_agent_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sources` from the Query Agent\n",
    "\n",
    "The response from the Weaviate Query Agent contains a `sources` field that can be used to help users understand what influenced the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Source(object_id='01a7eda0-729a-466a-bdfe-3ad8e18c36f6', collection='Blogs'),\n",
       " Source(object_id='ba0ef4c5-a4f0-45c6-8318-fd91e739ae64', collection='Blogs'),\n",
       " Source(object_id='f5c5d781-75eb-4728-a037-6ee2406e4c26', collection='Blogs'),\n",
       " Source(object_id='58d43298-0672-42f5-a0ed-2cccf49c4d16', collection='Blogs')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to pass `sources` --> `Lynx` Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Result #1\n",
      "\n",
      "first step**, you can see that the entry point for the search is in the center, and then the long-range connections allow jumping to the edges. This means that when a query comes, it will quickly move in the appropriate direction.<br/>\n",
      "The **second**, **third**, and **final steps** highlight the nodes reachable within **three**, **six**, and **nine** hops from the entry node.\n",
      "\n",
      "HNSW, on the other hand, implements the same idea a bit differently. Instead of having all information together on a flat graph, it has a hierarchical representation distributed across multiple layers. The top layers only contain long-range connections, and as you dive deeper into the layers, your query is routed to the appropriate region where you can look more locally for your answer. So your search starts making only big jumps across the top layers until it finally looks for the closest points locally in the bottom layers.\n",
      "\n",
      "## Performance comparison\n",
      "So, how do they perform? Let's take a look in terms of speed as well as recall.\n",
      "\n",
      "The chart below illustrates a comparison of the C++ Vamana [reference code](https://github.com/microsoft/DiskANN) provided by Microsoft and our [HNSW implementation](https://github.com/weaviate/weaviate/tree/master/adapters/repos/db/vector/hnsw) when using Sift1M. Following Microsoftâ€™s experiments, we have used sift-query.fvecs (100,000 vectors sample) for building the index and sift-query.f\n",
      "\n",
      "Search Result #2\n",
      "\n",
      " pride ourselves on our research acumen and on providing state-of-the-art solutions. So we took time to explore these solutions to identify and evaluate the right building blocks for Weaviate's future.  Here we share some of our findings from this research.\n",
      "\n",
      "## On the HNSW vs. Vamana comparison\n",
      "As the first step to disk-based vector indexing, we decided to explore Vamana â€“ the algorithm behind the DiskANN solution. Here are some key differences between Vamana and HNSW:\n",
      "\n",
      "### Vamana indexing - in short:\n",
      "* Build a random graph.\n",
      "* Optimize the graph, so it only connects vectors close to each other.\n",
      "* Modify the graph by removing some short connections and adding some long-range edges to speed up the traversal of the graph.\n",
      "\n",
      "### HNSW indexing - in short:\n",
      "* Build a hierarchy of layers to speed up the traversal of the nearest neighbor graph.\n",
      "* In this graph, the top layers contain only long-range edges.\n",
      "* The deeper the search traverses through the hierarchy, the shorter the distance between vectors captured in the edges.\n",
      "\n",
      "Put simply, Vamana can build a flat graph, in contrast to HNSW, which uses a hierarchical representation. And a flat graph may suffer less performance degradation from being stored on disk than a hierarchical representation might. The reason is that since the outgoing connections from each node are known, it is possible to store the information in such a way that we can calculate the exact position on the\n",
      "\n",
      "Search Result #3\n",
      "\n",
      "NSW works by organizing vectors into a hierarchical, multi-layered graph structure, which allows for fast navigation through the dataset during search. The structure of HNSW balances longer distances for faster search in upper layers and shorter distances for accurate search in lower layers.\n",
      "\n",
      "In Weaviate's implementation, HNSW is enhanced to support full [CRUD operations](https://weaviate.io/blog/crud-support-in-weaviate) and allows for real-time querying, updates, and deletions, with features like incremental disk writes for crash recovery and [asynchronous cleanup processes](https://github.com/nmslib/hnswlib/issues/4#issuecomment-678315156) for maintaining index freshness.\n",
      "\n",
      "Check out [Weaviate ANN benchmarks](https://weaviate.io/developers/weaviate/benchmarks/ann) to see how HNSW performed on realistic large-scale datasets. You can use it to compare the tradeoffs between recall, QPS, latency, and import time.\n",
      "\n",
      "You will find it interesting to see that Weaviate can maintain very high recall rates (\\>95%), whilst keeping high throughput and low latency (both in milliseconds). That is exactly what you need for fast, but reliable vector search\\!\n",
      "\n",
      "If youâ€™re interested in benchmarking for your own dataset, [check out this webinar](https://events.weaviate.io/benchmarking-webinar).\n",
      "\n",
      "### ANN vs. KNN\n",
      "\n",
      "kNN, or [k-nearest neighbors (k\n",
      "\n",
      "Search Result #4\n",
      "\n",
      " other (e.g., in clusters or a graph), so that you can later find similar objects faster. This process is called vector indexing. Note that the speed gains are traded in for some accuracy because the ANN approach returns only the approximate results.\n",
      "\n",
      "In the previous example, one option would be to pre-calculate some clusters, e.g., animals, fruits, and so on. So, when you query the database for \"Kitten\", you could start your search by only looking at the closest animals and not waste time calculating the distances between all fruits and other non-animal objects. More specifically, the ANN algorithm can help you to start the search in a region near, e.g., four-legged animals. Then, the algorithm would also prevent you from venturing further from relevant results.\n",
      "\n",
      "![Vector indexing for Approximate Nearest Neighbor](img/vector-indexing.jpg)\n",
      "\n",
      "The above example roughly describes the Hierarchical Navigable Small World (HNSW) algorithm. It is also the default ANN algorithm in Weaviate. However, there are several ANN algorithms to index the vectors, which can be categorized into the following groups:\n",
      "* Clustering-based index (e.g., [FAISS](https://github.com/facebookresearch/faiss))\n",
      "* Proximity graph-based index (e.g., [HNSW](https://arxiv.org/abs/1603.09320))\n",
      "* Tree-based index (e.g., [ANNOY](https://github.com/spotify/annoy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "blogs_collection = weaviate_client.collections.get(\"Blogs\")\n",
    "\n",
    "def format_source_content(sources, collection):\n",
    "    \"\"\"\n",
    "    Formats the content from source objects into a clean, numbered format.\n",
    "    \n",
    "    Args:\n",
    "        sources: List of source objects with object_id attribute\n",
    "        collection: Weaviate collection to query\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string of all search results\n",
    "    \"\"\"\n",
    "    source_uuids = [source.object_id for source in sources]\n",
    "    \n",
    "    objects = collection.query.fetch_objects_by_ids(\n",
    "        source_uuids\n",
    "    )\n",
    "    \n",
    "    # Format the search results in a clean, numbered format\n",
    "    search_results = []\n",
    "    for i, o in enumerate(objects.objects, 1):\n",
    "        content = o.properties.get('content', '')\n",
    "        search_results.append(f\"Search Result #{i}\\n\\n{content}\\n\")\n",
    "    \n",
    "    # Join all results into a single string\n",
    "    formatted_results = \"\\n\".join(search_results)\n",
    "    return formatted_results\n",
    "\n",
    "# Use the function with the response sources\n",
    "formatted_results = format_source_content(response.sources, blogs_collection)\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Query Agent Hallucination with Patronus `Lynx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "True\n",
      "- The QUESTION asks about how HNSW works.\n",
      "- The CONTEXT provides detailed information about HNSW, including its purpose, structure, and search process.\n",
      "- The ANSWER summarizes the key points from the CONTEXT, explaining that HNSW is an algorithm for efficient approximate nearest neighbor (ANN) search in high-dimensional spaces.\n",
      "- The ANSWER describes the hierarchical structure of HNSW, mentioning the multiple layers and the transition from long-range connections in upper layers to shorter-range connections in lower layers.\n",
      "- The ANSWER explains the search process, starting at the top layer with long-range connections and gradually focusing more locally as the search descends through the layers.\n",
      "- The ANSWER highlights the benefits of HNSW, including high recall, improved search speed, and low latency, by returning approximate results rather than exact nearest neighbors.\n",
      "- The ANSWER concludes that HNSW is well-suited for large-scale vector search scenarios, aligning with the CONTEXT that mentions its application in modern vector databases.\n"
     ]
    }
   ],
   "source": [
    "result = patronus_evaluator.evaluate(\n",
    "    task_input=\"How does HNSW work?\",\n",
    "    task_context=[formatted_results],\n",
    "    task_output=response.final_answer,\n",
    "    gold_answer=\"\"\n",
    ")\n",
    "\n",
    "print(result.score)\n",
    "print(result.pass_)\n",
    "print(result.explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Patronus AI](./images/patronus-gui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run more tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACORN stands for ANN Constraint-Optimized Retrieval Network. It is a filter strategy used to speed up filtered vector search, especially in systems like Weaviate. ACORN enhances traditional HNSW (Hierarchical Navigable Small World) algorithms by maintaining high connectivity in the graph through techniques like two-hop neighborhood expansion and adaptive traversal, making vector and hybrid searches more efficient under filter conditions. Unlike some pre-existing methods, ACORN is filter-agnostic and does not require advance knowledge of filter criteria, enabling robust and high-performance search even with low correlation between filters and queries.\n",
      "Evaluating with Lynx...\n",
      "1.0\n",
      "True\n",
      "- The QUESTION asks for the meaning of the term \"ACORN\".\n",
      "- The CONTEXT provides information about a new filter strategy called ANN Constraint-Optimized Retrieval Network, which is referred to as ACORN.\n",
      "- The ANSWER states that ACORN stands for ANN Constraint-Optimized Retrieval Network, which directly matches the information provided in the CONTEXT.\n",
      "- The ANSWER also explains that ACORN is a filter strategy used to speed up filtered vector search, which is consistent with the CONTEXT discussing the challenges of filtered search and the introduction of ACORN as a solution.\n",
      "- The ANSWER mentions that ACORN enhances traditional HNSW algorithms by maintaining high connectivity in the graph through techniques like two-hop neighborhood expansion and adaptive traversal, which is supported by the CONTEXT describing the innovative features of ACORN.\n",
      "- The ANSWER also notes that ACORN is filter-agnostic and does not require advance knowledge of filter criteria, which is consistent with the CONTEXT stating that the search will determine which objects match the filter without needing to know the filter criteria in advance.\n"
     ]
    }
   ],
   "source": [
    "def run_and_evaluate_query(query):\n",
    "    # Run the query\n",
    "    response = qa.run(query)\n",
    "    \n",
    "    print(response.final_answer)\n",
    "    print(\"Evaluating with Lynx...\")\n",
    "    \n",
    "    # Format the source content\n",
    "    formatted_results = format_source_content(response.sources, blogs_collection)\n",
    "    \n",
    "    # Evaluate with Patronus\n",
    "    result = patronus_evaluator.evaluate(\n",
    "        task_input=query,\n",
    "        task_context=[formatted_results],\n",
    "        task_output=response.final_answer,\n",
    "        gold_answer=\"\"\n",
    "    )\n",
    "    \n",
    "    print(result.score)\n",
    "    print(result.pass_)\n",
    "    print(result.explanation)\n",
    "\n",
    "query = \"What does ACORN stand for?\"\n",
    "run_and_evaluate_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization is a technique used to compress data, particularly vectors, by reducing the amount of information stored per dimension.\n",
      "\n",
      "In the context of vectors (commonly used in machine learning and search applications), quantization works by encoding each dimension of a float vector with fewer bits. A common and powerful form is Binary Quantization (BQ), where each dimension is represented by a single bit: 1 if the value is positive and 0 if it is negative. This drastically reduces memory usageâ€”as much as 32x compared to storing each dimension as a 32-bit floatâ€”and accelerates computations, as operations on binary vectors (using bitwise operations like XOR) are extremely fast.\n",
      "\n",
      "Here's how the process essentially works: imagine each vector is like a home address, with each number describing how to reach a location in space. Normally, these details are very precise, but take up a lot of space. Quantization keeps only the sign (direction) for each dimension; with binary quantization, itâ€™s as simple as â€œgo positive or negative along each axis.â€ While this approach loses information about the magnitude (how far to go), it retains overall direction, which, especially for high-dimensional data, is often enough to distinguish between items with minimal overlap/collision.\n",
      "\n",
      "For example, a 384-dimensional float vector becomes a 384-bit binary code after quantization. While this makes the representation much smaller and faster to compare, it also means different vectors that only vary in magnitude (not sign) become indistinguishable, so there is a trade-off between accuracy (recall) and efficiency.\n",
      "\n",
      "Other quantization techniques, like Product Quantization (PQ), split the vector into sections (segments) and compress each one using clustering methods like KMeans, further reducing memory while preserving accuracy better than just retaining the sign.\n",
      "\n",
      "Quantization is most effective when the data is well distributed (often normalized). Otherwise, many vectors may collapse into the same binary code, making them indistinguishable.\n",
      "Evaluating with Lynx...\n",
      "1.0\n",
      "True\n",
      "- The QUESTION asks about how Quantization works.\n",
      "- The CONTEXT provides detailed information on various aspects of Quantization, specifically focusing on Binary Quantization (BQ) and Product Quantization (PQ).\n",
      "- The ANSWER explains that Quantization is a technique used to compress data, particularly vectors, by reducing the amount of information stored per dimension.\n",
      "- The ANSWER mentions that BQ encodes each dimension with fewer bits, using 1 for positive values and 0 for negative values, which significantly reduces memory usage and accelerates computations.\n",
      "- The ANSWER also describes how BQ works by retaining only the sign of each dimension, which is sufficient for distinguishing between vectors in high-dimensional data.\n",
      "- The ANSWER notes that while BQ loses information about the magnitude, it is often enough to distinguish between items with minimal overlap/collision.\n",
      "- The ANSWER briefly mentions PQ as another quantization technique that splits the vector into sections and compresses each one using clustering methods, which better preserves accuracy compared to just retaining the sign.\n",
      "- The ANSWER concludes that Quantization is most effective when the data is well distributed (often normalized), otherwise, many vectors may collapse into the same binary code, making them indistinguishable.\n",
      "- The ANSWER is faithful to the CONTEXT because it accurately summarizes the key points about how Quantization, specifically BQ and PQ, work, and the trade-offs involved.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does Quantization work?\"\n",
    "run_and_evaluate_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matryoshka embeddings, also known as embeddings trained with Matryoshka Representation Learning (MRL), are a type of vector embedding designed to offer flexible trade-offs between accuracy, storage size, and computational efficiency. Unlike traditional embeddings where all dimensions are equally important, Matryoshka embeddings are structured so that the information is concentrated in the earlier dimensions of the vector. Subsequent dimensions add finer details, much like adding higher resolutions to an image.\n",
      "\n",
      "This organization allows you to use only a subset of the vector's dimensionsâ€”such as the first 8, 16, 32, or any other prefixâ€”to achieve increasingly detailed representations, depending on your accuracy or resource requirements. This enables you to store more embeddings at lower precision or cost and to search faster, with minimal loss in accuracy.\n",
      "\n",
      "Matryoshka Representation Learning works by modifying the training loss function: the model is trained to perform well not just with the full-length embedding but with multiple truncated versions of it. As a result, a single trained model produces vectors where the first N dimensions contain as much meaningful information as possible for any choice of N up to the model's maximum length. For example, OpenAI's 'text-embedding-3-small' and 'text-embedding-3-large' models use this approach, allowing their output size to be configurable at inference time, so the user can select the dimensionality that best fits their application.\n",
      "Evaluating with Lynx...\n",
      "1.0\n",
      "True\n",
      "- The QUESTION asks about the definition and characteristics of Matryoshka embeddings.\n",
      "- The CONTEXT provides detailed information about Matryoshka embeddings, including their purpose, structure, and benefits.\n",
      "- The ANSWER summarizes the key points from the CONTEXT, explaining that Matryoshka embeddings are designed for flexible trade-offs between accuracy, storage size, and computational efficiency.\n",
      "- The ANSWER also mentions that Matryoshka embeddings are structured so that information is concentrated in earlier dimensions, with subsequent dimensions adding finer details, which aligns with the CONTEXT.\n",
      "- Both the CONTEXT and the ANSWER highlight the ability to use subsets of the vector dimensions to achieve different levels of detail, which is a key feature of Matryoshka embeddings.\n",
      "- The ANSWER includes an example of how OpenAI models use Matryoshka Representation Learning to produce configurable output sizes, which is consistent with the information in the CONTEXT.\n",
      "- Overall, the ANSWER is faithful to the CONTEXT as it accurately reflects the definition, structure, and benefits of Matryoshka embeddings as described in the CONTEXT.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are Matryoshka embeddings?\"\n",
    "run_and_evaluate_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
