{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/cloud-hyperscalers/azure/azure-function-calling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate Query Agent with Azure OpenAI\n",
    "\n",
    "This notebook will show you how to define the Weaviate Query Agent as a tool through Azure OpenAI.\n",
    "\n",
    "### Requirements\n",
    "1. Weaviate Cloud instance (WCD): The Weaviate Query Agent is only accessible through WCD at the moment. You can create a serverless cluster or a free 14-day sandbox [here](https://console.weaviate.cloud/).\n",
    "2. Have an Azure project org and `azure_endpoint` and API key. More details can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview).\n",
    "3. Install the OpenAI package and import the `AzureOpenAI` library\n",
    "4. Install the Weaviate Agents package with `pip install weaviate-agents`\n",
    "5. You'll need a Weaviate cluster with data. If you don't have one, check out [this notebook](integrations/Weaviate-Import-Example.ipynb) to import the Weaviate Blogs.\n",
    "\n",
    "### Azure OpenAI Resources\n",
    "[Learn more](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-ai-foundry) about the Azure AI Foundry platform and how you can build generative AI applications with it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate_agents.query import QueryAgent\n",
    "import os\n",
    "import json\n",
    "\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WEAVIATE_URL\"] = \"\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Azure OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "deployment_name = \"query-agent-gpt-4\" # define the deployment name you want to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Query Agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_agent_request(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a query to the database and get the response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question or query to search for in the database. This can be any natural language question related to the content stored in the database.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the database containing relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    # connect to your Weaviate Cloud instance\n",
    "    weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=os.getenv(\"WEAVIATE_URL\"), \n",
    "        auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    "        headers={ # add the API key to the model provider from your Weaviate collection, for example `headers={\"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\"}` \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # connect the query agent to your Weaviate collection(s)\n",
    "    query_agent = QueryAgent(\n",
    "        client=weaviate_client,\n",
    "        collections=[\"Blogs\"] \n",
    "    )\n",
    "    return query_agent.run(query).final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define conversation flow and tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_rmAbogLS00maXseTrH3HU3cy', function=Function(arguments='{\"query\":\"run Weaviate on Docker\"}', name='query_agent_request'), type='function')])\n",
      "Function arguments: {'query': 'run Weaviate on Docker'}\n",
      "To run Weaviate on Docker, you must have Docker installed and configured on your system. Here are the steps to get Weaviate running using Docker:\n",
      "\n",
      "1. **Pull the Weaviate image from Docker Hub:**\n",
      "   Start by pulling the official Weaviate image from Docker Hub. You can do this by running the following command in your terminal or command prompt:\n",
      "   ```bash\n",
      "   docker pull semitechnologies/weaviate:latest\n",
      "   ```\n",
      "\n",
      "2. **Run Weaviate container:**\n",
      "   Once the image is pulled, you can run a Weaviate container using the following command:\n",
      "   ```bash\n",
      "   docker run -d --name weaviate -p 8080:8080 semitechnologies/weaviate:latest\n",
      "   ```\n",
      "   This command starts a detached (`-d`) container named `weaviate` and maps port `8080` from the container to port `8080` on your host machine, allowing you to access the Weaviate instance through `http://localhost:8080`.\n",
      "\n",
      "3. **Verify the container is running:**\n",
      "   To check if the Weaviate container is up and running, you can use:\n",
      "   ```bash\n",
      "   docker ps\n",
      "   ```\n",
      "   This command will list all active containers. Look for `weaviate` in the list to confirm it's running.\n",
      "\n",
      "4. **Access Weaviate:**\n",
      "   Now that your Weaviate instance is running, you can access it via `http://localhost:8080`. You may use the Weaviate RESTful API or connect using client libraries available for various programming languages.\n",
      "\n",
      "5. **Custom configurations:**\n",
      "   If you need to customize your Weaviate instance, such as connecting it with external modules or changing default configurations, refer to the Weaviate documentation for the respective version.\n",
      "\n",
      "By following these steps, you will have a Weaviate instance running on Docker, ready for use in your projects.\n"
     ]
    }
   ],
   "source": [
    "def run_conversation():\n",
    "    # Initial user message\n",
    "    messages = [{\"role\": \"user\", \"content\": \"How can I run Weaviate on Docker?\"}]\n",
    "\n",
    "    # Define the function for the model\n",
    "    tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "        \"name\": \"query_agent_request\",\n",
    "        \"description\": \"Send a query to the database and get the response\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any question or query to search for in the database\"\n",
    "            }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "\n",
    "    print(\"Model's response:\")  \n",
    "    print(response_message)\n",
    "\n",
    "\n",
    "    if response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            if tool_call.function.name == \"query_agent_request\":\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"Function arguments: {function_args}\")  \n",
    "                weaviate_query = query_agent_request(\n",
    "                    query=function_args.get(\"query\")\n",
    "                )\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": \"query_agent_request\",\n",
    "                    \"content\": weaviate_query,\n",
    "                })\n",
    "    else:\n",
    "        print(\"No tool calls were made by the model.\")  \n",
    "\n",
    "    \n",
    "    final_response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages,\n",
    "    )\n",
    "\n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "print(run_conversation())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
