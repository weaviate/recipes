{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing this notebook, you will need to install LlamaIndex version `0.8.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options=weaviate.embedded.EmbeddedOptions()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema was created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"blogpost_IhnmuS2PG63J\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-09-22T15:24:08Z\",\"took\":35458}\n"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"BlogPost\",\n",
    "           \"description\": \"Blog post from the Weaviate website.\",\n",
    "           \"vectorizer\": \"text2vec-openai\",\n",
    "           \"moduleConfig\": {\n",
    "               \"generative-openai\": { \n",
    "                    \"model\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "           },\n",
    "           \"properties\": [\n",
    "               {\n",
    "                  \"name\": \"Content\",\n",
    "                  \"dataType\": [\"text\"],\n",
    "                  \"description\": \"Content from the blog post\",\n",
    "               }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.delete_all()\n",
    "\n",
    "client.schema.create(schema)\n",
    "\n",
    "print(\"Schema was created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/site-packages/pkg_resources/__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "blogs = SimpleDirectoryReader('./data').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weaviate Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "import os\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# construct vector store\n",
    "vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n",
    "\n",
    "# setting up the storage for the embeddings\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "\n",
    "# set up the index\n",
    "index = VectorStoreIndex.from_documents(blogs, storage_context = storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query without Self-Corrrecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ref2Vec is a method of representing a data object based on the objects it references. It uses the average, or centroid vector, of the cross-referenced vectors to represent the referencing object. This way, it can be used to find more relevant objects.\n"
     ]
    }
   ],
   "source": [
    "base_query_engine = index.as_query_engine()\n",
    "query = \"What is Ref2Vec?\"\n",
    "\n",
    "response = base_query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Self-Correcting Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation.guideline_eval import GuidelineEvaluator, DEFAULT_GUIDELINES\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.indices.query.query_transform.feedback_transform import (\n",
    "    FeedbackQueryTransformation,\n",
    ")\n",
    "from llama_index.query_engine.retry_query_engine import (\n",
    "    RetryGuidelineQueryEngine,\n",
    ")\n",
    "\n",
    "# Guideline eval\n",
    "guideline_eval = GuidelineEvaluator(\n",
    "    guidelines=DEFAULT_GUIDELINES + \"\\nThe response should try to summarize where possible.\\n\"\n",
    "    \"The response should mention Weaviate and not be too vauge.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guideline eval evaluation result: The response does not mention Weaviate and is too vague. It should provide more specific information and use statistics or numbers when possible. It should also try to summarize where possible.\n",
      "Transformed query: Here is a previous bad answer.\n",
      "\n",
      "Ref2Vec is a method of representing a data object based on the objects it references. It uses the average, or centroid vector, of the cross-referenced vectors to represent the referencing object. This way, it can be used to find more relevant objects.\n",
      "Here is some feedback from the evaluator about the response given.\n",
      "The response does not mention Weaviate and is too vague. It should provide more specific information and use statistics or numbers when possible. It should also try to summarize where possible.\n",
      "Now answer the question.\n",
      "\n",
      "What is Ref2Vec and how does it work with Weaviate?\n"
     ]
    }
   ],
   "source": [
    "typed_response = response if isinstance(response, Response) else response.get_response()\n",
    "eval = guideline_eval.evaluate_response(query, typed_response)\n",
    "print(f\"Guideline eval evaluation result: {eval.feedback}\")\n",
    "\n",
    "feedback_query_transform = FeedbackQueryTransformation(resynthesize_query=True)\n",
    "transformed_query = feedback_query_transform.run(query, {\"evaluation\": eval})\n",
    "print(f\"Transformed query: {transformed_query.query_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ref2Vec is a method of representing a data object based on the objects it references, developed by Weaviate. It uses the average, or centroid vector, of the cross-referenced vectors to represent the referencing object. This way, it can be used to find more relevant objects, such as in recommendation, knowledge graph representation, and representing long or complex multimodal objects. Ref2Vec combines vector search with the ability to link classes to other classes through cross-references, allowing for a better search experience.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 84.51%, threshold set to 80.00%\",\"path\":\"/home/vscode/.local/share/weaviate\",\"time\":\"2023-09-22T15:38:06Z\"}\n"
     ]
    }
   ],
   "source": [
    "retry_guideline_query_engine = RetryGuidelineQueryEngine(\n",
    "    base_query_engine, guideline_eval, resynthesize_query=True\n",
    ")\n",
    "retry_guideline_response = retry_guideline_query_engine.query(query)\n",
    "print(retry_guideline_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
