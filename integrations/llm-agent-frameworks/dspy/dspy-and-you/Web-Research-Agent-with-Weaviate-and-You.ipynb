{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f87cc0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/dspy/dspy-and-you/Web-Research-Agent-with-Weaviate-and-You.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354ca53",
   "metadata": {},
   "source": [
    "# Combining Web Search with Vector DBs! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3fc604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.4.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.you_rm import YouRM\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "\n",
    "gpt4 = dspy.OpenAI(model=\"gpt-4\", max_tokens=4000, model_type=\"chat\")\n",
    "\n",
    "you_rm = YouRM(ydc_api_key=you_api_key, k=1)\n",
    "weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "weaviate_rm = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "dspy.settings.configure(lm=gpt4, rm=weaviate_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f8826c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At this year’s International Conference on Learning Representations (ICLR), we are presenting an unsupervised sentence-pair model we call a trans-encoder (paper, code), which improves on the prior state of the art by up to 5% on sentence similarity benchmarks.\\nA tale of two encoders\\nToday, there are basically two paradigms for sentence-pair tasks: cross-encoders and bi-encoders. The choice between the two comes down to the standard trade-off between computational efficiency and performance.\\nCross-encoder. In a cross-encoder, two sequences are concatenated and sent in one pass to the sentence pair model, which is usually built atop a Transformer-based language model like BERT or RoBERTa. The attention heads of a Transformer can directly model which elements of one sequence correlate with which elements of the other, enabling the computation of an accurate classification/relevance score.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "you_rm(\"What are cross encoders?\").passages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1802e7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.\n"
     ]
    }
   ],
   "source": [
    "with dspy.context(rm=weaviate_rm):\n",
    "    print(dspy.Retrieve(k=5)(\"What are cross encoders?\").passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1907aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\"\"\"\n",
    "    \n",
    "    blog_context = dspy.InputField(desc=\"Context from Weaviate Blog Posts.\")\n",
    "    web_context = dspy.InputField(desc=\"Context from a Web Search.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A detailed answer that is supported by the context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deecdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=1)\n",
    "        self.you_rm = you_rm\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        blog_contexts = self.retrieve(question).passages\n",
    "        blog_contexts = \"\".join(blog_contexts)\n",
    "        web_contexts = self.you_rm(question).passages\n",
    "        web_contexts = \"\".join(web_contexts)\n",
    "        pred = self.generate_answer(blog_context=blog_contexts, web_context=web_contexts, question=question).answer\n",
    "        return dspy.Prediction(answer=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271e8a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cross encoders are ranking models used for content-based re-ranking. They work by concatenating two sequences and sending them in one pass to the sentence pair model, which is usually built atop a Transformer-based language model like BERT or RoBERTa. The attention heads of a Transformer can directly model which elements of one sequence correlate with which elements of the other, enabling the computation of an accurate classification/relevance score. However, a cross-encoder needs to compute a new encoding for every pair of input sentences, resulting in high computational overhead. Therefore, cross-encoding is often impractical for tasks like information retrieval and clustering, which involve massive pairwise sentence comparisons. Converting pretrained language models into cross-encoders also requires fine-tuning on annotated data.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG()(\"What are cross encoders?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9083353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Context: Context from Weaviate Blog Posts.\n",
      "\n",
      "Web Context: Context from a Web Search.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: A detailed answer that is supported by the context.\n",
      "\n",
      "---\n",
      "\n",
      "Blog Context: [Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well) 1. [Metadata Rankers](#metadata-rankers) 1. [Score Rankers](#score-rankers) ## Cross Encoders Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.\n",
      "\n",
      "Web Context: At this year’s International Conference on Learning Representations (ICLR), we are presenting an unsupervised sentence-pair model we call a trans-encoder (paper, code), which improves on the prior state of the art by up to 5% on sentence similarity benchmarks. A tale of two encoders Today, there are basically two paradigms for sentence-pair tasks: cross-encoders and bi-encoders. The choice between the two comes down to the standard trade-off between computational efficiency and performance. Cross-encoder. In a cross-encoder, two sequences are concatenated and sent in one pass to the sentence pair model, which is usually built atop a Transformer-based language model like BERT or RoBERTa. The attention heads of a Transformer can directly model which elements of one sequence correlate with which elements of the other, enabling the computation of an accurate classification/relevance score.However, a cross-encoder needs to compute a new encoding for every pair of input sentences, resulting in high computational overhead. Cross-encoding is thus impractical for tasks like information retrieval and clustering, which involve massive pairwise sentence comparisons. Also, converting pretrained language models (PLMs) into cross-encoders always requires fine-tuning on annotated data. Bi-encoder. By contrast, in a bi-encoder, each sentence is encoded separately and mapped to a common embedding space, where the distances between them can be measured. As the encoded sentences can be cached and reused, bi-encoding is much more efficient, and the outputs of a bi-encoder can be used off-the-shelf as sentence embeddings for downstream tasks. That said, it is well known that in supervised learning, bi-encoders underperform cross-encoders, since they don’t explicitly model interactions between sentences. Trans-encoder: The best of both worldsTrans-encoder: The best of both worlds In our ICLR paper, we ask whether we can leverage the advantages of both bi- and cross-encoders to bootstrap an accurate sentence-pair model in an unsupervised manner. Our answer — the trans-encoder — is built on the following intuition: As a starting point, we can use bi-encoder representations to fine-tune a cross-encoder. With its more powerful inter-sentence modeling, the cross-encoder should extract more knowledge from the PLMs than the bi-encoder can given the same input data. In turn, the more powerful cross-encoder can distill its knowledge back into the bi-encoder, improving the accuracy of the more computationally practical model. We can repeat this cycle to iteratively bootstrap from both the bi- and cross-encoders. Specifically, the process of training a trans-encoder is as follows:Improving unsupervised sentence-pair comparison Method that captures advantages of cross-encoding and bi-encoding improves on predecessors by as much as 5%. Many tasks in natural-language processing and information retrieval involve pairwise comparisons of sentences — for example, sentence similarity detection, paraphrase identification, question-answer entailment, and textual entailment. The most accurate method of sentence comparison is so-called cross-encoding, which maps sentences against each other on a pair-by-pair basis. Training cross-encoders, however, requires annotated training data, which is labor intensive to collect. How can we train completely unsupervised models for sentence-pair tasks, eliminating the need for data annotation?Step 1. Transform PLMs into effective bi-encoders. To transform existing PLMs into bi-encoders, we leverage a simple contrastive tuning procedure. Given a sentence, we encode it twice, with two different PLMs. Because of dropout — a standard technique in which a fraction of neural-network nodes are randomly dropped during each pass through the training data, to prevent bottlenecks — the two PLMs will produce slightly different encodings. The bi-encoder is then trained to maximize the similarity of the two almost-identical encodings. This step primes the PLMs to be good at embedding sequences. Details can be found in prior work Mirror-BERT and SimCSE.Step 2. Self-distillation: bi- to cross-encoder. After obtaining a reasonably good bi-encoder from step one, we use it to create training data for a cross-encoder. Specifically, we label sentence pairs with the pairwise similarity scores computed by the bi-encoder and use them as training targets for a cross-encoder built on top of a new PLM. Step 3. Self-distillation: Cross- to bi-encoder. A natural next step is to distil the extra knowledge gained from the cross-encoder back into bi-encoder form, which is more useful for downstream tasks. More important, a better bi-encoder can produce even more self-labeled data for tuning the cross-encoder. In this way we can repeat steps two and three, continually bootstrapping the encoder performance. Our paper proposes other techniques, such as mutual distillation, to improve our model’s performance. Please refer to Section 2.4 of the paper for more details. Benchmark: A new state-of-the-art for sentence similarityWe experiment with the trans-encoder on seven sentence textual similarity (STS) benchmarks. We observe significant improvements upon previous unsupervised sentence-pair models across all datasets. We also benchmark binary-classification and domain transfer tasks. Please refer to section 5 of the paper for more details.\n",
      "\n",
      "Question: What are cross encoders?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m understand what cross encoders are. We can gather from the blog context that cross encoders are well-known ranking models used for content-based re-ranking. The web context provides more detailed information about how cross encoders work. In a cross-encoder, two sequences are concatenated and sent in one pass to the sentence pair model, which is usually built atop a Transformer-based language model like BERT or RoBERTa. The attention heads of a Transformer can directly model which elements of one sequence correlate with which elements of the other, enabling the computation of an accurate classification/relevance score. However, a cross-encoder needs to compute a new encoding for every pair of input sentences, resulting in high computational overhead. Cross-encoding is thus impractical for tasks like information retrieval and clustering, which involve massive pairwise sentence comparisons. Also, converting pretrained language models (PLMs) into cross-encoders always requires fine-tuning on annotated data.\n",
      "\n",
      "Answer: Cross encoders are ranking models used for content-based re-ranking. They work by concatenating two sequences and sending them in one pass to the sentence pair model, which is usually built atop a Transformer-based language model like BERT or RoBERTa. The attention heads of a Transformer can directly model which elements of one sequence correlate with which elements of the other, enabling the computation of an accurate classification/relevance score. However, a cross-encoder needs to compute a new encoding for every pair of input sentences, resulting in high computational overhead. Therefore, cross-encoding is often impractical for tasks like information retrieval and clustering, which involve massive pairwise sentence comparisons. Converting pretrained language models into cross-encoders also requires fine-tuning on annotated data.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt4.inspect_history(n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
