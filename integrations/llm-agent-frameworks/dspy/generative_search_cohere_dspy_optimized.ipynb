{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86d46f0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/dspy/generative_search_cohere_dspy_optimized.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d94b091",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "\n",
    "There are 4 basic steps to compile an LLM program with DSPy.\n",
    "1. Connect DSPy to any LLMs or tools, such as Weaviate :), that you want to use.\n",
    "2. Load your dataset, wrapping each example in a `dspy.Example` object.\n",
    "3. Define your LLM program.\n",
    "4. Define your Metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38ed87",
   "metadata": {},
   "source": [
    "## 1. Connect DSPy to Command R and Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb52e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "\n",
    "# Connect LMs\n",
    "command_r = dspy.Cohere(model=\"command-r\", max_tokens=4000, api_key=cohere_api_key)\n",
    "gpt4 = dspy.OpenAI(model=\"gpt-4\", max_tokens=4000, model_type=\"chat\")\n",
    "\n",
    "# Connect to Weaviate\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "\n",
    "# Set defaults in DSPy\n",
    "dspy.settings.configure(lm=command_r, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896ac11",
   "metadata": {},
   "source": [
    "## 2. Load your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8daaae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "gold_answers = []\n",
    "queries = []\n",
    "\n",
    "for row in dataset:\n",
    "    gold_answers.append(row[\"gold_answer\"])\n",
    "    queries.append(row[\"query\"])\n",
    "    \n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "trainset, devset, testset = data[:25], data[25:35], data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1ca21",
   "metadata": {},
   "source": [
    "## 3. Define your LLM Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eaf63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the the context and answer the question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Helpful information for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A detailed answer that is supported by the context.\")\n",
    "    \n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=k)\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        pred = self.generate_answer(context=context, question=question).answer\n",
    "        return dspy.Prediction(context=context, answer=pred, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec5011",
   "metadata": {},
   "source": [
    "## 4. Define your Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b5aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypedEvaluator(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a system's answer to a question according to a given criterion.\"\"\"\n",
    "    \n",
    "    criterion: str = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    question: str = dspy.InputField(desc=\"The question asked to the system.\")\n",
    "    ground_truth_answer: str = dspy.InputField(desc=\"An expert written Ground Truth Answer to the question.\")\n",
    "    predicted_answer: str = dspy.InputField(desc=\"The system's answer to the question.\")\n",
    "    rating: float = dspy.OutputField(desc=\"A float rating between 1 and 5\")\n",
    "\n",
    "\n",
    "def MetricWrapper(gold, pred, trace=None):\n",
    "    alignment_criterion = \"How aligned is the predicted_answer with the ground_truth?\"\n",
    "    return dspy.TypedPredictor(TypedEvaluator)(criterion=alignment_criterion,\n",
    "                                          question=gold.question,\n",
    "                                          ground_truth_answer=gold.gold_answer,\n",
    "                                          predicted_answer=pred.answer).rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7268ce8c",
   "metadata": {},
   "source": [
    "# Great! You are all setup to use DSPy's Compilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2854af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Depth: 1/2.\n",
      "At Depth 1/2, Evaluating Prompt Candidate #1/3 for Predictor 1 of 1.\n",
      "Average Metric: 13.5 / 3  (450.0%)\n",
      "At Depth 1/2, Evaluating Prompt Candidate #2/3 for Predictor 1 of 1.\n",
      "Average Metric: 14.0 / 3  (466.7%)\n",
      "At Depth 1/2, Evaluating Prompt Candidate #3/3 for Predictor 1 of 1.\n",
      "Average Metric: 12.5 / 3  (416.7%)\n",
      "Iteration Depth: 2/2.\n",
      "At Depth 2/2, Evaluating Prompt Candidate #1/3 for Predictor 1 of 1.\n",
      "Average Metric: 13.0 / 3  (433.3%)\n",
      "At Depth 2/2, Evaluating Prompt Candidate #2/3 for Predictor 1 of 1.\n",
      "Average Metric: 14.0 / 3  (466.7%)\n",
      "At Depth 2/2, Evaluating Prompt Candidate #3/3 for Predictor 1 of 1.\n",
      "Average Metric: 14.5 / 3  (483.3%)\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import COPRO\n",
    "\n",
    "COPRO_teleprompter = COPRO(prompt_model=gpt4,\n",
    "                          metric=MetricWrapper,\n",
    "                          breadth=3,\n",
    "                          depth=2,\n",
    "                          init_temperature=0.7,\n",
    "                          verbose=False,\n",
    "                          track_stats=True)\n",
    "kwargs = dict(num_threads=1, display_progress=False)\n",
    "\n",
    "COPRO_compiled_RAG = COPRO_teleprompter.compile(RAG(), trainset=trainset[:3], eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc9574",
   "metadata": {},
   "source": [
    "# Save the Compiled Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197db9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"RAG-with-Command-R-Example.json\"\n",
    "COPRO_compiled_RAG.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58c11f",
   "metadata": {},
   "source": [
    "# Load and Print the Optimized RAG Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59c4c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carefully examine the context provided, identify the key points and themes, comprehend the nuances, and then formulate a precise, comprehensive, and accurate answer to the question, ensuring that your response is directly supported by the information in the context.\n"
     ]
    }
   ],
   "source": [
    "def GenerateAnswer_instruction_from_dspy_json(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data[\"generate_answer\"][\"signature_instructions\"]\n",
    "\n",
    "task_description = GenerateAnswer_instruction_from_dspy_json(\"RAG-with-Command-R-Example.json\")\n",
    "print(task_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a63f24",
   "metadata": {},
   "source": [
    "# Query in Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a47cbeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context discusses Product Quantization (PQ) as a technique for compressing vectors to reduce memory requirements. Here are the key points and themes:\n",
      "\n",
      "- Product Quantization involves chopping up a high-dimensional vector into smaller segments and then compressing each segment independently. This allows for significant memory savings.\n",
      "- The trade-off with PQ is that it can decrease recall due to the loss of information during compression. The \"rescoring trick\" mentioned in the context is likely a technique to mitigate this issue and improve the recall of the compressed vectors.\n",
      "- The performance and efficiency of PQ depend on various factors, including the number of centroids used and the cost of fitting the KMeans clustering algorithm.\n",
      "- The provided content includes a detailed explanation of how PQ works, along with visual aids, and also highlights the potential memory savings achieved through PQ.\n",
      "- The context concludes with a set of highlights, including plans to discuss alternative encoders based on data distribution to improve PQ performance.\n",
      "\n",
      "Overall, the key theme is the use of Product Quantization as a vector compression technique to reduce memory requirements, along with the associated trade-offs and techniques for improving performance.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Product Quantization? Why is it helpful for Vector Databases?\"\n",
    "task_description += \"\\nQuery: {query}\\nAnswer:\"\n",
    "\n",
    "weaviate_blogs = weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
    "\n",
    "response = weaviate_blogs.generate.near_text(\n",
    "    query = query,\n",
    "    limit = 3,\n",
    "    grouped_task=task_description\n",
    ")\n",
    "\n",
    "print(response.generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
