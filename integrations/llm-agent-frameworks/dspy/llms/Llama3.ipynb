{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf09b78",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/dspy/llms/Llama3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd62388",
   "metadata": {},
   "source": [
    "# Llama3\n",
    "\n",
    "Learn more about Llama3 in Meta's [release notes!](https://ai.meta.com/blog/meta-llama-3/)\n",
    "\n",
    "Massive thank you to our friends at [Ollama](https://ollama.com/library/llama3:latest) for supporting this so quickly!\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. Show you how to build a RAG system with Llama3, Ollama, Weaviate, and DSPy\n",
    "2. Use DSPy's MIPRO optimizer to find the optimal RAG prompt for Llama3\n",
    "\n",
    "Please note the optimal prompt is not the same for all language models! We have recently published a blog post explaining this [here](https://weaviate.io/blog/dspy-optimizers) if interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf650430",
   "metadata": {},
   "source": [
    "### Connect to Llama3 (hosted with Ollama) and Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fd556d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cshorten/Desktop/DSPy-local/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "llama3_ollama = dspy.OllamaLocal(model=\"llama3:8b-instruct-q5_1\", max_tokens=4000, timeout_s=480)\n",
    "\n",
    "import weaviate\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client, k=10)\n",
    "\n",
    "dspy.settings.configure(lm=llama3_ollama, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e402881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_ollama(\"say hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977072a",
   "metadata": {},
   "source": [
    "### Load Dataset (Questions derived from Weaviate's Blog Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62aa8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "gold_answers = []\n",
    "queries = []\n",
    "\n",
    "for row in dataset:\n",
    "    gold_answers.append(row[\"gold_answer\"])\n",
    "    queries.append(row[\"query\"])\n",
    "    \n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "trainset, devset, testset = data[:25], data[25:35], data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d58d96",
   "metadata": {},
   "source": [
    "# Metric to Assess Response Quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb88ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypedEvaluator(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a system's answer to a question according to a given criterion.\"\"\"\n",
    "    \n",
    "    criterion: str = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    question: str = dspy.InputField(desc=\"The question asked to the system.\")\n",
    "    ground_truth_answer: str = dspy.InputField(desc=\"An expert written Ground Truth Answer to the question.\")\n",
    "    predicted_answer: str = dspy.InputField(desc=\"The system's answer to the question.\")\n",
    "    rating: float = dspy.OutputField(desc=\"A float rating between 1 and 5. IMPORTANT!! ONLY OUTPUT THE RATING!!\")\n",
    "\n",
    "\n",
    "def MetricWrapper(gold, pred, trace=None):\n",
    "    alignment_criterion = \"How aligned is the predicted_answer with the ground_truth?\"\n",
    "    return dspy.TypedPredictor(TypedEvaluator)(criterion=alignment_criterion,\n",
    "                                          question=gold.question,\n",
    "                                          ground_truth_answer=gold.gold_answer,\n",
    "                                          predicted_answer=pred.answer).rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5922bc6",
   "metadata": {},
   "source": [
    "### DSPy RAG Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc8165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the the context and answer the question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Helpful information for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!\")\n",
    "    \n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=k)\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        pred = self.generate_answer(context=context, question=question).answer\n",
    "        return dspy.Prediction(context=context, answer=pred, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1620c3",
   "metadata": {},
   "source": [
    "# Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a74b7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Quantization is a technique that translates vectors into a binary sequence, where each element in the vector is represented as either 0 or 1. This process condenses the information in the original vector while preserving its semantic structure. The Hamming distance between two strings can be computed by comparing the position of each bit in the sequence.\n"
     ]
    }
   ],
   "source": [
    "print(RAG()(\"What is binary quantization?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8cb91",
   "metadata": {},
   "source": [
    "# Compile with MIPRO\n",
    "\n",
    "What is the optimal prompt for Llama3 when answering questions about Weaviate?\n",
    "\n",
    "Starting with the prompt,\n",
    "\n",
    "`Assess the context and answer the question.`\n",
    "\n",
    "DSPy's MIPRO optimizers finds better performance with,\n",
    "\n",
    "`Given the provided context, your task is to understand the content and accurately answer the question based on the information available in the context. You should use formal English with technical terminologies where necessary and provide a detailed, relevant response.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2a1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
      "\n",
      "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
      "\n",
      "\u001b[93m- Task Model: \u001b[94m\u001b[1m5\u001b[0m\u001b[93m examples in dev set * \u001b[94m\u001b[1m3\u001b[0m\u001b[93m trials * \u001b[94m\u001b[1m# of LM calls in your program\u001b[0m\u001b[93m = (\u001b[94m\u001b[1m15 * # of LM calls in your program\u001b[0m\u001b[93m) task model calls\u001b[0m\n",
      "\u001b[93m- Prompt Model: # data summarizer calls (max \u001b[94m\u001b[1m10\u001b[0m\u001b[93m) + \u001b[94m\u001b[1m3\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program = \u001b[94m\u001b[1m13\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
      "\n",
      "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
      "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
      "\n",
      "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
      "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
      "\n",
      "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
      "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
      "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
      "\n",
      "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False`.\u001b[0m\n",
      "\n",
      "\u001b[93mAwaiting your input...\u001b[0m\n",
      "\n",
      "Do you wish to continue? (y/n): y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:04<00:18,  4.71s/it]\u001b[A\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:04<00:17,  4.49s/it]\u001b[A\n",
      "[I 2024-04-18 15:30:02,492] A new study created in memory with name: no-name-8e977397-d3d2-48f3-b319-fe46d1f20a25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #0\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Average Metric: 4.0 / 1  (400.0):   0%|                   | 0/5 [00:07<?, ?it/s]\u001b[A\n",
      "Average Metric: 4.0 / 1  (400.0):  20%|██▏        | 1/5 [00:07<00:29,  7.39s/it]\u001b[A\n",
      "Average Metric: 8.0 / 2  (400.0):  20%|██▏        | 1/5 [00:14<00:29,  7.39s/it]\u001b[A\n",
      "Average Metric: 8.0 / 2  (400.0):  40%|████▍      | 2/5 [00:14<00:21,  7.18s/it]\u001b[A\n",
      "Average Metric: 12.0 / 3  (400.0):  40%|████      | 2/5 [00:22<00:21,  7.18s/it]\u001b[A\n",
      "Average Metric: 12.0 / 3  (400.0):  60%|██████    | 3/5 [00:22<00:15,  7.64s/it]\u001b[A\n",
      "Average Metric: 16.0 / 4  (400.0):  60%|██████    | 3/5 [00:28<00:15,  7.64s/it]\u001b[A\n",
      "Average Metric: 16.0 / 4  (400.0):  80%|████████  | 4/5 [00:28<00:06,  6.93s/it]\u001b[A\n",
      "Average Metric: 20.0 / 5  (400.0):  80%|████████  | 4/5 [00:46<00:06,  6.93s/it]\u001b[A\n",
      "Average Metric: 20.0 / 5  (400.0): 100%|██████████| 5/5 [00:46<00:00,  9.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-18 15:30:49,208] Trial 0 finished with value: 400.0 and parameters: {'5971981264_predictor_instruction': 1, '5971981264_predictor_demos': 0}. Best is trial 0 with value: 400.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting trial #1\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Average Metric: 4.0 / 1  (400.0):   0%|                   | 0/5 [00:09<?, ?it/s]\u001b[A\n",
      "Average Metric: 4.0 / 1  (400.0):  20%|██▏        | 1/5 [00:09<00:37,  9.43s/it]\u001b[A\n",
      "Average Metric: 8.0 / 2  (400.0):  20%|██▏        | 1/5 [00:19<00:37,  9.43s/it]\u001b[A\n",
      "Average Metric: 8.0 / 2  (400.0):  40%|████▍      | 2/5 [00:19<00:29,  9.80s/it]\u001b[A\n",
      "Average Metric: 12.0 / 3  (400.0):  40%|████      | 2/5 [00:27<00:29,  9.80s/it]\u001b[A\n",
      "Average Metric: 12.0 / 3  (400.0):  60%|██████    | 3/5 [00:27<00:18,  9.07s/it]\u001b[A\n",
      "Average Metric: 16.0 / 4  (400.0):  60%|██████    | 3/5 [00:35<00:18,  9.07s/it]\u001b[A\n",
      "Average Metric: 16.0 / 4  (400.0):  80%|████████  | 4/5 [00:35<00:08,  8.51s/it]\u001b[A\n",
      "Average Metric: 20.0 / 5  (400.0):  80%|████████  | 4/5 [00:52<00:08,  8.51s/it]\u001b[A\n",
      "Average Metric: 20.0 / 5  (400.0): 100%|██████████| 5/5 [00:52<00:00, 10.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-18 15:31:41,450] Trial 1 finished with value: 400.0 and parameters: {'5971981264_predictor_instruction': 1, '5971981264_predictor_demos': 2}. Best is trial 0 with value: 400.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting trial #2\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Average Metric: 2.0 / 1  (200.0):   0%|                   | 0/5 [00:11<?, ?it/s]\u001b[A\n",
      "Average Metric: 2.0 / 1  (200.0):  20%|██▏        | 1/5 [00:11<00:44, 11.25s/it]\u001b[A\n",
      "Average Metric: 6.0 / 2  (300.0):  20%|██▏        | 1/5 [00:18<00:44, 11.25s/it]\u001b[A\n",
      "Average Metric: 6.0 / 2  (300.0):  40%|████▍      | 2/5 [00:18<00:26,  8.86s/it]\u001b[A\n",
      "Average Metric: 10.0 / 3  (333.3):  40%|████      | 2/5 [00:22<00:26,  8.86s/it]\u001b[A\n",
      "Average Metric: 10.0 / 3  (333.3):  60%|██████    | 3/5 [00:22<00:13,  6.88s/it]\u001b[A\n",
      "Average Metric: 14.0 / 4  (350.0):  60%|██████    | 3/5 [00:33<00:13,  6.88s/it]\u001b[A\n",
      "Average Metric: 14.0 / 4  (350.0):  80%|████████  | 4/5 [00:33<00:08,  8.23s/it]\u001b[A\n",
      "Average Metric: 18.0 / 5  (360.0):  80%|████████  | 4/5 [00:52<00:08,  8.23s/it]\u001b[A\n",
      "Average Metric: 18.0 / 5  (360.0): 100%|██████████| 5/5 [00:52<00:00, 10.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-18 15:32:33,545] Trial 2 finished with value: 360.0 and parameters: {'5971981264_predictor_instruction': 0, '5971981264_predictor_demos': 2}. Best is trial 0 with value: 400.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Returning generate_answer = Predict(StringSignature(context, question -> answer\n",
      "    instructions='Given the provided context, your task is to understand the content and accurately answer the question based on the information available in the context. You should use formal English with technical terminologies where necessary and provide a detailed, relevant response.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'Helpful information for answering the question.', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!', '__dspy_field_type': 'output', 'prefix': 'The answer to the question based on the context is:'})\n",
      "))\n",
      "trial_logs[0][program].generate_answer = Predict(StringSignature(context, question -> answer\n",
      "    instructions='Given the provided context, your task is to understand the content and accurately answer the question based on the information available in the context. You should use formal English with technical terminologies where necessary and provide a detailed, relevant response.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'Helpful information for answering the question.', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!', '__dspy_field_type': 'output', 'prefix': 'The answer to the question based on the context is:'})\n",
      "))\n",
      "trial_logs[1][program].generate_answer = Predict(StringSignature(context, question -> answer\n",
      "    instructions='Given the provided context, your task is to understand the content and accurately answer the question based on the information available in the context. You should use formal English with technical terminologies where necessary and provide a detailed, relevant response.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'Helpful information for answering the question.', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!', '__dspy_field_type': 'output', 'prefix': 'The answer to the question based on the context is:'})\n",
      "))\n",
      "trial_logs[2][program].generate_answer = Predict(StringSignature(context, question -> answer\n",
      "    instructions='Assess the the context and answer the question.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'Helpful information for answering the question.', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")) from continue_program\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import MIPRO\n",
    "\n",
    "import openai\n",
    "gpt4 = dspy.OpenAI(model=\"gpt-4\", max_tokens=4000, model_type=\"chat\")\n",
    "\n",
    "teleprompter = MIPRO(prompt_model=gpt4, \n",
    "                     task_model=llama3_ollama, \n",
    "                     metric=MetricWrapper, \n",
    "                     num_candidates=3, \n",
    "                     init_temperature=0.5)\n",
    "kwargs = dict(num_threads=1, \n",
    "              display_progress=True, \n",
    "              display_table=0)\n",
    "MIPRO_compiled_RAG = teleprompter.compile(RAG(), trainset=trainset[:5], num_trials=3, max_bootstrapped_demos=1, max_labeled_demos=0, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74bbe748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cross Encoders are one of the most well-known ranking models for content-based re-ranking, achieving high in-domain accuracy. They can be used with Weaviate using a specific syntax and can benefit from being chained behind Bi-Encoders in a multistage search pipeline to retrieve a list of result candidates and then rerank them for more accurate results.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIPRO_compiled_RAG(\"what are cross encoders?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ef3cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Given the provided context, your task is to understand the content and accurately answer the question based on the information available in the context. You should use formal English with technical terminologies where necessary and provide a detailed, relevant response.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: Helpful information for answering the question.\n",
      "Question: ${question}\n",
      "The answer to the question based on the context is: A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «With a vector database like [Weaviate](/), you can store and retrieve vectors and data efficiently using Bi-Encoder models to encode data and queries. A search pipeline can then continue with a Cross-Encoder model which reranks a list of retrieved search result candidates. This blog post was inspired by [Nils Reimer's work on Bi-Encoders and Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html). import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "Question: what are cross encoders?\n",
      "The answer to the question based on the context is:\u001b[32m Cross Encoders are one of the most well-known ranking models for content-based re-ranking, achieving high in-domain accuracy. They can be used with Weaviate using a specific syntax and can benefit from being chained behind Bi-Encoders in a multistage search pipeline to retrieve a list of result candidates and then rerank them for more accurate results.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nGiven the provided context, your task is to understand the content and accurately answer the question based on the information available in the context. You should use formal English with technical terminologies where necessary and provide a detailed, relevant response.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: Helpful information for answering the question.\\nQuestion: ${question}\\nThe answer to the question based on the context is: A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!\\n\\n---\\n\\nContext:\\n[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\\n[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.»\\n[3] «With a vector database like [Weaviate](/), you can store and retrieve vectors and data efficiently using Bi-Encoder models to encode data and queries. A search pipeline can then continue with a Cross-Encoder model which reranks a list of retrieved search result candidates. This blog post was inspired by [Nils Reimer's work on Bi-Encoders and Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html). import WhatNext from '/_includes/what-next.mdx'\\n\\n<WhatNext />»\\nQuestion: what are cross encoders?\\nThe answer to the question based on the context is:\\x1b[32m Cross Encoders are one of the most well-known ranking models for content-based re-ranking, achieving high in-domain accuracy. They can be used with Weaviate using a specific syntax and can benefit from being chained behind Bi-Encoders in a multistage search pipeline to retrieve a list of result candidates and then rerank them for more accurate results.\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_ollama.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d6f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
