{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2403ee2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/dspy/llms/Gemini-1.5-Pro-and-Flash.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d6941",
   "metadata": {},
   "source": [
    "# Gemini Evaluation 5/17/24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fabbaf",
   "metadata": {},
   "source": [
    "Hey everyone! Welcome to our notebook evaluating updates to Gemini at Google I/O 2024. Gemini **1.5 Pro** has released a new state-of-the-art input length for LLMs with 2 million tokens. Gemini **Flash** has also been released, offering faster and cheaper inference with a 1 million token input window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cdbf3",
   "metadata": {},
   "source": [
    "### There are 3 main parts to this notebook:\n",
    "\n",
    "1. Needle in the Haystack Test\n",
    "2. Gemini for Re-Ranking\n",
    "3. Many-Shot In-Context Learning with Gemini and Command R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7daccd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e49476",
   "metadata": {},
   "source": [
    "See Google Models avialable with `generativeai`\n",
    "\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "\n",
    "for value in genai.list_models():\n",
    "    print(value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f9f45c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "gemini_pro_1_5 = dspy.Google(model=\"gemini-1.5-pro-latest\", api_key=google_api_key)\n",
    "gemini_flash = dspy.Google(model=\"gemini-1.5-flash-latest\", api_key=google_api_key)\n",
    "command_r = dspy.Cohere(model=\"command-r\", \n",
    "                        max_input_tokens=32_000, max_tokens=4_000, api_key=cohere_api_key)\n",
    "\n",
    "lms = [{\"name\": \"Gemini Flash\", \"lm\": gemini_flash},\n",
    "       {\"name\": \"Gemini Pro\", \"lm\": gemini_pro_1_5},\n",
    "       {\"name\": \"Command R\", \"lm\": command_r}]\n",
    "\n",
    "dspy.settings.configure(lm=gemini_pro_1_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9731acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mResult for Gemini Flash\n",
      "\n",
      "\u001b[0m## Making Approximate Nearest Neighbor Search Faster and Cheaper\n",
      "\n",
      "Approximate Nearest Neighbor Search (ANNS) is a crucial technique for many applications, but it can be computationally expensive. Here are some strategies to make it faster and cheaper:\n",
      "\n",
      "**1. Data Preprocessing:**\n",
      "\n",
      "* **Dimensionality Reduction:** Reduce the number of dimensions in your data using techniques like PCA, t-SNE, or Autoencoders. This can significantly speed up search and reduce storage costs.\n",
      "* **Data Clustering:** Cluster similar data points together. This allows you to search within smaller clusters, reducing the search space.\n",
      "* **Data Indexing:** Create efficient data structures like k-d trees, ball trees, or hash tables to index your data. This allows for faster retrieval of potential nearest neighbors.\n",
      "\n",
      "**2. Search Algorithm Optimization:**\n",
      "\n",
      "* **Approximate Search Algorithms:** Use algorithms like Locality Sensitive Hashing (LSH), k-d tree search with approximate distance calculations, or graph-based methods like HNSW. These algorithms trade accuracy for speed and efficiency.\n",
      "* **Multi-Probe LSH:** Instead of using a single hash function, use multiple hash functions to increase the probability of finding the nearest neighbor.\n",
      "* **Hierarchical Navigable Small World (HNSW):** This graph-based method offers excellent performance for high-dimensional data.\n",
      "\n",
      "**3. Hardware Acceleration:**\n",
      "\n",
      "* **GPU Acceleration:** Utilize the parallel processing power of GPUs to accelerate search operations.\n",
      "* **Specialized Hardware:** Consider using specialized hardware like Approximate Nearest Neighbor (ANN) chips designed for efficient search.\n",
      "\n",
      "**4. Trade-offs and Considerations:**\n",
      "\n",
      "* **Accuracy vs. Speed:** Choose the right balance between accuracy and speed based on your application's requirements.\n",
      "* **Data Size and Dimensionality:** Different algorithms perform better for different data sizes and dimensions.\n",
      "* **Computational Resources:** Consider the available computational resources and choose algorithms and hardware accordingly.\n",
      "\n",
      "**5. Open-Source Libraries and Tools:**\n",
      "\n",
      "* **Faiss (Facebook AI Similarity Search):** A library offering efficient implementations of various ANNS algorithms.\n",
      "* **Annoy (Approximate Nearest Neighbors Oh Yeah):** A library focused on speed and simplicity.\n",
      "* **ScaNN (Scalable Nearest Neighbors):** A library designed for large-scale datasets.\n",
      "* **Opensearch:** A search engine with built-in ANNS capabilities.\n",
      "\n",
      "**6. Cloud-Based Solutions:**\n",
      "\n",
      "* **Amazon Kendra:** A cloud-based search service with ANNS capabilities.\n",
      "* **Google Cloud Search:** A cloud-based search service with advanced search features.\n",
      "\n",
      "By implementing these strategies, you can significantly improve the speed and cost-effectiveness of your Approximate Nearest Neighbor Search. Remember to carefully consider your specific application requirements and choose the best approach for your needs.\n",
      " \n",
      "\n",
      "\u001b[91mResult for Gemini Pro\n",
      "\n",
      "\u001b[0m## Making Approximate Nearest Neighbor Search (ANNS) Faster and Cheaper:\n",
      "\n",
      "ANNS is a powerful tool for finding similar items in large datasets, but it can be computationally expensive. Here are some strategies to make it faster and cheaper:\n",
      "\n",
      "**1. Data Structures and Algorithms:**\n",
      "\n",
      "* **Tree-based methods:** KD-trees, Ball trees, and Vantage Point trees partition the data space for efficient search.\n",
      "* **Hashing-based methods:** Locality Sensitive Hashing (LSH) maps similar items to the same hash bucket, enabling fast approximate search.\n",
      "* **Graph-based methods:**  Represent data points as nodes in a graph and use graph traversal algorithms for search. HNSW (Hierarchical Navigable Small World) is a popular example.\n",
      "* **Quantization:** Reduce the dimensionality of data by representing vectors with fewer bits, leading to faster distance computations. Product Quantization (PQ) is a common technique.\n",
      "\n",
      "**2. Hardware Acceleration:**\n",
      "\n",
      "* **GPUs:** Leverage the parallel processing power of GPUs to accelerate distance calculations and search operations.\n",
      "* **FPGAs:**  Use Field-Programmable Gate Arrays for custom hardware implementations tailored to specific ANNS algorithms.\n",
      "\n",
      "**3. Algorithm Optimization:**\n",
      "\n",
      "* **Early stopping:** Terminate the search process early if a sufficiently good solution is found.\n",
      "* **Beam search:** Explore a limited number of promising paths during the search, reducing the search space.\n",
      "* **Approximate distance computations:** Use faster but less accurate distance metrics to speed up calculations.\n",
      "\n",
      "**4. Data Preprocessing:**\n",
      "\n",
      "* **Dimensionality reduction:** Techniques like PCA or SVD can reduce the number of dimensions, making search faster.\n",
      "* **Data normalization:** Ensure all features have similar scales to avoid bias towards certain dimensions.\n",
      "* **Clustering:** Pre-cluster the data to limit the search space for each query.\n",
      "\n",
      "**5. Hybrid Approaches:**\n",
      "\n",
      "* **Combine different ANNS methods:** Leverage the strengths of different algorithms by combining them in a multi-stage approach.\n",
      "* **Use exact search for small datasets:** For small datasets, exact nearest neighbor search might be faster than approximate methods.\n",
      "\n",
      "**6. Cloud-based Solutions:**\n",
      "\n",
      "* **Managed ANNS services:** Utilize cloud providers like AWS, Google Cloud, or Azure, which offer optimized ANNS services with scalable infrastructure.\n",
      "\n",
      "**7. Open-Source Libraries:**\n",
      "\n",
      "* **Faiss:** Facebook AI Similarity Search, a library with efficient implementations of various ANNS algorithms.\n",
      "* **Annoy:** Spotify's Approximate Nearest Neighbors Oh Yeah, a library focused on tree-based methods.\n",
      "* **NMSLIB:** Non-Metric Space Library, a library supporting various distance metrics and search algorithms.\n",
      "\n",
      "**Choosing the right approach depends on factors like:**\n",
      "\n",
      "* **Dataset size and dimensionality:** Larger datasets and higher dimensionality require more sophisticated techniques.\n",
      "* **Accuracy requirements:**  Higher accuracy demands might necessitate more complex algorithms.\n",
      "* **Computational resources:** Available hardware and budget constraints influence the choice of methods.\n",
      "* **Specific application:** The nature of the application and the desired use case can guide the selection of appropriate techniques.\n",
      "\n",
      "By carefully considering these factors and implementing the appropriate strategies, you can significantly improve the speed and cost-effectiveness of your ANNS solutions. \n",
      " \n",
      "\n",
      "\u001b[91mResult for Command R\n",
      "\n",
      "\u001b[0mApproximate Nearest Neighbor (ANN) search can be made faster and more efficient by employing various techniques and algorithms designed specifically for this purpose. Some of the most effective methods include:\n",
      "\n",
      "1. Indexing: Creating an index data structure that organizes the data points can significantly speed up the search. Popular indexing methods include KD-trees, Ball-trees, and Hierarchical Navigable Small World (HNSW) graphs. These data structures divide the high-dimensional space and provide efficient ways to navigate and narrow down the search region.\n",
      "\n",
      "2. Hashing: Hashing techniques transform high-dimensional data points into a shorter binary code. This enables faster comparisons between data points. Methods like Product Quantization and Hierarchical Navigable Hash Tables (HNHT) can further accelerate the search process.\n",
      "\n",
      "3. Graph-based Methods: These methods construct a graph that connects nearby points in the data space. Algorithms such as the Hierarchical Dual Tree and the Nearest Neighbor Graph (NNG) can find approximate nearest neighbors by traversing the graph.\n",
      "\n",
      "4. Locality-Sensitive Hashing (LSH): LSH is a popular technique that uses random projections to map similar data points to the same hash buckets. This enables efficient similarity searches, especially in high-dimensional spaces.\n",
      "\n",
      "5. Quantization: This technique involves representing data points with a smaller number of quantized values. Product Quantization and Scalar Quantization are two approaches that reduce the dimensionality of the search space, making the search more manageable.\n",
      "\n",
      "6. Hierarchical Navigation: Hierarchical methods divide the data into a hierarchy of clusters or regions. Nearest neighbor searches can be performed efficiently by navigating through the hierarchy and focusing on specific clusters or areas.\n",
      "\n",
      "7. Machine Learning-based Approaches: Some algorithms learn the intrinsic structure of the data and use this knowledge to speed up the ANN search. Deep learning models, such as the Navigation Net, can be trained to predict rough locations of nearest neighbors.\n",
      "\n",
      "8. Approximate Distance Calculations: Instead of computing exact distances, which can be computationally expensive, using approximate distance measures can save time. Techniques like SDHash provide fast and reasonably accurate distance estimates.\n",
      "\n",
      "9. Dimensionality Reduction: Reducing the dimensionality of the data can make the search space smaller and less complex. Techniques like Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) can aid in this process.\n",
      "\n",
      "10. Hybrid Methods: Combining multiple techniques, such as using hashing with graph-based methods or integrating learning-based approaches with traditional indexing structures, can lead to improved ANN search performance.\n",
      "\n",
      "The choice of method depends on the specific characteristics of the data and the problem at hand. Each technique has its strengths and weaknesses, so selecting the most suitable approach can make the search for approximate nearest neighbors faster and more cost-effective. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"How can you make Approximate Nearest Neighbor Search faster and cheaper?\"\n",
    "\n",
    "for lm_dict in lms:\n",
    "    lm, name = lm_dict[\"lm\"], lm_dict[\"name\"]\n",
    "    with dspy.context(lm=lm):\n",
    "        print(f\"\\033[91mResult for {name}\\n\")\n",
    "        print(f\"\\033[0m{lm(question)[0]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c932f",
   "metadata": {},
   "source": [
    "# Test 2M Token Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5512b9f6",
   "metadata": {},
   "source": [
    "# Needle in the Haystack Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cce129d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "\n",
    "weaviate_blog_index = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate.connect_to_local())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c368871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Check out one of our free weekly workshops to help you understand what vector databases are and how they can help you build production-ready AI apps quickly and easily. If you’re curious, here are some of the most commonly asked questions we encountered:\n",
      "\n",
      "**What’s the difference between a vector database and a graph or relational database?**\n",
      "\n",
      "Graph databases are used to identify relationships between objects, and vector databases are used to find objects\n",
      "Relational databases store the relations between tables and build indexes for the fast lookup of joined tables. Vector databases, on the other hand, store the embeddings of structured and unstructured data for the quick retrieval of the objects. **What features does Weaviate offer to help me protect data privacy?**\n",
      "\n",
      "Weaviate is designed with robust security measures to ensure it meets the requirements of enterprise environments. Weaviate has achieved SOC 2 certification and is encrypted in transit and at rest.\n",
      "[2] ---\n",
      "title: How to Reduce Memory Requirements by up to 90%+ using Product Quantization\n",
      "slug: pq-rescoring\n",
      "authors: [abdel, zain]\n",
      "date: 2023-09-19\n",
      "tags: ['research','engineering']\n",
      "image: ./img/hero.png\n",
      "description: \"The details behind how you can compress vectors using PQ with little loss of recall!\"\n",
      "---\n",
      "![Product Quanitzation in Weaviate - How to Reduce Memory Requirements by up to 90%+](./img/hero.png)\n",
      "\n",
      "\n",
      ":::info Highlights\n",
      "- [Product Quantization](#product-quantizationpq) - how PQ reduces the memory requirements of running Weaviate\n",
      "- [Improving PQ with Rescoring](#the-problem-with-pq) - why compressing vectors decreases recall and how the rescoring trick can help reduce this\n",
      "- [Experimental results](#experiment-results) - QPS vs. recall, indexing times, and memory savings\n",
      "- [Benchmarking PQ](#benchmarking-pq-with-your-own-data) - scripts for benchmarking PQ with your own data\n",
      ":::\n",
      "\n",
      "-----\n",
      "\n",
      "## 📚What is Product Quantization(PQ)? Product Quantization is a way to compress vectors, allowing users to save on memory requirements. To understand how product quantization (PQ) compression works, imagine every vector you want to store is a unique house address. This address allows you to precisely locate where someone lives including country, state, city, street number, and even down to the house number.\n",
      "[3] The sign of the number at that dimension tells you which direction to move and the magnitude of the number tells you how much to move in that direction. The process of binary quantization(BQ) takes all the numbers stored per dimension of a vector and only retains a 1(for positive values) or a 0(for negative values) capturing the sign of the corresponding number. That seems a bit radical, how can you ever locate a vector without information on how far along to move in the direction of an axis? Though BQ might sound like a crazy proposition, it does work particularly well for high-dimensional vectors. Let's see why!\n",
      "\n",
      "<img\n",
      "  src={require('./img/bq_vecs.png').default}\n",
      "  alt=\"bq vectors\"\n",
      "  style={{ maxWidth: \"80%\", margin: \"auto auto\", display: \"block\" }}\n",
      "/>\n",
      "\n",
      "**Fig**: *Elementwise binary quantization of a vector from float32 to 1 bit.*\n",
      "\n",
      "\n",
      ":::info Enabling Binary Quantization in Weaviate\n",
      "To learn how to enable and configure Weaviate to use BQ refer to the docs [here](/developers/weaviate/configuration/bq-compression).\n",
      "[4] - [Benchmarking BQ](#benchmarking-bq-with-your-own-data) - Scripts for benchmarking BQ with your own data. :::\n",
      "\n",
      "-----\n",
      "\n",
      "## 🧮What is Binary Quantization? To perform search current vector databases build massive vector indices and search over them in memory. This allows for real-time responses to queries however, compute costs can add up. Binary quantization(BQ) is a vector compression algorithm that allows you to trade-off between retrieval accuracy and memory requirements.\n",
      "[5] ---\n",
      "title: '32x Reduced Memory Usage With Binary Quantization'\n",
      "slug: binary-quantization\n",
      "authors: [abdel, zain]\n",
      "date: 2024-04-02\n",
      "tags: ['research','engineering']\n",
      "image: ./img/hero.png\n",
      "description: \"In-depth technical breakdown of how binary quantization works and how to use it in Weaviate.\"\n",
      "---\n",
      "![Binary Quanitzation in Weaviate - How to Reduce Memory Requirements 32x](./img/hero.png)\n",
      "\n",
      "\n",
      ":::info Overview\n",
      "- [What is Binary Quantization?](#what-is-binary-quantization) - How BQ reduces the memory requirements of running Weaviate. - [Details of Working with Binarized Vectors](#details-of-working-with-binarized-vectors) - What to consider when using binary quantization. - [Weaviate Performance Improvements with BQ](#-performance-improvements-with-bq) - QPS vs. recall, indexing times, and memory savings. - [Comparing Product and Binary Quantization](#%EF%B8%8Fcomparing-product-and-binary-quantization) - Experiments comparing PQ and BQ.\n",
      "[6] Let's explore how PQ works. ## Product Quantization\n",
      "![ann](./img/Ann.png)\n",
      "\n",
      "If you already know the details behind how Product Quantization works feel free to skip this section!\n",
      "\n",
      "The main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently.\n",
      "[7] The past for vector searching definitely was not a “simpler time”, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.\n",
      "[8] Further reading [Why Is Vector Search So Fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)\n",
      "\n",
      "## Use Cases of Vector Databases\n",
      "\n",
      "Vector databases’ search capabilities can be used in various applications ranging from classical Machine Learning use cases, such as natural language processing, computer vision, and recommender systems, to providing long-term memory to LLMs in modern applications. The **most popular use case of vector search engines is naturally for search**. Because a vector database can help find similar objects, it is predestined for applications where you might want to find similar products, movies, books, songs, etc. That’s why vector search engines are also **used in recommendation systems** as a restated task of search. With the rise of LLMs, **vector databases have already been used to enhance modern Generative AI applications**.\n",
      "[9] What simple operation does it spend the majority of its time performing? If you guessed **vector distance calculations** … BINGO! 🎉\n",
      "\n",
      "While vector databases use many techniques and algorithms to improve performance (including locality graphs, quantization, hash based approaches), at the end of the day, efficient distance calculations between high-dimensional vectors is a requirement for a good vector database. In fact, when profiling Weaviate indexed using HNSW, we find that 40%-60% of the CPU time is spent doing vector distance calculations. So when someone tells us that they can make this quintessential process *much faster* they have our full attention! If you want to learn how to leverage algorithmic and hardware optimizations to make vector search 40% faster keep reading!\n",
      "\n",
      "In this post we’ll do a technical deep dive into different implementations for vector distance calculations and optimizations enabled by Intel’s new 5th Gen Xeon Processor - Emerald Rapids, parallelization techniques using SIMD with the Intel AVX-256 and Intel AVX-512 instruction sets, loop unrolling and compiler optimizations by transpiling C to Go assembly. We explain how we attained a **~40% QPS speed up at 90% Recall** in Weaviate running on Intel’s new Xeon Processor, Emerald Rapids.\n",
      "[10] We covered core concepts around vector databases, such as vector embeddings, and discussed that vector databases enable efficient vector search by leveraging ANN algorithms. Additionally, we explored the tool landscape around vector databases and discussed the advantages of vector databases over traditional and vector-capable databases and vector libraries. import WhatNext from  '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />\n",
      "[11] **Quantization and rescoring are all the rage**. We received a lot of questions from people interested in optimizing efficiency of their AI setup and how these concepts work within Weaviate. If you’re curious, check out [this blog post about binary quantization](/blog/binary-quantization) and [this one about product quantization and rescoring](/blog/pq-rescoring). ## Breakout session: From RAG to Autonomous apps with Weaviate and Gemini on Google Kubernetes Engine\n",
      "\n",
      "<div className=\"youtube\">\n",
      "    <iframe src=\"//youtube.com/embed/WORgeRAAN-4?si=RVoqcV84bJIlaShE\" \n",
      "    frameBorder=\"0\"\n",
      "    width=\"700\"\n",
      "    height=\"360\" \n",
      "    allowFullScreen></iframe>\n",
      "</div>\n",
      "\n",
      "Our CEO Bob joined Brian Kaufman, Senior Product Manager from Google Cloud, to teach users how to:\n",
      "\n",
      "* Deploy Weaviate on Google Kubernetes Engine\n",
      "* Set up a pipeline to ingest data from the Cloud Storage bucket\n",
      "* Query, RAG, and enhance the responses… all with just a few lines of code! \n",
      "\n",
      "If you want to give it a try for yourself, [explore the GitHub repository here](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo). ## Our favorite Google Cloud product announcements\n",
      "Next is Google Cloud’s big annual conference, so of course we were waiting to see what new products and features would be announced at the keynote.\n",
      "[12] ## Weaviate Features to Improve Indexing Speed\n",
      "In this section, we walk through some of the key Weaviate features we used to improve the indexing performance while preserving search quality. 1. [Async Indexing](#async-indexing)\n",
      "2. [Product Quantization](#product-quantization)\n",
      "3. [Vector Index Configuration](#vector-index-configuration)\n",
      "4.\n",
      "[13] ### Product Quantization\n",
      "While async indexing helped with importing, [product quantization](/developers/weaviate/concepts/vector-quantization#product-quantization) (PQ) was important for alleviating the memory footprint. PQ is a technique used to compress the size of vector embeddings by representing the vectors as centroid ids. The centroid ids are calculated by clustering each segment. We enabled PQ to reduce the memory requirements without slowing down the ingestion. You can either [manually](/developers/weaviate/configuration/pq-compression#manually-configure-pq) enable PQ or have it [automatically triggered](/developers/weaviate/configuration/pq-compression#configure-autopq) based on the number of vectors.\n",
      "[14] Interestingly enough, it takes longer to render the results than it takes the vector database to find the answer. *Note, a semantic search is unlike a regular keywords search (which matches keywords like-for-like), but instead, we are searching for answers based on the semantic meaning of our query and data.*\n",
      "\n",
      "The inevitable question that follows up this demonstration is always:\n",
      "\n",
      "> Why is this so incredibly fast? ## What is a vector search? To answer this question we need to look at how vector databases work. Vector databases index data, unlike other databases, based on data vectors (or vector embeddings).\n",
      "[15] - Backend improvements: Numerous improvements to make updates, imports, and deletions faster. - Python client update: General availability, support for all the latest features. Question: Does Weaviate support Binary Quantization? Answer:\n",
      "```\n",
      "</details>\n",
      "\n",
      "## Setting up our Pipeline for Optimization\n",
      "This blog post will illustrate how the BootstrapFewShot, COPRO, and MIPRO optimizers work through the lens of a RAG example. BootstrapFewShot optimizes **Examples**, COPRO optimizes **Instructions**, and MIPRO optimizes both **Instructions and Examples**.\n",
      "[16] ### Vector Database vs. Vector-Capable Database (SQL and NoSQL)\n",
      "\n",
      "Today, many existing databases have already enabled vector support and vector search. However, they usually don’t index the vector embeddings, which makes the vector search slow. Thus, an advantage of AI-native vector databases over vector-capable databases is their efficiency in vector search due to vector indexing. ### Vector Database vs.\n",
      "[17] ---\n",
      "title: A Gentle Introduction to Vector Databases\n",
      "slug: what-is-a-vector-database\n",
      "authors: [leonie]\n",
      "date: 2023-08-01\n",
      "image: ./img/hero.png\n",
      "tags: ['concepts']\n",
      "description: \"Vector databases explained by their core concepts of vector embeddings, vector search, and vector indexing\"\n",
      "\n",
      "---\n",
      "\n",
      "import Tabs from '@theme/Tabs';\n",
      "import TabItem from '@theme/TabItem';\n",
      "import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';\n",
      "import PyCode from '!!raw-loader!/_includes/code/automated-testing.py';\n",
      "import TSCode from '!!raw-loader!/_includes/code/automated-testing.ts';\n",
      "\n",
      "![A Gentle Introduction to Vector Databases](img/hero.png)\n",
      "\n",
      "If you have just recently heard the term “vector database” for the first time, you are not alone. Although vector databases have been around for a few years, they’ve just recently drawn the wider developer community’s attention. The excitement around vector databases is closely related to the release of ChatGPT. Since late 2022, the public has started to understand the capabilities of state-of-the-art large language models (LLMs), while developers have realized that vector databases can enhance these models further. This article will walk through what vector databases are and explain some of their core concepts, such as vector embeddings and vector search.\n",
      "[18] ---\n",
      "title: Vamana vs. HNSW - Exploring ANN algorithms Part 1\n",
      "slug: ann-algorithms-vamana-vs-hnsw\n",
      "authors: [abdel]\n",
      "date: 2022-10-11\n",
      "tags: ['research']\n",
      "image: ./img/hero.png\n",
      "description: \"Vector search on disks: How does Vamana compare to HNSW?\"\n",
      "---\n",
      "![Vamana vs HNSW - Exploring ANN algorithms Part 1](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "Vector databases must be able to search through a vast number of vectors at speed. This is a huge technical challenge that is only becoming more difficult over time as the vector dimensions and dataset sizes increase. Like many others, our current prevailing solution is to use Approximate Nearest Neighbor (ANN) algorithms to power Weaviate. But the key question is - which ones to use?\n",
      "[19] In addition, our code undergoes constant security scanning and we conduct regular penetration testing ensuring our defenses are always up to date against evolving security threats. Since Weaviate is open-source technology that can be installed in any environment, it enables customers to keep all data locally, without having to send to a third party over the internet. **How does Weaviate compare to AWS?**\n",
      "\n",
      "Weaviate and AWS compliment each other nicely. You can use Weaviate as your vector database with Amazon Bedrock. Start building with the [`text2vec-aws` module](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-aws) today!\n",
      "\n",
      "**What sets Weaviate apart from other vector databases?**\n",
      "\n",
      "Weaviate is an open-source vector database, built from the ground up for vector search.\n",
      "[20] If not, that's okay - these guides will help you!\n",
      "* [Learn about Weaviate](/developers/weaviate/concepts)\n",
      "* [Weaviate Installation](/developers/weaviate/installation)\n",
      "* [Docker Installation](https://docs.docker.com/engine/install/)\n",
      "\n",
      "\n",
      "[Check out this article](/blog/why-is-vector-search-so-fast) to learn more about why vector databases are so fast and how they work. We will provide code snippets in this post, but you can check out the full code base in the [Weaviate Examples GitHub repo](https://github.com/weaviate/weaviate-examples) under the `nearest-neighbor-dog-search` directory. We encourage you to follow along with us by cloning the repository!\n",
      "\n",
      "This blog post covers:\n",
      "1. [Image Vectorization](#image-vectorization)\n",
      "2. [Weaviate Database](#weaviate-database)\n",
      "3.\n",
      "[21] ---\n",
      "title: How to Create Your Own ChatGPT Plugin\n",
      "slug: how-to-chatgpt-plugin\n",
      "authors: [shukri, zain]\n",
      "date: 2023-04-27\n",
      "image: ./img/hero.png\n",
      "tags: ['how-to', 'integrations']\n",
      "description: \"A show-and-tell of how we created the Weaviate Retrieval Plugin for ChatGPT\"\n",
      "\n",
      "---\n",
      "![How To Create ChatGPT Plugins](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "## Introduction\n",
      "\n",
      "A couple weeks back in our blog on [ChatGPT plugins](/blog/weaviate-retrieval-plugin) we talked about the potential for plugins to help expand ChatGPT’s functionality by allowing it to leverage third-party resources to act upon the conversations that you have with it. The value for these plugins is greatest when they help make up for a current short-coming that ChatGPT has. For example ChatGPT is built on top of GPT 4.0 which is a large language model which doesn’t understand mathematical and algebraic reasoning as well as it does written language and thus using the WolframAlpha plugin as a “math-mode” when needing to solve mathematical problems makes perfect sense!\n",
      "\n",
      "Another short-coming of ChatGPT we spoke about was that it lacks the use of context in answering your questions unless this context is specifically conveyed in the body of the prompt. The solution to this shortcoming was the [ChatGPT retrieval plugin](https://github.com/openai/chatgpt-retrieval-plugin) which connects ChatGPT to a [vector database](https://weaviate.io/blog/what-is-a-vector-database) which provides a robust solution to the above problem. The vector database connected to ChatGPT can be used to store and refer to relevant information when answering prompts and acts as longterm memory of the LLM.\n",
      "[22] ![vector-database-definition.png](img/vector-database-definition.png)\n",
      "\n",
      "Because of its search capabilities, a vector database is sometimes also called a vector search engine. This section introduces the core concepts of vector databases. We will discuss what vector embeddings are, how they enable similarity search, and how they accelerate it. ### Vector Embeddings\n",
      "\n",
      "When you think of data, you probably think of neatly organized numbers in spreadsheets. This type of data is called structured data because it can easily be stored in tabular format.\n",
      "[23] In this article, we will cover the need for disk-based solutions, explore Vamana (how it works and contrast it against HNSW), and present the result of Vamana implementation. > If you are new to vector databases and ANN, I recommend you to read \"[Why is Vector Search so Fast?](/blog/why-is-vector-search-so-fast)\"<br/>\n",
      "> The article explains what vector databases are and how they work. ## Need for approximation\n",
      "In order for a vector database to efficiently search through a vast number of vectors, the database needs to be smart about it. A brute-force approach would calculate the distance between the query vector and every data vector in the queried collection, but this is computationally very expensive. For a database with millions of objects and thousands of high-dimensional vectors, this would take far too long.\n",
      "[24] ### Indexing Knobs\n",
      "For the sake of designing RAG systems, the most important indexing knob looks like vector compression settings. Launched in March 2023, Weaviate 1.18 introduced Product Quantization (PQ). PQ is a vector compression algorithm that groups contiguous segments of a vector, clusters their values across the collection, and then reduces the precision with centroids. For example, a contiguous segment of 4 32-bit floats requires 16 bytes to represent, a segment length of 4 with 8 centroids results in only needing 1 byte, a 16:1 memory reduction. Recent advances in PQ Rescoring help significantly with recall loss from this compression, but is still an important consideration with very high levels of compression.\n",
      "[25] And that Weaviate recently added a new algorithm for how this is done? In this post, we’ll dive into exactly the world of hybrid search to discuss how it works, how results are produced, the algorithms used, and more. So let’s get into it!\n",
      "\n",
      ":::info\n",
      "- Vector search and keyword search are also known as dense vector search and sparse vector search respectively. - Keyword search is also called a BM25 search in Weaviate, as it is based on the [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25) scoring algorithm. :::\n",
      "\n",
      "## How does hybrid search work, exactly?\n",
      "[26] > Distance Metrics convey how similar or dissimilar two vector embeddings are. In this article, we explore the variety of distance metrics, the idea behind each, how they are calculated, and how they compare to each other. ## Fundamentals of Vector Search\n",
      "\n",
      "If you already have a working knowledge of vector search, then you can skip straight to the [Cosine Distance](#cosine-similarity) section. ### Vectors in Multi-Dimensional Space\n",
      "\n",
      "Vector databases keep the semantic meaning of your data by representing each object as a vector embedding. Each embedding is a point in a high-dimensional space.\n",
      "[27] ### Example Use Cases\n",
      "Vector libraries are commonly used for applications that do not have changing data. For example, academic information retrieval benchmarks are designed to test performance on a **static snapshot** of data. When plugging an ANN index into production-ready applications, databases offer many appealing features not found in a library. <img\n",
      "    src={require('./img/vector-library-vs-database.png').default}\n",
      "    alt=\"alt\"\n",
      "    style={{ maxWidth: \"75%\" }}\n",
      "/>\n",
      "\n",
      "## Vector Databases\n",
      "One of the core features that set vector databases apart from libraries is the ability to store and update your data. Vector databases have full **CRUD** (create, read, update, and delete) support that solves the limitations of a vector library.\n",
      "[28] Finding information you are looking for and providing recommendations is possible because knowledge and information is placed in context. ## How does Weaviate work? Data usually refers to something in the real world. Traditional databases and search engines often struggle with understanding the context of data, a situation or a search query. The context of data is important, because it enables storage and search by fuzzy concepts instead of exact matching keywords.\n",
      "[29] These databases (like Weaviate) were created specifically to store, manage, and retrieve vector embeddings, and do so faster and more efficiently than traditional databases. While vector search is becoming a popular offering in the database world, many “latched on” solutions don’t meet the requirements of modern AI-native development, from reduced developer experience to problems with speed and accuracy. Which brings us to our next point…\n",
      "\n",
      "## The market is confusing\n",
      "\n",
      "The AI space is moving fast. And with so many new technologies and vendors, it’s no wonder re:Invent attendees were telling us that they were getting overwhelmed with the options. Luckily, your friendly neighborhood Weaviate crew is here to help.\n",
      "[30] Each vector dimension dictates the colour gradient on the heatmap. The image below shows the same vector, but we threshold the vector such that all positive valued dimensions are turned into 1(white) and negative valued dimensions are turned to 0(black). As a result, we get a black-and-white heatmap that looks kind of like a bar code. This is what the process of binary quantization looks like for vector embeddings! The resulting vectors are much smaller but also less detailed. |Sentence: *\"All your vector embeddings belong to you!\"*  |\n",
      "|:-------------------------:|\n",
      "| ![OriginalVec](./img/image14.jpg)  | \n",
      "|  ![BinaryVec](./img/image7.png)    |\n",
      "**Fig**: *Visualization of a sentence vector embedding when binary quantized*\n",
      "\n",
      "The concept of binary quantization simplifies the encoding of vectors by retaining only their directionality.\n",
      "[31] ## Compiling Weaviate’s `generative-search` modules with DSPy\n",
      "This article has demonstrated a few tools DSPy gives you to produce an optimized prompt for RAG! Now that you have this prompt, you can plug it into Weaviate’s `generative-search` modules. Weaviate’s `generative-search` module, [released in March 2023](https://www.youtube.com/watch?v=ro3ln4A9N8w), is our solution to help developers build Stateful AI. After we compile a DSPy program, we save the instructions with:\n",
      "```python\n",
      "COPRO_compiled_RAG.save(\"My-RAG-Prompt.json\")\n",
      "```\n",
      "\n",
      "We then load it into Weaviate by reading the JSON file,\n",
      "```python\n",
      "def GenerateAnswer_instruction_from_dspy_json(file_path):\n",
      "    with open(file_path, \"r\") as file:\n",
      "        data = json.load(file)\n",
      "    return data[\"generate_answer\"][\"signature_instructions\"]\n",
      "\n",
      "task_description = GenerateAnswer_instruction_from_dspy_json(\"RAG-with-Command-R-Example.json\")\n",
      "```\n",
      "\n",
      "We can then query in Weaviate using the v4 Python client with:\n",
      "```\n",
      "weaviate_blogs = weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
      "\n",
      "query = \"What is Product Quantization? Why is it helpful for Vector Databases?\"\n",
      "extended_task_description = task_description + \"\\nQuery: {query}\\nAnswer:\"\n",
      "\n",
      "response = weaviate_blogs.generate.near_text(\n",
      "    query = query,\n",
      "    limit = 3,\n",
      "    grouped_task=task_description\n",
      ")\n",
      "\n",
      "print(response.generated)\n",
      "```\n",
      "\n",
      "## Beyond Prompting: Gradient Descent and DSPy\n",
      "In this blog post, we covered how we can optimize the prompt that we feed to the language model. However, there is one additional way that we can boost our RAG performance – fine tuning with gradient descent.\n",
      "[32] ![graf](./img/image11.png)\n",
      "**Fig. 10**: *Heap usage while loading the data into the Weaviate server. Memory does not grow smoothly. Instead there is a higher peak at the beginning before the vectors are compressed.*\n",
      "\n",
      "Lets focus the comparison in three directions:\n",
      "- How much longer does it take to index compressed data? - How much does it affect the recall and latency?\n",
      "[33] Our people and culture team opens up this part of our town hall meetings by asking, “Who is your Weaviate Hero” —> this is how the name for our Champions Program was set. With the Weaviate Hero Program coming to life, we want to expand this internal initiative and also give kudos to people in our Community who help other members learn, grow, and thrive by sharing their knowledge, providing feedback, collaborating, and leading by example. ## The Weaviate Hero Program - in a Nutshell\n",
      "\n",
      "The Weaviate Hero Program was brought to life to appreciate, recognize & celebrate members of our Community who contribute and engage in ways that:\n",
      "\n",
      "> *“Make our Community a great and safe place where people can learn, make friends, and grow. A place for giving and receiving.”*\n",
      "\n",
      "Community members who ultimately develop and share practices that help others in the group thrive. With this global program, we enable members to accelerate their knowledge, skills, and growth in the areas of Vector Databases, Search, and AI-native applications and become thought leaders through their activities and initiatives across the Weaviate Ecosystem.\n",
      "[34] Because search in traditional databases relies on keyword matches, you would have to create a big query that queries all animals (e.g., contains \"dog\", or contains \"cat\", or contains \"wolf\", etc.). With semantic search, you could simply query for the concept of \"animals\". ![Traditional search vs. vector search](img/Traditional_Search_X_Vector_Search.jpg)\n",
      "\n",
      "Because most vector databases do not only store vector embeddings but store them together with the original source data, they are not only capable of vector search but also enable traditional keyword search. Some vector databases like [Weaviate](http://weaviate.io) even support [hybrid search](https://weaviate.io/blog/hybrid-search-explained) capabilities that combine vector search with keyword search.\n",
      "[35] Then, we will dive into the technical details of how distance metrics are used in vector search and how vector indexes enable efficient retrieval. Once we have a fundamental understanding of vector databases, we will discuss their use cases and the current tool landscape. ## What is a Vector Database? A vector database indexes, stores, and provides access to structured or unstructured data (e.g., text or images) alongside its [vector embeddings](https://weaviate.io/blog/vector-embeddings-explained), which are the data's numerical representation. It allows users to find and retrieve similar objects quickly at scale in production.\n",
      "[36] Here is an example [demo](https://github.com/weaviate/healthsearch-demo) called [HealthSearch](https://weaviate.io/blog/healthsearch-demo) of a [Weaviate](http://weaviate.io) vector database in action together with an LLM showcasing the potential of leveraging user-written reviews and queries to retrieve supplement products based on specific health effects. ![example-use-case-llm-vector-database](../2023-07-26-healthsearch-blogpost/img/hero.png)\n",
      "\n",
      "## Tool Landscape around Vector Databases\n",
      "\n",
      "Although vector databases are AI-native and specifically designed to handle vector embeddings and enable efficient vector search, alternatives like [vector libraries](https://weaviate.io/blog/vector-library-vs-vector-database) and vector-capable databases exist as well. ### Vector Database vs. Traditional (Relational) Database\n",
      "\n",
      "The main difference between a traditional (relational) database and a modern vector database comes from the type of data they were optimized for. While a relational database is designed to store structured data in columns, a vector database is also optimized to store unstructured data (e.g., text, images, or audio) and their vector embeddings.\n",
      "[37] Because vector databases and relational databases are optimized for different types of data, they also differ in the way data is stored and retrieved. In a relational database, data is stored in columns and retrieved by keyword matches in traditional search. In contrast, vector databases also store the vector embeddings of the original data, which enables efficient semantic search. Because vector search is capable of semantic understanding your search terms, it doesn't rely on retrieving relevant search results based on exact matches, which makes it robust to typos and synonyms. For example, imagine you have a database that stores Jeopardy questions, and you want to retrieve all questions that contain an animal.\n",
      "[38] For example, the vector for bananas (both the text and the image) is located near apples and not cats. <img\n",
      "  src={require('./img/vectors-example.png').default}\n",
      "  alt=\"Vectors Example\"\n",
      "  style={{ maxWidth: \"90%\" }}\n",
      "/>\n",
      "\n",
      "*The above image is a visual representation of a vector space. To perform a search, your search query is converted to a vector - similar to your data vectors. The vector database then computes the similarity between the search query and the collection of data points in the vector space.*\n",
      "\n",
      "### Vector Databases are Fast\n",
      "\n",
      "The best thing is, vector databases can **query large datasets**, containing **tens or hundreds of millions of objects** and still **respond** to queries in a tiny **fraction of a second**. Without getting too much into details, one of the big reasons why vector databases are so fast is because they use the **Approximate Nearest Neighbor** (ANN) algorithm to index data based on vectors.\n",
      "[39] This is why we have **Vector Databases** and **Vector Libraries**. They both use the Approximate Nearest Neighbor (ANN) algorithm to search through vectors in a tiny fraction of the time. You can learn more about this topic from [\"Why Vectors Search is so Fast.\"](/blog/why-is-vector-search-so-fast)\n",
      "\n",
      "## The Big Question\n",
      "So, if both vector databases and vector libraries allow you to efficiently search through your vectors. What are the key differences between them, and why/when should you choose one over the other? ## Vector Libraries\n",
      "Vector libraries store vector embeddings in in-memory indexes, in order to perform similarity search.\n",
      "[40] ## More Coming Soon\n",
      "We are very excited about the potential of Ref2Vec, and how it leverages existing symbolic data to augment vector searches in a new way. One of my favorite articles about Weaviate is Bob van Luijt's [\"The AI-First Database Ecosystem\"](/blog/semantic-search-with-wikipedia-and-weaviate). In this article, Bob describes emerging **waves** of database technology; from SQL, to NoSQL, and now, AI-first databases that focus \"on data that is processed by a machine learning model first, where the AI models help in processing, storing and searching through the data as opposed to traditional ways\". Although Weaviate puts Machine Learning data representations first, **this doesn't mean we discard symbolic data and many features of previous systems**. Rather, we are actively searching for how symbolic data can improve neural functionality and vice versa.\n",
      "[41] ![Hacktober video](img/hacktober.gif)\n",
      "\n",
      "### [Weaviate Academy](/developers/academy) & [Workshops](/community/events)\n",
      "Weaviate Academy and Workshops have had a fantastic year of learning and growth! We've been focusing on ensuring everyone has the chance to understand and use vector databases and get a grasp on Generative AI and data handling. Every week, [Zain](https://www.linkedin.com/in/zainhas/), [JP](https://www.linkedin.com/in/jphwang/), [Daniel](https://www.linkedin.com/in/malgamves/), and [Duda](https://www.linkedin.com/in/dudanogueira/) have been running introductory workshops on vector databases and Weaviate, which have been a hit. Plus, we're super excited about [JP Hwang](https://www.linkedin.com/in/jphwang/)'s initiative, the Weaviate Academy. It's a program that takes you from the basics all the way to production-level skills. Later in the year, we teamed up with [DeepLearningAI](https://www.deeplearning.ai/) to create a short course with [Sebastian](https://www.linkedin.com/in/sebawita/) and [Zain](https://www.linkedin.com/in/zainhas/), \"[Vector Databases: from Embeddings to Applications with Weaviate](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/).\" It’s been a year packed with learning.\n",
      "[42] You can read more about HNSW+BQ in the documentation [here](/developers/weaviate/concepts/vector-quantization#binary-quantization). :::\n",
      "\n",
      "Binarizing is not exclusive to embedding vectors, we can also understand it from the other domains in which it is used - for example in computer vision. Imagine you have an image. The process of binarizing that image entails looking at every pixel. If that pixel is larger than a certain threshold value, we will replace it with 1; otherwise, we will replace it with 0.\n",
      "[43] ANN algorithms organize indexes so that the vectors that are closely related are stored next to each other. Check out this article to learn [\"Why is Vector Search are so Fast\"](/blog/why-is-vector-search-so-fast) and how vector databases work. ### Why are there Different Distance Metrics? Depending on the machine learning model used, vectors can have ~100 dimensions or go into thousands of dimensions. The time it takes to calculate the distance between two vectors grows based on the number of vector dimensions.\n",
      "[44] ![Demo of Weaviate](./img/weaviate-demo.gif?i=8)\n",
      "\n",
      "## What is Weaviate and why does it need an API? Weaviate is an [open-source](https://github.com/weaviate/weaviate) Vector Database: for understandable knowledge representation, enabling semantic search and automatic classification. Weaviate does not only store data, but also its (automatically derived) context, consisting of linguistic information and relations to other concepts. The result is a network of knowledge, or a graph of data. One core question is: How do we interact with this new type of data storage?\n",
      "[45] ---\n",
      "title: Why is Vector Search so fast? slug: why-is-vector-search-so-fast\n",
      "authors: [laura]\n",
      "date: 2022-09-13\n",
      "tags: ['search']\n",
      "image: ./img/hero.png\n",
      "description: \"Vector Databases can run semantic queries on multi-million datasets in milliseconds. How is that possible?\"\n",
      "---\n",
      "![Why is Vector Search so fast?](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "## Why is this so incredibly fast? Whenever I talk about vector search, I like to demonstrate it with an example of a semantic search. To add the wow factor, I like to run my queries on a Wikipedia dataset, which is populated with over 28 million paragraphs sourced from Wikipedia.\n",
      "[46] Contrastingly, let's analyze another scenario. |Original Vectors            |  BQ Vectors |\n",
      "|:-------------------------:|:-------------------------: |\n",
      "| ![OriginalVectorsUN](./img/image17.png)  |  ![BinaryVectorsUN](./img/image16.png) |\n",
      "\n",
      "\n",
      "All of our data falls within the quadrant where both dimensions are positive. Consequently, all vectors are encoded as '11', making it challenging to discern between most vectors. This scenario exemplifies how the distribution of data can render binary quantization impractical. As previously noted, while normalization isn't obligatory, opting for normalized data provides reassurance in terms of data distribution, facilitating the use of binary quantization.\n",
      "[47] For instance, with 756 dimensions, you already have astonishing $2^{756}$ regions at your disposal - this makes collisions between vectors highly improbable even when you have billions or trillions of vectors. With 1.5K dimensions, the number of regions can easily accommodate any practical amount of vectors without a single collision. <img\n",
      "  src={require('./img/image4.png').default}\n",
      "  alt=\"collision probability\"\n",
      "  style={{ maxWidth: \"80%\", margin: \"0 auto\", display: \"block\" }}\n",
      "/>\n",
      "\n",
      "**Fig**: *Displays the exponentially decreasing probability for collisions in vector representations. In essence, this means that the higher the dimensionality of your data, the more effectively you can expect binary quantization to perform.*\n",
      "\n",
      "## 🚀 Performance Improvements with BQ\n",
      "\n",
      "Let's revisit the advantages of binary quantization. Typically, we employ quantization to conserve memory, encoding each dimension as a single bit.\n",
      "[48] Sometimes, the answer is both, which luckily for us happens to be hybrid search - the best of both worlds!\n",
      "In this article, we’ll get into the technology that powers hybrid search, learn how to use it in Weaviate and build a Next.js Application powered by hybrid search in Weaviate. ## How does Hybrid Search work? To understand hybrid search, we first need to understand the two technologies that power it - vector search and keyword search. ### Vector Search\n",
      "\n",
      "Vector search, also known as semantic search, uses machine learning to grasp text context, not just keywords. It converts text to numerical vectors, finding matches based on conceptual similarity for accurate, relevant search results.\n",
      "[49] Scalability: Most vector libraries cannot be queried while importing your data, which can be a scalability concern for applications that require importing millions or even billions of objects. Thus, vector libraries are a great solution for applications with a limited static snapshot of data. However, if your application requires real-time scalable semantic search at the production level, you should consider using a vector database. ## Summary\n",
      "\n",
      "This article explained that vector databases are a type of database that indexes, stores, and provides access to structured or unstructured data alongside its vector embeddings. Vector databases like [Weaviate](http://weaviate.io) allow for efficient similarity search and retrieval of data based on their vector distance or vector similarity at scale.\n",
      "[50] The demo is organized such that user interactions are outlined with a green box and moved to the top-left of the grid. The nearest neighbors to the ref2vec-centroid are then arranged from left-to-right top-down. import centroidVideo from './img/ref2vec-centroid-demo.mp4';\n",
      "\n",
      "<video width=\"100%\" autoplay loop controls>\n",
      "  <source src={centroidVideo} type=\"video/mp4\" />\n",
      "Your browser does not support the video tag. </video>\n",
      "\n",
      "This animation shows how selecting multiple backpacks (such as two school-style book bags) can help personalize the user's recommendations (so showing more backpacks in a similar style rather than the duffle bags). We also see here how selecting multiple digital LED screen watches moves results away from analog watches.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def format_RM_results(results: List[str]) -> str:\n",
    "    results = [result[\"long_text\"] for result in results]\n",
    "    results = \"\\n\".join([f\"[{i+1}] {item}\" for i, item in enumerate(results)]) \n",
    "    return results\n",
    "\n",
    "question = \"How does quantization help Vector Databases?\"\n",
    "\n",
    "search_results = format_RM_results(weaviate_blog_index(question, k=50))\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fbf7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeedleInTheHaystack(dspy.Signature):\n",
    "    \"\"\"Given a long context and a question, find the answer in the context.\"\"\"\n",
    "    \n",
    "    long_context = dspy.InputField()\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "rag = dspy.ChainOfThought(NeedleInTheHaystack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65051deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    rationale='Reasoning: Let\\'s think step by step in order to find the date the blog post about Vamana and HNSW was published. We can look for the title \"Vamana vs. HNSW - Exploring ANN algorithms Part 1\" and find the date associated with it.',\n",
       "    answer='October 11, 2022'\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer contained in result 18\n",
    "rag(long_context=results, question=\"When did Weaviate publish their blog post about Vamana and HNSW?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9ff8f",
   "metadata": {},
   "source": [
    "# Re-ranking Long Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80a61f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRelevanceCounter(dspy.Signature):\n",
    "    \"\"\"Given a numbered list of responses from a search engine to a query, count how many of them are relevant to the query.\"\"\"\n",
    "    \n",
    "    numbered_search_results: str = dspy.InputField()\n",
    "    query: str = dspy.InputField()\n",
    "    number_of_relevant_results: int = dspy.OutputField()\n",
    "    \n",
    "context_relevance_counter = dspy.TypedChainOfThought(ContextRelevanceCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "878fd44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning=\"Reasoning: Let's think step by step in order to produce the number of relevant results. We are looking for articles that discuss quantization in the context of vector databases. Articles 1, 4, 10, 16, 19, 23, and 38 all discuss quantization in this context.\",\n",
       "    number_of_relevant_results=7\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_relevance_counter(numbered_search_results=results, query=\"How does quantization help with Vector Databases?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf376f3",
   "metadata": {},
   "source": [
    "# RAG with *Many* (Question, Context, Rationale, Answer) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be64620",
   "metadata": {},
   "source": [
    "Agarwal et al. from Google DeepMind have recently published [**Many-Shot In-Context Learning**](https://arxiv.org/abs/2404.11018) on April 17th, 2024.\n",
    "\n",
    "Jiang et al. from Stanford University (including Andrew Ng) have also published [**Many-Shot In-Context Learning in Multimodal Foundation Models**](https://arxiv.org/abs/2405.09798) on May 16th, 2024.\n",
    "\n",
    "As a little bit of background,\n",
    "\n",
    "1. Prior to GPT-3, Machine Learning was achieved mostly by Supervised Learning with a large number of labeled examples.\n",
    "\n",
    "2. GPT-3 then changed the game with *In-Context Learning*, showing that you could perform a task by providing a **few** examples of the task in the input, rather than needing additional gradient descent training.\n",
    "\n",
    "3. ChatGPT then came along and showed the power of *Reinforcement Learning from Human Feedback*, shifting the AI world into Instruction following and formatting, rather than example labeling.\n",
    "\n",
    "4. Many-Shot In-Context Learning powered by LLMs **could** cause another paradigm shift in how we get AI systems to perform tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee3d6b",
   "metadata": {},
   "source": [
    "![Alt text](./images/many-shot.png \"Optional title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0398179",
   "metadata": {},
   "source": [
    "Image taken from **Many-Shot In-Context Learning** by Agarwal et al. 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d61c79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a bug in `BootstrapFewShot` when directly passing in the weaviate_blog_index\n",
    "# something with calling .deep_copy() on something with grpc, will fix later\n",
    "\n",
    "dspy.settings.configure(rm=weaviate_blog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba59bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the context and answer the question.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve()\n",
    "        # Note this `ChainOfThought`, \n",
    "        # => maybe the most important part of synthetic example generation\n",
    "        # as we imagine most RAG apps already have some kind of quesiton,answer gold dataset\n",
    "        # BUT => they do not have rationales of this praticular structure\n",
    "        # further, if the program is more complex than this, it will help to have intermediate labeling\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question, k=5):\n",
    "        context = self.retrieve(question, k=k).passages\n",
    "        pred = self.generate_answer(context=context, question=question).answer\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "089b7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "gold_answers = []\n",
    "queries = []\n",
    "\n",
    "for row in dataset:\n",
    "    gold_answers.append(row[\"gold_answer\"])\n",
    "    queries.append(row[\"query\"])\n",
    "    \n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "# converting all examples into Many-Shot Examples,\n",
    "# ToDo - add train / dev / test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8793044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ef6f4dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [06:24<00:00,  7.68s/it]\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "teacher_settings = {\"lm\": command_r}\n",
    "\n",
    "# optionally add a metric to assess the quality of synthetic examples\n",
    "compiler = BootstrapFewShot(teacher_settings=teacher_settings,\n",
    "                           max_bootstrapped_demos=50)\n",
    "\n",
    "many_shot_compiled_rag = compiler.compile(RAG(), trainset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ed895f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mResult for Gemini Flash\n",
      "\n",
      "\u001b[0mLong Context LLMs can potentially improve the performance of re-ranker models by providing them with more context and information to work with. This can lead to more accurate and relevant rankings, as the re-ranker model can better understand the relationship between the query and the retrieved documents. However, the context also mentions that there are trade-offs between performance and latency when using Long Context LLMs, which may need to be considered when implementing them in re-ranker models. \n",
      "\n",
      "\u001b[91mResult for Gemini Pro\n",
      "\n",
      "\u001b[0mReasoning: Let's think step by step in order to produce the answer. The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question.\n",
      "\n",
      "Answer: The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question. \n",
      "\n",
      "\u001b[91mResult for Command R\n",
      "\n",
      "\u001b[0mLong Context LLMs will likely have a significant impact on re-ranker models, as they enable the processing and generation of longer text sequences. This increased context length can provide more information for re-ranking, potentially improving the accuracy and relevance of search results. However, it also introduces challenges, such as the need for more sophisticated re-ranking models that can handle longer inputs and the potential for increased latency due to the larger context window. The context mentions that this is an area of ongoing research, and further experimentation is needed to fully understand the implications of Long Context LLMs on re-ranker models. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"How will Long Context LLMs impact re-ranker models?\"\n",
    "\n",
    "for lm_dict in lms:\n",
    "    lm, name = lm_dict[\"lm\"], lm_dict[\"name\"]\n",
    "    with dspy.context(lm=lm):\n",
    "        print(f\"\\033[91mResult for {name}\\n\")\n",
    "        print(f\"\\033[0m{many_shot_compiled_rag(question)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e53098",
   "metadata": {},
   "source": [
    "# Many-Shot In-Context Learning Visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f3baa",
   "metadata": {},
   "source": [
    "In the example above, Gemini Pro 1.5, Gemini Flash, and Command R all see **50** examples of `(question, context, answer)` tuples before the current inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "01a84c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Assess the context and answer the question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Note, the current implementation of hybrid search in Weaviate uses BM25/BM25F and vector search. If you’re interested to learn about how dense vector indexes are built and optimized in Weaviate, check out this [article](/blog/why-is-vector-search-so-fast). ### BM25\n",
      "BM25 builds on the keyword scoring method [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term-Frequency Inverse-Document Frequency) by taking the [Binary Independence Model](https://en.wikipedia.org/wiki/Binary_Independence_Model) from the IDF calculation and adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. The image below presents the scoring calculation of BM25:\n",
      "![BM25 calculation](./img/BM25-calculation.png) <div align=\"center\"><i>Source: Wikipedia page on Okapi BM25</i></div>\n",
      "\n",
      "The score of the document, query pair is determined by weighing the uniqueness of each keyword in the query relative to the collection of texts. BM25 contains additional static parameters, k1 and b that may help calibrate performance to particular datasets.»\n",
      "[2] «And that Weaviate recently added a new algorithm for how this is done? In this post, we’ll dive into exactly the world of hybrid search to discuss how it works, how results are produced, the algorithms used, and more. So let’s get into it!\n",
      "\n",
      ":::info\n",
      "- Vector search and keyword search are also known as dense vector search and sparse vector search respectively. - Keyword search is also called a BM25 search in Weaviate, as it is based on the [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25) scoring algorithm. :::\n",
      "\n",
      "## How does hybrid search work, exactly?»\n",
      "[3] «# Current Limitations of Using Weaviate with Non-English Languages\n",
      "\n",
      "As you saw, you can use the Weaviate vector database for semantic and generative search applications in non-English languages. However, we currently don’t have any official support/policy for non-English languages, which leads to some limitations. This limitation mainly affects [Weaviate’s keyword-based search functionality BM25](https://weaviate.io/developers/weaviate/search/bm25), as we currently only have the standard English tokenizer. This means the BM25 search will return an error for most non-alphabetic languages, especially ones requiring Unicode encoding. This limitation also affects [Weaviate’s hybrid search functionality](https://weaviate.io/developers/weaviate/search/hybrid).»\n",
      "[4] «We would really appreciate your input. Please let us know what you think!\n",
      "\n",
      "\n",
      "\n",
      "## Wrap-up\n",
      "\n",
      "Hybrid search in Weaviate offers a powerful blend of vector and keyword search, using the strengths of both to deliver semantically rich results while respecting precision of keyword searches. As we've explored, the introduction of `relativeScoreFusion` expands Weaviate’s hybrid search capabilities that began its life with the `rankedFusion` algorithm. We invite you to dive in, experiment with these fusion algorithms, and share your experiences. ### Further resources\n",
      "\n",
      "- [How-to: Hybrid search](/developers/weaviate/search/hybrid)\n",
      "- [API References: Hybrid search](/developers/weaviate/api/graphql/search-operators#hybrid)\n",
      "\n",
      "import WhatsNext from '/_includes/what-next.mdx';\n",
      "\n",
      "<WhatsNext />»\n",
      "[5] «In other use cases, we’ve found that [keyword search](https://weaviate.io/developers/weaviate/search/bm25) is more important for some applications. \"BM25 (Best match 25)\" or \"sparse vector\" search, returns objects that have the highest BM25F scores. Vector Search and Keyword Search each have their advantages, but often times, the results from vector search or keyword search are too restrictive and the optimal set of results are somewhere in-between. This is why we also have [hybrid search](https://weaviate.io/developers/weaviate/search/hybrid) which combines results of a vector search and a keyword (BM25F) search. You can set the [weights](https://weaviate.io/developers/weaviate/search/hybrid#balance-keyword-and-vector-search) or the [ranking method](https://weaviate.io/developers/weaviate/search/hybrid#change-the-ranking-method).»\n",
      "\n",
      "Question: What is the role of the Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To understand the role of the Binary Independence Model in the BM25 algorithm, we need to break down how BM25 works and how it utilizes this model.\n",
      "\n",
      "Answer: The Binary Independence Model is a component of the Inverse Document Frequency (IDF) calculation in the BM25 algorithm. It helps determine the uniqueness or rarity of a keyword within a collection of texts. By considering the frequency of a term in the entire corpus, the model assesses how informative or distinctive that term is. This information is then used to weigh the importance of the term in the overall scoring calculation, with rarer terms typically receiving higher scores.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Scalability: Most vector libraries cannot be queried while importing your data, which can be a scalability concern for applications that require importing millions or even billions of objects. Thus, vector libraries are a great solution for applications with a limited static snapshot of data. However, if your application requires real-time scalable semantic search at the production level, you should consider using a vector database. ## Summary\n",
      "\n",
      "This article explained that vector databases are a type of database that indexes, stores, and provides access to structured or unstructured data alongside its vector embeddings. Vector databases like [Weaviate](http://weaviate.io) allow for efficient similarity search and retrieval of data based on their vector distance or vector similarity at scale.»\n",
      "[2] «Vector Indexing Library\n",
      "\n",
      "Similarly to vector databases, [vector libraries](https://weaviate.io/blog/vector-library-vs-vector-database) also enable fast vector search. However, vector libraries only store vector embeddings of data objects, and they store them in in-memory indexes. This results in two key differences:\n",
      "\n",
      "1. Updatability: The index data is immutable, and thus, no real-time updates are possible. 2.»\n",
      "[3] «This is why we have **Vector Databases** and **Vector Libraries**. They both use the Approximate Nearest Neighbor (ANN) algorithm to search through vectors in a tiny fraction of the time. You can learn more about this topic from [\"Why Vectors Search is so Fast.\"](/blog/why-is-vector-search-so-fast)\n",
      "\n",
      "## The Big Question\n",
      "So, if both vector databases and vector libraries allow you to efficiently search through your vectors. What are the key differences between them, and why/when should you choose one over the other? ## Vector Libraries\n",
      "Vector libraries store vector embeddings in in-memory indexes, in order to perform similarity search.»\n",
      "[4] «Further reading [Why Is Vector Search So Fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)\n",
      "\n",
      "## Use Cases of Vector Databases\n",
      "\n",
      "Vector databases’ search capabilities can be used in various applications ranging from classical Machine Learning use cases, such as natural language processing, computer vision, and recommender systems, to providing long-term memory to LLMs in modern applications. The **most popular use case of vector search engines is naturally for search**. Because a vector database can help find similar objects, it is predestined for applications where you might want to find similar products, movies, books, songs, etc. That’s why vector search engines are also **used in recommendation systems** as a restated task of search. With the rise of LLMs, **vector databases have already been used to enhance modern Generative AI applications**.»\n",
      "[5] «Depending on the data type, vector embeddings are created using machine learning models that are able to translate the meaning of an object into a numerical representation in a high dimensional space. Thus, there are a variety of machine learning models able to create a variety of different types of embeddings, such as word embeddings, sentence embeddings, text embeddings, or image embeddings. Vector embeddings capture the semantic relationship between data objects in numerical values and thus, you can find similar data points by determining their nearest neighbors in the high dimensional vector space. This concept is also called similarity search and can be applied in different applications, such as text search, image search, or recommendation systems. Now that you have a good understanding of vector embeddings, you might also be interested in the following articles:\n",
      "* [What is a vector database?](https://weaviate.io/blog/what-is-a-vector-database)\n",
      "* [Distance metrics for nearest neighbor search](https://weaviate.io/blog/distance-metrics-in-vector-search)\n",
      "* [Why is vector search so fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)\n",
      "\n",
      "import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "\n",
      "Question: Why might vector libraries not be suitable for applications that require real-time updates and scalable semantic search?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's break down the question and identify the key points to address. First, we need to understand the capabilities and limitations of vector libraries. Then, we can assess why they may not be suitable for certain use cases. Vector libraries are designed to store vector embeddings of data objects, and they do so in in-memory indexes. This provides fast vector search capabilities. However, there are two main limitations to consider: 1. **Scalability**: Vector libraries cannot be queried while importing data. This becomes a bottleneck for applications that deal with large amounts of data, especially in the billions of objects. The library has to finish importing all the data before any queries can be made, which can be time-consuming and inefficient. 2. **Updatability**: The vector library indexes are immutable, meaning they cannot be changed once created. This results in an inability to perform real-time updates. Any changes to the data would require recreating the entire index, which is impractical for applications that require dynamic data updates.\n",
      "\n",
      "Answer: Vector libraries may not be suitable for applications that require real-time updates and scalable semantic search due to their limitations in updatability and scalability. The immutable nature of vector library indexes prevents real-time updates, and the lack of query capabilities during data import affects their scalability for large datasets. Therefore, applications that demand dynamic data changes and efficient handling of vast amounts of data would be better served by alternative solutions, such as vector databases.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «I recommend checking out the GitHub repository to test this out yourself!\n",
      "\n",
      "## Additional Resources\n",
      "• [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar. import WhatsNext from '/_includes/what-next.mdx';\n",
      "\n",
      "<WhatsNext />»\n",
      "[2] «We will review and help you out in the process. 💚\n",
      "\n",
      "You can also contribute by adding your own Weaviate examples. If you have other great ideas for contributions, let us know on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels, and we will figure out how to highlight it in Hacktoberfest. You don't need to be an expert to contribute to these demo projects!\n",
      "\n",
      "\n",
      "## Resources to Get Started\n",
      "\n",
      "We're thrilled to help you make your first open-source contribution! Here are some helpful resources to kickstart your journey:\n",
      "\n",
      "What is Open Source, and how do you contribute to it? - 🎯 [What is Open Source](https://www.digitalocean.com/community/tutorials/what-is-open-source)\n",
      "- 🎯 [Introduction to GitHub and Open-Source Projects](https://www.digitalocean.com/community/tutorial_series/an-introduction-to-open-source)\n",
      "- 🎯 [How to Contribute to Open Source](https://opensource.guide/how-to-contribute/)\n",
      "- 🎯 [GitHub Contribution Guide by Hugging Face](https://www.notion.so/Contribution-Guide-19411c29298644df8e9656af45a7686d?pvs=21)\n",
      "- 🎯 [How to Use Git](https://www.digitalocean.com/community/cheatsheets/how-to-use-git-a-reference-guide)\n",
      "- 🎯 [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide)\n",
      "\n",
      "If you're new to Weaviate, get up and running quickly with these beginner-friendly guides:\n",
      "\n",
      "- [Quickstart Guide](https://weaviate.io/developers/weaviate/quickstart) 🚀\n",
      "- [Weaviate Academy](https://weaviate.io/developers/academy) 🎓\n",
      "\n",
      "Dive deeper into specific topics with these detailed guides:\n",
      "\n",
      "- [How-to Search Guides](https://weaviate.io/developers/weaviate/search) 🔍\n",
      "- [Keyword, Vector, Hybrid, and Generative Search](https://github.com/weaviate-tutorials/generative-search/blob/main/GenerativeSearchDemo.ipynb) 🔍\n",
      "- [How-to Manage Data (CRUD Operations)](https://weaviate.io/developers/weaviate/manage-data) 💾\n",
      "- [Tutorial: Importing Data with Your Own Vectors](https://weaviate.io/developers/weaviate/tutorials/wikipedia) 📊\n",
      "- [Weaviate Architecture Concepts](https://weaviate.io/developers/weaviate/concepts#weaviate-architecture) 🏛️\n",
      "\n",
      "Join one of our [workshops](https://weaviate.io/learn/workshops) for an introduction to Weaviate.»\n",
      "[3] «It also provides a date, author, and tags related to integrations. Given this information, we can create a query that asks about the purpose of combining LangChain with Weaviate, as this is a specific topic that the document addresses. Query: What are the benefits of combining LangChain with Weaviate in the context of LLMs? ```\n",
      "</details>\n",
      "\n",
      "#### Grab Data out of Weaviate\n",
      "\n",
      "Now that we have the synthetic query + relevant chunk, we can export the data to fine-tune the model. We’ll grab 80% of the objects for training and 20% for validation.»\n",
      "[4] «In this example we used two research papers; however, there is the possibility to add Powerpoint presentations or even scanned letters to your Weaviate instance. Unstructured has really simplified the process of using visual document parsing for diverse document types. We tested a few queries above, but we can take this one step further by using [LangChain](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/weaviate). Once the documents are imported into Weaviate, you can build a simple chatbot to chat with your pdfs by using LangChain’s vectorstore. ```python\n",
      "from langchain.vectorstores.weaviate import Weaviate\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import ChatVectorDBChain\n",
      "import weaviate\n",
      "\n",
      "client = weaviate.Client(\"http://localhost:8080\")\n",
      "\n",
      "vectorstore = Weaviate(client, \"NAME_OF_CLASS\", \"NAME_OF_PROPERTY\")\n",
      "\n",
      "MyOpenAI = OpenAI(temperature=0.2,\n",
      "    openai_api_key=\"ENTER YOUR OPENAI KEY HERE\")\n",
      "\n",
      "qa = ChatVectorDBChain.from_llm(MyOpenAI, vectorstore)\n",
      "\n",
      "chat_history = []\n",
      "\n",
      "while True:\n",
      "    query = input(\"\")\n",
      "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
      "    print(result[\"answer\"])\n",
      "    chat_history = [(query, result[\"answer\"])]\n",
      "```\n",
      "\n",
      "import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "[5] «We ... Query: A short question uniquely answered by the document. ---\n",
      "\n",
      "Document: --- title: Combining LangChain and Weaviate slug: combining-langchain-and-weaviate authors: [erika] date: 2023-02-21 tags: ['integrations'] image: ./img/hero.png description: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\" --- ![Combining LangChain and Weaviate](./img/hero.png) Large Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale.»\n",
      "\n",
      "Question: What guide does the document recommend for learning about LangChain projects?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to identify which guide is specifically recommended for learning about LangChain projects within the provided context. We can scan the text for any mentions of \"LangChain\" and \"guide\" or \"tutorial\" to find the relevant information.\n",
      "\n",
      "Answer: The document recommends the \"LangChain Guide\" by Paul from CommandBar as a resource to learn about LangChain projects.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «If we compress the vectors then the memory requirements goes down to the 1572 MB to 2129 MB range. After compression, recall drops to values ranging from 0.7337 to 0.9545. Latency rises up to the 7521 to 37402 microsends range. A summary is shown in Table 3 below. |                       |              | Recall100@100 | Latency ($\\mu s$)         | Memory required (MB)         |\n",
      "|-----------------------|--------------|---------------|---------------------------|------------------------------|\n",
      "| Sift1M Low params     | Uncompressed | 0.91561       | 293                       | 1277                         |\n",
      "|                       | Compressed   | 0.91361       | 401               (x1.36) | 610                 (47.76%) |\n",
      "| Sift1M High params    | Uncompressed | 0.99974       | 1772                      | 1674                         |\n",
      "|                       | Compressed   | 0.99658       | 1937             (x1.09)  | 1478               (88.29%)  |\n",
      "| DeepImage Low params  | Uncompressed | 0.8644        | 827                       | 9420                         |\n",
      "|                       | Compressed   | 0.85666       | 1039             (x1.25)  | 4730               (50.21%)  |\n",
      "| DeepImage High params | Uncompressed | 0.99757       | 2601                      | 15226                        |\n",
      "|                       | Compressed   | 0.97023       | 2708             (x1.04)  | 12367             (81.22%)   |\n",
      "| Gist Low params       | Uncompressed | 0.74461       | 2133                      | 4218                         |\n",
      "|                       | Compressed   | 0.73376       | 7521             (x3.52)  | 1572              (37.26%)   |\n",
      "| Gist High params      | Uncompressed | 0.99628       | 15539                     | 5103                         |\n",
      "|                       | Compressed   | 0.95455       | 37402           (x2.40)   | 2129               (41.72%)  |\n",
      "\n",
      "**Tab.»\n",
      "[2] «If we compress the vectors then the memory requirements goes down to the 4730 MB to 12367 MB range. After compression, recall drops to values ranging from 0.8566 to 0.9702. Latency rises up to the 1039 to 2708 microsends range. For Gist we would require roughly 4218 MB to 5103 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.7446 to 0.9962 and latencies ranging from 2133 to 15539 microseconds.»\n",
      "[3] «Furthermore, you might be using Weaviate with your own data which might be even bigger than the datasets we report on below. If we extrapolate out, consider how big these memory requirements could grow as you add more objects or represent these objects with long vector embeddings. | DataSet     | Dimensions | Vectors   | Size in memory (MB) |\n",
      "|-------------|------------|-----------|---------------------|\n",
      "| Sift1M      | 128        | 1,000,000 | 512                 |\n",
      "| Gist        | 960        | 1,000,000 | 3840                |\n",
      "| DeepImage96 | 96         | 9,990,000 | 3836.16             |\n",
      "\n",
      "**Table 1**: *Description of datasets.*\n",
      "\n",
      "Increasing the number of objects vs. storing longer dimensional vectors has an equivalent effect on the overall memory required to store the vectors. As an example consider the Gist dataset, which contains 1,000,000 vectors, each with 960 dimensions.»\n",
      "[4] «| Memory footprint (MB)  | Compressed (hours) | Uncompressed (hours)|\n",
      "|----------------|--------------------|-------------|\n",
      "| Sphere         |  725 (15%)         | 5014    |\n",
      "| DBPedia        |  900 (14%)         | 6333     |\n",
      "**Tab. 2**: *Compression ratios of `~6.6x`.*\n",
      "\n",
      "Notice how using the compressed version we would barely need 15% of the memory we would need under uncompressed settings. To close the results up, we have also indexed the full Sphere dataset (nearly 1 billion vectors) with and without compression. The full dataset without compression needed nearly **2.7 Terabytes** of memory while the compressed version only required **0.7 Terabytes** which is again, a huge saving in terms of memory and costs. ## 📈Benchmarking PQ with your own data\n",
      "\n",
      "If you're trying to finetune HNSW and PQ parameters on your own data to optimally balance recall, latency, and memory footprint you might want to replicate these experiments yourself.»\n",
      "[5] «The main metric of concern in this setting is the latency with which we can obtain our replies. Product Quantization not only helps with reducing memory requirements in this case but also with cutting down on latency. The following table compares improvement in latency that can be achieved using PQ aggressively. | Dataset   | Segments | Centroids | Compression |              | Latency (ms) |\n",
      "|-----------|----------|-----------|-------------|--------------|--------------|\n",
      "| Sift      | 8        | 256       | x64         | Compressed   | 46 (x12)     |\n",
      "|           |          |           |             | Uncompressed | 547          |\n",
      "| DeepImage | 8        | 256       | x48         | Compressed   | 468 (x8.5)   |\n",
      "|           |          |           |             | Uncompressed | 3990         |\n",
      "| Gist      | 48       | 256       | x80         | Compressed   | 221 (x17.5)  |\n",
      "|           |          |           |             | Uncompressed | 3889         |\n",
      "\n",
      "**Tab. 2**: *Brute force search latency with high compression ratio.*\n",
      "\n",
      "## HNSW+PQ\n",
      "\n",
      "Our complete implementation of [FreshDiskANN](https://arxiv.org/abs/2105.09613) still requires a few key pieces, however at this point we have released the HNSW+PQ implementation with v1.18 for our users to take advantage of.»\n",
      "\n",
      "Question: What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Context: The Gist dataset has 1,000,000 vectors, each with 960 dimensions. When using uncompressed HNSW, the memory requirement ranges from 4218 MB to 5103 MB depending on the parameters. After compression, the memory requirement for Gist Low params is 1572 MB. Question: What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression? Reasoning: To find the percentage reduction in memory requirements, we can use the following formula: % reduction = ((initial value - final value) / initial value) * 100% We know that the initial memory requirement for Gist Low params is 4218 MB, and the final memory requirement after compression is 1572 MB. % reduction = ((4218 MB - 1572 MB) / 4218 MB) * 100% = 62.72% reduction\n",
      "\n",
      "Answer: There is a 62.72% reduction in memory requirements for the Gist dataset with low parameters after compression.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «### Testing queries\n",
      "\n",
      "Semantic (`nearText`) searches may be one of the most common (if not *the* most common) searches our users perform. So let's see how we might test semantic searches. A semantic search requires vectorizing the query, so a test will validate the integration with the vectorizer (`text2vec-openai` in this case). We'll run a query for \"chemistry\" and check that the top result is about \"sodium\". :::info Will the top result always be the same?»\n",
      "[2] «We choose the model because it’s faster than GPT4, but you can use any other embedding model (Cohere, HuggingFace, etc.). <details>\n",
      "  <summary>Product Schema</summary>\n",
      "\n",
      "```python\n",
      "class_obj = {\n",
      "        \"class\": \"Product\",\n",
      "        \"description\": \"Supplement products\",\n",
      "        \"properties\": [\n",
      "            {\n",
      "                \"dataType\": [\"text\"],\n",
      "                \"description\": \"The name of the product\",\n",
      "                \"name\": \"name\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": True,\n",
      "                        \"vectorizePropertyName\": False,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"text\"],\n",
      "                \"description\": \"The brand of the product\",\n",
      "                \"name\": \"brand\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": True,\n",
      "                        \"vectorizePropertyName\": False,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"text\"],\n",
      "                \"description\": \"The ingredients contained in the product.\",\n",
      "                \"name\": \"ingredients\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": False,\n",
      "                        \"vectorizePropertyName\": True,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"text[]\"],\n",
      "                \"description\": \"Reviews about the product\",\n",
      "                \"name\": \"reviews\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": True,\n",
      "                        \"vectorizePropertyName\": False,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"text\"],\n",
      "                \"description\": \"Image URL of the product\",\n",
      "                \"name\": \"image\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": True,\n",
      "                        \"vectorizePropertyName\": False,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"number\"],\n",
      "                \"description\": \"The Rating of the product\",\n",
      "                \"name\": \"rating\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": True,\n",
      "                        \"vectorizePropertyName\": False,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"text\"],\n",
      "                \"description\": \"The description of the product\",\n",
      "                \"name\": \"description\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": False,\n",
      "                        \"vectorizePropertyName\": True,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"text\"],\n",
      "                \"description\": \"The summary of the reviews\",\n",
      "                \"name\": \"summary\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": False,\n",
      "                        \"vectorizePropertyName\": True,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "            {\n",
      "                \"dataType\": [\"text\"],\n",
      "                \"description\": \"The health effects of the product\",\n",
      "                \"name\": \"effects\",\n",
      "                \"moduleConfig\": {\n",
      "                    \"text2vec-openai\": {\n",
      "                        \"skip\": False,\n",
      "                        \"vectorizePropertyName\": True,\n",
      "                    }\n",
      "                },\n",
      "            },\n",
      "        ],\n",
      "        \"moduleConfig\": {\"generative-openai\": {\"model\": \"gpt-3.5-turbo\"}},\n",
      "        \"vectorizer\": \"text2vec-openai\",\n",
      "    }\n",
      "```\n",
      "</details>\n",
      "\n",
      "To give you a clearer picture, we define a new class in the Weaviate database (similar to a blueprint) using a Python dictionary. This class includes fields like product name, brand, reviews, ratings, etc. Each field's datatype is specified, which can be text, image, number, etc., and a brief description and field name is added. Some fields are configured to be vectorized, meaning their semantic content will be incorporated into the overall vector indexed in the database.»\n",
      "[3] «Then, you just run the queries, as per usual:\n",
      "```javascript\n",
      "nearText = {\n",
      "    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\n",
      "    \"distance\": 0.6,\n",
      "}\n",
      "\n",
      "result = (\n",
      "    client.query\n",
      "    .get(\"Notes\", [\n",
      "        \"name\",\n",
      "        \"comment\",\n",
      "        \"_additional {certainty distance} \"])\n",
      "    .with_near_text(nearText)\n",
      "    .do()\n",
      ")\n",
      "```\n",
      "\n",
      "## Summary\n",
      "> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\n",
      "Check out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.»\n",
      "[4] «```python\n",
      "def test_query(documents):\n",
      "    LIMIT = 3\n",
      "    response = client.post(\"/query\", json={\"text\": \"lion\", \"limit\": LIMIT})\n",
      "\n",
      "    results = response.json()\n",
      "\n",
      "    assert len(results) == LIMIT\n",
      "    for result in results:\n",
      "        assert \"lion\" in result[\"document\"][\"text\"]\n",
      "```\n",
      "\n",
      "The implementation below will take in a query and return a list of retrieved documents and metadata. ```python\n",
      "@app.post(\"/query\", response_model=List[QueryResult])\n",
      "def query(query: Query, client=Depends(get_weaviate_client)) -> List[Document]:\n",
      "    \"\"\"\n",
      "    Query weaviate for documents\n",
      "    \"\"\"\n",
      "    query_vector = get_embedding(query.text)\n",
      "\n",
      "    results = (\n",
      "        client.query.get(INDEX_NAME, [\"document_id\", \"text\"])\n",
      "        .with_near_vector({\"vector\": query_vector})\n",
      "        .with_limit(query.limit)\n",
      "        .with_additional(\"certainty\")\n",
      "        .do()\n",
      "    )\n",
      "\n",
      "    docs = results[\"data\"][\"Get\"][INDEX_NAME]\n",
      "\n",
      "    return [\n",
      "        QueryResult(\n",
      "            document={\"text\": doc[\"text\"], \"document_id\": doc[\"document_id\"]},\n",
      "            score=doc[\"_additional\"][\"certainty\"],\n",
      "        )\n",
      "        for doc in docs\n",
      "    ]\n",
      "```\n",
      "</details>\n",
      "\n",
      "<details>\n",
      "  <summary>See details for /delete endpoint implementation.</summary>\n",
      "\n",
      "**Implement the `/delete` endpoint:**\n",
      "\n",
      "Here we simply want to check that the response returned correctly and that after removing one object we the number of total objects in the Weaviate instance goes down by one. ```python\n",
      "def test_delete(documents, weaviate_client):\n",
      "    num_docs_before_delete = weaviate_client.data_object.get()[\"totalResults\"]\n",
      "\n",
      "    response = client.post(\"/delete\", json={\"document_id\": \"3\"})\n",
      "    assert response.status_code == 200\n",
      "\n",
      "    num_docs_after_delete = weaviate_client.data_object.get()[\"totalResults\"]\n",
      "\n",
      "    assert num_docs_after_delete == num_docs_before_delete - 1\n",
      "```\n",
      "\n",
      "And the implementation of the endpoint is as follows:\n",
      "\n",
      "```python\n",
      "@app.post(\"/delete\")\n",
      "def delete(delete_request: DeleteRequest, client=Depends(get_weaviate_client)):\n",
      "    \"\"\"\n",
      "    Delete a document from weaviate\n",
      "    \"\"\"\n",
      "    result = client.batch.delete_objects(\n",
      "        class_name=INDEX_NAME,\n",
      "        where={\n",
      "            \"operator\": \"Equal\",\n",
      "            \"path\": [\"document_id\"],\n",
      "            \"valueText\": delete_request.document_id,\n",
      "        },\n",
      "    )\n",
      "\n",
      "    if result[\"results\"][\"successful\"] == 1:\n",
      "        return {\"status\": \"ok\"}\n",
      "    else:\n",
      "        return {\"status\": \"not found\"}\n",
      "```\n",
      "</details>\n",
      "\n",
      "Here we showed you how our endpoints work, this will be where your plugin will be most unique, depending on what functionality you want implemented you can create corresponding endpoints and test them. ::::tip\n",
      "Notice the docstrings we’ve included with all of our endpoints, these will be very important in the next step!\n",
      "::::\n",
      "\n",
      "**Step 2: Prepare plugin manifest files**\n",
      "\n",
      "This is where you specify to OpenAI and specifically ChatGPT which endpoints your plugin is exposing, how it can use those endpoints to accomplish specific tasks, what errors to expect if the endpoints are not used correctly and much more! [OpenAI instruction specify](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest) that you need to create two files: the [`openapi.yaml` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/openapi.yaml) and the [`ai-plugin.json` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/ai-plugin.json). As you can see both of these files need to be in the `.well-known` directory which must be mounted into the app as follows in order for ChatGPT to utilize them correctly.»\n",
      "[5] «Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Storing and using vector embeddings with a Weaviate vector database\n",
      "\n",
      "For this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules.»\n",
      "\n",
      "Question: How do you test semantic search functionality and result consistency using `text2vec-openai`?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Answer: Here is the response formatted as requested: Context: The text describes the process of setting up and testing a Weaviate database with a focus on semantic search functionality using the `text2vec-openai` module. It includes code snippets and explanations for various steps, such as defining the database schema, running queries, and implementing endpoints for a retrieval plugin. Question: How do you test semantic search functionality and result consistency using `text2vec-openai`? Reasoning: To test the semantic search functionality and result consistency using `text2vec-openai`, we need to follow the steps outlined in the context. First, we define the database schema, including the fields that will be vectorized, such as \"ingredients,\" \"description,\" and \"effects.\" Then, we can run queries using the `nearText` search parameter to perform semantic searches. In the example given, the query searches for \"chemistry\" and expects the top result to be about \"sodium.\" We can also specify the distance parameter to control the relevance of the results. To test the consistency of the results, we can use assertions in our test code. In the provided code snippet, the `test_query` function asserts that the length of the results is equal to the specified limit and that the query term (\"lion\" in this case) is present in the text of each result. By running this test for different queries and comparing the results, we can evaluate the consistency of the semantic search functionality.\n",
      "\n",
      "Answer: To test semantic search functionality and result consistency using `text2vec-openai`, we define the database schema, including fields for vectorization. We run queries using `nearText` and specify distance parameters. We can test consistency by asserting expected results and comparing outcomes for different queries.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «We recommend to use a Docker Compose setup for trying out or evaluating Weaviate and when developing with Weaviate locally. However, for a stable production environment, we [recommend to run Weaviate on Kubernetes](/developers/weaviate/installation/kubernetes). You can use Helm, there is an [officially supported Weaviate Helm Chart](https://github.com/weaviate/weaviate-helm). import WhatsNext from '/_includes/what-next.mdx';\n",
      "\n",
      "<WhatsNext />»\n",
      "[2] «Similarly, we want to give you more flexibility during ingestion time: how about [extracting PDFs](https://github.com/weaviate/weaviate/issues/2509) or applying [stemming](https://github.com/weaviate/weaviate/issues/2439) to your BM25 and hybrid search? <br></br>\n",
      "\n",
      "### Beyond Billion Scale: Large-Scale Performance\n",
      "![Billion Scale](./img/billion-scale.png)\n",
      "\n",
      "In 2022, we published the [Sphere Demo Dataset for Weaviate](/blog/sphere-dataset-in-weaviate). This marked the first time (to our knowledge) that more than a billion objects and vectors were imported into Weaviate. Dealing with ever-growing datasets is not only about being able to handle their size. Our users run complex queries in production and often have strict latency requirements.»\n",
      "[3] «---\n",
      "title: Multi-Tenancy Vector Search with millions of tenants\n",
      "slug: multi-tenancy-vector-search\n",
      "authors: [etienne]\n",
      "date: 2023-06-15\n",
      "image: ./img/hero.png\n",
      "tags: ['engineering', 'concepts']\n",
      "description: \"Learn about the new native multi-tenancy feature\"\n",
      "\n",
      "---\n",
      "\n",
      "![Multi-Tenancy ](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "Large-scale setups were always a great reason to choose Weaviate. Last year we wrote about the first time [a Weaviate setup ran with a billion objects & vectors](/blog/sphere-dataset-in-weaviate). What was a mere experiment back then is a regular production case today. But earlier this year, we saw a shift in usage patterns: As we onboarded more and more large-scale and enterprise users, the definition of scale shifted from the number of vectors to the number of individual tenants that can run on a single setup. Previously, Weaviate offered multiple ways to tackle multi-tenancy, but none were intended for a massive scale.»\n",
      "[4] «For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use. import WhatsNext from '/_includes/what-next.mdx';\n",
      "\n",
      "<WhatsNext />»\n",
      "[5] «## Monitoring and Observability\n",
      "![Monitoring and Observability](./img/monitoring-and-observability.png)\n",
      "\n",
      "One of the biggest challenges of running software in production is to understand what is happening under the hood. That is especially important when something goes wrong, or we need to anticipate in advance when more resources are required. ![It doesn't work.... why?](./img/what-is-happening.jpg)\n",
      "Without such insight, we end up looking at the black box, wondering what is going on. ### Announcement\n",
      "With Weaviate `1.14` you can get a lot more insight into the resources and the performance of different aspects of your Weaviate instance in Production.»\n",
      "\n",
      "Question: What does the document recommend for users running Weaviate at scale in production?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: The document provides insights into the Weaviate platform, a vector search engine, and its capabilities for large-scale usage. It recommends Docker Compose for trying out and evaluating Weaviate locally, while suggesting Weaviate on Kubernetes for a stable production environment. The text also highlights the benefits of using Weaviate at scale, including the ability to handle ever-growing datasets and complex queries with strict latency requirements. Additionally, it introduces the native multi-tenancy feature, which allows for a massive number of individual tenants on a single setup.\n",
      "\n",
      "Answer: The document recommends that users running Weaviate at scale in production consider enabling the native multi-tenancy feature. This feature addresses the shift in usage patterns towards a larger number of individual tenants, providing a solution for massive-scale multi-tenancy.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Document understanding techniques use an encoder-decoder pipeline that leverages the power of both computer vision and natural language processing methods. On the Weaviate Podcast, Brian Raymond described one of the founding motivations of Unstructured as follows: “Hey, HuggingFace is exploding over here with 10s of thousands of models and an incredible community. What if we did something similar to the left of HuggingFace, and we made it cheap, fast, and easy for data scientists to get through that data engineering step, so they can consume more of that!” Now that the stage is set, let’s explore how Unstructured works. Unstructured simplifies the process of importing a PDF and converting it into text. The core abstraction of Unstructured is the 'brick.' Unstructured uses bricks for document pre-processing: 1.»\n",
      "[2] «Check out one of our free weekly workshops to help you understand what vector databases are and how they can help you build production-ready AI apps quickly and easily. If you’re curious, here are some of the most commonly asked questions we encountered:\n",
      "\n",
      "**What’s the difference between a vector database and a graph or relational database?**\n",
      "\n",
      "Graph databases are used to identify relationships between objects, and vector databases are used to find objects\n",
      "Relational databases store the relations between tables and build indexes for the fast lookup of joined tables. Vector databases, on the other hand, store the embeddings of structured and unstructured data for the quick retrieval of the objects. **What features does Weaviate offer to help me protect data privacy?**\n",
      "\n",
      "Weaviate is designed with robust security measures to ensure it meets the requirements of enterprise environments. Weaviate has achieved SOC 2 certification and is encrypted in transit and at rest.»\n",
      "[3] «---\n",
      "layout: post\n",
      "title: What if you could understand your unstructured data? slug: understand-your-unstructured-data\n",
      "authors: [laura]\n",
      "date: 2021-01-20\n",
      "tags: ['concepts']\n",
      "image: ./img/hero.png\n",
      "# canonical-url: https://medium.com/semi-technologies/what-if-you-could-understand-your-unstructured-data-92f0861e016\n",
      "# canonical-name: Medium\n",
      "description: \"Learn how the AI-first vector database Weaviate unlocks the potential of unstructured data and why this is important.\"\n",
      "---\n",
      "![What if you could understand your unstructured data?](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "## Intro\n",
      "These days, more and more organizations are adopting a data-driven culture. Business processes and customer experience benefit from good data collection, management and analysis. But in order to really benefit from available data, it is essential to also understand the unstructured data, like free text in PDF documents, emails, invoices or voice transcriptions. Unstructured data is especially hard to index, manage and understand.»\n",
      "[4] «Since around [80% of all data is unstructured](https://www.forbes.com/sites/forbestechcouncil/2019/01/29/the-80-blind-spot-are-you-ignoring-unstructured-organizational-data/), it is hard to actually search and retrieve insights from most of the data. The Weaviate vector database unlocks the potential of unstructured data. Searching by fuzzy terms and classification of rich data like free text becomes possible with Weaviate. It uses AI-driven indexing and search technologies to enable real-time text processing. With machine learning methods, Weaviate automatically classifies texts.»\n",
      "[5] «These models are reaching new heights in performance because they leverage visual information, not just text. <figure>\n",
      "\n",
      "![Donut pipeline](./img/donut.png)\n",
      "<figcaption> Pipeline of Donut from Kim, G. et al (2022) </figcaption>\n",
      "</figure>\n",
      "\n",
      "\n",
      "## About Unstructured\n",
      "[Unstructured](https://www.unstructured.io/) is an open-source company working at the cutting edge of PDF processing and more. They allow businesses to ingest their diverse data sources, whether this be a `PDF`, `JPEG`, or `PPT`, and convert it into data that can be passed to a LLM. This means that you could take private documents from your company and pass it to a LLM to chat with your PDFs.\n",
      "\n",
      "Unstructured’s open-source [core library](https://github.com/Unstructured-IO/unstructured) is powered by document understanding models.»\n",
      "\n",
      "Question: What percentage of data is unstructured, and how does Weaviate help in processing it?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: The text mentions that around 80% of all data is unstructured, and provides the source for this statistic. This is a widely-accepted estimate, and a good indicator of the prevalence of unstructured data. Understanding and processing this unstructured data is a challenge, and this is where Weaviate comes in. Weaviate is an AI-first vector database that helps unlock the potential of unstructured data. It does this by using AI-driven indexing and search technologies, enabling real-time text processing and automatic classification of texts. With Weaviate, users can search using fuzzy terms and classify rich data like free text, PDFs, and more.\n",
      "\n",
      "Answer: Approximately 80% of all data is unstructured. Weaviate helps process this unstructured data by using AI-driven indexing and search, enabling real-time text processing and automatic classification.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The neighborhood: The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park.»\n",
      "[2] «| Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The space: This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\n",
      "[3] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher.»\n",
      "[4] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The gym is located in the building and is easily accessible. |\n",
      "\n",
      "</div>\n",
      "</details>\n",
      "\n",
      "## Deeper into Personalization\n",
      "Personalized digital experiences have been taken to a new level with the latest advances in AI technology.»\n",
      "[5] «|\n",
      "  </div>\n",
      "</details>\n",
      "\n",
      "<details>\n",
      "  <summary>\n",
      "    Cohere Generations\n",
      "  </summary>\n",
      "  <div>\n",
      "\n",
      "| Rank | Result |\n",
      "|------|--------|\n",
      "| 1    | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. |\n",
      "| 2    | THE VILLAGE OF HARLEM....NEW YORK !\\n\\nThis is the Village of Harlem.»\n",
      "\n",
      "Question: Does the apartment come with in-unit laundry appliances, and is it located near public transportation and Prospect Park?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to look for specific details mentioned in the context. We are specifically looking for information about laundry appliances, proximity to public transportation, and the location relative to Prospect Park.\n",
      "\n",
      "Answer: Yes, the apartment comes with in-unit laundry appliances, including a washer and dryer. It is also located near public transportation, with convenient access to the subway and multiple bus lines. Additionally, the apartment is a short walk away from Prospect Park, offering easy access to green space and leisure activities.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «The model trained to classify cats in the Paintbrush illustration will not perform as well as the model trained to classify cats in Photorealistic images, if the final use case is Photorealistic images!\n",
      "\n",
      "Domain differences are very common in Natural Language Processing (NLP), such as the difference between Legal Contracts, Financial Statements, Biomedical Scientific Papers, Wikipedia, or Reddit Conversations to give a few examples. ### Color-coded details\n",
      "For every model, Hugging Face displays a list of important **color-coded** details, such as:\n",
      "* Blue - the **dataset** it was trained on\n",
      "* Green - the **language** of the dataset\n",
      "* White or Purple - **additional details** about the model\n",
      "\n",
      "So, if we look at two Deep Learning models, we can see that [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large) was trained on **stsb_multi_mt**, which is a **French** dataset. ![Camembert Hugging Face Model Card](./img/camembert-model-card-min.png)\n",
      "\n",
      "While [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) was trained on **several datasets** in **English**. ![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)\n",
      "\n",
      "To put it as bluntly as possible, what makes `dangvantuan/sentence-camembert-large` better at French sentence embeddings than `sentence-transformers/all-MiniLM-L6-v2` is that… it was trained on **French** sentences! There are many examples like this, models trained on **biomedical text**, **legal documents**, or **Spanish** are generally going to perform better when tested on that domain compared to models that haven't been explicitly trained for the domain. Note that these tags are a part of Hugging Face’s **model cards**, an impressive effort to continue advancing the organization of Machine Learning models.»\n",
      "[2] «Particularly from my conversation with Nils Reimers, I have become very interested in the continual learning nature of this. For example, when we released the `ref2vec` module and discussed it on the podcast, the `all-miniLM-L6-v2` model has never seen ref2vec before in its training set. Additionally, a model fine-tuned up to podcast #30 will have never seen ref2vec either!\n",
      "\n",
      "    I am also very interested in the fine-tuning of cross-encoder models, which you can learn more about [here](/blog/cross-encoders-as-reranker). 3. Custom Benchmarking\n",
      "\n",
      "\tI have also been working on the BEIR benchmarking in Weaviate (nearly finished!).»\n",
      "[3] «You would need the following ingredients:\n",
      "* Raw Data\n",
      "* Hugging Face API token – which you can request from [their website](https://huggingface.co/settings/tokens)\n",
      "* A working Weaviate instance with the `text2vec-huggingface` enabled\n",
      "\n",
      "Then you would follow these steps. ### Step 1 – initial preparation – create schema and select the hf models\n",
      "Once you have a Weaviate instance up and running. Define your schema (standard stuff – pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class. This is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\n",
      "```javascript\n",
      "{\n",
      "    \"class\": \"Notes\",\n",
      "    \"moduleConfig\": {\n",
      "        \"text2vec-huggingface\": {\n",
      "            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\n",
      "            ...»\n",
      "[4] «---\n",
      "title: Weaviate Podcast Search\n",
      "slug: weaviate-podcast-search\n",
      "authors: [connor]\n",
      "date: 2023-01-31\n",
      "tags: ['how-to']\n",
      "image: ./img/hero.png\n",
      "description: \"Recent advances in AI are breathing new life into podcasting! The Whisper speech-to-text model is a game-changer!\"\n",
      "---\n",
      "![Weaviate Podcast Search](./img/hero.png)\n",
      "\n",
      "Recent advances in AI are breathing new life into podcasting! The Whisper speech-to-text model is a game-changer! It can transcribe podcasts with astonishing accuracy, allowing us to [index them into Weaviate](https://github.com/weaviate/weaviate-podcast-search)!\n",
      "I have been hosting the Weaviate podcast for a little over a year with 34 published episodes and am super excited to tell you how this will completely revamp our podcast, as well as the details behind how to build something like this for yourself. <!-- truncate -->\n",
      "\n",
      "Podcasts are easy to consume, we can listen to podcasts as we drive to work, take a walk, or play a video game. However, despite their convenience, podcasts have lacked a crucial aspect that other knowledge base mediums have - the ability to easily reference and search past content. So let’s dive into how to build it, see some queries, and then come back to how this will change podcasting!\n",
      "\n",
      "## Whisper\n",
      "\n",
      "OpenAI has taken a swing at unleashing the potential of AI technology, breaking open a piñata of new applications. Among the bounty of treats spilling out, podcasting shines bright like a glittering candy! - Written by ChatGPT\n",
      "\n",
      "```python\n",
      "!pip install git+https://github.com/openai/whisper.git\n",
      "\n",
      "import whisper\n",
      "\n",
      "# available models = [\"tiny.en\", \"tiny\", \"base.en\", \"base\", \"small.en\", \"small\", \"medium.en\", \"medium\", \"large-v1\", \"large-v2\", \"large\"]\n",
      "\n",
      "model = whisper.load_model(\"large-v2\").to(\"cuda:0\")\n",
      "\n",
      "import time\n",
      "start = time.time()\n",
      "result = model.transcribe(\"20.mp3\")\n",
      "print(f\"Transcribed in {time.time() - start} seconds.\")\n",
      "\n",
      "f = open(\"20-text-dump.txt\", \"w+\")\n",
      "f.write(result[\"text\"])\n",
      "f.close()\n",
      "```\n",
      "\n",
      "From here you get a big text file from the podcast.»\n",
      "[5] «Although a bigger machine (see below) is needed for importing the data, the serving is done on a 12 CPU, 100 GB RAM, 250Gb SSD Google Cloud VM with 1 x NVIDIA Tesla P4. The ML-models used are [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) and [bert-large-uncased-whole-word-masking-finetuned-squad](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad) both are available as [pre-built modules](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers#pre-built-images) in Weaviate. 📄 The complete dataset and code is open-source and available [on GitHub](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate). ![Demo GIF of Weaviate using the Wikipedia dataset](./img/weaviate-using-the-Wikipedia-dataset.gif)\n",
      "*Example semantic search queries in Weaviate's GraphQL interface — GIF by Author*\n",
      "\n",
      "## Importing the Data In Two Steps\n",
      "> You can also directly import a backup into Weaviate without doing the import your self as outlined [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/tree/main#step-3-load-from-backup). To import the data we use two different methods.»\n",
      "\n",
      "Question: What module was discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Context: The Weaviate podcast discusses recent advances in AI and how they can be applied to podcasting. It specifically mentions the Whisper speech-to-text model from OpenAI as a game-changer for transcribing podcasts. The podcast also covers the release of the `ref2vec` module, which is relevant to the question. Question: What module was discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on? Reasoning: The context mentions that the 'all-miniLM-L6-v2' model had not been exposed to the `ref2vec` module during its training. This is specifically highlighted in the second paragraph, where it is stated that the model \"has never seen ref2vec before in its training set.\"\n",
      "\n",
      "Answer: The module discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on is the `ref2vec` module.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «The below animation visualizes a real example of this in e-Commerce images.\"\n",
      "        },\n",
      "        {\n",
      "          \"content\": \"The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\\n\\n![Ref2Vec Image](./img/ref2vec.png)\\n\\nSuch a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User's preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User's interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User's product and style preferences. We envision Ref2Vec to have great potential in multiple application areas. Let's take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\\nMany of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.\"\n",
      "        },\n",
      "        {\n",
      "          \"content\": \"## More Coming Soon\\nWe are very excited about the potential of Ref2Vec, and how it leverages existing symbolic data to augment vector searches in a new way.»\n",
      "[2] «The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\n",
      "\n",
      "![Ref2Vec Image](./img/ref2vec.png)\n",
      "\n",
      "Such a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User's preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User's interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User's product and style preferences. We envision Ref2Vec to have great potential in multiple application areas. Let's take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\n",
      "Many of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.»\n",
      "[3] «If a user clicks on 3 shoe images on an e-commerce store, it is a safe bet that they want to see more shoes. Ref2Vec captures this intuition by calculating vectors that aggregate each User's interaction with another class. The below animation visualizes a real example of this in e-Commerce images.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 0.34444344\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\\n\\n![Ref2Vec Image](./img/ref2vec.png)\\n\\nSuch a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User's preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User's interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User's product and style preferences. We envision Ref2Vec to have great potential in multiple application areas.»\n",
      "[4] «## What is Ref2Vec? The name Ref2Vec is short for reference-to-vector, and it offers the ability to vectorize a data object with its cross-references to other objects. The Ref2Vec module currently holds the name ref2vec-**centroid** because it uses the average, or centroid vector, of the cross-referenced vectors to represent the **referencing** object.\"\n",
      "        },\n",
      "        {\n",
      "          \"content\": \"In other words, the User vector is being updated in real-time here to take into account their preferences and actions, which helps to produce more relevant results at speed. Another benefit of Ref2Vec is that this calculation is not compute-heavy, leading to low overhead. With Ref2Vec, you can use Weaviate to provide Recommendation with \\\"user-as-query\\\".»\n",
      "[5] «Let's take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\\nMany of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 0.007121429\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"In other words, the User vector is being updated in real-time here to take into account their preferences and actions, which helps to produce more relevant results at speed. Another benefit of Ref2Vec is that this calculation is not compute-heavy, leading to low overhead. With Ref2Vec, you can use Weaviate to provide Recommendation with \\\"user-as-query\\\". This is a very common and powerful way to build Home Feed style features in apps.»\n",
      "\n",
      "Question: What is Ref2Vec, and how does it integrate with Weaviate to provide real-time user-based recommendations?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to understand what Ref2Vec is and how it works with Weaviate to deliver real-time recommendations based on user behavior.\n",
      "\n",
      "Answer: Ref2Vec, or reference-to-vector, is a technique that enables the vectorization of a data object by leveraging its cross-references to other objects. In simple terms, it calculates vectors that represent a user's interactions with different items or classes. This is achieved by aggregating the representations of the referenced objects to create a vector for the referencing object. In the given context, Ref2Vec is used to capture a user's preferences and actions on an e-commerce platform. Now, how does Ref2Vec integrate with Weaviate to provide real-time user-based recommendations? Weaviate is a vector database and search engine that can power high-quality, fast recommendations. Ref2Vec enhances Weaviate's capabilities by allowing it to learn from user behavior and preferences in real time. When a user interacts with multiple items on an e-commerce platform (for example, clicking on shoe images), Ref2Vec calculates a vector that represents the user's interest in that particular category or style. This vector is created by aggregating the vectors of the referenced products (the shoes the user clicked on). By integrating Ref2Vec with Weaviate, the system can dynamically update user vectors based on their interactions and preferences. This enables Weaviate to provide relevant and personalized recommendations in real time. For instance, if a user has purchased or shown interest in boots, shorts, and a Weaviate t-shirt, Ref2Vec can aggregate these product representations to characterize the user's overall style preferences. As a result, Weaviate can offer recommendations that align closely with the user's tastes, improving their shopping experience. In summary, Ref2Vec enables the vectorization of user preferences by aggregating their interactions with various items. This information is then used by Weaviate to update user vectors in real time, leading to more accurate and personalized recommendations. The combination of Ref2Vec and Weaviate thus enhances the user experience by delivering relevant and timely suggestions based on their behavior and interests.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «We have three documents labeled `A`, `B`, and `C` and have run a BM25 and Dense search. In this example, we have set the constant *k* to 0. | BM25 Ranking | Dense Ranking | Results\n",
      "| --- | --- | --- |\n",
      "| A | B | A: 1/1 + 1/3 = 1.3 |\n",
      "| B | C | B: 1/2 + 1/1 = 1.5 |\n",
      "| C | A | C: 1/3 + 1/2 = 0.83|\n",
      "\n",
      "The above table shows the ranked order of the BM25 and Dense search. To fuse the two lists together, we need to take the sum of the reciprocal ranks. Based on the results, the top document is `Document B` with a ranking of 1.5, then `Document A` at 1.3, and `Document C` at 0.83.»\n",
      "[2] «The dense vector representation is able to disambiguate “catch” as meaning fishing rather than baseball or sickness. The sparse vector search will match the phrase “Alaskan Pollock” only. This example query shows where hybrid search combines the best of both sparse and dense vectors. ## Reciprocal Rank Fusion (RRF)\n",
      "\n",
      "While researching hybrid search, we needed a way to combine the results of BM25 and dense vector search into a single ranked list. We came across a paper from Benham and Culpepper exploring rank fusion techniques.»\n",
      "[3] «Note, the current implementation of hybrid search in Weaviate uses BM25/BM25F and vector search. If you’re interested to learn about how dense vector indexes are built and optimized in Weaviate, check out this [article](/blog/why-is-vector-search-so-fast). ### BM25\n",
      "BM25 builds on the keyword scoring method [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term-Frequency Inverse-Document Frequency) by taking the [Binary Independence Model](https://en.wikipedia.org/wiki/Binary_Independence_Model) from the IDF calculation and adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. The image below presents the scoring calculation of BM25:\n",
      "![BM25 calculation](./img/BM25-calculation.png) <div align=\"center\"><i>Source: Wikipedia page on Okapi BM25</i></div>\n",
      "\n",
      "The score of the document, query pair is determined by weighing the uniqueness of each keyword in the query relative to the collection of texts. BM25 contains additional static parameters, k1 and b that may help calibrate performance to particular datasets.»\n",
      "[4] «In this blog post, you will learn about the implementation of hybrid search in Weaviate and how to use it. ## Sparse and Dense Vectors\n",
      "Sparse and dense vectors are calculated with distinct algorithms. Sparse vectors have mostly zero values with only a few non-zero values, while dense vectors mostly contain non-zero values. Sparse embeddings are generated from algorithms like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) and [SPLADE](https://arxiv.org/abs/2107.05720). Dense embeddings are generated from machine learning models like\n",
      "[GloVe](https://text2vec.org/glove.html) and [Transformers](https://huggingface.co/docs/transformers/index).»\n",
      "[5] «And that Weaviate recently added a new algorithm for how this is done? In this post, we’ll dive into exactly the world of hybrid search to discuss how it works, how results are produced, the algorithms used, and more. So let’s get into it!\n",
      "\n",
      ":::info\n",
      "- Vector search and keyword search are also known as dense vector search and sparse vector search respectively. - Keyword search is also called a BM25 search in Weaviate, as it is based on the [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25) scoring algorithm. :::\n",
      "\n",
      "## How does hybrid search work, exactly?»\n",
      "\n",
      "Question: How are BM25 and Dense search rankings combined using reciprocal ranks, and what is the resulting order of documents A, B, and C?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: The context provides a table with the BM25 and Dense search rankings for documents A, B, and C, along with their respective reciprocal ranks. To combine the rankings and determine the final order, we need to calculate the sum of the reciprocal ranks for each document.\n",
      "\n",
      "Answer: The BM25 and Dense search rankings are combined by calculating the reciprocal rank for each document in both lists and then summing them up. The resulting order of documents A, B, and C is: 1. Document B (sum of reciprocal ranks: 1/2 + 1/1 = 1.5) 2. Document A (sum of reciprocal ranks: 1/1 + 1/3 = 1.33) 3. Document C (sum of reciprocal ranks: 1/3 + 1/2 = 0.83) So, the final ranking is B > A > C.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «So syntax that is currently like this:\n",
      "\n",
      ":::note Classic client syntax\n",
      "```python\n",
      "response['data']['Get']['Article'][0]['title']  # Get the `title` property of the first object\n",
      "response['data']['Get']['Article'][0]['_additional']['id']  # Get the ID of the first object\n",
      "response['data']['Get']['Article'][0]['_additional']['generate']['singleResult']  # Get the generated text from a `singlePrompt` request\n",
      "response['data']['Get']['Article'][0]['_additional']['generate']['groupedResult']  # Get the generated text from a `groupedTask` request\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "Become:\n",
      "\n",
      ":::info `Collections` client syntax\n",
      "\n",
      "```python\n",
      "response.objects[0].properties['title']  # Get the `title` property of the first object\n",
      "response.objects[0].uuid  # Get the ID of the first object\n",
      "response.objects[0].generated  # Get the generated text from a `singlePrompt` request\n",
      "response.generated  # Get the generated text from a `groupedTask` request\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "We think that these changes will reduce errors, increase productivity, and make the code easier to read and understand. ### Collections-first approach\n",
      "\n",
      "The other big change is that the `collections` client focuses on individual collections for interaction. This means that you will no longer need to specify the collection name in every request. Instead, you will create an object for each collection that you want to interact with, and then use that object for all subsequent requests. For example, take the following syntax for performing a simple request to retrieve a few objects from the database:\n",
      "\n",
      ":::note Classic client syntax\n",
      "\n",
      "```python\n",
      "response = (\n",
      "    client.query.get(\n",
      "        class_name=\"Article\",\n",
      "        properties=[\"title\", \"body\", \"url\"]\n",
      "    )\n",
      "    .with_limit(2)\n",
      "    .do()\n",
      ")\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "Becomes:\n",
      "\n",
      ":::info `Collections` client syntax\n",
      "\n",
      "```python\n",
      "articles = client.collection.get(\"Article\")\n",
      "response = articles.query.fetch_objects(limit=2)\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "You'll see that a search is now a method that originates from the collection object.»\n",
      "[2] «:::\n",
      "\n",
      ":::note What you will be able to do using the Web bundle\n",
      "Only Read operations powered by GraphQL. :::\n",
      "\n",
      "\n",
      "\n",
      "### Collections-first approach\n",
      "\n",
      "The other big change is that the `collections` client focuses on individual collections for interaction. This means that you will no longer need to specify the collection name in every request. Instead, you will create an object for each collection that you want to interact with, and then use that object for all subsequent requests. For example, take the following syntax for performing a simple request to retrieve a few objects from the database:\n",
      "\n",
      ":::note Classic TypeScript syntax\n",
      "\n",
      "```typescript\n",
      "const response = await client.graphql\n",
      "  .get()\n",
      "  .withClassName('Article')\n",
      "  .withLimit(2)\n",
      "  .withFields('title body url')\n",
      "  .do();\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "Becomes:\n",
      "\n",
      ":::info `Collections` client syntax\n",
      "\n",
      "```typescript\n",
      "const articles = client.collection.get('Article');\n",
      "const response = articles.query.fetchObjects({\n",
      "  limit: 2,\n",
      "  returnProperties: ['title','body','url']\n",
      "});\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "You'll see that a search is now a method that originates from the collection object.»\n",
      "[3] «This can be a hurdle when you are working with less often-used options, trying new APIs, or when you are new to Weaviate. import codeCompletionImgUrl from './img/code-completion.gif';\n",
      "\n",
      "<img src={codeCompletionImgUrl} alt=\"Code completion and type hints are added in the new Python client\" width=\"85%\"/>\n",
      "\n",
      "\n",
      "Types are introduced for the data objects as well at creation time, as well as when retrieving them from the database. This means that you can access the properties of the data object directly. So syntax that is currently like this:\n",
      "\n",
      ":::note Classic client syntax\n",
      "```typescript\n",
      "response.data?.Get?.Article?.[0].title  // Get the `title` property of the first object\n",
      "response.data?.Get?.Article?.[0]['_additional']?.id  // Get the ID of the first object\n",
      "response.data?.Get?.Article?.[0]['_additional']?.generate?.singleResult  // Get the generated text from a `singlePrompt` request\n",
      "response.data?.Get?.Article?.[0]['_additional']?.generate.groupedResult  // Get the generated text from a `groupedTask` request\n",
      "response.data?.Get?.Article?.[0]['_additional']?.creationTimeUnix // Get the timestamp when the object was created\n",
      "```\n",
      "Look at all those `?` operators and the necessary `['_additional']` index due to the `_` in the key name!\n",
      "\n",
      ":::\n",
      "\n",
      "Becomes:\n",
      "\n",
      ":::info `Collections` client syntax\n",
      "\n",
      "```typescript\n",
      "response.objects[0].properties.title  // Get the `title` property of the first object\n",
      "response.objects[0].uuid  // Get the ID of the first object\n",
      "response.objects[0].generated  // Get the generated text from a `singlePrompt` request\n",
      "response.generated  // Get the generated text from a `groupedTask` request\n",
      "response.metadata?.creationTime // Get the creation time as a native JS Date value\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "We think that these changes will reduce errors, increase productivity, and make the code easier to read and understand. ### Separated Node and Web Versions\n",
      "\n",
      "For a long time, we've used a single client to interact with Weaviate from both the Web and Node.js.»\n",
      "[4] «### Simplified methods\n",
      "\n",
      "Standalone methods with parameters now replace the builder pattern (`with_` methods) for queries. So what used to be a chain of methods like this:\n",
      "\n",
      ":::note Classic client syntax\n",
      "\n",
      "```python\n",
      "response = (\n",
      "    client.query.get(\n",
      "        class_name=\"JeopardyQuestion\",\n",
      "        properties=[\"question\", \"answer\"]\n",
      "    )\n",
      "    .with_near_text({\"concepts\": [\"the space race\"]})\n",
      "    .with_generate(\n",
      "        grouped_task=\"Write a haiku about these facts!\",\n",
      "    )\n",
      "    .with_limit(2)\n",
      "    .do()\n",
      ")\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "Becomes:\n",
      "\n",
      ":::info `Collections` client syntax\n",
      "\n",
      "```python\n",
      "questions = client.collection.get(\"JeopardyQuestion\")\n",
      "response = questions.generate.near_text(\n",
      "    query=\"the space race\",\n",
      "    limit=2,\n",
      "    grouped_task=\"Write a haiku about these facts!\"\n",
      ")\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "### Property/metadata return defaults\n",
      "\n",
      "You might have noticed that above examples do not specify properties to be returned!\n",
      "\n",
      "We have changed the default behavior to return most properties and metadata such as the object ID, creation time, vector search distance and so on. We think this will make it easier to get started, and for production use-cases where you want to optimize the response size, you can still specify the properties you want to return. ### Batch import typing\n",
      "\n",
      "Batch object, now called `insert_many`, also gets a refresh with the introduction of a `DataObject` class. This class is used to define the properties of the object to be inserted, and is then passed to the `insert_many` method.»\n",
      "[5] «### Simplified methods\n",
      "\n",
      "Standalone methods with parameters now replace the builder pattern (`.with` methods) for queries. So what used to be a chain of methods like this:\n",
      "\n",
      ":::note Classic client syntax\n",
      "\n",
      "```typescript\n",
      "generatePrompt = 'Convert this quiz question: {question} and answer: {answer} into a trivia tweet.';\n",
      "\n",
      "result = await client.graphql\n",
      "  .get()\n",
      "  .withClassName('JeopardyQuestion')\n",
      "  .withGenerate({\n",
      "    singlePrompt: generatePrompt,\n",
      "  })\n",
      "  .withNearText({\n",
      "    concepts: ['World history'],\n",
      "  })\n",
      "  .withFields('round _additional { distance }')\n",
      "  .withLimit(2)\n",
      "  .do();\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "Becomes:\n",
      "\n",
      ":::info `Collections` client syntax\n",
      "\n",
      "```typescript\n",
      "const generatePrompt = `Convert this quiz question: {question} and answer: {answer} into a trivia tweet.`;\n",
      "\n",
      "const myCollection = client.collections.get('JeopardyQuestion');\n",
      "const result = await myCollection.generate.nearText(['World history'], {\n",
      "  singlePrompt: generatePrompt\n",
      "}, {\n",
      "  limit: 2,\n",
      "  returnMetadata: ['distance']\n",
      "\n",
      "})\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "### Property/metadata return defaults\n",
      "\n",
      "You might have noticed that the above examples do not specify properties to be returned!\n",
      "\n",
      "We have changed the default behavior to return all non-reference properties, e.g. `text` and `object` types, and the IDs of the objects. We think this will make it easier to get started, and for production use cases where you want to optimize the response size, you can still specify the properties you want to return. To request additional metadata and any cross-references, you can use the `returnMetadata` and `returnReferences` options for each of the search queries. ### Batch import typing\n",
      "\n",
      "Batch object, now called `insertMany()`, also gets a refresh with the introduction of a `DataObject` class.»\n",
      "\n",
      "Question: What are the syntax differences between the classic client and the collections client for accessing data object properties in the described programming environment?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to identify and compare the syntax examples provided for both the classic client and the collections client when accessing data object properties. By examining the context, we can determine the specific differences in syntax between the two clients.\n",
      "\n",
      "Answer: The syntax differences between the classic client and the collections client for accessing data object properties are as follows: Classic Client: - Uses a chain of methods with the `with_` prefix, such as `with_limit` and `with_generate`, to specify query parameters. - Requires specifying the collection name in every request. - Uses a builder pattern for queries. - Requires using the `?` operator for optional chaining and the `['_additional']` index due to the underscore in the key name. - Accesses properties using bracket notation, such as `response['data']['Get']['Article'][0]['title']`. Collections Client: - Uses standalone methods with parameters, such as `fetch_objects` and `generate.near_text`, to perform queries. - Does not require specifying the collection name in every request. Instead, a collection object is created and used for subsequent requests. - Simplifies the syntax by removing the need for the `?` operator and the `['_additional']` index. - Accesses properties using dot notation, such as `response.objects[0].properties['title']`. - Introduces types for data objects, allowing direct access to their properties. - Returns most properties and metadata by default, including object IDs, creation times, and vector search distances. The collections client aims to improve readability, reduce errors, and increase productivity by simplifying the syntax and introducing standalone methods. It also eliminates the need to specify the collection name in every request, streamlining the code.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «It significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks. 2. It requires much less compute while achieving better performance (<30% of DALLE)\n",
      "3. MM-RAG capable models also generate images much more faithful to the retrieved context - meaning the quality of the generated images is better and grounded in the retrieved context image. 4.»\n",
      "[2] «Are capable of multimodal in-context learning (e.g., image generation from demonstrations) - meaning that we can feed any demonstration images and text so that the model generates an image that follows the visual characteristics of these in-context images. MM-RAG gives us a way to further control the awesome generative power of these new multimodal models to produce more useful results for use in industry. ## MM-RAG Code Demo\n",
      "\n",
      "Below we provide code that allows you to implement MM-RAG by retrieving from the multimodal Weaviate collection that we set up earlier and then stuffing a `base64` encoded image along with a text prompt to the GPT4-Vision model released recently by OpenAI. We then take the generated description and pass it into DALL-E-3 to recreate the image from the description. The full code for the example below can be found in this [Jupyter notebook](https://github.com/weaviate-tutorials/multimodal-workshop/blob/main/2-multimodal/2-multimedia-rag.ipynb).»\n",
      "[3] «This tiny ambassador of affection sits proudly, almost as if understanding the joy it brings to those around it. With its compact size and affectionate demeanor, this little canine looks ready to dispense unlimited love and puppy kisses on demand. ### Use Text to Reconstruct the Image from DALL-E-3 (Diffusion Model):\n",
      "\n",
      "Currently, GPT4-V can't produce images so to generate an image from the above description we will use the new DALL-E-3 model instead:\n",
      "\n",
      "<details>\n",
      "  <summary>Using OpenAI DALL-E-3</summary>\n",
      "\n",
      "```python\n",
      "from openai import OpenAI\n",
      "def generate_image_dalee3(prompt):\n",
      "  openai_client = OpenAI()\n",
      "  response_oai = openai_client.images.generate(\n",
      "    model=\"dall-e-3\",\n",
      "    prompt=str(prompt),\n",
      "    size=\"1024x1024\",\n",
      "    quality=\"standard\",\n",
      "    n=1,\n",
      "  )\n",
      "  result = response_oai.data[0].url\n",
      "  print(f\"Generated image url: {result}\")\n",
      "  return result\n",
      "image_url = generate_image_dalee3(GENERATED_DESCRIPTION)\n",
      "```\n",
      "\n",
      "</details>\n",
      "\n",
      "**Generated Image:**\n",
      "![generated puppy](./img/puppy_dalle.png)\n",
      "\n",
      "## Conclusion\n",
      "In this blog, we covered how we can extend the concept of RAG to include retrieval from a multimodal knowledge base. We also explained how multimedia can be embedded into a unified vector space and consequently how we can leverage vector databases to power any-to-any search. I hope you found this article useful! I'd love to connect on **X** at [@zainhasan6](https://twitter.com/ZainHasan6)!\n",
      "\n",
      "import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "[4] «In order to train a diffusion model we ask it to remove the noise from the noised images step by step until it recovers something as close as possible to the original image. This process is known as “de-noising”, is illustrated below and, is carried out for each image in the training set with multiple levels of random noise added. Once the diffusion model is trained in this way it becomes an expert at taking images that are less likely to be seen in the dataset (noisy images) and incrementally turning them into something that is more likely to be seen in the training set. By teaching the model to “de-noise” images we have developed a way to alter images to make them more like images from the training set. ![denoising gif](./img/denoise.gif)\n",
      "*[Source](https://yang-song.net/blog/2021/score/)*\n",
      "\n",
      "![denoising images](./img/denoisingimage.png)\n",
      "*[Source](https://huggingface.co/blog/annotated-diffusion)*\n",
      "\n",
      "Now if we take this trained diffusion model and just give it a random static image and run the de-noising process it will transform the static image into an image that resembles images in the training set!\n",
      "\n",
      "![noising denoising images](./img/noising_denoising.png)\n",
      "\n",
      "## How Text Prompts Control the Image Generation Process\n",
      "\n",
      "So far we have explained the general idea behind how diffusion models can start off from static noise and incrementally alter the pixel values so that the picture all together gains meaning and follows the distribution of the training set.»\n",
      "[5] «---\n",
      "title: How A.I. Creates Art - A Gentle Introduction to Diffusion Models\n",
      "slug: how-ai-creates-art\n",
      "authors: [zain]\n",
      "date: 2023-01-24\n",
      "tags: ['concepts']\n",
      "image: ./img/hero.png\n",
      "description: \"Machine learning models can create beautiful and novel images. Learn how Diffusion Models work and how you could make use of them.\"\n",
      "---\n",
      "![How A.I. Creates Art](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "One of the major developments this past year were the advancements made in machine learning models that can create beautiful and novel images such as the ones below. Though machine learning models with the capability to create images have existed for a while, this previous year we saw a marked improvement in the quality and photo-realism of the images created by these models. ![Perception of the world](./img/perception_of_the_world.jpg)\n",
      "\n",
      "Models like [DALL·E 2](https://openai.com/product/dall-e-2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion) and others which are the technologies underlying many platforms such as Lensa and Midjourney are being used by millions of people and are quickly becoming main stream as people realize their potential. These models not only have the ability to dream up photo-realistic images when prompted with text input but can also modify given images to add details, replace objects or even paint in a given artists style.»\n",
      "\n",
      "Question: Which model outperforms DALL-E and CM3 in image and caption generation tasks while requiring less computational power and producing images more faithful to the retrieved context?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: In the provided context, there is a direct comparison made between different models, including DALL-E, CM3, and MM-RAG capable models. To answer the question, we need to understand the performance and capabilities of each model.\n",
      "\n",
      "Answer: MM-RAG capable models outperform DALL-E and CM3 in image and caption generation tasks. They require less computational power (<30% of DALL-E) while producing higher-quality images that are more faithful to the retrieved context. This is because MM-RAG provides a way to control the generative power of multimodal models, resulting in more useful outcomes for industrial applications.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «| Welcome to your home away from home in the heart of Manhattan! This huge 2 bedroom is located just steps away from Central Park and all the best that New York City has to offer.\\n\\nThe apartment has been newly renovated with stainless steel appliances, exposed brick, and beautiful hardwood floors. You'll love the convenience of being located in a very safe and convenient area, with a variety of restaurants and stores nearby. The subway is also very close, making it easy to get around the city.\\n\\nWhether you're here for business or pleasure, this is the perfect place to call home during your stay. Don't miss out on the opportunity to explore all that New York City has to offer from the comfort of this amazing apartment. Book your stay today and start planning your adventure! |\n",
      "| Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\n",
      "[2] «| Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The space: This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\n",
      "[3] «It is newly renovated with stainless steel appliances, exposed brick, and beautiful hardwood floors. The apartment is located in a very safe and convenient area, with a variety of restaurants and stores nearby. The subway is also very close, making it easy to get around the city. So what are you waiting for? Book your stay today and experience the best of New York City! | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\n",
      "[4] «|\n",
      "  </div>\n",
      "</details>\n",
      "\n",
      "<details>\n",
      "  <summary>\n",
      "    Cohere Generations\n",
      "  </summary>\n",
      "  <div>\n",
      "\n",
      "| Rank | Result |\n",
      "|------|--------|\n",
      "| 1    | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. |\n",
      "| 2    | THE VILLAGE OF HARLEM....NEW YORK !\\n\\nThis is the Village of Harlem.»\n",
      "[5] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher.»\n",
      "\n",
      "Question: What features does the renovated 2 bedroom New York City apartment offer?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To determine the features of the renovated 2-bedroom apartment in New York City, we need to carefully examine the provided information and identify the key attributes mentioned. We should also pay attention to any unique or standout features that set this apartment apart from others.\n",
      "\n",
      "Answer: The renovated 2-bedroom apartment in New York City boasts several attractive features. Here is a list of what it offers: - Prime location: Situated in the heart of Manhattan, just a short distance from Central Park, providing easy access to the best attractions of New York City. - Newly renovated: Features stainless steel appliances, exposed brick walls, and beautiful hardwood floors, creating a modern and stylish ambiance. - Convenience and safety: Located in a safe and convenient neighborhood, with a variety of restaurants, stores, and the subway nearby, making daily life and navigation around the city effortless. - Comfort and privacy: Offers a private entrance, providing added privacy and comfort during your stay. - Spacious and well-equipped kitchen: The large kitchen is equipped with brand new stainless steel appliances, perfect for cooking and entertaining. - Comfortable bedrooms: Includes two bedrooms, each furnished with a queen-size bed, ensuring a good night's rest for up to four people. - Full bathroom: Features a soaking tub for a relaxing bath after a long day of exploring the city. - Additional amenities: Depending on the specific unit, some apartments also offer a washer, dryer, and dishwasher for added convenience during extended stays. - Easy transportation: The proximity to the subway and bus lines makes getting around the city a breeze, and it's also within walking distance of Prospect Park for leisure and outdoor activities. This apartment seems to be an excellent choice for anyone seeking a comfortable, convenient, and well-located stay in New York City, whether for business or pleasure.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «---\n",
      "title: Fine-Tuning Cohere's Reranker\n",
      "slug: fine-tuning-coheres-reranker\n",
      "authors: [erika]\n",
      "date: 2024-02-13\n",
      "image: ./img/hero.png\n",
      "tags: ['how-to', 'search']\n",
      "description: \"Learn how to fine-tune Cohere's reranker and generate synthetic data using DSPy!\"\n",
      "---\n",
      "\n",
      "![Hero image](./img/hero.png)\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Search engines and retrieval augmented generation (RAG) applications rely heavily on retrieval accuracy. There are many techniques for achieving highly accurate retrieval. One of the most common patterns we’ve seen is multi-stage search. Typically this is in two stages: 1. Gather the relevant objects with the first-stage retriever 2.»\n",
      "[2] «Furthermore, options like defining filters or metadata that were defined through string parameters in the `Classic` client are now defined through custom Python classes. import nearTextQueryImgUrl from './img/neartext_query_hints.gif';\n",
      "\n",
      "<img src={nearTextQueryImgUrl} alt=\"IDE autocompletion in queries have been added\" width=\"85%\"/>\n",
      "\n",
      "### Retrieval augmented generation\n",
      "\n",
      "As you saw briefly earlier, we have a `.generate` submodule for retrieval augmented generation. The structure of these mirror those of the queries, with additional parameters added for the generation task. For example, this query:\n",
      "\n",
      "```python\n",
      "response = questions.query.near_text(\n",
      "    query=\"the space race\",\n",
      "    limit=2,\n",
      ")\n",
      "```\n",
      "\n",
      "Can be converted to a retrieval augmented generation task by switching the submodule to `.generate` and adding the `grouped_task` parameter:\n",
      "\n",
      "```python\n",
      "# highlight-start\n",
      "response = questions.generate.near_text(\n",
      "# highlight-end\n",
      "    query=\"the space race\",\n",
      "    limit=2,\n",
      "    # highlight-start\n",
      "    grouped_task=\"Write a haiku about these facts!\"\n",
      "    # highlight-end\n",
      ")\n",
      "```\n",
      "\n",
      "## Now it's your turn\n",
      "\n",
      "![Tell us what you think](https://media.giphy.com/media/g2QPc8eIPrYkTBwC4v/giphy.gif)\n",
      "\n",
      "If you've made it this far, you're probably excited to try out the new client. We're soo excited for you to try it out too!\n",
      "\n",
      "We have prepared a [dedicated page](/developers/weaviate/client-libraries/python) with instructions on how to install and use the new client.»\n",
      "[3] «---\n",
      "title: 'Verba: Building an Open Source, Modular RAG Application'\n",
      "slug: verba-open-source-rag-app\n",
      "authors: [victoria, edward]\n",
      "date: 2024-03-07\n",
      "image: ./img/hero.png\n",
      "tags: ['how-to', 'concepts']\n",
      "description: 'Verba is an open source Retrieval Augmented Generation (RAG) application built using a modular, customizable architecture that makes it easy for anyone to use AI methods to get personalized answers on their own data. '\n",
      "---\n",
      "\n",
      "![Hero image](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "import example from './img/example.mp4';\n",
      "import chunks from './img/chunks.mp4';\n",
      "import ingestion from './img/ingestion.mp4';\n",
      "\n",
      "Have you ever wished that you could ask a question and get a personalized, relevant answer without having to dig through pages of search results? That's exactly what Retrieval Augmented Generation (RAG) lets you do. RAG applications search for relevant sources and create a personalized answer, using provided data sources instead of the general knowledge Large Language Models (LLMs) are trained on. But as cool as RAG is, creating a functional application can be overwhelming.»\n",
      "[4] «We won't get into the details of gRPC here, but if you are interested, you can read more about it [here](https://grpc.io/docs/what-is-grpc/introduction/). We will also be writing more about it in the future. ## Object typing in focus\n",
      "\n",
      "> **Highlights: Strong typing in queries, retrieval augmented generation, data operations, ...**\n",
      "\n",
      "We've mentioned object typing a few times already, but let's take a closer look at a few more concrete examples. Here you can see the new syntax for collection definition, queries and retrieval augmented generation, as well as types for each of thees tasks. To get started you can import the set of submodules like shown below, and use them as needed.»\n",
      "[5] «However, the modular architecture also allows for customization of each part of the pipeline if you needed to adjust the system to further support different use cases. We have lots of resources already about Verba, including:\n",
      "\n",
      "- ⚙️ The open source GitHub repository: [https://github.com/weaviate/verba](https://github.com/weaviate/verba)\n",
      "- 💻 A live demo ingested with Weaviate data, to answer your Weaviate-related questions: [https://verba.weaviate.io](https://verba.weaviate.io)\n",
      "- 📹 An in-depth video of how to use Verba with your own data: [https://youtu.be/IiNDCPwmqF8](https://youtu.be/IiNDCPwmqF8?feature=shared)\n",
      "\n",
      "![What is RAG? Animation](./img/rag_flow.gif)\n",
      "\n",
      "## What is Retrieval Augmented Generation (RAG)? Retrieval Augmented Generation (RAG) is an advanced method to enhance traditional search techniques by using a Large Language Model (LLM) to help identify and summarize answers. Instead of having to manually comb through documents to find an answer, a RAG system allows you to retrieve relevant sources and return a response in natural, conversational language.»\n",
      "\n",
      "Question: What is Retrieval Augmented Generation and what are its common applications?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's break down the context to understand Retrieval Augmented Generation (RAG) and its common applications. First, the context mentions that RAG is a technique used in search engines and information retrieval systems. It is described as an advanced method that enhances traditional search techniques by leveraging a Large Language Model (LLM). The main idea behind RAG is to improve the accuracy and relevance of search results by combining information retrieval techniques with the power of LLMs. By using an LLM, RAG systems can identify and summarize answers from relevant sources, providing users with more concise and personalized responses. Now, let's identify the common applications of RAG: - **Search Engines**: RAG is particularly useful for search engines, where the goal is to provide users with the most relevant and accurate results for their queries. By incorporating RAG techniques, search engines can offer more personalized and contextually relevant answers, enhancing the user experience. - **Question Answering Systems**: RAG is ideal for question-answering systems, where the goal is to provide direct answers to user queries. By retrieving relevant sources and using LLMs to generate responses, RAG systems can offer more accurate and contextually rich answers. - **Information Retrieval Applications**: Any application that involves retrieving information from a large corpus of data can benefit from RAG. This includes document retrieval systems, knowledge bases, and content recommendation engines. RAG enables these applications to provide more targeted and relevant results to users. - **Natural Language Processing (NLP) Tasks**: RAG can be applied to various NLP tasks beyond just question answering. It can be used for text summarization, information extraction, sentiment analysis, and language understanding tasks, where the ability to retrieve relevant context is crucial.\n",
      "\n",
      "Answer: Retrieval Augmented Generation (RAG) is an advanced technique used in search engines and information retrieval systems. It enhances traditional search techniques by leveraging a Large Language Model (LLM) to identify and summarize relevant answers. RAG is particularly useful for search engines, question-answering systems, information retrieval applications, and various Natural Language Processing (NLP) tasks, providing more accurate, personalized, and contextually relevant responses to users.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «More importantly, every tenant has a dedicated high-performance vector index providing query speeds as if the tenant was the only user on your cluster. With more features in the pipeline, such as tenant-specific rate limiting or tenant-specific replication factors, you can customize performance per tenant even further. ### GDPR-compliant and efficient deletes\n",
      "When discussing solutions made for many users, our first intuition is to worry about how we onboard and serve them. But deleting them is equally important – both for technical and legal reasons. Take GDPR as an example.»\n",
      "[2] «You can easily run with 50,000+ active shards per node. With just 20 nodes, you can support 1M concurrently active tenants. And with support for inactive tenants (coming soon - read below), there is no limit per node at all. ### Fast and efficient querying\n",
      "You don’t need to set a filter to restrict a query to a tenant. The simple addition of a tenant key is enough, and Weaviate will find the right tenant’s shard for you.»\n",
      "[3] «From a performance perspective, you would build a giant monolithic vector index with potentially billions of vectors, yet you would only ever query a tiny fraction of it. With a median tenant storing between 1,000 and 100,000 objects, you would typically query less than 0.01% of the index. What a waste of resources. Additionally, dropping many tenants simultaneously would lead to expensive repair operations on the shared index. Resource isolation for tenants was also not possible.»\n",
      "[4] «### Multi Tenancy\n",
      "\n",
      "[Multi tenancy](https://weaviate.io/developers/weaviate/manage-data/multi-tenancy) isolates data in individual tenants to avoid cross querying of search results. Each tenant is a separate shard in Weaviate, giving you high guarantees of isolation between data. This feature is ideal if you have different customers where each wants to load data into your enterprise application, but don’t want cross pollination of results between those tenants. The Weaviate multi tenancy scales to millions of tenants. If you’d like to learn more about our multi tenancy implementation, [read this blog post](https://weaviate.io/blog/multi-tenancy-vector-search) by our CTO, Etienne Dilocker.»\n",
      "[5] «With Weaviate’s rebalancing features (coming soon), you can ensure tenants are distributed across nodes exactly how you want them to – or leave it to Weaviate to do it for you. ### Active and inactive tenants\n",
      "With traditional search and other vector search solutions, infrastructure is typically sized for the number of objects or vectors that could be served. But why should you pay for expensive compute and memory resources for users who aren’t currently active? Enabled by Weaviate’s strict isolation between tenants, Weaviate allows you to distinguish between active and inactive tenants. A user hasn’t been logged in for an hour?»\n",
      "\n",
      "Question: How does the system ensure fast query speeds for individual tenants and what upcoming features will allow further customization of tenant performance?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to focus on the parts of the context that discuss query speeds and tenant performance. We can then identify the current mechanisms in place and the future features that will enhance them.\n",
      "\n",
      "Answer: The system ensures fast query speeds for individual tenants by providing each tenant with a dedicated high-performance vector index. This isolation means that queries are restricted to a specific tenant's shard, allowing for quick and efficient retrieval of data. Upcoming features that will allow further customization of tenant performance include tenant-specific rate limiting and tenant-specific replication factors. These features will provide finer-grained control over how tenants utilize resources and interact with the system, ultimately resulting in even faster query speeds and improved performance for individual tenants.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «The below `curl` command will back up all classes in **W1**, and call the backup `my-very-first-backup`. ```bash\n",
      "curl \\\n",
      "-X POST \\\n",
      "-H \"Content-Type: application/json\" \\\n",
      "-d '{\n",
      "     \"id\": \"my-very-first-backup\"\n",
      "    }' \\\n",
      "http://localhost:8080/v1/backups/filesystem\n",
      "```\n",
      "\n",
      ":::note The `backup_id` must be unique. The ID value is used to create a subdirectory in the backup location, and attempting to reuse an existing ID will cause Weaviate to throw an error. Delete the existing directory if one already exists. :::\n",
      "\n",
      "Now try running `3_backup` yourself to back up data from **W1**.»\n",
      "[2] «```bash\n",
      "scripts/3_backup.sh\n",
      "```\n",
      "\n",
      "If you check the contents of the backup directory again, you should see a new directory called `my-very-first-backup` containing the backup data files. Restoring this data can be done with a similarly short piece of code. The `curl` command below will restore our backup:\n",
      "```bash\n",
      "curl \\\n",
      "-X POST \\\n",
      "-H \"Content-Type: application/json\" \\\n",
      "-d '{\n",
      "     \"id\": \"my-very-first-backup\"\n",
      "    }' \\\n",
      "http://localhost:8090/v1/backups/filesystem/my-very-first-backup/restore\n",
      "```\n",
      "\n",
      "Try running `4_restore` yourself to restore the **W1** backup data to **W2**. ```bash\n",
      "scripts/4_restore.sh\n",
      "```\n",
      "\n",
      "Now, check the schemas again for **W1** and **W2**. ```bash\n",
      "scripts/0_query_instances.sh\n",
      "```\n",
      "\n",
      "Do they *both* now contain the same schema?»\n",
      "[3] «For example, the below `curl` command will restore only the `Author` class regardless of whether any other classes have been also included in `my-very-first-backup`. ```bash\n",
      "curl \\\n",
      "-X POST \\\n",
      "-H \"Content-Type: application/json\" \\\n",
      "-d '{\n",
      "     \"id\": \"my-very-first-backup\",\n",
      "     \"include\": [\"Author\"]\n",
      "    }' \\\n",
      "http://localhost:8090/v1/backups/filesystem/my-very-first-backup/restore\n",
      "```\n",
      "\n",
      "Delete everything in **W2** first with `8_delete_w2`, and try out the partial restore with `4a_partial_restore`. ```bash\n",
      "scripts/8_delete_w2.sh\n",
      "scripts/4a_partial_restore.sh\n",
      "```\n",
      "\n",
      "You should see that **W2** will only contain one class even though its data was restored from a backup that contains multiple classes. The restore function allows you to restore a class as long as the target Weaviate instance does not already contain that class. So if you run another operation to restore the `Book` class to **W2**, it will result in an instance containing both `Author` and `Book` classes.»\n",
      "[4] «For example, you can create a backup called **first_backup** and push it to **GCS**, like this:\n",
      "\n",
      "```js\n",
      "POST /v1/backups/gcs/\n",
      "{\n",
      "  \"id\": \"first_backup\"\n",
      "}\n",
      "```\n",
      "\n",
      "Then, you can check the backup status by calling:\n",
      "\n",
      "```js\n",
      "GET /v1/backups/gcs/first_backup\n",
      "```\n",
      "\n",
      "### Restore\n",
      "To restore a backup, you can call:\n",
      "\n",
      "```js\n",
      "POST /v1/backups/{store}/{backup_id}/restore\n",
      "```\n",
      "\n",
      "So, using our previous example, you can restore the **first_backup**, like this:\n",
      "\n",
      "```js\n",
      "POST /v1/backups/gcs/first_backup/restore\n",
      "```\n",
      "\n",
      "You can also, check the status of an ongoing restoration by calling:\n",
      "\n",
      "```js\n",
      "GET /v1/backups/gcs/first_backup/restore\n",
      "```\n",
      "\n",
      "### Cross-cloud\n",
      "Here is one interesting thing that you might not have noticed. You can use this setup to run Weaviate with one cloud provider but then store and restore backups to/from another cloud provider. So, for example, you can run Weaviate on AWS and use GCS for your backup needs. How cool is that? ### Class backups\n",
      "You can also create backups for specific classes or select which classes you want to restore.»\n",
      "[5] «### Asynchronous operations\n",
      "In some cases, Weaviate's response to your `backup` or `restore` request may have  `\"status\":\"STARTED\"`.<br/>\n",
      "*Isn't it interesting that the status was not indicative of a completion?*\n",
      "\n",
      "That is because Weaviate's backup operation can be initiated and monitored asynchronously. This means that you don't need to maintain a connection to the server for the operation to complete. And you can look in on the status of a restore operation with a command like:\n",
      "```bash\n",
      "curl http://localhost:8090/v1/backups/filesystem/my-very-first-backup/restore\n",
      "```\n",
      "\n",
      "Weaviate remains available for read and write operations while backup operations are ongoing. And you can poll the endpoint to check its status, without worrying about any potential downtime. Check out `3a_check_backup_status.sh` and `4b_check_restore_status.sh` for examples of how to query **W1** for the backup status, or **W2** for the restore status respectively.»\n",
      "\n",
      "Question: How do you create a unique backup called `my-very-first-backup` for classes in W1 using a curl command?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can refer to the provided context to identify the steps to create a unique backup called 'my-very-first-backup' for classes in W1 using a curl command.\n",
      "\n",
      "Answer: To create a unique backup called 'my-very-first-backup' for classes in W1, you can use the following curl command: ```bash curl \\ -X POST \\ -H \"Content-Type: application/json\" \\ -d '{ \"id\": \"my-very-first-backup\" }' \\ http://localhost:8080/v1/backups/filesystem ``` This command sends a POST request to the Weaviate instance at 'http://localhost:8080' to create a backup with the unique ID 'my-very-first-backup'. The '-X POST' option specifies the HTTP method as POST, and the '-d' option provides the request body, which includes the 'id' field set to the desired backup name. The '-H \"Content-Type: application/json\"' option sets the content type of the request to JSON. By executing this command, you initiate the process of creating a backup of all classes in W1 with the specified unique name.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\n",
      "let them do it for you.\"\n",
      "\n",
      "## Support for Hugging Face Inference API in Weaviate\n",
      "Starting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\n",
      "> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\n",
      "[2] «---\n",
      "title: Support for Hugging Face Inference API in Weaviate\n",
      "slug: hugging-face-inference-api-in-weaviate\n",
      "authors: [sebastian]\n",
      "date: 2022-09-27\n",
      "tags: ['integrations']\n",
      "image: ./img/hero.png\n",
      "description: \"Running ML Model Inference in production is hard. You can use Weaviate – a vector database – with Hugging Face Inference module to delegate the heavy lifting.\"\n",
      "---\n",
      "![Support for Hugging Face Inference API in Weaviate](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "Vector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data – text, image, audio, etc. – into vectors and use that for context-based queries) and many more use cases. All of these use cases require `Machine Learning model inference` – a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) – which is a compute-heavy process.»\n",
      "[3] «Then, you just run the queries, as per usual:\n",
      "```javascript\n",
      "nearText = {\n",
      "    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\n",
      "    \"distance\": 0.6,\n",
      "}\n",
      "\n",
      "result = (\n",
      "    client.query\n",
      "    .get(\"Notes\", [\n",
      "        \"name\",\n",
      "        \"comment\",\n",
      "        \"_additional {certainty distance} \"])\n",
      "    .with_near_text(nearText)\n",
      "    .do()\n",
      ")\n",
      "```\n",
      "\n",
      "## Summary\n",
      "> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\n",
      "Check out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.»\n",
      "[4] «Now, that's a lot of new models. 😉\n",
      "\n",
      "#### How this works\n",
      "The way the module works, Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. You need a `Hugging Face API Token` to use the Hugging Face module. You can [request it here](https://huggingface.co/login?next=%2Fsettings%2Ftokens).»\n",
      "[5] «On the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:\n",
      "\n",
      "```yaml\n",
      "DEFAULT_VECTORIZER_MODULE: text2vec-huggingface\n",
      "ENABLE_MODULES: text2vec-huggingface\n",
      "```\n",
      "\n",
      "## How to get started\n",
      "\n",
      ":::note\n",
      "This article is not meant as a hands-on tutorial. For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). :::\n",
      "\n",
      "The overall process to use a Hugging Face module with Weaviate is fairly straightforward. ![Recipe for using the Hugging Face module](./img/how-to-get-started-recipe.png)\n",
      "If this was a cooking class and you were following a recipe.»\n",
      "\n",
      "Question: How can I use Hugging Face modules with Weaviate without running the Inference API myself?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We know from the context that Weaviate offers a way to use Hugging Face modules without having to run the Inference API directly. The process is described as straightforward, and we can identify the key steps to follow: - Weaviate includes a Hugging Face module that provides support for Hugging Face Inference. This is available from version 1.15 onwards. - The Hugging Face module delegates the model inference tasks to the Hugging Face Inference service. - To use the module, one needs to set 'text2vec-huggingface' as the default vectorizer in the configuration. - An API token from Hugging Face is also required, which can be obtained from their website. - With these settings in place, Weaviate coordinates the data imports, updates, and queries, communicating with the Hugging Face Inference API on the user's behalf.\n",
      "\n",
      "Answer: To use Hugging Face modules with Weaviate without running the Inference API yourself, follow these steps: 1. Ensure you are using Weaviate version 1.15 or newer. 2. Set 'text2vec-huggingface' as the default vectorizer in your Weaviate configuration. 3. Obtain a Hugging Face API token from the Hugging Face website. 4. Provide your API token when setting up the Hugging Face module in Weaviate. 5. With these settings, Weaviate will handle the communication with the Hugging Face Inference API, allowing you to focus on your application's requirements.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «---\n",
      "title: Better automated testing with Embedded Weaviate\n",
      "slug: automated-testing\n",
      "authors: [dan]\n",
      "date: 2023-07-18\n",
      "image: ./img/hero.png\n",
      "tags: ['how-to']\n",
      "description: \"Learn how to make testing less of a chore with Embedded Weaviate, and other tips for better automated testing.\"\n",
      "\n",
      "---\n",
      "\n",
      "import Tabs from '@theme/Tabs';\n",
      "import TabItem from '@theme/TabItem';\n",
      "import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';\n",
      "import PyCode from '!!raw-loader!/_includes/code/automated-testing.py';\n",
      "import TSCode from '!!raw-loader!/_includes/code/automated-testing.ts';\n",
      "import { DownloadButton } from '/src/theme/Buttons';\n",
      "\n",
      "![Automated testing for Weaviate applications](./img/hero.png)\n",
      "\n",
      "As a software engineer with experience in test automation, I firmly believe in [Test-Driven Development](https://en.wikipedia.org/wiki/Test-driven_development), and more specifically, incorporating [integration testing](https://en.wikipedia.org/wiki/Integration_testing) from the very early stages of developing an application. <!-- truncate -->\n",
      "\n",
      "But you probably know that writing tests is quite a task in itself, and in many cases running them can also be a chore. For example, the test suite may need to set up and tear down a separate service such as a database, which can be time-consuming and error-prone. I'm here to tell you that it doesn't have to be that way. In this article, I'll show you how to make testing easier with [Embedded Weaviate](/developers/weaviate/installation/embedded), and other tips for better automated testing.»\n",
      "[2] «<!-- truncate -->\n",
      "\n",
      "\n",
      "## 1. Install the client library\n",
      "\n",
      "The Python and TypeScript client libraries support running Weaviate embedded on Linux, and starting with versions 3.21.0 and 1.2.0 respectively, on macOS as well. <Tabs groupId=\"languages\">\n",
      "  <TabItem value=\"py\" label=\"Python\">\n",
      "\n",
      "  ```bash\n",
      "  pip install weaviate-client  --upgrade\n",
      "  ```\n",
      "\n",
      "  </TabItem>\n",
      "\n",
      "  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n",
      "\n",
      "  ```bash\n",
      "  npm install weaviate-ts-embedded typescript ts-node jest  # also install support for TypeScript and Jest testing\n",
      "  ```\n",
      "\n",
      "  </TabItem>\n",
      "</Tabs>\n",
      "\n",
      "\n",
      "## 2. Run the code\n",
      "\n",
      "<Tabs groupId=\"languages\">\n",
      "  <TabItem value=\"py\" label=\"Python\">\n",
      "\n",
      "  Save as `embedded.py` and run `python embedded.py`:\n",
      "  <br/>\n",
      "\n",
      "  <FilteredTextBlock\n",
      "    text={PyCode}\n",
      "    startMarker=\"# START 10lines\"\n",
      "    endMarker=\"# END 10lines\"\n",
      "    language=\"py\"\n",
      "  />\n",
      "  </TabItem>\n",
      "\n",
      "  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n",
      "\n",
      "  Save as `embedded.ts` and run `node --loader=ts-node/esm embedded.ts`:\n",
      "  <br/>\n",
      "\n",
      "  <FilteredTextBlock\n",
      "    text={TSCode}\n",
      "    startMarker=\"// START 10lines\"\n",
      "    endMarker=\"// END 10lines\"\n",
      "    language=\"js\"\n",
      "  />\n",
      "  </TabItem>\n",
      "</Tabs>\n",
      "\n",
      "\n",
      "## <i class=\"fa-solid fa-screwdriver-wrench\"></i> How does this work? Essentially, what happens behind the scenes is that the client library downloads the server binary, spawns it in a separate process, connects to it, then terminates it on exit.»\n",
      "[3] «With Embedded Weaviate, you can just run the test suite and be done with it. But Embedded Weaviate is not the only way to make testing easier. In the following sections, we'll look at other ways to make testing easier, and how to make the most of Embedded Weaviate. ## Scoping tests\n",
      "\n",
      "While you may be familiar with tests and integration tests in general, here are some specific suggestions for Weaviate-powered applications:\n",
      "* **Whether to test search quality**: This depends primarily on the model used for vectorization, such as by a [Weaviate vectorizer module](/developers/weaviate/modules/retriever-vectorizer-modules). We suggest evaluating models separately, but not tested as a part of the application.»\n",
      "[4] «* **Focus on interactions with the inference provider**: Search itself is a core Weaviate functionality that we can trust. So, we suggest any integration tests focus on the interaction with the inference provider. For example,\n",
      "  * is the vectorization model the expected one? * if switching to a different inference provider or model, does the application still function as expected? * **Other common issues to test** include:\n",
      "  * Connection or authentication issues with the inference provider\n",
      "  * Incomplete or incorrect data imports\n",
      "  * Specifying the vector correctly when [bringing your own vectors](/developers/weaviate/starter-guides/custom-vectors)\n",
      "  * Data definition issues, like invalid class names, properties, or data types\n",
      "\n",
      "\n",
      "## Testing with embedded Weaviate\n",
      "\n",
      "### Set up\n",
      "\n",
      "[Embedded Weaviate](/developers/weaviate/installation/embedded) lets us spawn a Weaviate server instance from the client, and automatically tear it down when the client terminates.»\n",
      "[5] «And doing so, you might just discover the hidden gem that is the value provided by adding tests to your applications. ## Testing and Weaviate\n",
      "\n",
      "In this article, we will focus on [integration tests](https://en.wikipedia.org/wiki/Integration_testing). Integrated testing is an important part of the development process, and especially so for complex applications. Weaviate-based apps usually fall in this category. For one, Weaviate must interact with the application in a variety of ways.»\n",
      "\n",
      "Question: How does Embedded Weaviate simplify the setup process for testing the Weaviate application?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We know from the context that Embedded Weaviate is a feature that allows users to spawn a Weaviate server instance directly from the client. This suggests that it simplifies the setup process by eliminating the need for a separate server instance. Let's continue reading to confirm and elaborate on this.\n",
      "\n",
      "Answer: Embedded Weaviate simplifies the setup process for testing a Weaviate application by allowing users to spawn a Weaviate server instance directly from the client. This eliminates the need for a separate server instance, streamlining the testing process. With Embedded Weaviate, users can run the test suite without the hassle of setting up and tearing down a separate service, making it more convenient and efficient to test Weaviate-based applications.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «You can reach us through:\n",
      "\n",
      "- Join our dedicated Hacktoberfest channel in our [Discourse community forum](https://forum.weaviate.io/), where we're ready to answer your questions. - Join our dedicated #hacktoberfest channel in our [Weaviate Community Slack](https://weaviate.slack.com/) channel to stay connected and receive real-time support. - Join our [newsletter](https://newsletter.weaviate.io/) and follow us on [Twitter](https://twitter.com/weaviate_io) and [Linkedin](https://www.linkedin.com/company/weaviate-io/mycompany/verification/) to receive updates. - Stay up to date with Weaviate's development by exploring the [Weaviate GitHub Repository](https://github.com/weaviate/weaviate). Don’t forget to give us a ⭐️ while you are there!\n",
      "\n",
      ":::info Pro Tip\n",
      "Share your process online and tag us on [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://nl.linkedin.com/company/weaviate-io).»\n",
      "[2] «- 4th October 2023 (20:00-21:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232574048361254) with Zain Hasan, Senior Dev Advocate @ Weaviate\n",
      "- 5th October 2023 (17:00-18:00 UTC+2): [NEW Python `Collections` Client API Preview](https://form.jotform.com/232683153137859) with JP Hwang - Educator @ Weaviate\n",
      "- 18th October 2023 (15:00-16:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232602295283859) with JP Hwang - Educator @ Weaviate\n",
      "\n",
      "Expand your knowledge with these supplementary resources:\n",
      "\n",
      "- [Weaviate YouTube Channel](https://www.youtube.com/@Weaviate) 📺\n",
      "- [Weaviate Blog](https://weaviate.io/blog) 📰\n",
      "- [Weaviate Recipes](https://github.com/weaviate/recipes) 🍳\n",
      "\n",
      "\n",
      "## What's in it for you? The repositories you contribute to participate in [Hacktoberfest](https://hacktoberfest.com/) and are a great opportunity for your first-ever Hacktoberfest PR. We will also assist you throughout the whole process. You might even receive some swag in the end. ## Connect with the Weaviate Community!\n",
      "\n",
      "To make your Hacktoberfest experience successful, connect with the Weaviate community for collaboration and assistance.»\n",
      "[3] «Share what you build with Weaviate in [Slack](https://weaviate.slack.com/), on our [Forum](https://forum.weaviate.io/), or on socials. ## Embracing Open Source and Sharing Knowledge\n",
      "\n",
      "As AI accelerated throughout the year with ever-new innovations popping up, so did the community's curiosity to learn and share knowledge in that area. As an open-source solution, **community** is a foundational pillar of Weaviate. ### [Hacktoberfest](https://weaviate.io/blog/hacktoberfest-2023)\n",
      "\n",
      "Celebrating the spirit of **open source**, we participated in our first [Hacktoberfest](https://hacktoberfest.com/) this October, which was organized by our very own **[Leonie Monigatti](https://www.linkedin.com/in/804250ab/)**! This global event, aimed at engineers and machine learning enthusiasts, fosters collaboration and contributions to open-source technology. Participants who had four pull requests (PRs) accepted between October 1 and 31, 2023, earned a unique digital reward and some Weaviate Merch! Contributions varied in scope, ranging from minor, non-coding inputs to more substantial technical improvements.»\n",
      "[4] «Use the hashtag #hacktoberfest2023 for increased visibility. :::\n",
      "\n",
      "\n",
      "## FAQ\n",
      "\n",
      "- **Will this count towards Hacktoberfest?** Yes, it definitely does! If your PR/MR is created between **October 1** and **October 31** (in any time zone, UTC-12 thru UTC+14), we will add the \"HACKTOBERFEST-ACCEPTED\" label to it. - **Where do I get help?** For any questions or assistance, contact us on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels. - **I have a cool contribution idea. Can I still participate?** Awesome! Connect with us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels and we will figure it out.»\n",
      "[5] «---\n",
      "title: Hacktoberfest 2023 - Celebrating Open Source with Weaviate\n",
      "slug: hacktoberfest-2023\n",
      "authors: [leonie]\n",
      "date: 2023-10-02\n",
      "tags: []\n",
      "image: ./img/weaviate-hacktoberfest-2023.png\n",
      "description: \"Join us in celebrating Hacktoberfest, a month-long celebration of open source!\"\n",
      "---\n",
      "import hacktober_demo from './img/hacktoberfest_2023_demo.mp4';\n",
      "\n",
      "![Celebrating Hacktoberfest 2023 with Weaviate](./img/weaviate-hacktoberfest-2023.png)\n",
      "\n",
      "At [Weaviate](https://weaviate.io/), we love open source! Join us in celebrating [Hacktoberfest](https://hacktoberfest.com/), a month-long celebration of open source!\n",
      "Participants with four pull requests (PR) accepted between **October 1 - 31, 2023** will receive a unique digital reward [from Hacktoberfest](https://hacktoberfest.com/participation/). ## The Task\n",
      "\n",
      "Welcome to our demo sprint!\n",
      "\n",
      "It is aimed at onboarding engineers and machine learning practitioners to open-source. In our [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page, we showcase what you can do with a [vector database](https://weaviate.io/blog/what-is-a-vector-database). Some of these demos have received more love than others recently, and we want to give them a little polish. ![Untitled](./img/weaviate-demos.png)\n",
      "\n",
      "The gallery contains demos using Weaviate in different states.»\n",
      "\n",
      "Question: How can I participate in the Weaviate community's Hacktoberfest event and stay updated on their developments?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can break down the question into two parts: \"How can I participate in the Weaviate community's Hacktoberfest event?\" and \"How can I stay updated on their developments?\" To participate in the Weaviate community's Hacktoberfest event: - Join their dedicated Hacktoberfest channel on the Weaviate Community Forum (Discourse) or their Slack channel to connect with the community and seek assistance. - Contribute to the Weaviate repositories during October 1-31, 2023, and aim for four accepted pull requests to earn the rewards. - Share your process and what you build with Weaviate on their Slack, Forum, or social media platforms (Twitter, LinkedIn). To stay updated on the Weaviate community's developments: - Follow their newsletter, Twitter, and LinkedIn accounts for updates. - Explore the Weaviate GitHub Repository to stay up to date with the project's development and contribute if you wish. - Utilize their supplementary resources: the Weaviate YouTube Channel, Blog, and Recipes repository.\n",
      "\n",
      "Answer: To participate in the Weaviate community's Hacktoberfest event, join their dedicated channels on Discourse and Slack, contribute to their repositories during the specified period, and share your process and creations. To stay updated on their developments, follow their newsletter and social media accounts, explore their GitHub repository, and utilize their supplementary resources.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «On the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:\n",
      "\n",
      "```yaml\n",
      "DEFAULT_VECTORIZER_MODULE: text2vec-huggingface\n",
      "ENABLE_MODULES: text2vec-huggingface\n",
      "```\n",
      "\n",
      "## How to get started\n",
      "\n",
      ":::note\n",
      "This article is not meant as a hands-on tutorial. For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). :::\n",
      "\n",
      "The overall process to use a Hugging Face module with Weaviate is fairly straightforward. ![Recipe for using the Hugging Face module](./img/how-to-get-started-recipe.png)\n",
      "If this was a cooking class and you were following a recipe.»\n",
      "[2] «Then, you just run the queries, as per usual:\n",
      "```javascript\n",
      "nearText = {\n",
      "    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\n",
      "    \"distance\": 0.6,\n",
      "}\n",
      "\n",
      "result = (\n",
      "    client.query\n",
      "    .get(\"Notes\", [\n",
      "        \"name\",\n",
      "        \"comment\",\n",
      "        \"_additional {certainty distance} \"])\n",
      "    .with_near_text(nearText)\n",
      "    .do()\n",
      ")\n",
      "```\n",
      "\n",
      "## Summary\n",
      "> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\n",
      "Check out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.»\n",
      "[3] «You would need the following ingredients:\n",
      "* Raw Data\n",
      "* Hugging Face API token – which you can request from [their website](https://huggingface.co/settings/tokens)\n",
      "* A working Weaviate instance with the `text2vec-huggingface` enabled\n",
      "\n",
      "Then you would follow these steps. ### Step 1 – initial preparation – create schema and select the hf models\n",
      "Once you have a Weaviate instance up and running. Define your schema (standard stuff – pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class. This is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\n",
      "```javascript\n",
      "{\n",
      "    \"class\": \"Notes\",\n",
      "    \"moduleConfig\": {\n",
      "        \"text2vec-huggingface\": {\n",
      "            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\n",
      "            ...»\n",
      "[4] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\n",
      "let them do it for you.\"\n",
      "\n",
      "## Support for Hugging Face Inference API in Weaviate\n",
      "Starting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\n",
      "> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\n",
      "[5] «Each `text2vec-*` module uses an external API (like `text2vec-openai` or `text2vec-huggingface`) or a local instance like `text2vec-transformers` to produce a vector for each object. Let's try vectorizing data with the `text2vec-cohere` module. We will be using data from `tiny_jeopardy.csv` [available here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain) containing questions from the game show Jeopardy. We'll just use a few (20) questions here, but the [full dataset on Kaggle](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) includes 200k+ questions. Load the data into a Pandas dataframe, then populate Weaviate like this:\n",
      "\n",
      "```python\n",
      "client.batch.configure(batch_size=100)  # Configure batch\n",
      "with client.batch as batch:\n",
      "    for i, row in df.iterrows():\n",
      "        properties = {\n",
      "            \"question\": row.Question,\n",
      "            \"answer\": row.Answer\n",
      "        }\n",
      "        batch.add_data_object(properties, \"Question\")\n",
      "```\n",
      "\n",
      "This should add a series of `Question` objects with text properties like this:\n",
      "\n",
      "```text\n",
      "{'question': 'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger',\n",
      " 'answer': \"McDonald's\"}\n",
      "```\n",
      "\n",
      "Since we use the `text2vec-cohere` module to vectorize our data, we can query Weaviate to find data objects most similar to any input text.»\n",
      "\n",
      "Question: How do you set up a Weaviate instance with the `text2vec-huggingface` module?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can refer to the provided context, which outlines the process of setting up the Weaviate instance with the 'text2vec-huggingface' module. Firstly, it is mentioned that to use the Hugging Face module in Weaviate open source, one needs to set 'text2vec-huggingface' as the default vectorizer in the configuration file (usually YAML). This is a crucial step to ensure Weaviate uses the correct module for vectorization. Secondly, the context provides a step-by-step guide, comparing the process to following a recipe in a cooking class. It emphasizes the need for specific 'ingredients', which include raw data, a Hugging Face API token, and a working Weaviate instance with the 'text2vec-huggingface' module enabled. Thirdly, the guide instructs users to define their schema, including class names, properties, and data types. This is a standard procedure when working with structured data in Weaviate. As part of the schema definition, users need to specify which Hugging Face model they want to use for each schema class. This is achieved by adding a 'moduleConfig' property to the schema, where the desired model name is provided. Finally, the context mentions that Weaviate 'v1.15' and newer versions include the Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. This implies that ensuring the Weaviate instance is updated to the latest version is essential for accessing the Hugging Face module. With these steps in mind, we can now formulate a response.\n",
      "\n",
      "Answer: To set up a Weaviate instance with the 'text2vec-huggingface' module, follow these steps: 1. Ensure you have the necessary ingredients: raw data to work with, a Hugging Face API token (obtainable from their website), and a functioning Weaviate instance. 2. Configure your Weaviate instance by setting 'text2vec-huggingface' as the default vectorizer in the configuration file. 3. Define your schema, including class names, properties, and data types. As part of this step, specify the desired Hugging Face model for each schema class by adding a 'moduleConfig' property to the schema definition. 4. Update your Weaviate instance to the latest version ('v1.15' or newer) to access the built-in Hugging Face module, which facilitates Hugging Face Inference straight from the vector database. By following these steps, you can effectively set up Weaviate to utilize the 'text2vec-huggingface' module for vectorization and model inference tasks.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «`go_memstats_heap_inuse_bytes` should always be present even with an empty schema. ![datadog summary](./img/datadog-summary.png)\n",
      "\n",
      "\n",
      "## Key Metrics\n",
      "\n",
      "Below are some key Weaviate metrics to monitor. Standard CPU, Disk, Network metrics are also useful as are [Kubernetes\n",
      "events](https://grafana.com/blog/2023/01/23/how-to-use-kubernetes-events-for-effective-alerting-and-monitoring/). Note that some Weaviate metrics will not appear until an operation has occurred (for instance batch operations). ### Heap Usage\n",
      "\n",
      "For heap usage, the expectation is the memory will have a standard jagged pattern underload but that memory will drop periodically\n",
      "due to the Go garbage collection.»\n",
      "[2] «```\n",
      "go_memstats_heap_inuse_bytes{job=\"weaviate\"}\n",
      "```\n",
      "### Dashboards\n",
      "\n",
      "One benefit of this approach is that you can now reuse the existing Weaviate Grafana dashboards. Steps to import these dashboards:\n",
      "\n",
      "1\\. Download and import the [preexisting dashboards](https://github.com/weaviate/weaviate/tree/master/tools/dev/grafana/dashboards). 2\\. If you're using Grafana Cloud hosted Prometheus you will need to patch the dashboards to change the datasource uid to be `grafanacloud-prom` as below.»\n",
      "[3] «If memory is not dropping and is staying very close to the [GOMEMLIMIT](/blog/gomemlimit-a-game-changer-for-high-memory-applications), you may need to increase resources. ```\n",
      "go_memstats_heap_inuse_bytes\n",
      "```\n",
      "\n",
      "### Batch Latency\n",
      "\n",
      "Batch latency is important as batch operations are the most efficient way to write data to\n",
      "Weaviate. Monitoring this can give an indication if there is a problem with indexing data. This metric has a label `operation` which\n",
      "allows you to see how long objects, vectors, and inverted index sub operations take. If you are using a [vectorizer module](/developers/weaviate/modules/retriever-vectorizer-modules) you will see additional latency due to the overhead of sending data to the module.»\n",
      "[4] «In other words, the more the user imports, the more the heap will grow. That makes it quite unpredictable to know what live heap we will end up with eventually. In other words, Weaviate – like any other memory-intensive Go application – is a perfect candidate to benefit from GOMEMLIMT. It also comes out of the box with [Prometheus-compatible monitoring](/developers/weaviate/configuration/monitoring), so it should be easy to visualize the heap usage. For this experiment, we will use the latest Weaviate version compiled with Go 1.19.»\n",
      "[5] «The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually. All this means that you can also use the schema to tweak Weaviate's vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level.»\n",
      "\n",
      "Question: Should the `go_memstats_heap_inuse_bytes` metric be present in Weaviate even if the schema is empty, and what is its expected behavior under load?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to understand the context provided, which discusses Weaviate, a tool for managing and monitoring data. We also need to refer to the specific statements related to the 'go_memstats_heap_inuse_bytes' metric. The context mentions that the 'go_memstats_heap_inuse_bytes' metric should always be present, even with an empty schema. It is listed as one of the key Weaviate metrics to monitor. This indicates that this particular metric is essential for understanding the performance and behavior of Weaviate. Under load, the expected behavior of the 'go_memstats_heap_inuse_bytes' metric is described as a standard jagged pattern. This suggests that memory usage will fluctuate, but it should also drop periodically due to Go garbage collection. This garbage collection process helps to manage memory usage and prevent excessive memory consumption.\n",
      "\n",
      "Answer: Yes, the 'go_memstats_heap_inuse_bytes' metric should be present in Weaviate even if the schema is empty. Under load, this metric is expected to exhibit a jagged pattern, with periodic drops in memory usage due to Go garbage collection.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «One of the primary reasons is the computational cost associated with processing and storing longer sequences of tokens. The longer the sequence, the more memory and processing power required to operate on it, which can be a significant challenge for even the most powerful computing systems. The relatively long input window of LLMs is what drives the integration with semantic search. For example, we can use this entire blog post as input in addition to questions if we want the LLM to answer questions such as “What are LLM Chains?” However, when we want to give the LLM an entire book or pages of search results, we need more clever techniques to decompose this task. This is where the `CombineDocuments` chain comes to play! Note, that one method is not better than another, and the performance is dependent solely on your application.»\n",
      "[2] «Then we take this new query and hit the Weaviate vector database to get context to answer the question. The ChatVectorDB chain we use has a default value of k = 4 search results, if we use longer search results we will need another CombineDocuments chain here as well! With the 4 search results, we answer the question with this final prompt:\n",
      "\n",
      "```python\n",
      "Prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. {context}\n",
      "Question: {question}\n",
      "Helpful Answer:\"\"\"\n",
      "```\n",
      "\n",
      "Hopefully this was a nice look under the hood of how the ChatVectorDB chain works. Let’s get into how we can use this with Weaviate!\n",
      "\n",
      "### The Code\n",
      "\n",
      "If this is your first time using Weaviate, please check out the [Quickstart tutorial](/developers/weaviate/quickstart).»\n",
      "[3] «We will look at the [ChatVectorDB chain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db), it lets you build an LLM that stores chat history and retrieves context from Weaviate to help with generation. To begin, the chat history in this chain uses the `stuff` configuration of CombineDocuments. This means we take as much of the chat history as we can fit in our context window and use it for a query reformulation prompt. The prompt is as follows:\n",
      "\n",
      "```python\n",
      "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History:\n",
      "{chat_history}\n",
      "Follow Up Input: {question}\n",
      "Standalone question:\"\"\"\n",
      "```\n",
      "\n",
      "We put the chat_history and the latest user input in the curly bracket syntax.»\n",
      "[4] «Further reading [Why Is Vector Search So Fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)\n",
      "\n",
      "## Use Cases of Vector Databases\n",
      "\n",
      "Vector databases’ search capabilities can be used in various applications ranging from classical Machine Learning use cases, such as natural language processing, computer vision, and recommender systems, to providing long-term memory to LLMs in modern applications. The **most popular use case of vector search engines is naturally for search**. Because a vector database can help find similar objects, it is predestined for applications where you might want to find similar products, movies, books, songs, etc. That’s why vector search engines are also **used in recommendation systems** as a restated task of search. With the rise of LLMs, **vector databases have already been used to enhance modern Generative AI applications**.»\n",
      "[5] «Since around [80% of all data is unstructured](https://www.forbes.com/sites/forbestechcouncil/2019/01/29/the-80-blind-spot-are-you-ignoring-unstructured-organizational-data/), it is hard to actually search and retrieve insights from most of the data. The Weaviate vector database unlocks the potential of unstructured data. Searching by fuzzy terms and classification of rich data like free text becomes possible with Weaviate. It uses AI-driven indexing and search technologies to enable real-time text processing. With machine learning methods, Weaviate automatically classifies texts.»\n",
      "\n",
      "Question: Why are techniques like the `CombineDocuments` chain necessary for processing large texts with LLMs?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's break down the context to identify the key points that explain the need for techniques like the `CombineDocuments` chain. The context mentions that one of the primary challenges of processing large texts with LLMs is the computational cost and memory requirements. As the input sequence length increases, so do the processing power and memory needs, which can be a significant hurdle even for powerful computing systems. The `CombineDocuments` chain is introduced as a solution to this problem. It is a technique used to decompose large texts into manageable segments that can be more easily processed by LLMs. By taking portions of the chat history and user input, the `CombineDocuments` chain reformulates the question into a standalone query that can be answered using a smaller context window. Additionally, the context mentions that vector databases, such as Weaviate, are used in conjunction with LLMs to enhance their capabilities. Vector databases excel at searching and retrieving similar objects, making them useful for applications beyond just search, including recommendation systems and Generative AI. Weaviate, in particular, enables the search and retrieval of insights from unstructured data by using AI-driven indexing and search technologies.\n",
      "\n",
      "Answer: Techniques like the `CombineDocuments` chain are necessary for processing large texts with LLMs because they help address the computational cost and memory requirements associated with longer input sequences. By decomposing the task and reformulating queries, the `CombineDocuments` chain enables LLMs to handle larger contexts without overwhelming their resource limitations. Additionally, vector databases like Weaviate further enhance the capabilities of LLMs by providing efficient search and retrieval capabilities for unstructured data.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «What does replication get us? A big one is *availability*. With no replication, any node being down will make its data unavailable. But in a Kubernetes setup composed of say, three Weaviate nodes (three Kubernetes “pods”) and a replication factor of three, you can have any one of the three nodes down and still reach consensus. This reflects Weaviate’s leaderless replication architecture, meaning any node can be down without affecting availability at a cluster level as long as the right data is available somewhere.»\n",
      "[2] «<details>\n",
      "  <summary>Optional: Try it yourself (with minikube)</summary>\n",
      "\n",
      "You can try running a local, multi-node Weaviate cluster with `minikube`, which can conveniently run a local Kubernetes cluster. We note that deploying Weaviate on a cloud provider’s kubernetes service follows a similar process. <br/>\n",
      "\n",
      "First, install `minikube` and `helm` for your system by following these guides ([minikube](https://minikube.sigs.k8s.io/docs/start), [helm](https://helm.sh/docs/intro/install)). We also recommend installing `kubectl` ([by following this guide](https://kubernetes.io/docs/tasks/tools/#kubectl)). <br/>\n",
      "\n",
      "Once minikube is installed, start a three-node minikube cluster by running the following from the shell:\n",
      "\n",
      "```shell\n",
      "minikube start --nodes 3\n",
      "```\n",
      "\n",
      "Once the nodes have been created, you should be able to interact with them through the `kubectl` command-line tool.»\n",
      "[3] «Full dynamic scalability will be added in a future release. # highlight-start\n",
      "replicas: 3\n",
      "# highlight-end\n",
      "... ```\n",
      "\n",
      "You can now deploy Weaviate in this configuration by running:\n",
      "\n",
      "```shell\n",
      "kubectl create namespace weaviate\n",
      "\n",
      "helm upgrade --install \\\n",
      "  \"weaviate\" \\\n",
      "  weaviate/weaviate \\\n",
      "  --namespace \"weaviate\" \\\n",
      "  --values ./values.yaml\n",
      "```\n",
      "\n",
      "This will deploy the Weaviate clusters. You can check the status of the deployment by running:\n",
      "\n",
      "```shell\n",
      "kubectl get pods -n weaviate\n",
      "```\n",
      "\n",
      "This should produce an output similar to the following:\n",
      "\n",
      "```shell\n",
      "NAME         READY   STATUS    RESTARTS   AGE\n",
      "weaviate-0   1/1     Running   0          3m00s\n",
      "weaviate-1   1/1     Running   0          2m50s\n",
      "weaviate-2   1/1     Running   0          2m40s\n",
      "```\n",
      "\n",
      "Now, you need to expose the Weaviate service to the outside world - i.e. to your local machine. You can do this by running:\n",
      "\n",
      "```shell\n",
      "minikube service weaviate --namespace weaviate\n",
      "```\n",
      "\n",
      "This should show an output similar to the following that shows the URL to access the Weaviate cluster:\n",
      "\n",
      "```shell\n",
      "|-----------|----------|-------------|------------------------|\n",
      "| NAMESPACE |   NAME   | TARGET PORT |          URL           |\n",
      "|-----------|----------|-------------|------------------------|\n",
      "| weaviate  | weaviate |             | http://127.0.0.1:54847 |\n",
      "|-----------|----------|-------------|------------------------|\n",
      "```\n",
      "\n",
      "And it should also open a browser window showing the list of Weaviate endpoints.»\n",
      "[4] «To see a list of the newly spun up nodes, run:\n",
      "\n",
      "```shell\n",
      "kubectl get nodes -o wide\n",
      "```\n",
      "\n",
      "You should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\n",
      "\n",
      "```shell\n",
      "NAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\n",
      "minikube       Ready    control-plane   134m   v1.27.3   192.168.49.2   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n",
      "minikube-m02   Ready    <none>          134m   v1.27.3   192.168.49.3   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n",
      "minikube-m03   Ready    <none>          133m   v1.27.3   192.168.49.4   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n",
      "```\n",
      "\n",
      "Now, add the Weaviate helm repository to your local helm configuration by running:\n",
      "\n",
      "```shell\n",
      "helm repo add weaviate https://weaviate.github.io/weaviate-helm\n",
      "```\n",
      "\n",
      "And save the default configuration with:\n",
      "\n",
      "```shell\n",
      "helm show values weaviate/weaviate > values.yaml\n",
      "```\n",
      "\n",
      "Edit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it. ```yaml\n",
      "... # Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\n",
      "# to cases where no data is imported yet. Scaling down after importing data may\n",
      "# break usability.»\n",
      "[5] «<br/>\n",
      "\n",
      "Just like that, you’ve got a multi-node Weaviate cluster. Remember that when you create a class, you must have replication enabled. You can do this by adding the `replicationConfig` parameter to the collection definition, like so:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"class\": \"ClassWithReplication\",\n",
      "  \"properties\": [\n",
      "    {\n",
      "      \"name\": \"exampleProperty\",\n",
      "      \"dataType\": [\"text\"]\n",
      "    }\n",
      "  ],\n",
      "  // highlight-start\n",
      "  \"replicationConfig\": {\n",
      "    \"factor\": 3\n",
      "  }\n",
      "  // highlight-end\n",
      "}\n",
      "```\n",
      "\n",
      "And when you insert objects into `ClassWithReplication`, they will be replicated across the three nodes. You can verify this by visiting the `/nodes` endpoint, which will verify that each node contains the same number of objects. </details>\n",
      "\n",
      "## Benefits of replication\n",
      "\n",
      "So, let’s cover the implications of doing this.»\n",
      "\n",
      "Question: How can I check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas?\n",
      "\n",
      "Reasoning: Let's think step by step in order to answer the question. We can refer to the provided context, which outlines the process of setting up a local, multi-node Weaviate cluster using 'minikube' and 'helm'. First, we need to ensure that we have the necessary tools installed, including 'minikube', 'helm', and 'kubectl'. Then, we can start a three-node minikube cluster using the provided command. To check the status and details of the nodes in the cluster, we can use the 'kubectl get nodes' command, which will provide an overview of the nodes, including their status, roles, ages, and IP addresses. Once we have confirmed that the nodes are up and running, we can proceed with deploying Weaviate. We add the Weaviate helm repository to our local configuration and save the default values to a 'values.yaml' file. We then edit this file to specify three replicas for the Weaviate deployment. Finally, we can deploy Weaviate using the provided 'helm upgrade' command, which will create the Weaviate clusters with the specified number of replicas. By following these steps, we can check the status and details of nodes in the Kubernetes cluster and prepare to deploy Weaviate with three replicas.\n",
      "\n",
      "Answer: To check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas, follow these steps: 1. Install 'minikube', 'helm', and 'kubectl' on your system. 2. Start a three-node minikube cluster using the command: 'minikube start --nodes 3'. 3. Use 'kubectl get nodes -o wide' to view the status and details of the nodes, including their roles, ages, IP addresses, and more. 4. Add the Weaviate helm repository to your local configuration with 'helm repo add weaviate https://weaviate.github.io/weaviate-helm'. 5. Save the default configuration using 'helm show values weaviate/weaviate > values.yaml'. 6. Edit 'values.yaml' to specify three replicas by changing 'replicas: 1' to 'replicas: 3'. 7. Deploy Weaviate using the 'helm upgrade' command provided in the context. By executing these steps, you can verify the status and details of the nodes in the Kubernetes cluster and set up the deployment of Weaviate with three replicas.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «For more information about this new feature, read this [blog post](/blog/ref2vec-centroid) by Connor Shorten. Weaviate also has a bunch of example use cases on [GitHub](https://github.com/weaviate/weaviate-examples). Find your favorite example, give it a star, and try to recreate it yourself!\n",
      "\n",
      "## Feature Comparison - Library versus Database\n",
      "The table below summarizes the differences between vector libraries and databases. This is by no means an exhaustive list of features, and not every library or database has the same features. |**Feature**|**Vector Library**|**Vector Database** (Weaviate as an example)|\n",
      "| ----- | ----- | ----- |\n",
      "| Filtering (in combination with Vector Search)| No| Yes|\n",
      "| Updatability (CRUD)| No (some do, e.g. hnswlib)| Yes|\n",
      "| Incremental importing, concurrent reading while importing| No (some do, e.g. hnswlib) | Yes |\n",
      "| Stores objects and vectors | No | Yes|\n",
      "|Speed | Typically faster than full-blown database|  Typically slower than pure library|\n",
      "| Performance optimized for | In-memory similarity search| End2end-callstack, including: vector search, object retrieval from persistent storage, optional inverted index filtering, network requests, etc.»\n",
      "[2] «Vector Indexing Library\n",
      "\n",
      "Similarly to vector databases, [vector libraries](https://weaviate.io/blog/vector-library-vs-vector-database) also enable fast vector search. However, vector libraries only store vector embeddings of data objects, and they store them in in-memory indexes. This results in two key differences:\n",
      "\n",
      "1. Updatability: The index data is immutable, and thus, no real-time updates are possible. 2.»\n",
      "[3] «This is why we have **Vector Databases** and **Vector Libraries**. They both use the Approximate Nearest Neighbor (ANN) algorithm to search through vectors in a tiny fraction of the time. You can learn more about this topic from [\"Why Vectors Search is so Fast.\"](/blog/why-is-vector-search-so-fast)\n",
      "\n",
      "## The Big Question\n",
      "So, if both vector databases and vector libraries allow you to efficiently search through your vectors. What are the key differences between them, and why/when should you choose one over the other? ## Vector Libraries\n",
      "Vector libraries store vector embeddings in in-memory indexes, in order to perform similarity search.»\n",
      "[4] «### Example Use Cases\n",
      "Vector libraries are commonly used for applications that do not have changing data. For example, academic information retrieval benchmarks are designed to test performance on a **static snapshot** of data. When plugging an ANN index into production-ready applications, databases offer many appealing features not found in a library. <img\n",
      "    src={require('./img/vector-library-vs-database.png').default}\n",
      "    alt=\"alt\"\n",
      "    style={{ maxWidth: \"75%\" }}\n",
      "/>\n",
      "\n",
      "## Vector Databases\n",
      "One of the core features that set vector databases apart from libraries is the ability to store and update your data. Vector databases have full **CRUD** (create, read, update, and delete) support that solves the limitations of a vector library.»\n",
      "[5] «---\n",
      "title: Vector Library versus Vector Database\n",
      "slug: vector-library-vs-vector-database\n",
      "authors: [erika]\n",
      "date: 2022-12-1\n",
      "tags: ['concepts', 'search']\n",
      "image: ./img/hero.png\n",
      "description: \"Learn more about the differences between vector libraries and vector databases!\"\n",
      "---\n",
      "![Vector library versus Vector Database](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "In the world of Vector Search, we use **vector embeddings** – generated by **Machine Learning models** – to represent data objects (text, images, audio, etc.). The key idea here is that embeddings that are semantically similar to each other have a smaller distance between them. We can use [vector distance functions](/blog/distance-metrics-in-vector-search) like euclidean distance or cosine distance to determine if these objects are similar to other objects in the collection. However, to do this we need to compare the distances between the query vector and **every** vector in the collection. This type of calculation can be difficult to scale to millions or billions of vectors.»\n",
      "\n",
      "Question: What are the differences between vector libraries and vector databases like Weaviate in terms of features such as filtering, updatability, and performance?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: The context provides a comparison between vector libraries and vector databases, highlighting their key differences. By referring to the provided table and the subsequent explanations, we can identify the specific differences in terms of features, updatability, and performance.\n",
      "\n",
      "Answer: Vector libraries and vector databases like Weaviate differ in several ways: 1. Filtering: Vector databases support filtering in combination with vector search, while vector libraries do not. 2. Updatability: Vector databases offer CRUD (Create, Read, Update, Delete) operations, allowing real-time updates. Vector libraries, on the other hand, have immutable index data, making updates impossible. 3. Incremental Importing: Vector databases support incremental importing and concurrent reading while importing. While some vector libraries may offer this feature (e.g., hnswlib), it is not a standard capability. 4. Storage: Vector databases store both objects and vectors, whereas vector libraries only store vector embeddings. 5. Speed: Vector libraries are typically faster than full-blown databases due to their in-memory nature. Vector databases are optimized for the end-to-end call stack, which includes various operations beyond just vector search. 6. Performance Optimization: Vector libraries are optimized for in-memory similarity search, making them efficient for specific tasks. Vector databases, on the other hand, are optimized for the entire end-to-end call stack, including vector search, object retrieval, filtering, and network requests. In summary, vector libraries are suitable for applications with static data and in-memory similarity search requirements. Vector databases, like Weaviate, are preferred when you need to store objects and vectors together, perform updates, and have additional features like filtering and CRUD operations.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «---\n",
      "title: Using Cross-Encoders as reranker in multistage vector search\n",
      "slug: cross-encoders-as-reranker\n",
      "authors: [laura]\n",
      "date: 2022-08-09\n",
      "tags: ['search']\n",
      "image: ./img/hero.png\n",
      "description: \"Learn about bi-encoder and cross-encoder machine learning models, and why combining them could improve the vector search experience.\"\n",
      "---\n",
      "![Using Cross-Encoders as reranker in multistage vector search](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "Semantic search overcomes the shortcomings of keyword-based search. If you search for \"programming for data scientists\", a keyword search will find \"An intro to programming for data scientists\", but it won't return \"Python for Machine Learning use-cases\". Vector databases can use machine learning to capture the context and semantics of data and search queries. Usually, machine learning models have a tradeoff between high accuracy and speed. The higher the accuracy, the more computationally expensive the model is.»\n",
      "[4] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[5] «With a vector database like [Weaviate](/), you can store and retrieve vectors and data efficiently using Bi-Encoder models to encode data and queries. A search pipeline can then continue with a Cross-Encoder model which reranks a list of retrieved search result candidates. This blog post was inspired by [Nils Reimer's work on Bi-Encoders and Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html). import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "\n",
      "Question: Why are Cross-Encoders considered impractical for semantic search applications with large datasets?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's break down the context to understand the impracticality of Cross-Encoders for large-scale semantic search. The context discusses the trade-off between accuracy and efficiency in machine learning models, specifically in the context of semantic search. Semantic search aims to understand the context and semantics of data and queries, and it relies on machine learning models for this task. Cross-Encoders are introduced as highly accurate models for semantic search. However, the issue lies in their computational expense. When performing a search, a Cross-Encoder model needs to process every single data item in combination with the query. This means that for large datasets with thousands or millions of objects, the search process becomes extremely time-consuming and inefficient.\n",
      "\n",
      "Answer: Cross-Encoders are considered impractical for semantic search applications with large datasets because they are computationally expensive and inefficient. The need to process every data item individually during a search makes it time-consuming and impractical for large-scale datasets.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here. Be the first to experience this great room and write a review! Don't miss out on this amazing opportunity - book now! |\n",
      "| This beautiful two bedroom apartment is located in the Upper East Side of Manhattan, just a short walk away from Central Park. The apartment is situated in the East Harlem neighbourhood, and is surrounded by a variety of restaurants, shops, and other attractions. The apartment is available for rent for a minimum of seven nights, and is priced at $190 per night. The apartment is equipped with all the necessary amenities, and is available for rent for 249 days out of the year.»\n",
      "[2] «| Welcome to this beautiful two bedroom apartment in the Upper East Side of Manhattan! Located in the East Harlem neighbourhood, this apartment is just a short walk away from Central Park and surrounded by a variety of restaurants, shops, and other attractions. Enjoy all the necessary amenities in this apartment, available for rent for a minimum of seven nights at a price of $190 per night. With 249 days available for rent out of the year, this is the perfect place to stay for your next vacation or business trip! Book now and experience the best of Manhattan! |\n",
      "| This cozy private room is located in the Flatbush neighbourhood of Brooklyn. Enjoy the convenience of being in the city while still having access to the peacefulness of the countryside. The room is available for a minimum of one night and is available for booking all year round.»\n",
      "[3] «The location is convenient, with easy access to public transportation and a variety of shops and restaurants nearby. Although there are no reviews yet, you can rest assured that the host is experienced and dedicated to providing a comfortable and enjoyable stay. With availability 365 days a year, this listing is a great option for anyone looking for a long-term stay in Brooklyn. |\n",
      "| 2    | This spacious 2 bedroom apartment is located in the vibrant East Harlem neighbourhood of Manhattan, just a stone's throw away from Central Park. The entire home is available for rent, providing guests with ample space and privacy during their stay.»\n",
      "[4] «With a minimum stay of 7 nights, this listing is perfect for those looking for a longer-term rental option. Although there are currently no reviews for this property, guests can rest assured that they will be staying in a well-maintained and comfortable space. The host has two listings on Airbnb and the availability for this property is 249 days out of the year. Book now to experience the excitement of New York City from the comfort of this spacious apartment. |\n",
      "| 3    | This private room is located in the Flatbush neighborhood of Brooklyn, offering a unique country space in the heart of the city.»\n",
      "[5] «The beautiful Central Park is also a short walk away.\\n\\nCome and experience the real New York City in the Village of Harlem. I look forward to meeting you and showing you around my home. |\n",
      "| 3    | Huge 2 BR Upper East  Cental Park\\n\\nHost Sing\\n\\nEast Harlem\\nManhattan\\n\\nPrice: $190\\n\\nThis huge 2 bedroom is located in the heart of Manhattan, steps away from Central Park. It is newly renovated with stainless steel appliances, exposed brick, and beautiful hardwood floors. The apartment is located in a very safe and convenient area, with a variety of restaurants and stores nearby.»\n",
      "\n",
      "Question: What is the minimum stay and nightly rate for the new two-bedroom apartment rental in East Harlem near Central Park with 249 days of annual availability?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to identify the relevant information from the context. We are looking for a two-bedroom apartment in East Harlem near Central Park with 249 days of annual availability.\n",
      "\n",
      "Answer: The minimum stay for the new two-bedroom apartment rental in East Harlem near Central Park with 249 days of annual availability is seven nights, and the nightly rate is $190.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «In other words, GOMEMLIMIT is precisely the missing piece that we outlined before. If the live heap is low (e.g., 100MB), we can delay the next GC cycle until the heap has doubled (200MB). But if the heap has grown close to our limit (e.g., 4GB), the GC runs more often to prevent us from ever running OOM. ### Why a soft limit? What is the difference between a soft and a hard limit?»\n",
      "[2] «* We could complete our full import without problems. * The memory usage never exceeded our soft limit of 2.75GiB and, therefore, never our hard limit of 3GiB. * GOMEMLIMIT made the GC more aggressive when less memory was available but kept it pretty relaxed when there was plenty. ## Conclusion: GOMEMLIMIT is a game-changer!\n",
      "We have covered quite a few points in this article, so let me summarize the key takeaways:\n",
      "* Memory is either allocated on the stack (cheap) or the heap (expensive). Long-lived memory must live on the heap.»\n",
      "[3] «The Go authors explicitly label GOMEMLIMIT a \"soft\" limit. That means that the Go runtime does not guarantee that the memory usage will exceed the limit. Instead, it uses it as a target. The goal is to fail fast in an impossible-to-solve situation: Imagine we set the limit to a value just a few kilobytes larger than the live heap. The GC would have to run constantly.»\n",
      "[4] «With the new parameter, I will invoke Weaviate like this:\n",
      "\n",
      "```\n",
      "docker run -it \\\n",
      "  -m 3GiB \\\n",
      "  -e \"GOMEMLIMIT=2750MiB\" \\\n",
      "  -e \"GOGC=100\" \\\n",
      "  -e \"PERSISTENCE_DATA_PATH=/var/lib/weaviate\" \\\n",
      "  -e \"AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true\" \\\n",
      "  -e \"PROMETHEUS_MONITORING_ENABLED=true\" \\\n",
      "  -e \"CLUSTER_HOSTNAME=node1\" \\\n",
      "  -e \"GODEBUG=gctrace=1\" \\\n",
      "  -v \"$PWD/data:/var/lib/weaviate\" \\\n",
      "  -p \"8080:8080\" \\\n",
      "  -p \"2112:2112\" \\\n",
      "  weaviate\n",
      "```\n",
      "\n",
      "Now it's time to run the same experiment again… and, I have some good news! The import succeeded. Let's run a quick sanity check to prove that there are indeed 400,000 objects imported:\n",
      "\n",
      "```bash\n",
      "curl localhost:8080/v1/graphql \\\n",
      "  -H 'content-type: application/json' \\\n",
      "  -d '{\"query\":\"{Aggregate{Example{meta{count}}}}\"}'\n",
      "\n",
      "{\"data\":{\"Aggregate\":{\"Example\":[{\"meta\":{\"count\":400000}}]}}}\n",
      "```\n",
      "\n",
      "As you can see, the imports are all present, let's take a look at the heap usage according to the Prometheus metrics:\n",
      "\n",
      "![GOMEMLIMIT helps to collect garbage before a OOM crash](./img/gomemlimit-in-action.jpg)\n",
      "\n",
      "The chart shows exactly what we expected:\n",
      "* At no time did the test exceed our hard 3GiB limit (hence no OOM kill)\n",
      "* As it approached the limit the heap hovered around our configured 2750MiB GOMEMLIMIT\n",
      "* After the import finished, the stable long-lived heap dropped to just above 2GiB\n",
      "\n",
      "Let us also take a look at the most recent GC log:\n",
      "\n",
      "```\n",
      "gc 313 @692.771s 2%: 0.36+393+0.13 ms clock,\n",
      "5.7+337/1572/1065+2.1 ms cpu,\n",
      "2214->2272->2039 MB,\n",
      "2296 MB goal, 0 MB stacks, 0 MB globals, 16 P\n",
      "```\n",
      "\n",
      "Again, this log output backs up our claims from above:\n",
      "* After completing the import, the live heap is just over 2GiB (2039MB)\n",
      "* As we got closer to our soft limit, the heap goal was no longer twice the live heap. Instead, it is barely above the live heap at 2296MB. * The Go runtime has done a conscious trade-off to sacrifice a bit of CPU time to prevent us from running OOM. We can see that the self-reported cost of GC is now at 2%\n",
      "\n",
      "### In summary, how did GOMEMLIMIT save us from OOM?»\n",
      "[5] «A stack allocation is short-lived and typically very cheap. No Garbage Collection is required for a stack allocation as the end of the function is also the end of the variable's lifetime. On the other hand, a heap allocation is long-lived and considerably more expensive. When allocating onto the heap, the runtime must find a contiguous piece of memory where the new variable fits. Additionally, it must be garbage-collected when the variable is no longer used.»\n",
      "\n",
      "Question: What is the role of GOMEMLIMIT in garbage collection cycles, and how does it differ when the live heap is low versus when it is near the limit?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to understand the role of GOMEMLIMIT in managing memory usage and its impact on garbage collection cycles. GOMEMLIMIT is a \"soft\" limit introduced in the Go programming language to optimize memory usage and prevent out-of-memory (OOM) situations. It is a target value that guides the garbage collector (GC) but is not strictly enforced. When the live heap is low: - GOMEMLIMIT allows the GC cycle to be delayed. The GC runs less frequently, conserving resources and improving performance. - The GC is more relaxed, as there is plenty of memory available, and it can take its time to collect and manage memory efficiently. When the live heap is near the limit: - GOMEMLIMIT triggers the GC to run more often. It becomes more aggressive to prevent an OOM situation. - The GC tries to keep the memory usage below the soft limit, sacrificing some CPU time if needed. It becomes a conscious trade-off between memory usage and CPU usage. - The goal of the GC is no longer to double the live heap but to stay barely above it, ensuring that memory usage remains within manageable limits.\n",
      "\n",
      "Answer: GOMEMLIMIT plays a crucial role in optimizing garbage collection cycles by providing a target memory limit. When the live heap is low, GOMEMLIMIT delays GC cycles and allows for a more relaxed collection. However, when the live heap is near the limit, GOMEMLIMIT triggers more frequent and aggressive GC cycles to prevent OOM. This adaptive behavior ensures efficient memory management and prevents potential crashes due to memory constraints.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «In addition to these two models, however, you can use any model from the Hugging Face Hub (or your own) by following [this guide](/developers/weaviate/modules/reader-generator-modules/sum-transformers#use-another-summarization-module-from-hugging-face). Even when looking only at language models that are trained for summarization tasks, there is still a wide range of choices in terms of sheer numbers, which vary in the target domain (e.g. medical, legal, scientific, etc.) and size (number of parameters, i.e. speed). If you have specific needs, we recommend investigating other models. ### Avoid too long an input\n",
      "\n",
      "All transformer models have a maximum input length size. For example, `bart-large-cnn` has a maximum limit of 1024 tokens, where each token is part of a word (i.e. a few characters).»\n",
      "[2] «This module can shorten a piece of text into a pithy, to-the-point summary by passing the text retrieved from Weaviate to a language model trained specifically for summarization. By using Weaviate to summarize your data, you can reduce the amount of too long; did not read (TL;DR) content in your life and reduce the problem of information overload. The `sum-transformers` module uses the `bart-large-cnn` model by default, with an option for the `pegasus-xsum` model, but any model from Hugging Face Hub can be used. import WhatsNext from '/_includes/what-next.mdx';\n",
      "\n",
      "<WhatsNext />»\n",
      "[3] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\n",
      "let them do it for you.\"\n",
      "\n",
      "## Support for Hugging Face Inference API in Weaviate\n",
      "Starting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\n",
      "> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\n",
      "[4] «## Overview\n",
      "![Overview](./img/hugging-face-module-overview.png)\n",
      "\n",
      "The Hugging Face module is quite incredible, for many reasons. ### Public models\n",
      "You get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case. In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). ### Private models\n",
      "If you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate.»\n",
      "[5] «At the time of this writing, model cards still rely on **manual tagging**. It may be the case that the developer uploading the model hasn’t filled out these details. If you are new to Hugging Face, please consider annotating your uploaded models by adding a model card - YAML sections in the `README.md`, like this:\n",
      "\n",
      "<img\n",
      "    src={require('./img/how-to-populate-model-card-min.png').default}\n",
      "    alt=\"How to populate a Hugging Face model card\"\n",
      "    style={{ maxWidth: \"70%\" }}\n",
      "/>\n",
      "\n",
      "### Private models\n",
      "A large part of the beauty of Weaviate's integration with Hugging Face is that **anyone** can upload their models to Hugging Face and use them in Weaviate's vector database. For example, I am doing research on COVID-19 literature, so I have fine-tuned a model on CORD-19 title to abstract matching and uploaded it to [CShorten/CORD-19-Title-Abstracts](https://huggingface.co/CShorten/CORD-19-Title-Abstracts-1-more-epoch). ### Domain recap\n",
      "As a quick recap, **Domain** largely describes the high-level notion of **what** the dataset is about.»\n",
      "\n",
      "Question: How can I use a different summarization model from the Hugging Face Hub for my project?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer to this question. We first need to identify which part of the context is relevant to the question. In this case, the relevant information is found in the first paragraph, where it mentions that any model from the Hugging Face Hub can be used for summarization tasks by following a specific guide.\n",
      "\n",
      "Answer: To use a different summarization model from the Hugging Face Hub, you need to follow the guide mentioned in the context. This guide will provide step-by-step instructions on how to utilize a custom model for your specific project. The flexibility to choose from a wide range of models allows you to select one that aligns with your project's target domain and size requirements.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Hamilton was granted honorary Brazilian citizenship in 2022. </details>\n",
      "\n",
      "<details>\n",
      "\n",
      "  <summary>\n",
      "    The Loch Ness Monster (52% reduction)\n",
      "  </summary>\n",
      "\n",
      ":::note Summarized text\n",
      "The Loch Ness Monster is said to be a large, long-necked creature. Popular belief in the creature has varied since it was brought to worldwide attention in 1933. Evidence of its existence is disputed, with a number of disputed photographs and sonar readings. The pseudoscience and subculture of cryptozoology has placed particular emphasis on the creature.»\n",
      "[2] «The ability to combine modules also allows for relations between e.g. a text object and a corresponding image object. As this article will show, Weaviate is also not restricted by some of the limitations of popular ANN libraries. ## When do you call a database a database? If you were to ask one hundred engineers what defines a database you will most likely get as many different answers. We might take the following points for granted in the world of SQL and NoSQL databases, yet they are anything but ubiquitous in the context of vector searches.»\n",
      "[3] «Our people and culture team opens up this part of our town hall meetings by asking, “Who is your Weaviate Hero” —> this is how the name for our Champions Program was set. With the Weaviate Hero Program coming to life, we want to expand this internal initiative and also give kudos to people in our Community who help other members learn, grow, and thrive by sharing their knowledge, providing feedback, collaborating, and leading by example. ## The Weaviate Hero Program - in a Nutshell\n",
      "\n",
      "The Weaviate Hero Program was brought to life to appreciate, recognize & celebrate members of our Community who contribute and engage in ways that:\n",
      "\n",
      "> *“Make our Community a great and safe place where people can learn, make friends, and grow. A place for giving and receiving.”*\n",
      "\n",
      "Community members who ultimately develop and share practices that help others in the group thrive. With this global program, we enable members to accelerate their knowledge, skills, and growth in the areas of Vector Databases, Search, and AI-native applications and become thought leaders through their activities and initiatives across the Weaviate Ecosystem.»\n",
      "[4] «Imagine a fictional company called ACME Accounting Group that offers online accounting services that use AI to make accounting easy and fun. The company has over one million customers. Each customer is a company that can have many users and even more documents. Alice, who works for AliceCorp, should never be able to see the accounting information of Bob, who works for BobInc. Therefore, AliceCorp and BobInc are tenants from the perspective of ACME Accounting.»\n",
      "[5] «Know someone who made a difference in your Weaviate journey? Nominate them as a Weaviate Hero! Let's recognize the people who make our community great. 💚\n",
      "\n",
      "Nominate your hero [here](https://docs.google.com/forms/d/e/1FAIpQLSePtgvTPZGx2gbCzVdyjitQ0WHrq4gNNZKiaqrQhphmJc3vJA/viewform). As we keep building, learning, and collaborating, we're excited to see the innovations that 2024 will bring. 💫\n",
      "\n",
      "![Weaviate Hero](img/Weaviate-hero_(1).gif)\n",
      "\n",
      "import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "\n",
      "Question: Who was granted honorary Brazilian citizenship in 2022?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to identify the person mentioned in the context who was granted honorary Brazilian citizenship in 2022.\n",
      "\n",
      "Answer: Hamilton was granted honorary Brazilian citizenship in 2022.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «## Representing long objects\n",
      "\n",
      "One of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly exceed the 512 token input limit on Deep Transformer Neural Networks. This problem is a large part of what motivates our interest in Hybrid Search techniques that combine the flexibility of Vector Search with the sparse BM25 word counting algorithm well suited for >512 token text sequences. We think Ref2Vec can also help address this challenge. To be clear, Weaviate already offers a solution to represent long documents with Cross-References! As an example, [the Wikipedia Demo](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) breaks the long Wikipedia articles into a (Paragraph, inArticle, Article) data schema in which each Paragraph contains less than 512 tokens of text.»\n",
      "[2] «We can then vectorize this text description using off-the-shelf models from OpenAI, Cohere, HuggingFace, and others to unlock semantic search. We recently presented an example of this idea for [AirBnB listings](https://weaviate.io/blog/generative-feedback-loops-with-llms), translating tabular data about each property’s price, neighborhood, and more into a text description. Huge thanks to Svitlana Smolianova for creating the following animation of the concept. <img\n",
      "    src={require('./img/gen-example.gif').default}\n",
      "    alt=\"alt\"\n",
      "    style={{ width: \"100%\" }}\n",
      "/>\n",
      "\n",
      "### Text Chunking\n",
      "Similarly related to the 512 token length for vectorizing text chunks, we may consider using the Large Language Model to identify good places to cut up text chunks. For example, if we have a list of items, it might not be best practice to separate the list into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.»\n",
      "[3] «This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in “Orchestrating Tuning”. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents.»\n",
      "[4] «When dealing with dimensions ranging from 8 to 128, we found that using AVX-256 instructions with loop unrolling yields the most favorable results. In the past, we conducted experiments with AVX-512 to assess its performance potential. However, the results didn’t show a significant increase in performance, particularly on older Intel processors. This lack of notable improvement could be due to thermal and power constraints in these older processor architectures. Such constraints could lead to [reductions in CPU frequency](https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/), limiting the capacity of AVX-512 instructions to deliver enhanced performance under these conditions.»\n",
      "[5] «| When you want to control the batch size. |\n",
      "| `rate_limit` | The number of objects sent to Weaviate is rate limited (specified as n_objects per minute). | When you want to avoid hitting third-party vectorization API rate limits. |\n",
      "\n",
      "<Tabs groupId=\"languages\">\n",
      "    <TabItem value=\"wcs\" label=\"Dynamic\">\n",
      "        <FilteredTextBlock\n",
      "            text={PythonCode}\n",
      "            startMarker=\"# START BatchDynamic\"\n",
      "            endMarker=\"# END BatchDynamic\"\n",
      "            language=\"py\"\n",
      "        />\n",
      "    </TabItem>\n",
      "    <TabItem value=\"local\" label=\"Fixed size\">\n",
      "        <FilteredTextBlock\n",
      "            text={PythonCode}\n",
      "            startMarker=\"# START BatchFixedSize\"\n",
      "            endMarker=\"# END BatchFixedSize\"\n",
      "            language=\"py\"\n",
      "        />\n",
      "    </TabItem>\n",
      "    <TabItem value=\"embedded\" label=\"Rate limit\">\n",
      "        <FilteredTextBlock\n",
      "            text={PythonCode}\n",
      "            startMarker=\"# START BatchRateLimit\"\n",
      "            endMarker=\"# END BatchRateLimit\"\n",
      "            language=\"py\"\n",
      "        />\n",
      "    </TabItem>\n",
      "</Tabs>\n",
      "\n",
      "#### Error handling in batch operations\n",
      "\n",
      "The `v4` client introduces additional error handling operations. You can now monitor the number of errors in an ongoing batching operation, or simply review the errors after the operation has completed.»\n",
      "\n",
      "Question: What is the strategy for chunking text for vectorization when dealing with a 512 token length limit?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To effectively chunk text for vectorization when faced with a 512 token length constraint, we need to identify optimal breakpoints in the text that ensure each chunk remains within the limit while preserving the integrity of the information.\n",
      "\n",
      "Answer: The strategy suggested is to use Large Language Models (LLMs) to identify suitable places to divide the text. This ensures that meaningful segments of text remain together, even if they exceed the 512 token limit. Additionally, the text chunking process is an important step in optimizing retrieval and ensuring compliance with LLM token limits.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «You can find other dev container templates [here](https://containers.dev/templates). **Step 2. Test the Setup**\n",
      "\n",
      "1. After setting up the environment we tested that everything worked by:\n",
      "Create a [dummy endpoint](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-c16fbf0c6f7b90a46b94b36f88893c2d174476088608841f7254afba0e81373d) which will simply respond with a `{“Hello”: “World”}` object when called. ```python\n",
      "from fastapi import FastAPI\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    \"\"\"\n",
      "    Say hello to the world\n",
      "    \"\"\"\n",
      "    return {\"Hello\": \"World\"}\n",
      "```\n",
      "\n",
      "2.»\n",
      "[2] «These functions give ChatGPT the ability to add objects to the Weaviate instance, query and search through objects in the Weaviate instance and lastly delete objects if needed. Upon interacting with ChatGPT while the plugin is enabled it can be instructed to use a particular endpoint via prompt but will also independently decide when to use the appropriate endpoint to complete the response to a query! These endpoints are what extend the functionality of ChatGPT and enable it to interact with the vector database. We developed these three endpoints through test driven development, as such we will display the tests that each endpoint must first pass and then the implementation that satisfies these tests. In preparation to setup the Weaviate instance for these tests we added the following test documents through a fixture:\n",
      "```python\n",
      "@pytest.fixture\n",
      "def documents(weaviate_client):\n",
      "    docs = [\n",
      "        {\"text\": \"The lion is the king of the jungle\", \"document_id\": \"1\"},\n",
      "        {\"text\": \"The lion is a carnivore\", \"document_id\": \"2\"},\n",
      "        {\"text\": \"The lion is a large animal\", \"document_id\": \"3\"},\n",
      "        {\"text\": \"The capital of France is Paris\", \"document_id\": \"4\"},\n",
      "        {\"text\": \"The capital of Germany is Berlin\", \"document_id\": \"5\"},\n",
      "    ]\n",
      "\n",
      "    for doc in docs:\n",
      "        client.post(\"/upsert\", json=doc)\n",
      "```\n",
      "\n",
      "**Implementing the `/upsert` endpoint:**\n",
      "\n",
      "After using the `/upsert` endpoint we mainly want to test that that we got the appropriate status code in addition to checking that the content, id and vector’s were all upserted correctly. Here's the test that carries this out:\n",
      "\n",
      "```python\n",
      "def test_upsert(weaviate_client):\n",
      "    response = client.post(\"/upsert\", json={\"text\": \"Hello World\", \"document_id\": \"1\"})\n",
      "    assert response.status_code == 200\n",
      "\n",
      "    docs = weaviate_client.data_object.get(with_vector=True)[\"objects\"]\n",
      "    assert len(docs) == 1\n",
      "    assert docs[0][\"properties\"][\"text\"] == \"Hello World\"\n",
      "    assert docs[0][\"properties\"][\"document_id\"] == \"1\"\n",
      "    assert docs[0][\"vector\"] is not None\n",
      "```\n",
      "\n",
      "The implementation below satisfies all of these requirements and tests above:\n",
      "```python\n",
      "@app.post(\"/upsert\")\n",
      "def upsert(doc: Document, client=Depends(get_weaviate_client)):\n",
      "    \"\"\"\n",
      "    Insert a document into weaviate\n",
      "    \"\"\"\n",
      "    client.batch.configure(batch_size=100)  # Configure batch\n",
      "    with client.batch as batch:\n",
      "        batch.add_data_object(\n",
      "            data_object=doc.dict(),\n",
      "            class_name=INDEX_NAME,\n",
      "            vector=get_embedding(doc.text),\n",
      "        )\n",
      "\n",
      "    return {\"status\": \"ok\"}\n",
      "```\n",
      "\n",
      "The `/query` and `/delete` endpoints were developed similarly, if you're interested you can read below!\n",
      "\n",
      "<details>\n",
      "  <summary>See details for /query endpoint implementation.</summary>\n",
      "\n",
      "**Implement the `/query` endpoint:**\n",
      "\n",
      "For this endpoint we mainly want to check that it returns the right number of objects and that the required document that we were expecting is part of the returned objects.»\n",
      "[3] «Since none of the other services will be accessed outside of Snowflake, I only needed it for the Jupyter service. ```\n",
      "GRANT USAGE ON SERVICE JUPYTER TO ROLE WEAVIATE_ROLE;\n",
      "```\n",
      "\n",
      "### Step 4: Generate Product Data with Mistral\n",
      "Next, I needed to access the Jupyter Notebooks endpoint. You can find your endpoint by running this command:\n",
      "\n",
      "```sql\n",
      "SHOW ENDPOINTS IN SERVICE JUPYTER;\n",
      "```\n",
      "\n",
      "This showed me the endpoint for Jupyter that I can use to access it. I grabbed the URL for the Jupyter endpoint and pasted it into my browser. Using the Snowflake Login I set up earlier, I logged into the Jupyter Notebooks interface.»\n",
      "[4] «See the example below:\n",
      "\n",
      "![error](./img/error.png)\n",
      "\n",
      "**Take-aways:** ChatGPT doesn’t have hard-coded instructions on when and how to use the plugin endpoints. You need to be very careful about how you describe your plugin and endpoints to ChatGPT so that they can be used as intended! The `openapi.json` spec that FastAPI generates for you is based on how you documented the endpoints in your code i.e. the function’s name, docstring, query description and the field’s description in your pydantic models. The steps used to do this are outside the scope of this blog post, for more details please refer to the FastAPI documentation. In general you want to have complete and comprehensive documentation for your plugin because the documentation is what will allow it to be used correctly!\n",
      "\n",
      "Additionally you have to be careful when specifying the descriptions, doc strings etc. not to exceed the context length since the plugin description, API requests, and API responses are all inserted into the conversation with ChatGPT.»\n",
      "[5] «<br/>\n",
      "\n",
      "Just like that, you’ve got a multi-node Weaviate cluster. Remember that when you create a class, you must have replication enabled. You can do this by adding the `replicationConfig` parameter to the collection definition, like so:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"class\": \"ClassWithReplication\",\n",
      "  \"properties\": [\n",
      "    {\n",
      "      \"name\": \"exampleProperty\",\n",
      "      \"dataType\": [\"text\"]\n",
      "    }\n",
      "  ],\n",
      "  // highlight-start\n",
      "  \"replicationConfig\": {\n",
      "    \"factor\": 3\n",
      "  }\n",
      "  // highlight-end\n",
      "}\n",
      "```\n",
      "\n",
      "And when you insert objects into `ClassWithReplication`, they will be replicated across the three nodes. You can verify this by visiting the `/nodes` endpoint, which will verify that each node contains the same number of objects. </details>\n",
      "\n",
      "## Benefits of replication\n",
      "\n",
      "So, let’s cover the implications of doing this.»\n",
      "\n",
      "Question: How do you create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed, we can follow these steps: 1. Define the endpoint route: We want our endpoint to be accessible at the root URL \"/\", so we define it using the `@app.get(\"/\")` decorator. 2. Create a function for the endpoint: We need a function that will be executed when the endpoint is accessed. In this case, we want to return `{\"Hello\": \"World\"}`. 3. Add a docstring to the function: Although not necessary for the functionality, adding a docstring helps with documentation and understanding the purpose of the endpoint. 4. Return the desired JSON object: Inside the function, we simply return `{\"Hello\": \"World\"}`.\n",
      "\n",
      "Answer: Here's the code snippet that creates the dummy endpoint: ```python from fastapi import FastAPI app = FastAPI() @app.get(\"/\") def read_root(): \"\"\" Say hello to the world \"\"\" return {\"Hello\": \"World\"} ``` This code defines a FastAPI app and creates an endpoint at \"/\" using the `@app.get(\"/\")` decorator. The `read_root()` function is executed when the endpoint is accessed, and it returns `{\"Hello\": \"World\"}`. The docstring provides a description of the endpoint's purpose.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\n",
      "\n",
      "![Reduced memory usage](./img/reduced-memory-usage.jpg)\n",
      "\n",
      "As part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\n",
      "\n",
      "First, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs).»\n",
      "[2] «There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.»\n",
      "[3] «[Better control over Garbage Collector](#better-control-over-garbage-collector) - with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 1. [Faster imports for ordered data](#faster-imports-for-ordered-data) - by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n)\n",
      "1. [More efficient filtered aggregations](#more-efficient-filtered-aggregations) - thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 1.»\n",
      "[4] «Next, let's see how we might test imports. ### Testing imports\n",
      "\n",
      "One particularly common issue we see is skipped objects during import due to rate limits from the vectorization provider. So, let's see how we might test that all objects were imported correctly. In this section, we'll import a small subset (100 objects) of the [original Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions). As always, we'll use [batching](/developers/weaviate/manage-data/import) for optimal speed.»\n",
      "[5] «---\n",
      "title: Weaviate 1.15 release\n",
      "slug: weaviate-1-15-release\n",
      "authors: [connor, erika, laura, sebastian]\n",
      "date: 2022-09-07\n",
      "tags: ['release']\n",
      "image: ./img/hero.png\n",
      "description: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\n",
      "---\n",
      "![Weaviate 1.15 release](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "We are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\n",
      "\n",
      "If you like your content brief and to the point, here is the TL;DR of this release:\n",
      "1. [☁️Cloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\n",
      "1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1.»\n",
      "\n",
      "Question: What optimization has Weaviate introduced to manage memory usage during parallel data imports?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: The context mentions that Weaviate has introduced new optimizations to reduce RAM usage without compromising performance. Specifically, it talks about a thread pooling optimization that addresses parallel imports.\n",
      "\n",
      "Answer: Weaviate introduced thread pooling to manage memory usage during parallel data imports. This optimization ensures that the number of parallel imports doesn't exceed the number of CPU cores, reducing memory spikes and providing maximum performance without unnecessary memory usage.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Similarly to the original Gorilla paper’s use of Abstract Syntax Tree evaluation, we are also considering an n-gram match where we construct keywords for each query such as “bm25”, “query”, “title” and check how many are contained in the generated query. We can also use the finer-grained perplexity metric that measures the log probability of the ground truth tokens at each step of decoding. We are currently using a simple greedy decoding algorithm to sample from the LoRA fine-tuned LlaMA 7B LLM. To ground the evaluation discussion further, let’s take a look at an incorrect query:\n",
      "\n",
      "```graphql\n",
      "{\n",
      "\tGet {\n",
      "\t\tJobListing(\n",
      "\t\t\tbm25: {query: “software”}\n",
      "\t\t\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
      "\t\t){\n",
      "\t\ttitle\n",
      "\t\tdescription\n",
      "\t\tisRemote\n",
      "\t\tpostedBy {\n",
      "\t\t\tname\n",
      "\t\t  }\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "```\n",
      "\n",
      "Almost there! But unfortunately the missing comma from the `bm25` to `where` query will prevent this query from successfully executing. As discussed we may have other cases where although the syntax is correct and the query executes, it does not achieve what was specified in the natural language command.»\n",
      "[2] «Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate. Only use the API reference to understand the syntax of the request. ##Natural Language Query\n",
      "%s\n",
      "\n",
      "## Schema\n",
      "%s\n",
      "\n",
      "## API reference\n",
      "%s\n",
      "\n",
      "## Answer\n",
      "```graphql\n",
      "“”” % (nlcommand, customSchema, apiReference)\n",
      "```\n",
      "\n",
      "## Quantitative Evaluation\n",
      "\n",
      "Hopefully these visualizations of the Gorilla output on unseen test examples provides a solid indication of where the model is at. We are still kicking off the process of rigorous quantitative measurement, here are some of our thoughts on quantitative evaluation so far.»\n",
      "[3] «The GraphQL query editor features:\n",
      "\n",
      "- Syntax highlighting\n",
      "- Intelligent type ahead of fields, arguments, types, and more\n",
      "- Real-time error highlighting and reporting for queries and variables\n",
      "- Automatic query and variables completion\n",
      "\n",
      "\n",
      "![Weaviate Cloud Service GraphQL IDE ](./img/wcs-console-example-query.png)\n",
      "\n",
      "## Control Access to Weaviate - API Key Management\n",
      "To help make it easy to control access to your vector databases, all Weaviate Cloud instances can generate API keys with full admin (read and write) access. Read-only API keys are also supported for paid clusters. ![API Key Management in Weaviate Cloud Service](./img/wcs-get-key.png)\n",
      "\n",
      "## Next Stop, GA…\n",
      "The Weaviate Cloud is in public beta. General Availability is planned for release later in 2023, after we’ve addressed feedback from public beta users. If you’d like to try Weaviate Cloud, you can create a [free Weaviate Cloud Sandbox here](https://console.weaviate.cloud/).»\n",
      "[4] «:::\n",
      "\n",
      ":::note What you will be able to do using the Web bundle\n",
      "Only Read operations powered by GraphQL. :::\n",
      "\n",
      "\n",
      "\n",
      "### Collections-first approach\n",
      "\n",
      "The other big change is that the `collections` client focuses on individual collections for interaction. This means that you will no longer need to specify the collection name in every request. Instead, you will create an object for each collection that you want to interact with, and then use that object for all subsequent requests. For example, take the following syntax for performing a simple request to retrieve a few objects from the database:\n",
      "\n",
      ":::note Classic TypeScript syntax\n",
      "\n",
      "```typescript\n",
      "const response = await client.graphql\n",
      "  .get()\n",
      "  .withClassName('Article')\n",
      "  .withLimit(2)\n",
      "  .withFields('title body url')\n",
      "  .do();\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "Becomes:\n",
      "\n",
      ":::info `Collections` client syntax\n",
      "\n",
      "```typescript\n",
      "const articles = client.collection.get('Article');\n",
      "const response = articles.query.fetchObjects({\n",
      "  limit: 2,\n",
      "  returnProperties: ['title','body','url']\n",
      "});\n",
      "```\n",
      "\n",
      ":::\n",
      "\n",
      "You'll see that a search is now a method that originates from the collection object.»\n",
      "[5] «This can be done by sending queries like this to Weaviate:\\n\\n```graphql\\n{\\n  Get {\\n    Product (\\n      nearObject: {\\n        id: \\\"8abc5-4d5...\\\" # id for the User object with vector defined by ref2vec-centroid\\n      }\\n    ) {\\n      product_name\\n      price\\n    }\\n  }\\n}\\n```\\n\\nThis short query encapsulates the power of Ref2Vec.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 5.5508026e-06\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"---\\ntitle: What is Ref2Vec and why you need it for your recommendation system\\nslug: ref2vec-centroid\\nauthors: [connor]\\ndate: 2022-11-23\\ntags: ['integrations', 'concepts']\\nimage: ./img/hero.png\\ndescription: \\\"Weaviate introduces Ref2Vec, a new module that utilises Cross-References for Recommendation!\\\"\\n---\\n![Ref2vec-centroid](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWeaviate 1.16 introduced the [Ref2Vec](/developers/weaviate/modules/retriever-vectorizer-modules/ref2vec-centroid) module. In this article, we give you an overview of what Ref2Vec is and some examples in which it can add value such as recommendations or representing long objects. ## What is Ref2Vec? The name Ref2Vec is short for reference-to-vector, and it offers the ability to vectorize a data object with its cross-references to other objects. The Ref2Vec module currently holds the name ref2vec-**centroid** because it uses the average, or centroid vector, of the cross-referenced vectors to represent the **referencing** object.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 4.7478566e-06\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"## More Coming Soon\\nWe are very excited about the potential of Ref2Vec, and how it leverages existing symbolic data to augment vector searches in a new way.»\n",
      "\n",
      "Question: What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's think step by step to identify the syntax error in the provided GraphQL query example and how it relates to the evaluation of n-gram matches. We'll start by examining the query and identifying any discrepancies from the expected syntax.\n",
      "\n",
      "Answer: The syntax error in the provided GraphQL query example is the missing comma between the `bm25` and `where` fields. This error would prevent the query from executing successfully and is thus relevant to the evaluation of n-gram matches, as it affects the structure and validity of the query.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «#### Test\n",
      "We designed an extreme stress test that would represent the \"worst-case\" scenario for recovery. It has multiple very large, independent Write-Ahead Logs that required for recovery. Before, this could take many hours to recover, while now it takes only a few seconds. ### Full changelog\n",
      "These are few of the many improvements and bug fixes that were included in this release. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list.»\n",
      "[2] «#### Problem\n",
      "If Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\n",
      "\n",
      "- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\n",
      "- The recovery process was single-threaded.»\n",
      "[3] «#### Solution\n",
      "We addressed each of the points above individually and improved the overall MTTR substantially:\n",
      "\n",
      "- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.»\n",
      "[4] «To attach to the logs of a specific containers you can run:\n",
      "\n",
      "```bash\n",
      "docker compose logs -f <container-name>\n",
      "```\n",
      "\n",
      "for example:\n",
      "\n",
      "```bash\n",
      "docker compose logs -f weaviate\n",
      "```\n",
      "\n",
      "You can also omit the -d option and keep your terminal attached to all logs. To see the names and status of each container you can run:\n",
      "\n",
      "```bash\n",
      "docker compose ps\n",
      "```\n",
      "\n",
      "## How do I know when Weaviate is up and ready? Weaviate implements a readiness check at `GET /v1/.well-known/ready`. It will return a `2xx` HTTP status code once everything is ready. To check for the readiness programmatically you can use `curl` in this simple `bash` loop:\n",
      "\n",
      "```bash\n",
      "until curl --fail -s localhost:8080/v1/.well-known/ready; do\n",
      "  sleep 1\n",
      "done\n",
      "```\n",
      "\n",
      "Make sure to adjust localhost:8080 if you are running on a different bind address.»\n",
      "[5] «We were able to import 200 million objects and more, while the import performance remained constant throughout the process. [See more on github](https://github.com/weaviate/weaviate/pull/1976). ### Drastically improved Mean-Time-To-Recovery (MTTR)\n",
      "Weaviate `1.14` fixes an issue where a crash-recovery could take multiple minutes, or even hours in some extreme cases. It is now a matter of just seconds. So even in the rare event that your instance crashes, it will come back up very quickly.»\n",
      "\n",
      "Question: How was the MTTR improved in the system with regards to handling large write-ahead logs?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer by understanding the problem and the solutions implemented. In the given context, the problem is that the recovery process for Write-Ahead Logs (WALs) could be slow, especially with large unflushed memtables and a single-threaded recovery process.\n",
      "\n",
      "Answer: The MTTR was improved by addressing the following: 1. Deduplication of large WALs: By adding a deduplication process, redundant data in the WALs is removed, reducing the recovery time. 2. Parallel recovery: The recovery process now runs in parallel, allowing multiple places requiring recovery to work independently and simultaneously, improving the speed. 3. Idle memtable flushing: Any memtable idle for 60 seconds or more is flushed, reducing the amount of data that needs to be recovered and ensuring faster recovery. These changes ensure that the recovery process for large WALs is faster and more efficient, improving the overall MTTR of the system.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «---\n",
      "title: Weaviate 2023 Recap\n",
      "slug: 2023-recap\n",
      "authors: [femke]\n",
      "date: 2023-12-26\n",
      "tags: []\n",
      "image: ./img/hero.png\n",
      "description: \"A reflection on 2023 from team Weaviate!\"\n",
      "---\n",
      "![hero](img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "It’s hard to imagine that less than a year ago, so very few people even knew about the concept of vector databases and how AI could benefit from them. Those who did still had many questions about how they worked and whether they could at all be helpful. Meanwhile, curiosity and interest in AI spiked, especially after OpenAI launched ChatGPT. Curiosity has sped up our progress and made more people aware of the opportunities AI offers, transforming our landscape. Let's all take a moment to reflect and appreciate the start of a momentous change in how we can communicate, learn, teach, and collaborate so much faster and more effectively by leveraging AI.»\n",
      "[2] «---\n",
      "title: What to expect from Weaviate in 2023\n",
      "slug: what-to-expect-from-weaviate-in-2023\n",
      "authors: [etienne]\n",
      "date: 2023-02-14\n",
      "tags: ['engineering']\n",
      "image: ./img/hero.png\n",
      "description: \"Learn about the six pillars outlining how Weaviate will evolve in 2023.\"\n",
      "---\n",
      "![What to expect from Weaviate in 2023](./img/hero.png)\n",
      "\n",
      "Without a doubt, 2022 has been the most exciting year for Weaviate so far. The company and the product have grown tremendously, and we are incredibly excited about 2023. Weaviate’s usage numbers are through the roof, and so are your feedback and requests for what you’re still missing from Weaviate. <!-- truncate -->\n",
      "\n",
      "In this blog post, I will introduce you to the six pillars outlining how Weaviate will get even better in 2023. Weaviate development is highly dynamic – we don’t waterfall-plan for the entire year – but nevertheless, we want to give you the best possible overview of what to expect in the coming year.»\n",
      "[3] «---\n",
      "title: Weaviate’s re:Invent 2023 recap\n",
      "slug: reinvent-2023\n",
      "authors: [ieva, alea, erika]\n",
      "date: 2023-12-05\n",
      "image: ./img/hero.png\n",
      "tags: []\n",
      "description: \"Recap the first Weaviate visit to Amazon re:Invent in Las Vegas\"\n",
      "---\n",
      "\n",
      "![AWS re:Invent](./img/hero.png)\n",
      "\n",
      "This November the Weaviate team went to our first ever AWS re:Invent! Over 50,000 people from around the world came together to learn about the latest developments in cloud technology and to share what they’ve been building with the AWS developer community. Let’s dive into our highlight reel!\n",
      "\n",
      "## GenAI is (still) heating up\n",
      "\n",
      "This year’s re:Invent focus theme was Generative AI and everyone seemed to be talking about it. In the sessions our team attended, we noticed a particular emphasis on [Retrieval Augmented Generation (RAG)](https://weaviate.io/blog/rag-evaluation). Initially when people came to our booth on Monday, roughly 40% of people knew about RAG. By the last day, nearly everyone that talked to us knew what it was because so many different sessions had covered the topic.»\n",
      "[4] «I am proud of you all and highly excited about the future. Thank you all, and let’s make 2023 the most exciting year for Weaviate users so far!\n",
      "\n",
      "<br></br>\n",
      "\n",
      "import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "[5] «This way, you can make sure that your voice is heard, and we can see what all of you need the most. <br></br>\n",
      "\n",
      "## Conclusion: Proud of how far we’ve come, excited about the future\n",
      "In the beginning, I mentioned that not just the product but also the company grew significantly last year. I am incredibly proud of what we have achieved – both overall and in the past year. This wouldn’t have been possible without an absolutely fantastic team. Everyone working on Weaviate – whether a full-time employee or open-source contributor – is doing a fantastic job.»\n",
      "\n",
      "Question: What is the title of the Weaviate team's reflection on the year 2023?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to find the title of the Weaviate team's reflection on the year 2023. We can look for a blog post or article published by the Weaviate team that recaps the year.\n",
      "\n",
      "Answer: \"Weaviate 2023 Recap\"\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «This will give you an idea of the Weaviate cluster’s performance over the course of this massive import process. Let’s start off with a bird’s eye view of the whole run. As mentioned above, the entire process took approximately 48 hours and on a high level the system performed very well. The graph beneath the text show the number of vectors imported over time:\n",
      "\n",
      "![Number of vectors imported](./img/number-of-vectors-imported.png)\n",
      "\n",
      "As can be seen the slope, indicating import speed, is almost linear. meaning that we see pretty much no slow down in the import times regardless of the quantity of objects already imported.»\n",
      "[2] «```python\n",
      "def test_query(documents):\n",
      "    LIMIT = 3\n",
      "    response = client.post(\"/query\", json={\"text\": \"lion\", \"limit\": LIMIT})\n",
      "\n",
      "    results = response.json()\n",
      "\n",
      "    assert len(results) == LIMIT\n",
      "    for result in results:\n",
      "        assert \"lion\" in result[\"document\"][\"text\"]\n",
      "```\n",
      "\n",
      "The implementation below will take in a query and return a list of retrieved documents and metadata. ```python\n",
      "@app.post(\"/query\", response_model=List[QueryResult])\n",
      "def query(query: Query, client=Depends(get_weaviate_client)) -> List[Document]:\n",
      "    \"\"\"\n",
      "    Query weaviate for documents\n",
      "    \"\"\"\n",
      "    query_vector = get_embedding(query.text)\n",
      "\n",
      "    results = (\n",
      "        client.query.get(INDEX_NAME, [\"document_id\", \"text\"])\n",
      "        .with_near_vector({\"vector\": query_vector})\n",
      "        .with_limit(query.limit)\n",
      "        .with_additional(\"certainty\")\n",
      "        .do()\n",
      "    )\n",
      "\n",
      "    docs = results[\"data\"][\"Get\"][INDEX_NAME]\n",
      "\n",
      "    return [\n",
      "        QueryResult(\n",
      "            document={\"text\": doc[\"text\"], \"document_id\": doc[\"document_id\"]},\n",
      "            score=doc[\"_additional\"][\"certainty\"],\n",
      "        )\n",
      "        for doc in docs\n",
      "    ]\n",
      "```\n",
      "</details>\n",
      "\n",
      "<details>\n",
      "  <summary>See details for /delete endpoint implementation.</summary>\n",
      "\n",
      "**Implement the `/delete` endpoint:**\n",
      "\n",
      "Here we simply want to check that the response returned correctly and that after removing one object we the number of total objects in the Weaviate instance goes down by one. ```python\n",
      "def test_delete(documents, weaviate_client):\n",
      "    num_docs_before_delete = weaviate_client.data_object.get()[\"totalResults\"]\n",
      "\n",
      "    response = client.post(\"/delete\", json={\"document_id\": \"3\"})\n",
      "    assert response.status_code == 200\n",
      "\n",
      "    num_docs_after_delete = weaviate_client.data_object.get()[\"totalResults\"]\n",
      "\n",
      "    assert num_docs_after_delete == num_docs_before_delete - 1\n",
      "```\n",
      "\n",
      "And the implementation of the endpoint is as follows:\n",
      "\n",
      "```python\n",
      "@app.post(\"/delete\")\n",
      "def delete(delete_request: DeleteRequest, client=Depends(get_weaviate_client)):\n",
      "    \"\"\"\n",
      "    Delete a document from weaviate\n",
      "    \"\"\"\n",
      "    result = client.batch.delete_objects(\n",
      "        class_name=INDEX_NAME,\n",
      "        where={\n",
      "            \"operator\": \"Equal\",\n",
      "            \"path\": [\"document_id\"],\n",
      "            \"valueText\": delete_request.document_id,\n",
      "        },\n",
      "    )\n",
      "\n",
      "    if result[\"results\"][\"successful\"] == 1:\n",
      "        return {\"status\": \"ok\"}\n",
      "    else:\n",
      "        return {\"status\": \"not found\"}\n",
      "```\n",
      "</details>\n",
      "\n",
      "Here we showed you how our endpoints work, this will be where your plugin will be most unique, depending on what functionality you want implemented you can create corresponding endpoints and test them. ::::tip\n",
      "Notice the docstrings we’ve included with all of our endpoints, these will be very important in the next step!\n",
      "::::\n",
      "\n",
      "**Step 2: Prepare plugin manifest files**\n",
      "\n",
      "This is where you specify to OpenAI and specifically ChatGPT which endpoints your plugin is exposing, how it can use those endpoints to accomplish specific tasks, what errors to expect if the endpoints are not used correctly and much more! [OpenAI instruction specify](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest) that you need to create two files: the [`openapi.yaml` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/openapi.yaml) and the [`ai-plugin.json` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/ai-plugin.json). As you can see both of these files need to be in the `.well-known` directory which must be mounted into the app as follows in order for ChatGPT to utilize them correctly.»\n",
      "[3] «But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50–100m, the import process would slow down to a walking pace. #### Solution\n",
      "To address this problem, we changed how the HNSW index grows. We implemented a relative growth pattern, where the HNSW index size increases by either 25% or 25'000 objects (whichever is bigger). ![HNSW index growth chart](./img/hnsw-index-growth.jpg)\n",
      "\n",
      "#### Test\n",
      "After introducing the relative growth patterns, we've run a few tests.»\n",
      "[4] «These functions give ChatGPT the ability to add objects to the Weaviate instance, query and search through objects in the Weaviate instance and lastly delete objects if needed. Upon interacting with ChatGPT while the plugin is enabled it can be instructed to use a particular endpoint via prompt but will also independently decide when to use the appropriate endpoint to complete the response to a query! These endpoints are what extend the functionality of ChatGPT and enable it to interact with the vector database. We developed these three endpoints through test driven development, as such we will display the tests that each endpoint must first pass and then the implementation that satisfies these tests. In preparation to setup the Weaviate instance for these tests we added the following test documents through a fixture:\n",
      "```python\n",
      "@pytest.fixture\n",
      "def documents(weaviate_client):\n",
      "    docs = [\n",
      "        {\"text\": \"The lion is the king of the jungle\", \"document_id\": \"1\"},\n",
      "        {\"text\": \"The lion is a carnivore\", \"document_id\": \"2\"},\n",
      "        {\"text\": \"The lion is a large animal\", \"document_id\": \"3\"},\n",
      "        {\"text\": \"The capital of France is Paris\", \"document_id\": \"4\"},\n",
      "        {\"text\": \"The capital of Germany is Berlin\", \"document_id\": \"5\"},\n",
      "    ]\n",
      "\n",
      "    for doc in docs:\n",
      "        client.post(\"/upsert\", json=doc)\n",
      "```\n",
      "\n",
      "**Implementing the `/upsert` endpoint:**\n",
      "\n",
      "After using the `/upsert` endpoint we mainly want to test that that we got the appropriate status code in addition to checking that the content, id and vector’s were all upserted correctly. Here's the test that carries this out:\n",
      "\n",
      "```python\n",
      "def test_upsert(weaviate_client):\n",
      "    response = client.post(\"/upsert\", json={\"text\": \"Hello World\", \"document_id\": \"1\"})\n",
      "    assert response.status_code == 200\n",
      "\n",
      "    docs = weaviate_client.data_object.get(with_vector=True)[\"objects\"]\n",
      "    assert len(docs) == 1\n",
      "    assert docs[0][\"properties\"][\"text\"] == \"Hello World\"\n",
      "    assert docs[0][\"properties\"][\"document_id\"] == \"1\"\n",
      "    assert docs[0][\"vector\"] is not None\n",
      "```\n",
      "\n",
      "The implementation below satisfies all of these requirements and tests above:\n",
      "```python\n",
      "@app.post(\"/upsert\")\n",
      "def upsert(doc: Document, client=Depends(get_weaviate_client)):\n",
      "    \"\"\"\n",
      "    Insert a document into weaviate\n",
      "    \"\"\"\n",
      "    client.batch.configure(batch_size=100)  # Configure batch\n",
      "    with client.batch as batch:\n",
      "        batch.add_data_object(\n",
      "            data_object=doc.dict(),\n",
      "            class_name=INDEX_NAME,\n",
      "            vector=get_embedding(doc.text),\n",
      "        )\n",
      "\n",
      "    return {\"status\": \"ok\"}\n",
      "```\n",
      "\n",
      "The `/query` and `/delete` endpoints were developed similarly, if you're interested you can read below!\n",
      "\n",
      "<details>\n",
      "  <summary>See details for /query endpoint implementation.</summary>\n",
      "\n",
      "**Implement the `/query` endpoint:**\n",
      "\n",
      "For this endpoint we mainly want to check that it returns the right number of objects and that the required document that we were expecting is part of the returned objects.»\n",
      "[5] «Partitioning 2. Cleaning, 3. Staging. Partitioning bricks take an unstructured document and extract structured content from it. It takes the document and breaks it down into elements like `Title`, `Abstract`, and `Introduction`.»\n",
      "\n",
      "Question: At what number of imported objects did the slow down begin according to the document?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Context: The provided text discusses the process of importing data into a Weaviate cluster and the implementation of various endpoints for interacting with the data. It mentions a slowdown in the import process, but does not explicitly state the number of imported objects at which this slowdown occurs. Question: At what number of imported objects did the slowdown begin according to the document? Reasoning: To answer this question, we need to carefully examine the relevant portions of the provided context. The text mentions a slowdown in the import process, but does not provide a specific number of imported objects at which this slowdown starts. However, it gives us a range to work with. The relevant excerpt from the context is: \"[3] «But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50–100m, the import process would slow down to a walking pace.\" Here, the document is stating that at around 25 million objects, the import process started to become noticeably slower. However, it also mentions that the import process further slowed down to a \"walking pace\" between 50 and 100 million objects. Therefore, we can conclude that the slowdown began around 25 million imported objects, but it became more pronounced and significant as the number of objects increased toward 50 million and beyond.\n",
      "\n",
      "Answer: The slowdown in the import process began around 25 million imported objects, but it became more significant as the number of objects approached 50 million and beyond.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «1**: *Suppose we have vectors $x$ and $y$ represented in their original space. We apply a compression function $C$ to obtain a shorter representation of $x$ ($x'$) and $y$ ($y'$) on a compressed space but would require a decompression function $C'$ from the compressed space into the original space to be able to use the original distance function. In this case we would obtain $x''$ and $y''$ from $x'$ and $y'$ respectively and apply the distance on the approximations of the original $x$ and $y$ so $d(x,y)=d(x'',y'') + \\delta$ where $\\delta$ is the distortion added to the distance calculation due of the reconstruction of the original vectors. The compression/decompression mechanisms should be such that the distortion is minimized.*\n",
      "\n",
      "![comp2](./img/image2.jpg)\n",
      "**Fig. 2**: *Suppose we have vectors $x$ and $y$ represented in their original space.»\n",
      "[2] «In Weaviate, floating-point vectors are represented as `float32` arrays, yielding a compression rate of 1:32, which is already commendable. However, there's a significant secondary benefit: bitwise operations now suffice for distance calculations between compressed binary vectors. This calculation involves a straightforward exclusive OR (XOR) operation between two binary arrays, tallying the resulting ones. Moreover, Go offers SIMD-optimized operations for these binary functions, resulting in notably faster computation than with original vectors. But how much faster exactly?»\n",
      "[3] «## Vector Distance Calculations\n",
      "\n",
      "Vector databases use vector representations generated from machine learning models to capture the meaning of data. The fact that these vectors are numerical representations of data objects allows us to apply mathematical operations to them, such as calculating the distance between two vector representations to determine their similarity. To calculate the distance between two vectors, you can use several similarity measures, detailed below:\n",
      "\n",
      "\n",
      "![Vector Distance Calculations](./img/vector_distance.png)\n",
      "\n",
      "\n",
      "As you can see, there are many different similarity measures - cosine, dot product or euclidean (also know as L2) are the most commonly used in practice. To learn more about the different distance metrics, you can continue reading our blog post on [What are Distance Metrics in Vector Search?](/blog/distance-metrics-in-vector-search/)\n",
      "\n",
      "When a vector database needs to retrieve semantically similar objects it has to calculate one of these distance metrics to assess how close a query point is to candidate points in the database. In fact, this vector distance calculation needs to be conducted when you build the index as well as anytime you perform any CRUD operation.»\n",
      "[4] «The next natural question is: How do we compress vectors? ## How to Efficiently Compress Vectors\n",
      "\n",
      "The main idea behind vector compression is to have a “good-enough” representation of the vectors (as opposed to a perfect representation) so they take up less space in memory while still allowing us to calculate the distance between them in a performant and accurate way. Compression could come from different sources. We could, for example, aim to reduce redundant data to store information more efficiently. We could also sacrifice accuracy in the data in favor of space.»\n",
      "[5] «What simple operation does it spend the majority of its time performing? If you guessed **vector distance calculations** … BINGO! 🎉\n",
      "\n",
      "While vector databases use many techniques and algorithms to improve performance (including locality graphs, quantization, hash based approaches), at the end of the day, efficient distance calculations between high-dimensional vectors is a requirement for a good vector database. In fact, when profiling Weaviate indexed using HNSW, we find that 40%-60% of the CPU time is spent doing vector distance calculations. So when someone tells us that they can make this quintessential process *much faster* they have our full attention! If you want to learn how to leverage algorithmic and hardware optimizations to make vector search 40% faster keep reading!\n",
      "\n",
      "In this post we’ll do a technical deep dive into different implementations for vector distance calculations and optimizations enabled by Intel’s new 5th Gen Xeon Processor - Emerald Rapids, parallelization techniques using SIMD with the Intel AVX-256 and Intel AVX-512 instruction sets, loop unrolling and compiler optimizations by transpiling C to Go assembly. We explain how we attained a **~40% QPS speed up at 90% Recall** in Weaviate running on Intel’s new Xeon Processor, Emerald Rapids.»\n",
      "\n",
      "Question: What is the impact of vector compression and decompression on the accuracy of distance calculations between the original vectors?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer to this question, we need to understand the impact of compression and decompression on the original vectors and how it affects the accuracy of distance calculations.\n",
      "\n",
      "Answer: Vector compression and decompression can introduce distortion to the original vectors, and this distortion can affect the accuracy of distance calculations. The compression function $C$ applied to the original vectors $x$ and $y$ results in a shorter representation, but when these compressed vectors are decompressed back into the original space using $C'$, there is a slight difference in the reconstructed vectors $x''$ and $y''$. This distortion, denoted as $\\delta$, adds to the original distance calculation, so the new distance between $x$ and $y$ is given by $d(x, y) = d(x'', y'') + \\delta$. The impact of this distortion on the accuracy of distance calculations depends on the specific compression and decompression functions used and the similarity measure employed to calculate vector distances. In the context of Weaviate, the compression rate of 1:32 for floating-point vectors to `float32` arrays is mentioned, and it is stated that this compression allows for faster computation through bitwise operations. However, the specific impact on accuracy is not explicitly mentioned and would depend on the chosen similarity measure and the characteristics of the data being compressed. In general, vector compression techniques aim to find a balance between reducing storage requirements and maintaining the accuracy of vector operations. The choice of compression algorithm and the acceptable level of distortion depend on the specific use case and the sensitivity of the application to errors in vector calculations.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «It is one of my favorite topics about Go memory management, and I'd be happy to write this post. Please let me know. For this one, it's enough to understand that sometimes heap-allocations happen even when we think they shouldn't. That is important to know because those allocations will put pressure on the GC, which is required for an unexpected OOM situation. ## Why would you run OOM – even when there is enough memory available?»\n",
      "[2] «In this article, I will invite you on a journey. We will cover:\n",
      "* How memory allocations work in Go.\n",
      "* When the Garbage Collector runs, and what the implications are. * Why it was far too easy to run out of memory before Go 1.19. * What GOMEMLIMT is and why it can prevent premature OOM kills. * Run an experiment first without and then with GOMEMLIMIT\n",
      "\n",
      "## What is a garbage-collected language?»\n",
      "[3] «However, as the next few paragraphs show, there are some caveats (and a terrific new solution called GOMEMLIMIT). If you aren't careful, you can run OOM even when you shouldn't. But before we dive into this, we need to talk about stack and heap allocations and why something ends up on the heap. ## The stack vs. the heap\n",
      "In short, there are two ways to allocate memory: On the stack or the heap.»\n",
      "[4] «We found that the data structures relied on dynamic allocations. So, even if we knew that an array would never be longer than 64 elements, the Go runtime could still decide to allocate an array[100] in the background when the array reaches 51 elements. To fix that, we switched to static allocations, and Weaviate instructs the Go runtime to allocate the exact number of elements. This reduced **static** memory usage even when idle. ### Results\n",
      "\n",
      "🎉 Between these two major updates, plus some smaller ones, we saw a **significant reduction in memory usage of 10-30%**🚀.»\n",
      "[5] «* After introducing `GOMEMLIMIT=2750MiB`, our application no longer crashed and could efficiently use the available memory. The import succeeded. Does that mean that GOMEMLIMIT is your *get-out-of-jail-free* card for heap allocations? No. A Go application that gets heavy usage still has to ensure allocation efficiency. Simply setting a GOMEMLIMIT will not make an inefficient application efficient.»\n",
      "\n",
      "Question: Why can heap allocations in Go lead to out-of-memory errors even when there appears to be enough memory available?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to understand the memory management system in Go and the role of the Garbage Collector (GC). Go has automatic memory management, which means the Go runtime handles memory allocation and deallocation. Memory allocations can happen on the stack or the heap. The heap is a dynamic memory allocation area, and the stack is a static memory allocation area with a fixed size. When objects are allocated on the heap, the Go runtime keeps track of these allocations and their sizes. The Garbage Collector is then responsible for deallocating unused memory on the heap. However, the GC runs periodically, and if there are a large number of allocations happening in a short period, it can lead to a situation where the available memory is fragmented and not contiguous, even if the total memory available seems sufficient. Additionally, as mentioned in the context, the Go runtime may sometimes allocate more memory than what is apparently necessary. For example, in context [4], it is mentioned that an array of 51 elements was allocated as an array of 100 elements. This can further contribute to the issue of fragmented memory and unexpected out-of-memory (OOM) errors. Reasoning: Let's understand this step by step to provide a comprehensive answer.\n",
      "\n",
      "Answer: Heap allocations in Go can lead to out-of-memory errors, even when there seems to be enough memory available due to the dynamic nature of memory allocation on the heap and the periodic garbage collection process. The Go runtime may allocate more memory than needed, and if a large number of allocations occur in a short period, it can fragment the available memory. As a result, the GC may not be able to deallocate memory efficiently, leading to unexpected OOM errors. Additionally, the runtime's behavior of over-allocating memory can further contribute to this issue.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «This pillar is all about performance. The first big step will be the move towards a [Native Roaring Bitmap Index](https://github.com/weaviate/weaviate/issues/2511). In the most extreme case, this new index time can speed up filtered vector search [by a factor of 1000](https://twitter.com/etiennedi/status/1621180981519458305). But it doesn’t stop there; we are already thinking about the next steps. Whether you want faster aggregations or new types of specialized indexes, we will ensure you can hit all your p99 latency targets with Weaviate.»\n",
      "[2] «When we generate these symbolic structures like “room_type”, we can then use it in filtered vector search! Filtered Vector Search has recently been made [1000(!) times faster in Weaviate thanks to roaring bitmap indexing](/blog/weaviate-1-18-release#faster-filtering-with-roaring-bitmaps)!\n",
      "\n",
      "Another powerful idea is the emergence of `Knowledge Graphs 2.0`. We can use LLMs to suggest relations between pieces of content and then connect these nodes together with the relational edges. Check out this example of GraphGPT!\n",
      "\n",
      "## From ChatGPT to AutoGPT and the role of Weaviate\n",
      "Another title for this section would be “Complex Tasks that run overnight”. The latest imagination inspiring feat of AI has been the rise of AutoGPT! AutoGPT is a loop of prompts in which the LLM breaks a complex task into subtasks, executes them, reflects on the plan, and so on until the complex task has been completed. Generative Feedback Loops are one of the key pieces to this puzzle.»\n",
      "[3] «With the Hybrid-SaaS option, Weaviate will set up and manage your database cluster on a dedicated private cloud system on the cloud platform of your choice or on premises in your data center. ## The Full Power of the Weaviate Vector Database\n",
      "The Weaviate Cloud is always updated with the latest [Weaviate open source database](https://github.com/weaviate/weaviate) version and functionality. If you’ve been using Weaviate Cloud since its release, you’ve seen it enhanced with new core Weaviate database functionality, such as:\n",
      "\n",
      "- [ChatGPT plug-in](https://weaviate.io/blog/weaviate-retrieval-plugin)\n",
      "- [Bitmap indexing](https://weaviate.io/blog/weaviate-1-18-release#faster-filtering-with-roaring-bitmaps) for faster filtered search\n",
      "- [HNSW-PQ index and data compression](https://weaviate.io/blog/weaviate-1-18-release#hnsw-pq) for greater cost efficiency\n",
      "- [Database replication](https://weaviate.io/blog/weaviate-1-18-release#replication)\n",
      "- [WAND and BM25 search](https://weaviate.io/blog/weaviate-1-18-release#improvements-to-bm25-and-hybrid-search)\n",
      "- [Cursor API](https://weaviate.io/blog/weaviate-1-18-release#cursor-api) for processing query results\n",
      "\n",
      "## Develop More Productively - Interactive Admin & Query Console\n",
      "We care a lot about the Weaviate user experience, so developer productivity is a top design priority. Hence, we implemented many new features in the Weaviate Cloud Console, which now allows you to administer your Weaviate clusters - create, delete, restore, monitor your clusters and activate high-availability settings. ![Weaviate Cloud Service Console ](./img/wcs-connected-instances.png)\n",
      "\n",
      "The console also features a GraphQL IDE that allows you to interactively query your Weaviate databases.»\n",
      "[4] «Add any ordered input, for example, 1, 2, 3, 4 and see how the tree stays balanced. ### Results\n",
      "We've run a few local tests to paint a better picture of what you could expect. First, we saw that the RB-Tree is a factor of 20 faster than the binary tree when adding objects with sequential keys (just the tree, without anything else). With a full import test, we saw a **3x performance improvement** 🚀. * Weaviate `1.14.1` - import time **~38 minutes**\n",
      "* Weaviate `1.15.0` - import time **~13 minutes** 🔥\n",
      "\n",
      "## More efficient filtered aggregations\n",
      "\n",
      "![More efficient filtered aggregations](./img/filtered-aggregation.png)\n",
      "\n",
      "Recently we've been working with a customer who was running multiple filtered aggregations on a large dataset.»\n",
      "[5] «---\n",
      "title: Weaviate 1.15 release\n",
      "slug: weaviate-1-15-release\n",
      "authors: [connor, erika, laura, sebastian]\n",
      "date: 2022-09-07\n",
      "tags: ['release']\n",
      "image: ./img/hero.png\n",
      "description: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\n",
      "---\n",
      "![Weaviate 1.15 release](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "We are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\n",
      "\n",
      "If you like your content brief and to the point, here is the TL;DR of this release:\n",
      "1. [☁️Cloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\n",
      "1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1.»\n",
      "\n",
      "Question: What is the expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: In the provided context, it is mentioned that the new Native Roaring Bitmap Index can speed up filtered vector search by a factor of 1000. This is specifically stated in the first paragraph of the context.\n",
      "\n",
      "Answer: Therefore, the expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index is a factor of 1000.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «import GenerativeFeedbackLoopsLoop from '/_includes/code/generative.feedback.loops.loop.mdx';\n",
      "\n",
      "<GenerativeFeedbackLoopsLoop/>\n",
      "\n",
      "Let's see the resulting ads and the descriptions they were sourced from:\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>\n",
      "    OpenAI Generations\n",
      "  </summary>\n",
      "  <div>\n",
      "\n",
      "| Description | Ad |\n",
      "|-------------|----|\n",
      "| This cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn is the perfect place to stay for an extended period of time. Located at 40.68876 latitude and -73.94312 longitude, this room is clean and quiet, and has a minimum stay of 60 nights. With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here. There are no reviews yet, so be the first to experience this great room! | Are you looking for a place to stay for an extended period of time? Look no further than this cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean and quiet, and has a minimum stay of 60 nights.»\n",
      "[2] «We will add `target` as a property to the `Ad` class, like this:\n",
      "\n",
      "```python\n",
      "target_property = {\n",
      "   \"dataType\": [\n",
      "       \"text\"\n",
      "   ],\n",
      "   \"name\": \"target\",\n",
      "   \"description\": \"High-level audience target for this ad.\"\n",
      "}\n",
      "\n",
      "\n",
      "client.schema.property.create(\"Ad\", target_property)\n",
      "```\n",
      "\n",
      "<details>\n",
      "  <summary>\n",
      "    OpenAI Generations\n",
      "  </summary>\n",
      "  <div>\n",
      "\n",
      "\n",
      "| Ad Target | Ad |\n",
      "|-----------|----|\n",
      "| For Young Couples | Are you looking for a cozy place to stay for an extended period of time? Look no further than this private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean, quiet, and perfect for young couples. With a minimum stay of 60 nights and availability of 365 days a year, you can be sure to find a spot here. Plus, you can be the first to experience this great room - there are no reviews yet! Don't miss out on this great opportunity - book your stay today! |\n",
      "| For Elderly Couples | Are you an elderly couple looking for a place to stay for an extended period of time? Look no further than this cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean and quiet, and has a minimum stay of 60 nights.»\n",
      "[3] «With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here. There are no reviews yet, so be the first to experience this great room! Enjoy the peace and quiet of this room and make it your home away from home. |\n",
      "| For Single Travelers | Are you a single traveler looking for a place to stay for an extended period of time? Look no further than this cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean, quiet, and has a minimum stay of 60 nights. With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here.»\n",
      "[4] «| Welcome to this beautiful two bedroom apartment in the Upper East Side of Manhattan! Located in the East Harlem neighbourhood, this apartment is just a short walk away from Central Park and surrounded by a variety of restaurants, shops, and other attractions. Enjoy all the necessary amenities in this apartment, available for rent for a minimum of seven nights at a price of $190 per night. With 249 days available for rent out of the year, this is the perfect place to stay for your next vacation or business trip! Book now and experience the best of Manhattan! |\n",
      "| This cozy private room is located in the Flatbush neighbourhood of Brooklyn. Enjoy the convenience of being in the city while still having access to the peacefulness of the countryside. The room is available for a minimum of one night and is available for booking all year round.»\n",
      "[5] «By emphasizing the use of real-time information stored in Weaviate, we continuously update the language model with the latest data, bypassing the limitation of its encoded knowledge. Ultimately combining the peanut butter of factual information from the database with the jelly of LLM language understanding. ### Generate a description for each Listing\n",
      "\n",
      "import GenerativeFeedbackLoopsDescription from '/_includes/code/generative.feedback.loops.description.mdx';\n",
      "\n",
      "<GenerativeFeedbackLoopsDescription/>\n",
      "\n",
      "Now let's test a semantic search through the Listings, each vectorized with the respective LLM-generated `description`:\n",
      "\n",
      "import GenerativeFeedbackLoopsSearch from '/_includes/code/generative.feedback.loops.search.mdx';\n",
      "\n",
      "<GenerativeFeedbackLoopsSearch/>\n",
      "\n",
      "<details>\n",
      "  <summary>\n",
      "    OpenAI Generations\n",
      "  </summary>\n",
      "  <div>\n",
      "\n",
      "| Rank | Result |\n",
      "|------|--------|\n",
      "| 1    | This private room in the Bedford-Stuyvesant neighbourhood of Brooklyn is the perfect spot for a long-term stay. The space is clean and quiet, providing a peaceful retreat from the hustle and bustle of the city. With a minimum stay of 60 nights, you'll have plenty of time to explore all that Brooklyn has to offer.»\n",
      "\n",
      "Question: What is the minimum stay requirement for the private room available in the Bedford-Stuyvesant neighbourhood of Brooklyn?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To determine the minimum stay requirement for the private room in the Bedford-Stuyvesant neighbourhood of Brooklyn, we need to examine the provided information carefully. We are given details about a private room in this neighbourhood, including its location, amenities, and availability. By focusing on the relevant sentences, we can extract the necessary information to answer the question.\n",
      "\n",
      "Answer: The minimum stay requirement for the private room in the Bedford-Stuyvesant neighbourhood of Brooklyn is 60 nights. This information is mentioned twice in the context, emphasizing the duration of the minimum stay for potential guests.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «This case is quite similar to our discussion of Multi-Index Routing and we can similarly evaluate generations with a prompt that explains the needs for SQL and Vector Databases and then asks the LLM whether the router made the right decision. We can also use the RAGAS Context Relevance score for the results of the SQL query. <img\n",
      "  src={require('./img/sql-router.png').default}\n",
      "  alt=\"SQL Router Query Engine\"\n",
      "  style={{ maxWidth: \"60%\" }}\n",
      "/>\n",
      "\n",
      "Concluding our discussion of “From RAG to Agent Evaluation”, we believe that it is still too early to tell what the common patterns will be for agent use. We have intentionally shown the multi-hop query engine and query router because these are relatively straightforward to understand. Once we add more open-ended planning loops, tool use and the associated evaluation of how well the model can format API requests to the tool, and more meta internal memory management prompts such as the ideas in MemGPT, it is very difficult to provide a general abstraction around how Agents will be evaluated.»\n",
      "[2] «A system decided to break it into the sub questions {sub_question_1} and {sub_question_2}. Does this decomposition of the question make sense?”. We then have two separate RAG evaluations for each of the sub questions, and then an evaluation of whether the LLM was able to combine the answers from each question to answer the original question. As another example of evolving complexity from RAG to Agents, let’s consider Routing Query Engines. The following visual illustrates an agent routing a query to either an SQL or Vector Database query.»\n",
      "[3] «We covered core concepts around vector databases, such as vector embeddings, and discussed that vector databases enable efficient vector search by leveraging ANN algorithms. Additionally, we explored the tool landscape around vector databases and discussed the advantages of vector databases over traditional and vector-capable databases and vector libraries. import WhatNext from  '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />»\n",
      "[4] «We don’t just want the items with the most semantic similarity to the query, but also those that are less than $100. LlamaIndex’s [Query Engine](https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html#building-multi-document-agents) presents really nice abstractions to leverage this concept of LLM Query Understanding. In the following illustration, the LLM adds the filter, where “animal” = “dog” to facilitate searching for information about Goldendoodles. This is done by prompting the LLM with information about the data schema and the syntax for formatting structured vector searches in Weaviate. <img\n",
      "    src={require('./img/self-query.gif').default}\n",
      "    alt=\"alt\"\n",
      "    style={{ width: \"100%\" }}\n",
      "/>\n",
      "\n",
      "## Index Construction\n",
      "Large Language Models can also completely change the way we index data for search engines, resulting in better search quality down the line.»\n",
      "[5] «### Vector Database vs. Vector-Capable Database (SQL and NoSQL)\n",
      "\n",
      "Today, many existing databases have already enabled vector support and vector search. However, they usually don’t index the vector embeddings, which makes the vector search slow. Thus, an advantage of AI-native vector databases over vector-capable databases is their efficiency in vector search due to vector indexing. ### Vector Database vs.»\n",
      "\n",
      "Question: What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases as discussed in the document?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To evaluate agents in the context of Multi-Index Routing and SQL and Vector Databases, we need to consider the following: - The complexity of the evaluation increases when more open-ended planning loops, tool use, and meta internal memory management prompts are introduced. Therefore, it is challenging to provide a general abstraction for agent evaluation. - The document mentions evaluating generations with a prompt that explains the needs for SQL and Vector Databases and then asking the LLM whether the router made the correct decision. - The RAGAS Context Relevance score can be used for the results of the SQL query to assess the agent's performance. - Another aspect to consider is the decomposition of questions and the ability of the LLM to combine answers from sub-questions to answer the original question. - The visualization provided in the document illustrates an agent routing a query to either an SQL or a Vector Database query, showing the potential for LLMs to understand and facilitate structured searches efficiently.\n",
      "\n",
      "Answer: The document discusses that it is too early to determine the standard patterns for agent evaluation as the field is constantly evolving. However, some key considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases include: - Using prompts to explain the needs for SQL and Vector Databases and then assessing the LLM's decision-making. - Employing the RAGAS Context Relevance score for evaluating the results of the SQL query. - Considering the decomposition of questions into sub-questions and the LLM's ability to combine answers. - Visualizing agent query routing to either SQL or Vector Databases and understanding the efficiency of vector searches. - Evaluating the agent's performance in open-ended planning loops, tool use, and meta internal memory management prompts.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Note, how it doesn’t even mention the Cohere model, as all of that is done through configuration. ```python\n",
      "def semantic_serch(query):\n",
      "   client = _setup.connect_to_weaviate()\n",
      "\n",
      "   nearText = {\n",
      "       \"concepts\": [query],\n",
      "       \"distance\": 0.7,\n",
      "   }\n",
      "\n",
      "   properties = [\n",
      "       \"name\", \"content\", \"enName\", \"link\", \"lang\",\n",
      "       \"_additional {certainty distance}\"\n",
      "   ]\n",
      "\n",
      "   result = (\n",
      "       client.query\n",
      "       .get(\"Document\", properties)\n",
      "       .with_near_text(nearText)\n",
      "       .with_limit(5)\n",
      "       .do()\n",
      "   )\n",
      "\n",
      "   return result['data']['Get']['Document']\n",
      "```\n",
      "## How does this work\n",
      "It is also important to understand how Weaviate and Cohere communicate with each other. Weaviate fully automates communication with Cohere’s AI Platform. There are two key scenarios when Weaviate will communicate with Cohere:\n",
      "\n",
      "* Data import and modifications – to vectorize data objects\n",
      "* Query time – to vectorize the query\n",
      "\n",
      "### Data import + modifications\n",
      "When you import (or update) data into Weaviate, Weaviate will:\n",
      "1. Receive the data object\n",
      "1.»\n",
      "[2] «But until now, getting outputs from Cohere into Weaviate required a few extra steps. This included sending requests to Cohere to vectorize text objects, retrieving the output vectors, and then importing it  into Weaviate along with the matching objects. ### Text2vec-cohere\n",
      "Over the last few weeks, we’ve partnered with Cohere to create a new Weaviate module – [text2vec-cohere](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) – it allows you to use Cohere’s LLMs to run complex NLP queries straight from Weaviate. You can learn all about [text2vec-cohere from the docs](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere). And there is a **lot** that you can do with Cohere, including some very new, exciting developments.»\n",
      "[3] «Send the relevant properties for vectorization to Cohere\n",
      "1. Store the data object and index the vector in Weaviate\n",
      "\n",
      "![Weaviate Import with Cohere](./img/cohere-import.png)\n",
      "\n",
      "This allows Weaviate to efficiently store and query your data. ### Query\n",
      "When you query Weaviate, Weaviate will:\n",
      "1. Receive the query\n",
      "1. Send the query to Cohere to vectorize it\n",
      "1.»\n",
      "[4] «The `Blogs` collection, will have two properties: `content` and `synthetic_query`. ```python\n",
      "schema = {\n",
      "   \"classes\": [\n",
      "       {\n",
      "           \"class\": \"Blogs\",\n",
      "           \"description\": \"Weaviate blogs\",\n",
      "           \"vectorizer\": \"text2vec-cohere\",\n",
      "           \"properties\": [\n",
      "               {\n",
      "                   \"name\": \"content\",\n",
      "                   \"dataType\": [\"text\"],\n",
      "                   \"description\": \"Content from the blogs.\",\n",
      "               },\n",
      "               {\n",
      "                   \"name\": \"synthetic_query\",\n",
      "                   \"dataType\": [\"text\"],\n",
      "                   \"description\": \"Synthetic query generated from a LM.\"\n",
      "               }\n",
      "           ]\n",
      "       }\n",
      "   ]\n",
      "}\n",
      "\n",
      "client.schema.create(schema)\n",
      "```\n",
      "\n",
      "If you need to reset your schema and delete objects in a collection, run:\n",
      "`client.schema.delete_all()` or `client.schema.delete_class(\"Blogs\")`\n",
      "\n",
      "#### Synthetic Queries\n",
      "\n",
      "We are using DSPy’s signature and chain-of-thought module to generate synthetic queries using Cohere’s `command-nightly` model. The signature implements the prompt that: 1. Describes the task, 2. Gives an example of the `input` and `output` fields and 3.»\n",
      "[5] «Capturing these variations is important for this specific use case and it’s also universally applicable to other use cases involving user reviews. But how did we accomplish this with Weaviate? The process is surprisingly straightforward. We created `Product` objects embedded with their reviews, summaries, and other metadata, to enable semantic search. To optimize performance, we vectorize specific fields using the `gpt-3.5-turbo` model and leave out fields that do not require vectorization.»\n",
      "\n",
      "Question: How does Weaviate process and vectorize queries using Cohere?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's break down the process step by step to understand how Weaviate utilizes Cohere for processing and vectorizing queries.\n",
      "\n",
      "Answer: Weaviate, a powerful vector search engine, integrates with Cohere's AI Platform to enhance its capabilities. Here's how Weaviate processes and vectorizes queries using Cohere: 1. **Query Receipt**: When a query is received by Weaviate, it recognizes the need to vectorize the query text. 2. **Communication with Cohere**: Weaviate sends the query to Cohere's AI Platform for vectorization. This communication is automated, thanks to the text2vec-cohere module, a collaboration between Weaviate and Cohere. 3. **Vectorization by Cohere**: Cohere's large language models, such as the `command-nightly` model, perform complex natural language processing on the query. This includes understanding the query intent, generating chain-of-thought responses, and producing output vectors that represent the query's semantic meaning. 4. **Query Execution**: Once the query is vectorized by Cohere, Weaviate uses this vector to search for relevant data objects in its database. It compares the query vector with the vectors of stored data objects to find the most similar matches. 5. **Result Retrieval**: Weaviate retrieves the data objects with vectors closest to the query vector and returns them as the query results. This ensures that the results are contextually relevant to the user's query. By leveraging Cohere's advanced language models, Weaviate can process and understand queries more effectively, enabling it to provide accurate and semantically meaningful search results. This integration enhances Weaviate's capabilities in handling complex NLP queries and improves the overall user experience.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Typically this is in the context of recommendation in which we have metadata about users, as well as the documents or items. So for example, say we have features that describe a Users looking for Movies such as:\n",
      "\n",
      "User Features - (Age, Gender, Location, Occupation, Preferences)\n",
      "Movie Features - (Release Year, Genre, Box Office, Duration). So together, the Metadata ranker takes as input something like: [Age, Gender, Location, Occupation, Preferences, Release year, Genre, Box Office, Duration] and predicts a score of how much this User will like the movie. We can fix the User features and rotate in each Document to get a score for each of the candidate movies (retrieved with something like ref2vec) to rank with. In addition to vectors, Weaviate also enables storing metadata features about objects such as `price`, or `color`.»\n",
      "[2] «They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.»\n",
      "[3] «However, most of the LLM APIs don’t actually give us these probabilities. Further, this is probably pretty slow. We will keep an eye on it, but it doesn’t seem like the next step to take for now. ## Metadata Rankers\n",
      "Whereas I would describe Cross-Encoders as `content-based` re-ranking, I would say Metadata rankers are `context-based` re-rankers. Metadata rankers describe using symbolic features to rank relevance.»\n",
      "[4] «We can send these features to wherever our Metadata Ranker is hosted and get those scores back to Weaviate to sort our search results. This is also closely related to another category of ranking methods that use models like XGBoost to combine features, as well as say the bm25 score, vector distance, and maybe even the cross encoder score as well. This is a pretty interesting technique when you additionally factor in multiple properties. For example, we could have a bm25, vector, and cross encoders for the `title`, as well as `content` properties and use a learned model to combine these into a final ranking score. I recently came across a paper titled “Injecting the BM25 Score as Text Improves BERT-Based Re-rankers” published in ECIR 2023.»\n",
      "[5] «As described in our [previous article](https://weaviate.io/blog/ranking-models-for-better-search), re-ranking models are new to the scene of zero-shot generalization. The story of re-rankers has mostly been tabular user features combined with tabular product or item features, fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt. Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score. This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.»\n",
      "\n",
      "Question: How does the Metadata ranker predict which movies a user will like based on their personal features and movie characteristics?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to understand how the Metadata Ranker utilizes user and movie data to make predictions. We can break down the process into the following steps: Step 1: Understand User Features: The Metadata Ranker takes into account various user features such as age, gender, location, occupation, and preferences. These features provide a personalized profile of the user, allowing the system to tailor movie recommendations accordingly. Step 2: Analyze Movie Metadata: For each movie, the ranker considers metadata such as release year, genre, box office performance, and duration. This information helps match movies to the user's tastes and preferences. Step 3: Combine User and Movie Data: By integrating user features with movie metadata, the Metadata Ranker can make context-aware predictions. It predicts a score or ranking for each movie, indicating how much the user is likely to enjoy it. Step 4: Rank and Recommend: Finally, the movies are ranked based on the predicted scores, and the system can recommend movies with the highest predicted relevance or appeal to the user.\n",
      "\n",
      "Answer: The Metadata Ranker utilizes user features (e.g., age, gender, preferences) and movie characteristics (e.g., genre, release year) to predict a score indicating how much a user will like a particular movie. By combining these features, the ranker can provide personalized movie recommendations tailored to the user's tastes and interests.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Please boost relevance based on recency and if the Author is “Connor Shorten”. ```\n",
      "\n",
      "Each search result then comes packaged with their associated metadata in a key-value array. This offers the additional benefit of allowing business practitioners to easily swap out the ranking logic. This also holds the benefit of dramatically increasing the interpretability of recommendation systems, since LLMs can easily be prompted to provide an explanation of the ranking in addition to the ranking itself. ## Search Result Compression\n",
      "\n",
      "Traditionally, search results are presented to human users as a long list of relevant websites or passages.»\n",
      "[2] «---\n",
      "title: Distance Metrics in Vector Search\n",
      "slug: distance-metrics-in-vector-search\n",
      "authors: [erika]\n",
      "date: 2023-08-15\n",
      "image: ./img/hero.png\n",
      "tags: ['concepts']\n",
      "description: \"Learn about why you need distance metrics in vector search and the metrics implemented in Weaviate (Cosine, Dot Product, L2-Squared, Manhattan, and Hamming).\"\n",
      "\n",
      "---\n",
      "\n",
      "![Distance Metrics in Vector Search](img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "[Vector databases](/blog/what-is-a-vector-database) - like [Weaviate](/developers/weaviate) - use **machine learning models** to analyze data and **calculate vector embeddings**. The vector embeddings are **stored together with the data** in a database, and later are used to query the data. In a nutshell, a vector embedding is an array of numbers, that is used to describe an object. For example, strawberries could have a vector `[3, 0, 1]` – more likely the array would be a lot longer than that. *Note*, the meaning of each value in the array, depends on what machine learning model we use to generate them.»\n",
      "[3] «It specifies various options, such as the batch size, Weaviate host, and API key for authentication. One of the key features of this integration is its seamless interaction with Confluent's Schema Registry. The integration automatically extracts the schema ID from the message's value and queries the Schema Registry to retrieve the associated schema. This schema is then used to deserialize the rest of the data, ensuring that the data is accurately streamed into Weaviate. ### Write The Streaming DataFrame\n",
      "With your Spark environment initialized and your data stream configured, you're now ready to start the actual data streaming process from Confluent Cloud to Weaviate.»\n",
      "[4] «How, you ask? Having redundancy is key to achieving this. And such redundancy, or replication, has been available for a while in Weaviate for production use. In fact, configuring replication is actually easy and simple, and using it can lead to huge benefits. In this post, we will explain the benefits of replication and show you how to configure it.»\n",
      "[5] «They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.»\n",
      "\n",
      "Question: What are the benefits of packaging search results with associated metadata in a key-value array?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, let's break down the relevant information from the context and analyze the benefits mentioned.\n",
      "\n",
      "Answer: There are two main benefits mentioned in the context. Firstly, it allows business practitioners to easily swap out the ranking logic, providing flexibility and control over the ranking algorithm used. Secondly, it dramatically increases the interpretability of recommendation systems. By having the metadata packaged with the search results, LLMs can provide explanations for the ranking, making the system more transparent and understandable to users. This added interpretability can increase user trust and satisfaction.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «<div align=\"center\">\n",
      "\n",
      "![\"Bob talks about feedback loops\"](img/bob-gen-ai.jpg)\n",
      "##### Our CEO Bob talking about the next level of vector database capabilities - Generative Feedback Loops\n",
      "\n",
      "</div>\n",
      "\n",
      "We also got tons of great feedback on our open-source RAG app, Verba, which we used to demo Weaviate. It’s something you can use to quickly build RAG apps that fit your use case. Check out [our intro video](https://youtu.be/IiNDCPwmqF8?si=cZxfsnY6vW75c4BD) or dive right in to [our GitHub repo](https://youtu.be/IiNDCPwmqF8?si=cZxfsnY6vW75c4BD)!\n",
      "\n",
      "<div align=\"center\">\n",
      "\n",
      "![\"Adam demos Verba\"](img/adam-verba.jpg)\n",
      "##### Adam from our Developer Growth team demoing Verba, powered by Weaviate!\n",
      "\n",
      "</div>\n",
      "\n",
      "It was great hearing about the different use cases and industries in which folks are building AI apps – these included chatbots and search applications for things like customer support, document search, image search, and even video search. A common pattern we saw was that organizations of all sizes are evaluating vector database platforms and want guidance on how to find the right database for them. One of our recommendations is to look for a vector database purpose-built for AI.»\n",
      "[2] «---\n",
      "title: 'Enterprise Use Cases of Weaviate Vector Database'\n",
      "slug: enterprise-use-cases-weaviate\n",
      "authors: [adam]\n",
      "date: 2024-03-12\n",
      "image: ./img/hero.png\n",
      "tags: ['concepts', 'application']\n",
      "description: 'Explore enterprise use cases heavily used by our customers adopting generative AI features, search capabilities, and RAG with Weaviate vector database.'\n",
      "---\n",
      "# Enterprise Use Cases for Weaviate\n",
      "\n",
      "![Hero image](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "import verba_overview_video from './img/verba_overview_video.mp4';\n",
      "import ragsend from './img/ragsend.mp4';\n",
      "import generative_feedback_loops from './img/generative_feedback_loops.mp4';\n",
      "\n",
      "The world was taken by storm by OpenAI ChatGPT in November of 2022. The interesting thing about this is that the technology that powers ChatGPT and related language models has actually been around for some time before then and was originally discussed in [this scientific paper](https://arxiv.org/abs/1706.03762v1), released by Google, in 2017. It was also around this time that we, at Weaviate, began exploring the representations of data in different formats that carried semantic meaning to these transformer models, as described by Google’s paper. If you check out the [Weaviate GitHub repository](https://github.com/weaviate/), you’ll see that we began leveraging semantic relationships of vector embeddings as early as the publication of “Attention is All You Need” from Google. In 2019, SeMi Technologies was born, later renamed to Weaviate, and since then we’ve been building and iterating on a highly performant, open source, AI Native vector database that’s closely coupled with language models of your choosing.»\n",
      "[3] «---\n",
      "title: 'Hurricane: Writing Blog Posts with Generative Feedback Loops'\n",
      "slug: hurricane-generative-feedback-loops\n",
      "authors: [connor]\n",
      "date: 2024-03-19\n",
      "image: ./img/hero.png\n",
      "tags: ['how-to', 'concepts']\n",
      "description: 'Hurricane is a web application to demonstrate Generative Feedback Loops with blog posts.'\n",
      "---\n",
      "\n",
      "![HERO image](./img/hero.png)\n",
      "\n",
      "import demo from './img/Hurricane-Demo.mp4';\n",
      "import phoenix from './img/Phoenix-Demo.mp4';\n",
      "\n",
      "Back in May 2023, we published an initial vision for [Generative Feedback Loops](/blog/generative-feedback-loops-with-llms), which we have defined to describe saving the outputs of generative AI systems back into databases for future use. We have since extended our experiments from AirBnB listings to [Amazon products](https://medium.com/@iamleonie/recreating-amazons-new-generative-ai-feature-product-review-summaries-50640e40872a), even running every component of a Generative Feedback Loop within [Snowpark Container Services](/blog/generative-feedback-loop-with-weaviate-and-spcs#implementing-a-generative-feedback-loop-with-weaviate). Both of these applications illustrate the ability of generative AI to enhance our data. With the emergence of DSPy, we are now entering a new era of control with generative AI systems and are able to create even more compelling examples of Generative Feedback Loops. When we initially published our predictions for Generative Feedback Loops, we presented Ben Thompson’s [Aggregation Theory](https://stratechery.com/2015/aggregation-theory/) to describe some of the impacts of this technology.»\n",
      "[4] «A generative feedback loop first leverages RAG or generative search in order to yield a set of results based on some analytics and usually the prompt contains some relevant data for your specific user and their past behaviors. From there, you embed the the LLM output back into the vector database to apply search over it again, and this query could contain questions that the prior generation will have some insight over. If you’d like to learn more about this, we’ve authored [a blog post](https://weaviate.io/blog/generative-feedback-loops-with-llms) about how you could leverage generative feedback loops to create targeted ads. <figure>\n",
      "  <video width=\"100%\" autoplay loop muted controls>\n",
      "    <source src={generative_feedback_loops} type=\"video/mp4\" />\n",
      "    Your browser does not support the video tag. </video>\n",
      "  <figcaption>Apply a generative feedback loop to Weaviate</figcaption>\n",
      "</figure>\n",
      "\n",
      "\n",
      "### Ecommerce Recommendations and Multi Modal Search\n",
      "\n",
      "Within ecommerce, the applications of vector database can be used to create recommendation systems, image search systems, and general search systems.»\n",
      "[5] «## When Do Generative Feedback Loops Run? Having illustrated the general concept of Generative Feedback Loops, where we use data from the database to supplement the factual knowledge of generative models and then write the generated outputs back to the database for future use, let’s discuss when to kick this off. ## Data Ingestion\n",
      "Data Ingestion is a shorthand reference to importing data into the database. For example, every time a new User is added to the AirBnB database, we may want to kick off a Generative Feedback Loop of retrieving information about a listing and this new user and save the new ads targeted to this user. On the other hand, we may want to kick this off when a new Listing is added, writing ads for each of our users!\n",
      "\n",
      "As another example, imagine that we have a collection of podcast episodes.»\n",
      "\n",
      "Question: Who is the CEO that discussed the next level of vector database capabilities related to Generative Feedback Loops?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can identify the relevant information from the context, which mentions the CEO discussing the next level of vector database capabilities.\n",
      "\n",
      "Answer: Bob is the CEO who discussed the next level of vector database capabilities related to Generative Feedback Loops.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «It specifies various options, such as the batch size, Weaviate host, and API key for authentication. One of the key features of this integration is its seamless interaction with Confluent's Schema Registry. The integration automatically extracts the schema ID from the message's value and queries the Schema Registry to retrieve the associated schema. This schema is then used to deserialize the rest of the data, ensuring that the data is accurately streamed into Weaviate. ### Write The Streaming DataFrame\n",
      "With your Spark environment initialized and your data stream configured, you're now ready to start the actual data streaming process from Confluent Cloud to Weaviate.»\n",
      "[2] «**Automatic Deserialization**: Thanks to the integration with Confluent's Schema Registry, the data in the Kafka message has been automatically deserialized. Each key in the message's data becomes its own property in the corresponding Weaviate object. For example, **`city`**, **`first_name`**, and **`last_name`** are all individual properties in Weaviate, making the data immediately usable for your AI applications. ## Closing CTAs\n",
      "\n",
      "Not yet a Weaviate customer? Kickstart your AI journey with a [free sandbox environment](https://console.weaviate.cloud/) in Weaviate Cloud today - no credit card required.»\n",
      "[3] «It specifies the necessary JAR packages, including Avro and Kafka SQL libraries, to ensure compatibility and functionality with Confluent Cloud. Additionally, a custom JAR file, `confluent-connector_2.12-3.4.0_0.0.1.jar`, is included to establish the connection between Spark and Weaviate. By running this script, you initialize a Spark session with all the required configurations, laying the groundwork for streaming data from Confluent Cloud to Weaviate Cloud. ### Create A Schema In Weaviate\n",
      "\n",
      "Before you can start streaming data into Weaviate, you'll need to create a schema to define the structure of the Kafka messages you'll be storing. Here's how to do it:\n",
      "\n",
      "```python\n",
      "with open(\"../src/it/resources/schema.json\", \"r\") as f:\n",
      "    weaviate_schema = json.load(f)\n",
      "\n",
      "client.schema.create_class(weaviate_schema)\n",
      "```\n",
      "\n",
      "This code snippet reads [a JSON file](https://github.com/weaviate/confluent-connector/blob/main/src/it/resources/schema.json) containing the schema definition and uses the Weaviate client's **`create_class`** method to create the schema in Weaviate.»\n",
      "[4] «This sets the stage for the data you'll be streaming from Confluent Cloud. ### Create A Streaming DataFrame To Stream A Topic From Confluent Cloud\n",
      "\n",
      "After initializing your Spark environment, the next step is to configure the Kafka data stream that you'll be pulling from Confluent Cloud. Here's how to set it up:\n",
      "\n",
      ":::caution\n",
      "In this example, we've already set up a topic on Confluent Cloud that receives data from the DataGen source, specifically utilizing the clickstream user template. :::\n",
      "\n",
      "```python\n",
      "clickstreamDF = (\n",
      "    spark.readStream.format(\"kafka\")\n",
      "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
      "    .option(\"subscribe\", confluentTopicName)\n",
      "    .option(\"startingOffsets\", \"latest\")\n",
      "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
      "    .option(\n",
      "        \"kafka.sasl.jaas.config\",\n",
      "        \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(\n",
      "            confluentApiKey, confluentSecret\n",
      "        ),\n",
      "    )\n",
      "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
      "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
      "    .option(\"failOnDataLoss\", \"false\")\n",
      "    .option(\"name\", \"clickStreamReadFromConfluent\")\n",
      "    .load()\n",
      ")\n",
      "```\n",
      "\n",
      "This code snippet creates a streaming DataFrame named **`clickstreamDF`** in Spark, configured to read from your Kafka topic in Confluent Cloud. It specifies various options like the bootstrap servers, topic name, and security protocols, ensuring a secure and efficient data stream.»\n",
      "[5] «**Kafka Topic in Confluent Cloud:** The journey begins in Confluent Cloud, where you create a fully managed Kafka topic that holds the data you wish to stream into Weaviate. 2. **Spark Cluster with confluent-connector Library:** Next, you'll spin up a Spark cluster loaded with our specialized Confluent-Connector library. This cluster acts as the data processing engine that facilitates the data flow between Confluent Cloud and Weaviate. 3.»\n",
      "\n",
      "Question: How does Weaviate handle Kafka message data with Confluent's Schema Registry integration?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to understand how Weaviate interacts with Kafka message data and utilizes Confluent's Schema Registry integration. By breaking down the context, we can identify the steps involved in the process.\n",
      "\n",
      "Answer: Weaviate, in conjunction with Confluent's Schema Registry integration, handles Kafka message data by automatically extracting the schema ID from the message's value. This schema ID is used to query the Schema Registry and retrieve the associated schema. The integration then deserializes the data in the Kafka message, mapping each key to its own property in the corresponding Weaviate object. This automatic deserialization ensures that data from Kafka is seamlessly transformed into a format that Weaviate can utilize for AI applications.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «A dataset labeled for Question Answering is very different from Image Classification, or even Duplicate Question Detection. ### Task Benchmarks\n",
      "Two excellent benchmarks that collect Supervised Learning tasks to evaluate Sentence Transformers are Knowledge Intensive Language Tasks ([KILT](https://ai.facebook.com/tools/kilt/)) and Benchmarking Information Retrieval ([BEIR](https://arxiv.org/abs/2104.08663)). **KILT** uses the same domain for all tasks, Wikipedia. Each task is labeled with a different task: Slot Filling, Question Answering, Dialogue, Fact Checking, or Entity Linking. ![KILT Benchmark](./img/KILT-min-correct.png)\n",
      "\n",
      "**BEIR** tests many different tasks: Fact Checking, Citation Prediction, Duplicate Question Retrieval, Argument Retrieval and more.»\n",
      "[2] «The model must learn an intermediate function of entailment. These intermediate functions must be learned for Natural Language Inference, Fact Verification, Question Answering, and so on more so than tasks like aligning English to French translations or detecting Question paraphrases. ### Question Answering vs Fact Verification\n",
      "These two high-level categories are a good starting place for taxonomizing these datasets. However, there are still noticeable differences within each category. For example, what makes **Question Answering** different from **Fact Verification** for evaluating the quality of a Sentence Transformer?»\n",
      "[3] «---\n",
      "title: Weaviate 1.2 release - transformer models\n",
      "slug: weaviate-1-2-transformer-models\n",
      "authors: [etienne]\n",
      "date: 2021-03-30\n",
      "tags: ['release']\n",
      "image: ./img/hero.png\n",
      "# canonical-url: https://medium.com/semi-technologies/weaviate-version-1-2-x-now-supports-transformer-models-4a12d858cce3\n",
      "# canonical-name: Medium\n",
      "description: \"Weaviate v1.2 introduced support for transformers (DistilBERT, BERT, RoBERTa, Sentence-BERT, etc) to vectorize and semantically search through your data.\"\n",
      "---\n",
      "![Weaviate 1.2 release - transformer models](./img/hero.png)\n",
      "\n",
      "In the v1.0 release of Weaviate ([docs](/developers/weaviate/) — [GitHub](https://github.com/weaviate/weaviate)) we introduced the concept of [modules](/developers/weaviate/concepts/modules). Weaviate modules are used to extend the vector database with vectorizers or functionality that can be used to query your dataset. With the release of Weaviate v1.2, we have introduced the use of transformers ([DistilBERT](https://arxiv.org/abs/1910.01108), [BERT](https://github.com/google-research/bert), [RoBERTa](https://arxiv.org/abs/1907.11692), Sentence-[BERT](https://arxiv.org/abs/1908.10084), etc) to vectorize and semantically search through your data. <!-- truncate -->\n",
      "\n",
      "### Weaviate v1.2 introduction video\n",
      "\n",
      "<div className=\"youtube\">\n",
      "    <iframe src=\"//www.youtube.com/embed/S4lXPPZvGPQ\" frameBorder=\"0\" allowFullScreen></iframe>\n",
      "</div>\n",
      "\n",
      "## What are transformers? A [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) (e.g., [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))) is a deep learning model that is used for NLP tasks.»\n",
      "[4] «The model trained to classify cats in the Paintbrush illustration will not perform as well as the model trained to classify cats in Photorealistic images, if the final use case is Photorealistic images!\n",
      "\n",
      "Domain differences are very common in Natural Language Processing (NLP), such as the difference between Legal Contracts, Financial Statements, Biomedical Scientific Papers, Wikipedia, or Reddit Conversations to give a few examples. ### Color-coded details\n",
      "For every model, Hugging Face displays a list of important **color-coded** details, such as:\n",
      "* Blue - the **dataset** it was trained on\n",
      "* Green - the **language** of the dataset\n",
      "* White or Purple - **additional details** about the model\n",
      "\n",
      "So, if we look at two Deep Learning models, we can see that [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large) was trained on **stsb_multi_mt**, which is a **French** dataset. ![Camembert Hugging Face Model Card](./img/camembert-model-card-min.png)\n",
      "\n",
      "While [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) was trained on **several datasets** in **English**. ![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)\n",
      "\n",
      "To put it as bluntly as possible, what makes `dangvantuan/sentence-camembert-large` better at French sentence embeddings than `sentence-transformers/all-MiniLM-L6-v2` is that… it was trained on **French** sentences! There are many examples like this, models trained on **biomedical text**, **legal documents**, or **Spanish** are generally going to perform better when tested on that domain compared to models that haven't been explicitly trained for the domain. Note that these tags are a part of Hugging Face’s **model cards**, an impressive effort to continue advancing the organization of Machine Learning models.»\n",
      "[5] «---\n",
      "title: How to choose a Sentence Transformer from Hugging Face\n",
      "slug: how-to-choose-a-sentence-transformer-from-hugging-face\n",
      "authors: [connor]\n",
      "date: 2022-10-04\n",
      "tags: ['integrations']\n",
      "image: ./img/hero.png\n",
      "description: \"Learn about the various Sentence Transformers from Hugging Face!\"\n",
      "---\n",
      "![How to choose a Sentence Transformer from Hugging Face](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "[Weaviate](/developers/weaviate/) has recently unveiled a new module which allows users to easily [integrate models from Hugging Face](/blog/hugging-face-inference-api-in-weaviate) to vectorize their data and incoming queries. At the time of this writing, there are [over 700 models](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) that can be easily plugged into Weaviate. You may ask: **Why are there so many models and how do they differ?**<br/>\n",
      "And more importantly: **How to choose a Sentence Transformer for Semantic Search?**\n",
      "\n",
      "There are too many models to summarize in one flowchart. So instead, we will describe factors that differentiate these models and give you tools to **choose the perfect model for your use case**. <img\n",
      "    src={require('./img/huggingface-sentence-transformers.png').default}\n",
      "    alt=\"Weaviate Sentence Transformers\"\n",
      "    style={{ maxWidth: \"70%\" }}\n",
      "/>\n",
      "\n",
      "## Differences in Deep Learning models\n",
      "\n",
      "Not too long ago, Deep Learning models were typically differentiated based on architectural decisions.»\n",
      "\n",
      "Question: What are the tasks included in the KILT benchmark for evaluating Sentence Transformers?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: To answer this question, we need to refer to the section in the context that mentions the KILT benchmark. The relevant sentence states: \"Each task is labeled with a different task: Slot Filling, Question Answering, Dialogue, Fact Checking, or Entity Linking.\"\n",
      "\n",
      "Answer: The tasks included in the KILT benchmark for evaluating Sentence Transformers are Slot Filling, Question Answering, Dialogue, Fact Checking, and Entity Linking.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «---\n",
      "title: How to choose a Sentence Transformer from Hugging Face\n",
      "slug: how-to-choose-a-sentence-transformer-from-hugging-face\n",
      "authors: [connor]\n",
      "date: 2022-10-04\n",
      "tags: ['integrations']\n",
      "image: ./img/hero.png\n",
      "description: \"Learn about the various Sentence Transformers from Hugging Face!\"\n",
      "---\n",
      "![How to choose a Sentence Transformer from Hugging Face](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "[Weaviate](/developers/weaviate/) has recently unveiled a new module which allows users to easily [integrate models from Hugging Face](/blog/hugging-face-inference-api-in-weaviate) to vectorize their data and incoming queries. At the time of this writing, there are [over 700 models](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) that can be easily plugged into Weaviate. You may ask: **Why are there so many models and how do they differ?**<br/>\n",
      "And more importantly: **How to choose a Sentence Transformer for Semantic Search?**\n",
      "\n",
      "There are too many models to summarize in one flowchart. So instead, we will describe factors that differentiate these models and give you tools to **choose the perfect model for your use case**. <img\n",
      "    src={require('./img/huggingface-sentence-transformers.png').default}\n",
      "    alt=\"Weaviate Sentence Transformers\"\n",
      "    style={{ maxWidth: \"70%\" }}\n",
      "/>\n",
      "\n",
      "## Differences in Deep Learning models\n",
      "\n",
      "Not too long ago, Deep Learning models were typically differentiated based on architectural decisions.»\n",
      "[2] «## Overview\n",
      "![Overview](./img/hugging-face-module-overview.png)\n",
      "\n",
      "The Hugging Face module is quite incredible, for many reasons. ### Public models\n",
      "You get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case. In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). ### Private models\n",
      "If you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate.»\n",
      "[3] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\n",
      "let them do it for you.\"\n",
      "\n",
      "## Support for Hugging Face Inference API in Weaviate\n",
      "Starting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\n",
      "> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\n",
      "[4] «Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Storing and using vector embeddings with a Weaviate vector database\n",
      "\n",
      "For this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules.»\n",
      "[5] «The model trained to classify cats in the Paintbrush illustration will not perform as well as the model trained to classify cats in Photorealistic images, if the final use case is Photorealistic images!\n",
      "\n",
      "Domain differences are very common in Natural Language Processing (NLP), such as the difference between Legal Contracts, Financial Statements, Biomedical Scientific Papers, Wikipedia, or Reddit Conversations to give a few examples. ### Color-coded details\n",
      "For every model, Hugging Face displays a list of important **color-coded** details, such as:\n",
      "* Blue - the **dataset** it was trained on\n",
      "* Green - the **language** of the dataset\n",
      "* White or Purple - **additional details** about the model\n",
      "\n",
      "So, if we look at two Deep Learning models, we can see that [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large) was trained on **stsb_multi_mt**, which is a **French** dataset. ![Camembert Hugging Face Model Card](./img/camembert-model-card-min.png)\n",
      "\n",
      "While [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) was trained on **several datasets** in **English**. ![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)\n",
      "\n",
      "To put it as bluntly as possible, what makes `dangvantuan/sentence-camembert-large` better at French sentence embeddings than `sentence-transformers/all-MiniLM-L6-v2` is that… it was trained on **French** sentences! There are many examples like this, models trained on **biomedical text**, **legal documents**, or **Spanish** are generally going to perform better when tested on that domain compared to models that haven't been explicitly trained for the domain. Note that these tags are a part of Hugging Face’s **model cards**, an impressive effort to continue advancing the organization of Machine Learning models.»\n",
      "\n",
      "Question: How do I choose the right Sentence Transformer model from Hugging Face for Semantic Search?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We know that there are a vast number of Sentence Transformer models available from Hugging Face, and that the Weaviate module provides access to these models for semantic search. The context provides some guidelines on how to choose the right model. Firstly, it is important to understand the differences in the models. The models can vary based on architectural decisions, the datasets they were trained on, and the language of the dataset. The specific domain the model is intended for is also a critical factor. For example, a model trained on French sentences will perform better for French sentence embeddings than a model trained on English sentences. The Hugging Face model cards provide color-coded details about each model, including the dataset, language, and additional information. These details can be used to select a model that aligns with the specific requirements of the semantic search use case.\n",
      "\n",
      "Answer: To choose the right Sentence Transformer model from Hugging Face for Semantic Search, consider the following factors: - Language: Select a model that has been trained on a dataset in the same language as your semantic search use case. - Domain: Choose a model that is tailored to the specific domain of your data, such as legal, financial, biomedical, or general text. - Dataset: Review the Hugging Face model cards to understand the datasets the models were trained on and select one that aligns with your data. - Architectural differences: While not discussed in detail in the context, architectural decisions can also impact the performance of the model. Consider the specific architecture of each model and how it aligns with your requirements. By considering these factors and utilizing the information provided in the Hugging Face model cards, you can select the most suitable Sentence Transformer model for your semantic search application.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «---\n",
      "title: Weaviate introduces an all-new business model for vector search\n",
      "slug: weaviate-cloud-services\n",
      "authors: [bob]\n",
      "date: 2022-09-01\n",
      "tags: ['release']\n",
      "image: ./img/hero.png\n",
      "description: \"Press Release: Pay-as-you-grow comes to Vector Search.\"\n",
      "---\n",
      "![Weaviate introduces an all-new business model for vector search](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "**Amsterdam NL - September 1, 2022**\n",
      "\n",
      "Weaviate, the leader in open-source vector database technology, announces an [entirely new pricing model](/pricing) for its Weaviate Cloud. Inspired by \"pay-as-you-grow\" pricing used in cloud storage, Weaviate has introduced a new business model that makes it easier and more affordable for enterprises of any scale to take advantage of its Weaviate vector database. Beginning today, users have the option of paying a usage-based rate for search on a per-dimension basis. Pricing begins at $0.05 per million dimensions. (No, that's not a typo; the rate is five cents per million.)\n",
      "\n",
      "> \"At this point, as we've reached 1,500,000 downloads, a lot of people know and love our open-source software, but they want us to run it for them,\" says Weaviate's co-founder Bob van Luijt.\" So, we've created a ‘NoOps' option that allows them to pay only for what they use on our optimized Weaviate Cloud.\"\n",
      "\n",
      "In addition to remarkably convenient access to the latest vector database capabilities, Weaviate Cloud customers get dedicated customer support on a private Slack channel.»\n",
      "[2] «---\n",
      "title: Make Real-Time AI a Reality with Weaviate + Confluent\n",
      "slug: confluent-and-weaviate\n",
      "authors: [shukri]\n",
      "date: 2023-09-26\n",
      "tags: ['integrations', 'how-to']\n",
      "image: ./img/hero.png\n",
      "description: \"Learn how to build an application using Weaviate and Confluent\"\n",
      "---\n",
      "import confluentVideo from './img/weaviate-confluent.mp4';\n",
      "\n",
      "<video width=\"100%\" autoplay loop controls>\n",
      "  <source src={confluentVideo} type=\"video/mp4\" />\n",
      "Your browser does not support the video tag. </video>\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "Today, we’re excited to announce our new integration with [Confluent Cloud](https://www.confluent.io/confluent-cloud/?utm_campaign=tm.pmm_cd.2023_partner_cwc_weaviate_generic&utm_source=weaviate&utm_medium=partnerref). Weaviate users now have simple access to data streams from across their entire business to build a real-time, contextual, and trustworthy knowledge base fueling their AI applications. Confluent’s data streaming platform bridges the legacy and modern data stack, providing a continuous supply of AI-ready data for development of sophisticated customer experiences with constant awareness of what’s happening in the world and their business right now. > “With our new integration with Confluent Cloud, Weaviate is taking a giant leap forward in empowering businesses to build AI applications that are not just smart, but also real-time and context-aware.»\n",
      "[3] «### Data Privacy and Compliance\n",
      "\n",
      "Of course, with any enterprise tool, compliance and data privacy are top of mind for any product adoption. As an open source vector database, we support many deployment models from being self-hostable in your own infrastructure using docker or kubernetes, to being deployed into your own VPC in AWS or GCP via the marketplaces, or if you’re compliance requirements allows it, you can also deploy on Weaviate Cloud which will reduce your overhead of managing your Weaviate clusters as well. You can learn more about our security standards [here](https://weaviate.io/security). ## Application of GenAI and Weaviate in Business Use Cases\n",
      "\n",
      "Enough with features, what are some applications of Weaviate and GenAI in the enterprise? Across many of our customers, there are a few common architectural patterns in the applications they are building.»\n",
      "[4] «---\n",
      "layout: post\n",
      "title: What if you could understand your unstructured data? slug: understand-your-unstructured-data\n",
      "authors: [laura]\n",
      "date: 2021-01-20\n",
      "tags: ['concepts']\n",
      "image: ./img/hero.png\n",
      "# canonical-url: https://medium.com/semi-technologies/what-if-you-could-understand-your-unstructured-data-92f0861e016\n",
      "# canonical-name: Medium\n",
      "description: \"Learn how the AI-first vector database Weaviate unlocks the potential of unstructured data and why this is important.\"\n",
      "---\n",
      "![What if you could understand your unstructured data?](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "## Intro\n",
      "These days, more and more organizations are adopting a data-driven culture. Business processes and customer experience benefit from good data collection, management and analysis. But in order to really benefit from available data, it is essential to also understand the unstructured data, like free text in PDF documents, emails, invoices or voice transcriptions. Unstructured data is especially hard to index, manage and understand.»\n",
      "[5] «The model is deep learning neural network with many layers and weights distributed across the entire network representing the probablistic nature of your dataset. ![neural-network](./img/neural-network.png)\n",
      "\n",
      "Through this simple input output system, we can leverage these foundational large language models to generate new content based on natural language prompts. And because they contain massive amounts of data, we don’t necessarily have to train custom neural networks to leverage their capabilities in real world business use cases. Beyond incredible innovation in drug discovery and chip design, these systems allow for the complete automation, or human in the loop based systems, to augment business to consumer experiences or increase enterprise output. ### Vector Databases\n",
      "\n",
      "Vector databases are a fairly new class of database that is specifically designed to store and look up vector embeddings.»\n",
      "\n",
      "Question: What are the new capabilities available to Weaviate users for managing their business data?\n",
      "\n",
      "Reasoning: Let's think step by step in order to answer the question. We know from the context that Weaviate has introduced a new business model and pricing structure, but we also need to consider the integration with Confluent Cloud and the mention of data privacy and compliance.\n",
      "\n",
      "Answer: Weaviate users now have access to new capabilities that enable them to manage their business data more effectively. Firstly, Weaviate introduced a \"pay-as-you-grow\" pricing model, making it more affordable for enterprises of any scale to adopt its vector database technology. This new business model allows users to pay a usage-based rate for search on a per-dimension basis, starting at $0.05 per million dimensions. Additionally, the integration with Confluent Cloud provides Weaviate users with access to data streams from across their entire business. This enables them to build a real-time, contextual, and trustworthy knowledge base that fuels their AI applications. By bridging the legacy and modern data stack, Confluent's data streaming platform ensures a continuous supply of AI-ready data, helping enterprises develop sophisticated, context-aware AI applications. Furthermore, Weaviate supports various deployment models to address data privacy and compliance concerns. Enterprises can self-host Weaviate in their infrastructure using Docker or Kubernetes, deploy it into their own VPC in AWS or GCP marketplaces, or opt for Weaviate Cloud if their compliance requirements allow it.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «The following image presents a high-level illustration of chunking text. ![chunking](img/chunk.png)\n",
      "\n",
      "### Retrieval \n",
      "There are four major knobs to tune in Retrieval: Embedding models, Hybrid search weighting, whether to use AutoCut, and Re-ranker models. Most RAG developers may instantly jump to tuning the embedding model used, such as OpenAI, Cohere, Voyager, Jina AI, Sentence Transformers, and many others! Developers also need to consider the dimensionality of the models and how it affects the PQ compression. The next key decision is how to weight the aggregation of sparse and dense retrieval methods in Hybrid Search. The weighting is based on the `alpha` parameter.»\n",
      "[2] «* **Index Construction:** LLMs can transform information to facilitate building search indexes. This can range from summarizing long content to extracting structured data, identifying text chunks, managing document updates, or transformation structured data to text for vectorization. * **LLMs in Re-Ranking:** Ranking models, distinctly from Retrieval, explicitly takes the query and/or a user description, as well as each candidate document as input to the neural network to output a fine-grained score of relevance. LLMs can now do this off-the-shelf without any extra training. Quite interestingly, we can prompt the LLMs to rank with symbolic preferences such as “price” or “recency”, in addition to the unstructured text content.»\n",
      "[3] «---\n",
      "title: Running Large Language Models Privately - privateGPT and Beyond\n",
      "slug: private-LLM\n",
      "authors: zain\n",
      "date: 2023-05-30\n",
      "image: ./img/hero.png\n",
      "tags: ['concepts','how-to']\n",
      "description: \"A discussion on data privacy and privacy-preserving machine learning for LLMs\"\n",
      "\n",
      "---\n",
      "\n",
      "![Private LLMs](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "Large Language Models (LLM’s) have revolutionized how we access and consume information, shifting the pendulum from a search engine market that was predominantly retrieval-based (where we asked for source documents containing concepts relevant to our search query), to one now that is growingly memory-based and performs generative search (where we ask LLMs to generate answers to questions based on their knowledge of and training on massive datasets). More importantly, what’s different this time is that with the advent of widely accessible products like ChatGPT that have exposed the underlying technology of these models to the wider consumer market, we’re seeing these models revolutionize how we work, learn and interact on a scale never seen before. This wide-scale adoption of LLMs makes the concerns and challenges around privacy and data security paramount, and ones that each organization needs to address. In this blog post we will explore some of the different potential approaches organizations can take to ensure robust data privacy while harnessing the power of these LLMs.\n",
      "\n",
      "## Understanding the Privacy Challenge\n",
      "\n",
      "LLMs are typically trained on vast amounts of data to develop a statistical understanding of human language patterns. If you’d like to get an introductory explanation of how these models work, please read our previous blogpost: [How LLMs Work](/blog/what-are-llms).»\n",
      "[4] «As described in our [previous article](https://weaviate.io/blog/ranking-models-for-better-search), re-ranking models are new to the scene of zero-shot generalization. The story of re-rankers has mostly been tabular user features combined with tabular product or item features, fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt. Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score. This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.»\n",
      "[5] «In the future, we expect further optionality ablating the capacity of the model due to inherent trade-offs of performance and latency that may make sense for some applications but not others. Discovering jointly which capacity re-ranker is needed and how many retrieved results to re-rank is another challenge for tuning the knobs in retrieval. This is also one of the lowest hanging fruit opportunities for fine-tuning custom models in the RAG stack, which we will discuss further in “Tuning Orchestration”. Another interesting knob to tune is Multi-Index Search. Similar to our discussion on chunking, this is a tricky one that may involve structural changes to the database.»\n",
      "\n",
      "Question: How will Long Context LLMs impact re-ranker models?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Answer: The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question.\n",
      "\n",
      "Answer:\u001b[32mReasoning: Let's think step by step in order to produce the answer. The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question.\n",
      "\n",
      "Answer: The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question. \n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAssess the context and answer the question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nContext:\\n[1] «Note, the current implementation of hybrid search in Weaviate uses BM25/BM25F and vector search. If you’re interested to learn about how dense vector indexes are built and optimized in Weaviate, check out this [article](/blog/why-is-vector-search-so-fast). ### BM25\\nBM25 builds on the keyword scoring method [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term-Frequency Inverse-Document Frequency) by taking the [Binary Independence Model](https://en.wikipedia.org/wiki/Binary_Independence_Model) from the IDF calculation and adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. The image below presents the scoring calculation of BM25:\\n![BM25 calculation](./img/BM25-calculation.png) <div align=\"center\"><i>Source: Wikipedia page on Okapi BM25</i></div>\\n\\nThe score of the document, query pair is determined by weighing the uniqueness of each keyword in the query relative to the collection of texts. BM25 contains additional static parameters, k1 and b that may help calibrate performance to particular datasets.»\\n[2] «And that Weaviate recently added a new algorithm for how this is done? In this post, we’ll dive into exactly the world of hybrid search to discuss how it works, how results are produced, the algorithms used, and more. So let’s get into it!\\n\\n:::info\\n- Vector search and keyword search are also known as dense vector search and sparse vector search respectively. - Keyword search is also called a BM25 search in Weaviate, as it is based on the [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25) scoring algorithm. :::\\n\\n## How does hybrid search work, exactly?»\\n[3] «# Current Limitations of Using Weaviate with Non-English Languages\\n\\nAs you saw, you can use the Weaviate vector database for semantic and generative search applications in non-English languages. However, we currently don’t have any official support/policy for non-English languages, which leads to some limitations. This limitation mainly affects [Weaviate’s keyword-based search functionality BM25](https://weaviate.io/developers/weaviate/search/bm25), as we currently only have the standard English tokenizer. This means the BM25 search will return an error for most non-alphabetic languages, especially ones requiring Unicode encoding. This limitation also affects [Weaviate’s hybrid search functionality](https://weaviate.io/developers/weaviate/search/hybrid).»\\n[4] «We would really appreciate your input. Please let us know what you think!\\n\\n\\n\\n## Wrap-up\\n\\nHybrid search in Weaviate offers a powerful blend of vector and keyword search, using the strengths of both to deliver semantically rich results while respecting precision of keyword searches. As we\\'ve explored, the introduction of `relativeScoreFusion` expands Weaviate’s hybrid search capabilities that began its life with the `rankedFusion` algorithm. We invite you to dive in, experiment with these fusion algorithms, and share your experiences. ### Further resources\\n\\n- [How-to: Hybrid search](/developers/weaviate/search/hybrid)\\n- [API References: Hybrid search](/developers/weaviate/api/graphql/search-operators#hybrid)\\n\\nimport WhatsNext from \\'/_includes/what-next.mdx\\';\\n\\n<WhatsNext />»\\n[5] «In other use cases, we’ve found that [keyword search](https://weaviate.io/developers/weaviate/search/bm25) is more important for some applications. \"BM25 (Best match 25)\" or \"sparse vector\" search, returns objects that have the highest BM25F scores. Vector Search and Keyword Search each have their advantages, but often times, the results from vector search or keyword search are too restrictive and the optimal set of results are somewhere in-between. This is why we also have [hybrid search](https://weaviate.io/developers/weaviate/search/hybrid) which combines results of a vector search and a keyword (BM25F) search. You can set the\\xa0[weights](https://weaviate.io/developers/weaviate/search/hybrid#balance-keyword-and-vector-search)\\xa0or the\\xa0[ranking method](https://weaviate.io/developers/weaviate/search/hybrid#change-the-ranking-method).»\\n\\nQuestion: What is the role of the Binary Independence Model in the BM25 algorithm used by Weaviate\\'s hybrid search?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To understand the role of the Binary Independence Model in the BM25 algorithm, we need to break down how BM25 works and how it utilizes this model.\\n\\nAnswer: The Binary Independence Model is a component of the Inverse Document Frequency (IDF) calculation in the BM25 algorithm. It helps determine the uniqueness or rarity of a keyword within a collection of texts. By considering the frequency of a term in the entire corpus, the model assesses how informative or distinctive that term is. This information is then used to weigh the importance of the term in the overall scoring calculation, with rarer terms typically receiving higher scores.\\n\\n---\\n\\nContext:\\n[1] «Scalability: Most vector libraries cannot be queried while importing your data, which can be a scalability concern for applications that require importing millions or even billions of objects. Thus, vector libraries are a great solution for applications with a limited static snapshot of data. However, if your application requires real-time scalable semantic search at the production level, you should consider using a vector database. ## Summary\\n\\nThis article explained that vector databases are a type of database that indexes, stores, and provides access to structured or unstructured data alongside its vector embeddings. Vector databases like [Weaviate](http://weaviate.io) allow for efficient similarity search and retrieval of data based on their vector distance or vector similarity at scale.»\\n[2] «Vector Indexing Library\\n\\nSimilarly to vector databases, [vector libraries](https://weaviate.io/blog/vector-library-vs-vector-database) also enable fast vector search. However, vector libraries only store vector embeddings of data objects, and they store them in in-memory indexes. This results in two key differences:\\n\\n1. Updatability: The index data is immutable, and thus, no real-time updates are possible. 2.»\\n[3] «This is why we have **Vector Databases** and **Vector Libraries**. They both use the Approximate Nearest Neighbor (ANN) algorithm to search through vectors in a tiny fraction of the time. You can learn more about this topic from [\"Why Vectors Search is so Fast.\"](/blog/why-is-vector-search-so-fast)\\n\\n## The Big Question\\nSo, if both vector databases and vector libraries allow you to efficiently search through your vectors. What are the key differences between them, and why/when should you choose one over the other? ## Vector Libraries\\nVector libraries store vector embeddings in in-memory indexes, in order to perform similarity search.»\\n[4] «Further reading [Why Is Vector Search So Fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)\\n\\n## Use Cases of Vector Databases\\n\\nVector databases’ search capabilities can be used in various applications ranging from classical Machine Learning use cases, such as natural language processing, computer vision, and recommender systems, to providing long-term memory to LLMs in modern applications. The **most popular use case of vector search engines is naturally for search**. Because a vector database can help find similar objects, it is predestined for applications where you might want to find similar products, movies, books, songs, etc. That’s why vector search engines are also **used in recommendation systems** as a restated task of search. With the rise of LLMs, **vector databases have already been used to enhance modern Generative AI applications**.»\\n[5] «Depending on the data type, vector embeddings are created using machine learning models that are able to translate the meaning of an object into a numerical representation in a high dimensional space. Thus, there are a variety of machine learning models able to create a variety of different types of embeddings, such as word embeddings, sentence embeddings, text embeddings, or image embeddings. Vector embeddings capture the semantic relationship between data objects in numerical values and thus, you can find similar data points by determining their nearest neighbors in the high dimensional vector space. This concept is also called similarity search and can be applied in different applications, such as text search, image search, or recommendation systems. Now that you have a good understanding of vector embeddings, you might also be interested in the following articles:\\n* [What is a vector database?](https://weaviate.io/blog/what-is-a-vector-database)\\n* [Distance metrics for nearest neighbor search](https://weaviate.io/blog/distance-metrics-in-vector-search)\\n* [Why is vector search so fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)\\n\\nimport WhatNext from \\'/_includes/what-next.mdx\\'\\n\\n<WhatNext />»\\n\\nQuestion: Why might vector libraries not be suitable for applications that require real-time updates and scalable semantic search?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s break down the question and identify the key points to address. First, we need to understand the capabilities and limitations of vector libraries. Then, we can assess why they may not be suitable for certain use cases. Vector libraries are designed to store vector embeddings of data objects, and they do so in in-memory indexes. This provides fast vector search capabilities. However, there are two main limitations to consider: 1. **Scalability**: Vector libraries cannot be queried while importing data. This becomes a bottleneck for applications that deal with large amounts of data, especially in the billions of objects. The library has to finish importing all the data before any queries can be made, which can be time-consuming and inefficient. 2. **Updatability**: The vector library indexes are immutable, meaning they cannot be changed once created. This results in an inability to perform real-time updates. Any changes to the data would require recreating the entire index, which is impractical for applications that require dynamic data updates.\\n\\nAnswer: Vector libraries may not be suitable for applications that require real-time updates and scalable semantic search due to their limitations in updatability and scalability. The immutable nature of vector library indexes prevents real-time updates, and the lack of query capabilities during data import affects their scalability for large datasets. Therefore, applications that demand dynamic data changes and efficient handling of vast amounts of data would be better served by alternative solutions, such as vector databases.\\n\\n---\\n\\nContext:\\n[1] «I recommend checking out the GitHub repository to test this out yourself!\\n\\n## Additional Resources\\n• [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar. import WhatsNext from \\'/_includes/what-next.mdx\\';\\n\\n<WhatsNext />»\\n[2] «We will review and help you out in the process. 💚\\n\\nYou can also contribute by adding your own Weaviate examples. If you have other great ideas for contributions, let us know on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels, and we will figure out how to highlight it in Hacktoberfest. You don\\'t need to be an expert to contribute to these demo projects!\\n\\n\\n## Resources to Get Started\\n\\nWe\\'re thrilled to help you make your first open-source contribution! Here are some helpful resources to kickstart your journey:\\n\\nWhat is Open Source, and how do you contribute to it? - 🎯 [What is Open Source](https://www.digitalocean.com/community/tutorials/what-is-open-source)\\n- 🎯 [Introduction to GitHub and Open-Source Projects](https://www.digitalocean.com/community/tutorial_series/an-introduction-to-open-source)\\n- 🎯 [How to Contribute to Open Source](https://opensource.guide/how-to-contribute/)\\n- 🎯\\xa0[GitHub Contribution Guide by Hugging Face](https://www.notion.so/Contribution-Guide-19411c29298644df8e9656af45a7686d?pvs=21)\\n- 🎯 [How to Use Git](https://www.digitalocean.com/community/cheatsheets/how-to-use-git-a-reference-guide)\\n- 🎯 [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide)\\n\\nIf you\\'re new to Weaviate, get up and running quickly with these beginner-friendly guides:\\n\\n- [Quickstart Guide](https://weaviate.io/developers/weaviate/quickstart) 🚀\\n- [Weaviate Academy](https://weaviate.io/developers/academy) 🎓\\n\\nDive deeper into specific topics with these detailed guides:\\n\\n- [How-to Search Guides](https://weaviate.io/developers/weaviate/search) 🔍\\n- [Keyword, Vector, Hybrid, and Generative Search](https://github.com/weaviate-tutorials/generative-search/blob/main/GenerativeSearchDemo.ipynb) 🔍\\n- [How-to Manage Data (CRUD Operations)](https://weaviate.io/developers/weaviate/manage-data) 💾\\n- [Tutorial: Importing Data with Your Own Vectors](https://weaviate.io/developers/weaviate/tutorials/wikipedia) 📊\\n- [Weaviate Architecture Concepts](https://weaviate.io/developers/weaviate/concepts#weaviate-architecture) 🏛️\\n\\nJoin one of our [workshops](https://weaviate.io/learn/workshops) for an introduction to Weaviate.»\\n[3] «It also provides a date, author, and tags related to integrations. Given this information, we can create a query that asks about the purpose of combining LangChain with Weaviate, as this is a specific topic that the document addresses. Query: What are the benefits of combining LangChain with Weaviate in the context of LLMs? ```\\n</details>\\n\\n#### Grab Data out of Weaviate\\n\\nNow that we have the synthetic query + relevant chunk, we can export the data to fine-tune the model. We’ll grab 80% of the objects for training and 20% for validation.»\\n[4] «In this example we used two research papers; however, there is the possibility to add Powerpoint presentations or even scanned letters to your Weaviate instance. Unstructured has really simplified the process of using visual document parsing for diverse document types. We tested a few queries above, but we can take this one step further by using [LangChain](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/weaviate). Once the documents are imported into Weaviate, you can build a simple chatbot to chat with your pdfs by using LangChain’s vectorstore. ```python\\nfrom langchain.vectorstores.weaviate import Weaviate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ChatVectorDBChain\\nimport weaviate\\n\\nclient = weaviate.Client(\"http://localhost:8080\")\\n\\nvectorstore = Weaviate(client, \"NAME_OF_CLASS\", \"NAME_OF_PROPERTY\")\\n\\nMyOpenAI = OpenAI(temperature=0.2,\\n    openai_api_key=\"ENTER YOUR OPENAI KEY HERE\")\\n\\nqa = ChatVectorDBChain.from_llm(MyOpenAI, vectorstore)\\n\\nchat_history = []\\n\\nwhile True:\\n    query = input(\"\")\\n    result = qa({\"question\": query, \"chat_history\": chat_history})\\n    print(result[\"answer\"])\\n    chat_history = [(query, result[\"answer\"])]\\n```\\n\\nimport WhatNext from \\'/_includes/what-next.mdx\\'\\n\\n<WhatNext />»\\n[5] «We ... Query: A short question uniquely answered by the document. ---\\n\\nDocument: --- title: Combining LangChain and Weaviate slug: combining-langchain-and-weaviate authors: [erika] date: 2023-02-21 tags: [\\'integrations\\'] image: ./img/hero.png description: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\" --- ![Combining LangChain and Weaviate](./img/hero.png) Large Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale.»\\n\\nQuestion: What guide does the document recommend for learning about LangChain projects?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to identify which guide is specifically recommended for learning about LangChain projects within the provided context. We can scan the text for any mentions of \"LangChain\" and \"guide\" or \"tutorial\" to find the relevant information.\\n\\nAnswer: The document recommends the \"LangChain Guide\" by Paul from CommandBar as a resource to learn about LangChain projects.\\n\\n---\\n\\nContext:\\n[1] «If we compress the vectors then the memory requirements goes down to the 1572 MB to 2129 MB range. After compression, recall drops to values ranging from 0.7337 to 0.9545. Latency rises up to the 7521 to 37402 microsends range. A summary is shown in Table 3 below. |                       |              | Recall100@100 | Latency ($\\\\mu s$)         | Memory required (MB)         |\\n|-----------------------|--------------|---------------|---------------------------|------------------------------|\\n| Sift1M Low params     | Uncompressed | 0.91561       | 293                       | 1277                         |\\n|                       | Compressed   | 0.91361       | 401               (x1.36) | 610                 (47.76%) |\\n| Sift1M High params    | Uncompressed | 0.99974       | 1772                      | 1674                         |\\n|                       | Compressed   | 0.99658       | 1937             (x1.09)  | 1478               (88.29%)  |\\n| DeepImage Low params  | Uncompressed | 0.8644        | 827                       | 9420                         |\\n|                       | Compressed   | 0.85666       | 1039             (x1.25)  | 4730               (50.21%)  |\\n| DeepImage High params | Uncompressed | 0.99757       | 2601                      | 15226                        |\\n|                       | Compressed   | 0.97023       | 2708             (x1.04)  | 12367             (81.22%)   |\\n| Gist Low params       | Uncompressed | 0.74461       | 2133                      | 4218                         |\\n|                       | Compressed   | 0.73376       | 7521             (x3.52)  | 1572              (37.26%)   |\\n| Gist High params      | Uncompressed | 0.99628       | 15539                     | 5103                         |\\n|                       | Compressed   | 0.95455       | 37402           (x2.40)   | 2129               (41.72%)  |\\n\\n**Tab.»\\n[2] «If we compress the vectors then the memory requirements goes down to the 4730 MB to 12367 MB range. After compression, recall drops to values ranging from 0.8566 to 0.9702. Latency rises up to the 1039 to 2708 microsends range. For Gist we would require roughly 4218 MB to 5103 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.7446 to 0.9962 and latencies ranging from 2133 to 15539 microseconds.»\\n[3] «Furthermore, you might be using Weaviate with your own data which might be even bigger than the datasets we report on below. If we extrapolate out, consider how big these memory requirements could grow as you add more objects or represent these objects with long vector embeddings. | DataSet     | Dimensions | Vectors   | Size in memory (MB) |\\n|-------------|------------|-----------|---------------------|\\n| Sift1M      | 128        | 1,000,000 | 512                 |\\n| Gist        | 960        | 1,000,000 | 3840                |\\n| DeepImage96 | 96         | 9,990,000 | 3836.16             |\\n\\n**Table 1**: *Description of datasets.*\\n\\nIncreasing the number of objects vs. storing longer dimensional vectors has an equivalent effect on the overall memory required to store the vectors. As an example consider the Gist dataset, which contains 1,000,000 vectors, each with 960 dimensions.»\\n[4] «| Memory footprint (MB)  | Compressed (hours) | Uncompressed (hours)|\\n|----------------|--------------------|-------------|\\n| Sphere         |  725 (15%)         | 5014    |\\n| DBPedia        |  900 (14%)         | 6333     |\\n**Tab. 2**: *Compression ratios of `~6.6x`.*\\n\\nNotice how using the compressed version we would barely need 15% of the memory we would need under uncompressed settings. To close the results up, we have also indexed the full Sphere dataset (nearly 1 billion vectors) with and without compression. The full dataset without compression needed nearly **2.7 Terabytes** of memory while the compressed version only required **0.7 Terabytes** which is again, a huge saving in terms of memory and costs. ## 📈Benchmarking PQ with your own data\\n\\nIf you\\'re trying to finetune HNSW and PQ parameters on your own data to optimally balance recall, latency, and memory footprint you might want to replicate these experiments yourself.»\\n[5] «The main metric of concern in this setting is the latency with which we can obtain our replies. Product Quantization not only helps with reducing memory requirements in this case but also with cutting down on latency. The following table compares improvement in latency that can be achieved using PQ aggressively. | Dataset   | Segments | Centroids | Compression |              | Latency (ms) |\\n|-----------|----------|-----------|-------------|--------------|--------------|\\n| Sift      | 8        | 256       | x64         | Compressed   | 46 (x12)     |\\n|           |          |           |             | Uncompressed | 547          |\\n| DeepImage | 8        | 256       | x48         | Compressed   | 468 (x8.5)   |\\n|           |          |           |             | Uncompressed | 3990         |\\n| Gist      | 48       | 256       | x80         | Compressed   | 221 (x17.5)  |\\n|           |          |           |             | Uncompressed | 3889         |\\n\\n**Tab. 2**: *Brute force search latency with high compression ratio.*\\n\\n## HNSW+PQ\\n\\nOur complete implementation of [FreshDiskANN](https://arxiv.org/abs/2105.09613) still requires a few key pieces, however at this point we have released the HNSW+PQ implementation with v1.18 for our users to take advantage of.»\\n\\nQuestion: What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression?\\n\\nReasoning: Let\\'s think step by step in order to Context: The Gist dataset has 1,000,000 vectors, each with 960 dimensions. When using uncompressed HNSW, the memory requirement ranges from 4218 MB to 5103 MB depending on the parameters. After compression, the memory requirement for Gist Low params is 1572 MB. Question: What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression? Reasoning: To find the percentage reduction in memory requirements, we can use the following formula: % reduction = ((initial value - final value) / initial value) * 100% We know that the initial memory requirement for Gist Low params is 4218 MB, and the final memory requirement after compression is 1572 MB. % reduction = ((4218 MB - 1572 MB) / 4218 MB) * 100% = 62.72% reduction\\n\\nAnswer: There is a 62.72% reduction in memory requirements for the Gist dataset with low parameters after compression.\\n\\n---\\n\\nContext:\\n[1] «### Testing queries\\n\\nSemantic (`nearText`) searches may be one of the most common (if not *the* most common) searches our users perform. So let\\'s see how we might test semantic searches. A semantic search requires vectorizing the query, so a test will validate the integration with the vectorizer (`text2vec-openai` in this case). We\\'ll run a query for \"chemistry\" and check that the top result is about \"sodium\". :::info Will the top result always be the same?»\\n[2] «We choose the model because it’s faster than GPT4, but you can use any other embedding model (Cohere, HuggingFace, etc.). <details>\\n  <summary>Product Schema</summary>\\n\\n```python\\nclass_obj = {\\n        \"class\": \"Product\",\\n        \"description\": \"Supplement products\",\\n        \"properties\": [\\n            {\\n                \"dataType\": [\"text\"],\\n                \"description\": \"The name of the product\",\\n                \"name\": \"name\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": True,\\n                        \"vectorizePropertyName\": False,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"text\"],\\n                \"description\": \"The brand of the product\",\\n                \"name\": \"brand\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": True,\\n                        \"vectorizePropertyName\": False,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"text\"],\\n                \"description\": \"The ingredients contained in the product.\",\\n                \"name\": \"ingredients\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": False,\\n                        \"vectorizePropertyName\": True,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"text[]\"],\\n                \"description\": \"Reviews about the product\",\\n                \"name\": \"reviews\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": True,\\n                        \"vectorizePropertyName\": False,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"text\"],\\n                \"description\": \"Image URL of the product\",\\n                \"name\": \"image\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": True,\\n                        \"vectorizePropertyName\": False,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"number\"],\\n                \"description\": \"The Rating of the product\",\\n                \"name\": \"rating\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": True,\\n                        \"vectorizePropertyName\": False,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"text\"],\\n                \"description\": \"The description of the product\",\\n                \"name\": \"description\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": False,\\n                        \"vectorizePropertyName\": True,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"text\"],\\n                \"description\": \"The summary of the reviews\",\\n                \"name\": \"summary\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": False,\\n                        \"vectorizePropertyName\": True,\\n                    }\\n                },\\n            },\\n            {\\n                \"dataType\": [\"text\"],\\n                \"description\": \"The health effects of the product\",\\n                \"name\": \"effects\",\\n                \"moduleConfig\": {\\n                    \"text2vec-openai\": {\\n                        \"skip\": False,\\n                        \"vectorizePropertyName\": True,\\n                    }\\n                },\\n            },\\n        ],\\n        \"moduleConfig\": {\"generative-openai\": {\"model\": \"gpt-3.5-turbo\"}},\\n        \"vectorizer\": \"text2vec-openai\",\\n    }\\n```\\n</details>\\n\\nTo give you a clearer picture, we define a new class in the Weaviate database (similar to a blueprint) using a Python dictionary. This class includes fields like product name, brand, reviews, ratings, etc. Each field\\'s datatype is specified, which can be text, image, number, etc., and a brief description and field name is added. Some fields are configured to be vectorized, meaning their semantic content will be incorporated into the overall vector indexed in the database.»\\n[3] «Then, you just run the queries, as per usual:\\n```javascript\\nnearText = {\\n    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\\n    \"distance\": 0.6,\\n}\\n\\nresult = (\\n    client.query\\n    .get(\"Notes\", [\\n        \"name\",\\n        \"comment\",\\n        \"_additional {certainty distance} \"])\\n    .with_near_text(nearText)\\n    .do()\\n)\\n```\\n\\n## Summary\\n> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\\nCheck out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.»\\n[4] «```python\\ndef test_query(documents):\\n    LIMIT = 3\\n    response = client.post(\"/query\", json={\"text\": \"lion\", \"limit\": LIMIT})\\n\\n    results = response.json()\\n\\n    assert len(results) == LIMIT\\n    for result in results:\\n        assert \"lion\" in result[\"document\"][\"text\"]\\n```\\n\\nThe implementation below will take in a query and return a list of retrieved documents and metadata. ```python\\n@app.post(\"/query\", response_model=List[QueryResult])\\ndef query(query: Query, client=Depends(get_weaviate_client)) -> List[Document]:\\n    \"\"\"\\n    Query weaviate for documents\\n    \"\"\"\\n    query_vector = get_embedding(query.text)\\n\\n    results = (\\n        client.query.get(INDEX_NAME, [\"document_id\", \"text\"])\\n        .with_near_vector({\"vector\": query_vector})\\n        .with_limit(query.limit)\\n        .with_additional(\"certainty\")\\n        .do()\\n    )\\n\\n    docs = results[\"data\"][\"Get\"][INDEX_NAME]\\n\\n    return [\\n        QueryResult(\\n            document={\"text\": doc[\"text\"], \"document_id\": doc[\"document_id\"]},\\n            score=doc[\"_additional\"][\"certainty\"],\\n        )\\n        for doc in docs\\n    ]\\n```\\n</details>\\n\\n<details>\\n  <summary>See details for /delete endpoint implementation.</summary>\\n\\n**Implement the `/delete` endpoint:**\\n\\nHere we simply want to check that the response returned correctly and that after removing one object we the number of total objects in the Weaviate instance goes down by one. ```python\\ndef test_delete(documents, weaviate_client):\\n    num_docs_before_delete = weaviate_client.data_object.get()[\"totalResults\"]\\n\\n    response = client.post(\"/delete\", json={\"document_id\": \"3\"})\\n    assert response.status_code == 200\\n\\n    num_docs_after_delete = weaviate_client.data_object.get()[\"totalResults\"]\\n\\n    assert num_docs_after_delete == num_docs_before_delete - 1\\n```\\n\\nAnd the implementation of the endpoint is as follows:\\n\\n```python\\n@app.post(\"/delete\")\\ndef delete(delete_request: DeleteRequest, client=Depends(get_weaviate_client)):\\n    \"\"\"\\n    Delete a document from weaviate\\n    \"\"\"\\n    result = client.batch.delete_objects(\\n        class_name=INDEX_NAME,\\n        where={\\n            \"operator\": \"Equal\",\\n            \"path\": [\"document_id\"],\\n            \"valueText\": delete_request.document_id,\\n        },\\n    )\\n\\n    if result[\"results\"][\"successful\"] == 1:\\n        return {\"status\": \"ok\"}\\n    else:\\n        return {\"status\": \"not found\"}\\n```\\n</details>\\n\\nHere we showed you how our endpoints work, this will be where your plugin will be most unique, depending on what functionality you want implemented you can create corresponding endpoints and test them. ::::tip\\nNotice the docstrings we’ve included with all of our endpoints, these will be very important in the next step!\\n::::\\n\\n**Step 2: Prepare plugin manifest files**\\n\\nThis is where you specify to OpenAI and specifically ChatGPT which endpoints your plugin is exposing, how it can use those endpoints to accomplish specific tasks, what errors to expect if the endpoints are not used correctly and much more! [OpenAI instruction specify](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest) that you need to create two files: the [`openapi.yaml` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/openapi.yaml) and the [`ai-plugin.json` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/ai-plugin.json). As you can see both of these files need to be in the `.well-known` directory which must be mounted into the app as follows in order for ChatGPT to utilize them correctly.»\\n[5] «Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Storing and using vector embeddings with a Weaviate vector database\\n\\nFor this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules.»\\n\\nQuestion: How do you test semantic search functionality and result consistency using `text2vec-openai`?\\n\\nReasoning: Let\\'s think step by step in order to Answer: Here is the response formatted as requested: Context: The text describes the process of setting up and testing a Weaviate database with a focus on semantic search functionality using the `text2vec-openai` module. It includes code snippets and explanations for various steps, such as defining the database schema, running queries, and implementing endpoints for a retrieval plugin. Question: How do you test semantic search functionality and result consistency using `text2vec-openai`? Reasoning: To test the semantic search functionality and result consistency using `text2vec-openai`, we need to follow the steps outlined in the context. First, we define the database schema, including the fields that will be vectorized, such as \"ingredients,\" \"description,\" and \"effects.\" Then, we can run queries using the `nearText` search parameter to perform semantic searches. In the example given, the query searches for \"chemistry\" and expects the top result to be about \"sodium.\" We can also specify the distance parameter to control the relevance of the results. To test the consistency of the results, we can use assertions in our test code. In the provided code snippet, the `test_query` function asserts that the length of the results is equal to the specified limit and that the query term (\"lion\" in this case) is present in the text of each result. By running this test for different queries and comparing the results, we can evaluate the consistency of the semantic search functionality.\\n\\nAnswer: To test semantic search functionality and result consistency using `text2vec-openai`, we define the database schema, including fields for vectorization. We run queries using `nearText` and specify distance parameters. We can test consistency by asserting expected results and comparing outcomes for different queries.\\n\\n---\\n\\nContext:\\n[1] «We recommend to use a Docker Compose setup for trying out or evaluating Weaviate and when developing with Weaviate locally. However, for a stable production environment, we [recommend to run Weaviate on Kubernetes](/developers/weaviate/installation/kubernetes). You can use Helm, there is an [officially supported Weaviate Helm Chart](https://github.com/weaviate/weaviate-helm). import WhatsNext from \\'/_includes/what-next.mdx\\';\\n\\n<WhatsNext />»\\n[2] «Similarly, we want to give you more flexibility during ingestion time: how about [extracting PDFs](https://github.com/weaviate/weaviate/issues/2509) or applying [stemming](https://github.com/weaviate/weaviate/issues/2439) to your BM25 and hybrid search? <br></br>\\n\\n### Beyond Billion Scale: Large-Scale Performance\\n![Billion Scale](./img/billion-scale.png)\\n\\nIn 2022, we published the [Sphere Demo Dataset for Weaviate](/blog/sphere-dataset-in-weaviate). This marked the first time (to our knowledge) that more than a billion objects and vectors were imported into Weaviate. Dealing with ever-growing datasets is not only about being able to handle their size. Our users run complex queries in production and often have strict latency requirements.»\\n[3] «---\\ntitle: Multi-Tenancy Vector Search with millions of tenants\\nslug: multi-tenancy-vector-search\\nauthors: [etienne]\\ndate: 2023-06-15\\nimage: ./img/hero.png\\ntags: [\\'engineering\\', \\'concepts\\']\\ndescription: \"Learn about the new native multi-tenancy feature\"\\n\\n---\\n\\n![Multi-Tenancy ](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nLarge-scale setups were always a great reason to choose Weaviate. Last year we wrote about the first time [a Weaviate setup ran with a billion objects & vectors](/blog/sphere-dataset-in-weaviate). What was a mere experiment back then is a regular production case today. But earlier this year, we saw a shift in usage patterns: As we onboarded more and more large-scale and enterprise users, the definition of scale shifted from the number of vectors to the number of individual tenants that can run on a single setup. Previously, Weaviate offered multiple ways to tackle multi-tenancy, but none were intended for a massive scale.»\\n[4] «For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use. import WhatsNext from \\'/_includes/what-next.mdx\\';\\n\\n<WhatsNext />»\\n[5] «## Monitoring and Observability\\n![Monitoring and Observability](./img/monitoring-and-observability.png)\\n\\nOne of the biggest challenges of running software in production is to understand what is happening under the hood. That is especially important when something goes wrong, or we need to anticipate in advance when more resources are required. ![It doesn\\'t work.... why?](./img/what-is-happening.jpg)\\nWithout such insight, we end up looking at the black box, wondering what is going on. ### Announcement\\nWith Weaviate `1.14` you can get a lot more insight into the resources and the performance of different aspects of your Weaviate instance in Production.»\\n\\nQuestion: What does the document recommend for users running Weaviate at scale in production?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: The document provides insights into the Weaviate platform, a vector search engine, and its capabilities for large-scale usage. It recommends Docker Compose for trying out and evaluating Weaviate locally, while suggesting Weaviate on Kubernetes for a stable production environment. The text also highlights the benefits of using Weaviate at scale, including the ability to handle ever-growing datasets and complex queries with strict latency requirements. Additionally, it introduces the native multi-tenancy feature, which allows for a massive number of individual tenants on a single setup.\\n\\nAnswer: The document recommends that users running Weaviate at scale in production consider enabling the native multi-tenancy feature. This feature addresses the shift in usage patterns towards a larger number of individual tenants, providing a solution for massive-scale multi-tenancy.\\n\\n---\\n\\nContext:\\n[1] «Document understanding techniques use an encoder-decoder pipeline that leverages the power of both computer vision and natural language processing methods. On the Weaviate Podcast, Brian Raymond described one of the founding motivations of Unstructured as follows: “Hey, HuggingFace is exploding over here with 10s of thousands of models and an incredible community. What if we did something similar to the left of HuggingFace, and we made it cheap, fast, and easy for data scientists to get through that data engineering step, so they can consume more of that!” Now that the stage is set, let’s explore how Unstructured works. Unstructured simplifies the process of importing a PDF and converting it into text. The core abstraction of Unstructured is the \\'brick.\\' Unstructured uses bricks for document pre-processing: 1.»\\n[2] «Check out one of our free weekly workshops to help you understand what vector databases are and how they can help you build production-ready AI apps quickly and easily. If you’re curious, here are some of the most commonly asked questions we encountered:\\n\\n**What’s the difference between a vector database and a graph or relational database?**\\n\\nGraph databases are used to identify relationships between objects, and vector databases are used to find objects\\nRelational databases store the relations between tables and build indexes for the fast lookup of joined tables. Vector databases, on the other hand, store the embeddings of structured and unstructured data for the quick retrieval of the objects. **What features does Weaviate offer to help me protect data privacy?**\\n\\nWeaviate is designed with robust security measures to ensure it meets the requirements of enterprise environments. Weaviate has achieved SOC 2 certification and is encrypted in transit and at rest.»\\n[3] «---\\nlayout: post\\ntitle: What if you could understand your unstructured data? slug: understand-your-unstructured-data\\nauthors: [laura]\\ndate: 2021-01-20\\ntags: [\\'concepts\\']\\nimage: ./img/hero.png\\n# canonical-url: https://medium.com/semi-technologies/what-if-you-could-understand-your-unstructured-data-92f0861e016\\n# canonical-name: Medium\\ndescription: \"Learn how the AI-first vector database Weaviate unlocks the potential of unstructured data and why this is important.\"\\n---\\n![What if you could understand your unstructured data?](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n## Intro\\nThese days, more and more organizations are adopting a data-driven culture. Business processes and customer experience benefit from good data collection, management and analysis. But in order to really benefit from available data, it is essential to also understand the unstructured data, like free text in PDF documents, emails, invoices or voice transcriptions. Unstructured data is especially hard to index, manage and understand.»\\n[4] «Since around [80% of all data is unstructured](https://www.forbes.com/sites/forbestechcouncil/2019/01/29/the-80-blind-spot-are-you-ignoring-unstructured-organizational-data/), it is hard to actually search and retrieve insights from most of the data. The Weaviate vector database unlocks the potential of unstructured data. Searching by fuzzy terms and classification of rich data like free text becomes possible with Weaviate. It uses AI-driven indexing and search technologies to enable real-time text processing. With machine learning methods, Weaviate automatically classifies texts.»\\n[5] «These models are reaching new heights in performance because they leverage visual information, not just text. <figure>\\n\\n![Donut pipeline](./img/donut.png)\\n<figcaption> Pipeline of Donut from Kim, G. et al (2022) </figcaption>\\n</figure>\\n\\n\\n## About Unstructured\\n[Unstructured](https://www.unstructured.io/) is an open-source company working at the cutting edge of PDF processing and more. They allow businesses to ingest their diverse data sources, whether this be a `PDF`, `JPEG`, or `PPT`, and convert it into data that can be passed to a LLM. This means that you could take private documents from your company and pass it to a LLM to chat with your PDFs.\\n\\nUnstructured’s open-source [core library](https://github.com/Unstructured-IO/unstructured) is powered by document understanding models.»\\n\\nQuestion: What percentage of data is unstructured, and how does Weaviate help in processing it?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: The text mentions that around 80% of all data is unstructured, and provides the source for this statistic. This is a widely-accepted estimate, and a good indicator of the prevalence of unstructured data. Understanding and processing this unstructured data is a challenge, and this is where Weaviate comes in. Weaviate is an AI-first vector database that helps unlock the potential of unstructured data. It does this by using AI-driven indexing and search technologies, enabling real-time text processing and automatic classification of texts. With Weaviate, users can search using fuzzy terms and classify rich data like free text, PDFs, and more.\\n\\nAnswer: Approximately 80% of all data is unstructured. Weaviate helps process this unstructured data by using AI-driven indexing and search, enabling real-time text processing and automatic classification.\\n\\n---\\n\\nContext:\\n[1] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The neighborhood: The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park.»\\n[2] «| Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The space: This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\\n[3] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher.»\\n[4] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The gym is located in the building and is easily accessible. |\\n\\n</div>\\n</details>\\n\\n## Deeper into Personalization\\nPersonalized digital experiences have been taken to a new level with the latest advances in AI technology.»\\n[5] «|\\n  </div>\\n</details>\\n\\n<details>\\n  <summary>\\n    Cohere Generations\\n  </summary>\\n  <div>\\n\\n| Rank | Result |\\n|------|--------|\\n| 1    | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. |\\n| 2    | THE VILLAGE OF HARLEM....NEW YORK !\\\\n\\\\nThis is the Village of Harlem.»\\n\\nQuestion: Does the apartment come with in-unit laundry appliances, and is it located near public transportation and Prospect Park?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to look for specific details mentioned in the context. We are specifically looking for information about laundry appliances, proximity to public transportation, and the location relative to Prospect Park.\\n\\nAnswer: Yes, the apartment comes with in-unit laundry appliances, including a washer and dryer. It is also located near public transportation, with convenient access to the subway and multiple bus lines. Additionally, the apartment is a short walk away from Prospect Park, offering easy access to green space and leisure activities.\\n\\n---\\n\\nContext:\\n[1] «The model trained to classify cats in the Paintbrush illustration will not perform as well as the model trained to classify cats in Photorealistic images, if the final use case is Photorealistic images!\\n\\nDomain differences are very common in Natural Language Processing (NLP), such as the difference between Legal Contracts, Financial Statements, Biomedical Scientific Papers, Wikipedia, or Reddit Conversations to give a few examples. ### Color-coded details\\nFor every model, Hugging Face displays a list of important **color-coded** details, such as:\\n* Blue - the **dataset** it was trained on\\n* Green - the **language** of the dataset\\n* White or Purple - **additional details** about the model\\n\\nSo, if we look at two Deep Learning models, we can see that [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large) was trained on **stsb_multi_mt**, which is a **French** dataset. ![Camembert Hugging Face Model Card](./img/camembert-model-card-min.png)\\n\\nWhile [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) was trained on **several datasets** in **English**. ![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)\\n\\nTo put it as bluntly as possible, what makes `dangvantuan/sentence-camembert-large` better at French sentence embeddings than `sentence-transformers/all-MiniLM-L6-v2` is that… it was trained on **French** sentences! There are many examples like this, models trained on **biomedical text**, **legal documents**, or **Spanish** are generally going to perform better when tested on that domain compared to models that haven\\'t been explicitly trained for the domain. Note that these tags are a part of Hugging Face’s **model cards**, an impressive effort to continue advancing the organization of Machine Learning models.»\\n[2] «Particularly from my conversation with Nils Reimers, I have become very interested in the continual learning nature of this. For example, when we released the `ref2vec` module and discussed it on the podcast, the `all-miniLM-L6-v2` model has never seen ref2vec before in its training set. Additionally, a model fine-tuned up to podcast #30 will have never seen ref2vec either!\\n\\n    I am also very interested in the fine-tuning of cross-encoder models, which you can learn more about [here](/blog/cross-encoders-as-reranker). 3. Custom Benchmarking\\n\\n\\tI have also been working on the BEIR benchmarking in Weaviate (nearly finished!).»\\n[3] «You would need the following ingredients:\\n* Raw Data\\n* Hugging Face API token – which you can request from [their website](https://huggingface.co/settings/tokens)\\n* A working Weaviate instance with the `text2vec-huggingface` enabled\\n\\nThen you would follow these steps. ### Step 1 – initial preparation – create schema and select the hf models\\nOnce you have a Weaviate instance up and running. Define your schema (standard stuff – pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class. This is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\\n```javascript\\n{\\n    \"class\": \"Notes\",\\n    \"moduleConfig\": {\\n        \"text2vec-huggingface\": {\\n            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\\n            ...»\\n[4] «---\\ntitle: Weaviate Podcast Search\\nslug: weaviate-podcast-search\\nauthors: [connor]\\ndate: 2023-01-31\\ntags: [\\'how-to\\']\\nimage: ./img/hero.png\\ndescription: \"Recent advances in AI are breathing new life into podcasting! The Whisper speech-to-text model is a game-changer!\"\\n---\\n![Weaviate Podcast Search](./img/hero.png)\\n\\nRecent advances in AI are breathing new life into podcasting! The Whisper speech-to-text model is a game-changer! It can transcribe podcasts with astonishing accuracy, allowing us to [index them into Weaviate](https://github.com/weaviate/weaviate-podcast-search)!\\nI have been hosting the Weaviate podcast for a little over a year with 34 published episodes and am super excited to tell you how this will completely revamp our podcast, as well as the details behind how to build something like this for yourself. <!-- truncate -->\\n\\nPodcasts are easy to consume, we can listen to podcasts as we drive to work, take a walk, or play a video game. However, despite their convenience, podcasts have lacked a crucial aspect that other knowledge base mediums have - the ability to easily reference and search past content. So let’s dive into how to build it, see some queries, and then come back to how this will change podcasting!\\n\\n## Whisper\\n\\nOpenAI has taken a swing at unleashing the potential of AI technology, breaking open a piñata of new applications. Among the bounty of treats spilling out, podcasting shines bright like a glittering candy! - Written by ChatGPT\\n\\n```python\\n!pip install git+https://github.com/openai/whisper.git\\n\\nimport whisper\\n\\n# available models = [\"tiny.en\", \"tiny\", \"base.en\", \"base\", \"small.en\", \"small\", \"medium.en\", \"medium\", \"large-v1\", \"large-v2\", \"large\"]\\n\\nmodel = whisper.load_model(\"large-v2\").to(\"cuda:0\")\\n\\nimport time\\nstart = time.time()\\nresult = model.transcribe(\"20.mp3\")\\nprint(f\"Transcribed in {time.time() - start} seconds.\")\\n\\nf = open(\"20-text-dump.txt\", \"w+\")\\nf.write(result[\"text\"])\\nf.close()\\n```\\n\\nFrom here you get a big text file from the podcast.»\\n[5] «Although a bigger machine (see below) is needed for importing the data, the serving is done on a 12 CPU, 100 GB RAM, 250Gb SSD Google Cloud VM with 1 x NVIDIA Tesla P4. The ML-models used are [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) and [bert-large-uncased-whole-word-masking-finetuned-squad](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad) both are available as [pre-built modules](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers#pre-built-images) in Weaviate. 📄 The complete dataset and code is open-source and available [on GitHub](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate). ![Demo GIF of Weaviate using the Wikipedia dataset](./img/weaviate-using-the-Wikipedia-dataset.gif)\\n*Example semantic search queries in Weaviate\\'s GraphQL interface — GIF by Author*\\n\\n## Importing the Data In Two Steps\\n> You can also directly import a backup into Weaviate without doing the import your self as outlined [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/tree/main#step-3-load-from-backup). To import the data we use two different methods.»\\n\\nQuestion: What module was discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on?\\n\\nReasoning: Let\\'s think step by step in order to Context: The Weaviate podcast discusses recent advances in AI and how they can be applied to podcasting. It specifically mentions the Whisper speech-to-text model from OpenAI as a game-changer for transcribing podcasts. The podcast also covers the release of the `ref2vec` module, which is relevant to the question. Question: What module was discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on? Reasoning: The context mentions that the \\'all-miniLM-L6-v2\\' model had not been exposed to the `ref2vec` module during its training. This is specifically highlighted in the second paragraph, where it is stated that the model \"has never seen ref2vec before in its training set.\"\\n\\nAnswer: The module discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on is the `ref2vec` module.\\n\\n---\\n\\nContext:\\n[1] «The below animation visualizes a real example of this in e-Commerce images.\"\\n        },\\n        {\\n          \"content\": \"The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\\\\n\\\\n![Ref2Vec Image](./img/ref2vec.png)\\\\n\\\\nSuch a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User\\'s preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User\\'s interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User\\'s product and style preferences. We envision Ref2Vec to have great potential in multiple application areas. Let\\'s take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\\\\nMany of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.\"\\n        },\\n        {\\n          \"content\": \"## More Coming Soon\\\\nWe are very excited about the potential of Ref2Vec, and how it leverages existing symbolic data to augment vector searches in a new way.»\\n[2] «The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\\n\\n![Ref2Vec Image](./img/ref2vec.png)\\n\\nSuch a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User\\'s preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User\\'s interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User\\'s product and style preferences. We envision Ref2Vec to have great potential in multiple application areas. Let\\'s take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\\nMany of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.»\\n[3] «If a user clicks on 3 shoe images on an e-commerce store, it is a safe bet that they want to see more shoes. Ref2Vec captures this intuition by calculating vectors that aggregate each User\\'s interaction with another class. The below animation visualizes a real example of this in e-Commerce images.\"\\n        },\\n        {\\n          \"_additional\": {\\n            \"rerank\": [\\n              {\\n                \"score\": 0.34444344\\n              }\\n            ]\\n          },\\n          \"content\": \"The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\\\\n\\\\n![Ref2Vec Image](./img/ref2vec.png)\\\\n\\\\nSuch a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User\\'s preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User\\'s interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User\\'s product and style preferences. We envision Ref2Vec to have great potential in multiple application areas.»\\n[4] «## What is Ref2Vec? The name Ref2Vec is short for reference-to-vector, and it offers the ability to vectorize a data object with its cross-references to other objects. The Ref2Vec module currently holds the name ref2vec-**centroid** because it uses the average, or centroid vector, of the cross-referenced vectors to represent the **referencing** object.\"\\n        },\\n        {\\n          \"content\": \"In other words, the User vector is being updated in real-time here to take into account their preferences and actions, which helps to produce more relevant results at speed. Another benefit of Ref2Vec is that this calculation is not compute-heavy, leading to low overhead. With Ref2Vec, you can use Weaviate to provide Recommendation with \\\\\"user-as-query\\\\\".»\\n[5] «Let\\'s take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\\\\nMany of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.\"\\n        },\\n        {\\n          \"_additional\": {\\n            \"rerank\": [\\n              {\\n                \"score\": 0.007121429\\n              }\\n            ]\\n          },\\n          \"content\": \"In other words, the User vector is being updated in real-time here to take into account their preferences and actions, which helps to produce more relevant results at speed. Another benefit of Ref2Vec is that this calculation is not compute-heavy, leading to low overhead. With Ref2Vec, you can use Weaviate to provide Recommendation with \\\\\"user-as-query\\\\\". This is a very common and powerful way to build Home Feed style features in apps.»\\n\\nQuestion: What is Ref2Vec, and how does it integrate with Weaviate to provide real-time user-based recommendations?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to understand what Ref2Vec is and how it works with Weaviate to deliver real-time recommendations based on user behavior.\\n\\nAnswer: Ref2Vec, or reference-to-vector, is a technique that enables the vectorization of a data object by leveraging its cross-references to other objects. In simple terms, it calculates vectors that represent a user\\'s interactions with different items or classes. This is achieved by aggregating the representations of the referenced objects to create a vector for the referencing object. In the given context, Ref2Vec is used to capture a user\\'s preferences and actions on an e-commerce platform. Now, how does Ref2Vec integrate with Weaviate to provide real-time user-based recommendations? Weaviate is a vector database and search engine that can power high-quality, fast recommendations. Ref2Vec enhances Weaviate\\'s capabilities by allowing it to learn from user behavior and preferences in real time. When a user interacts with multiple items on an e-commerce platform (for example, clicking on shoe images), Ref2Vec calculates a vector that represents the user\\'s interest in that particular category or style. This vector is created by aggregating the vectors of the referenced products (the shoes the user clicked on). By integrating Ref2Vec with Weaviate, the system can dynamically update user vectors based on their interactions and preferences. This enables Weaviate to provide relevant and personalized recommendations in real time. For instance, if a user has purchased or shown interest in boots, shorts, and a Weaviate t-shirt, Ref2Vec can aggregate these product representations to characterize the user\\'s overall style preferences. As a result, Weaviate can offer recommendations that align closely with the user\\'s tastes, improving their shopping experience. In summary, Ref2Vec enables the vectorization of user preferences by aggregating their interactions with various items. This information is then used by Weaviate to update user vectors in real time, leading to more accurate and personalized recommendations. The combination of Ref2Vec and Weaviate thus enhances the user experience by delivering relevant and timely suggestions based on their behavior and interests.\\n\\n---\\n\\nContext:\\n[1] «We have three documents labeled `A`, `B`, and `C` and have run a BM25 and Dense search. In this example, we have set the constant *k* to 0. | BM25 Ranking | Dense Ranking | Results\\n| --- | --- | --- |\\n| A | B | A: 1/1 + 1/3 = 1.3 |\\n| B | C | B: 1/2 + 1/1 = 1.5 |\\n| C | A | C: 1/3 + 1/2 = 0.83|\\n\\nThe above table shows the ranked order of the BM25 and Dense search. To fuse the two lists together, we need to take the sum of the reciprocal ranks. Based on the results, the top document is `Document B` with a ranking of 1.5, then `Document A` at 1.3, and `Document C` at 0.83.»\\n[2] «The dense vector representation is able to disambiguate “catch” as meaning fishing rather than baseball or sickness. The sparse vector search will match the phrase “Alaskan Pollock” only. This example query shows where hybrid search combines the best of both sparse and dense vectors. ## Reciprocal Rank Fusion (RRF)\\n\\nWhile researching hybrid search, we needed a way to combine the results of BM25 and dense vector search into a single ranked list. We came across a paper from Benham and Culpepper exploring rank fusion techniques.»\\n[3] «Note, the current implementation of hybrid search in Weaviate uses BM25/BM25F and vector search. If you’re interested to learn about how dense vector indexes are built and optimized in Weaviate, check out this [article](/blog/why-is-vector-search-so-fast). ### BM25\\nBM25 builds on the keyword scoring method [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term-Frequency Inverse-Document Frequency) by taking the [Binary Independence Model](https://en.wikipedia.org/wiki/Binary_Independence_Model) from the IDF calculation and adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. The image below presents the scoring calculation of BM25:\\n![BM25 calculation](./img/BM25-calculation.png) <div align=\"center\"><i>Source: Wikipedia page on Okapi BM25</i></div>\\n\\nThe score of the document, query pair is determined by weighing the uniqueness of each keyword in the query relative to the collection of texts. BM25 contains additional static parameters, k1 and b that may help calibrate performance to particular datasets.»\\n[4] «In this blog post, you will learn about the implementation of hybrid search in Weaviate and how to use it. ## Sparse and Dense Vectors\\nSparse and dense vectors are calculated with distinct algorithms. Sparse vectors have mostly zero values with only a few non-zero values, while dense vectors mostly contain non-zero values. Sparse embeddings are generated from algorithms like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) and [SPLADE](https://arxiv.org/abs/2107.05720). Dense embeddings are generated from machine learning models like\\n[GloVe](https://text2vec.org/glove.html) and [Transformers](https://huggingface.co/docs/transformers/index).»\\n[5] «And that Weaviate recently added a new algorithm for how this is done? In this post, we’ll dive into exactly the world of hybrid search to discuss how it works, how results are produced, the algorithms used, and more. So let’s get into it!\\n\\n:::info\\n- Vector search and keyword search are also known as dense vector search and sparse vector search respectively. - Keyword search is also called a BM25 search in Weaviate, as it is based on the [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25) scoring algorithm. :::\\n\\n## How does hybrid search work, exactly?»\\n\\nQuestion: How are BM25 and Dense search rankings combined using reciprocal ranks, and what is the resulting order of documents A, B, and C?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: The context provides a table with the BM25 and Dense search rankings for documents A, B, and C, along with their respective reciprocal ranks. To combine the rankings and determine the final order, we need to calculate the sum of the reciprocal ranks for each document.\\n\\nAnswer: The BM25 and Dense search rankings are combined by calculating the reciprocal rank for each document in both lists and then summing them up. The resulting order of documents A, B, and C is: 1. Document B (sum of reciprocal ranks: 1/2 + 1/1 = 1.5) 2. Document A (sum of reciprocal ranks: 1/1 + 1/3 = 1.33) 3. Document C (sum of reciprocal ranks: 1/3 + 1/2 = 0.83) So, the final ranking is B > A > C.\\n\\n---\\n\\nContext:\\n[1] «So syntax that is currently like this:\\n\\n:::note Classic client syntax\\n```python\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'title\\']  # Get the `title` property of the first object\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'_additional\\'][\\'id\\']  # Get the ID of the first object\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'_additional\\'][\\'generate\\'][\\'singleResult\\']  # Get the generated text from a `singlePrompt` request\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'_additional\\'][\\'generate\\'][\\'groupedResult\\']  # Get the generated text from a `groupedTask` request\\n```\\n\\n:::\\n\\nBecome:\\n\\n:::info `Collections` client syntax\\n\\n```python\\nresponse.objects[0].properties[\\'title\\']  # Get the `title` property of the first object\\nresponse.objects[0].uuid  # Get the ID of the first object\\nresponse.objects[0].generated  # Get the generated text from a `singlePrompt` request\\nresponse.generated  # Get the generated text from a `groupedTask` request\\n```\\n\\n:::\\n\\nWe think that these changes will reduce errors, increase productivity, and make the code easier to read and understand. ### Collections-first approach\\n\\nThe other big change is that the `collections` client focuses on individual collections for interaction. This means that you will no longer need to specify the collection name in every request. Instead, you will create an object for each collection that you want to interact with, and then use that object for all subsequent requests. For example, take the following syntax for performing a simple request to retrieve a few objects from the database:\\n\\n:::note Classic client syntax\\n\\n```python\\nresponse = (\\n    client.query.get(\\n        class_name=\"Article\",\\n        properties=[\"title\", \"body\", \"url\"]\\n    )\\n    .with_limit(2)\\n    .do()\\n)\\n```\\n\\n:::\\n\\nBecomes:\\n\\n:::info `Collections` client syntax\\n\\n```python\\narticles = client.collection.get(\"Article\")\\nresponse = articles.query.fetch_objects(limit=2)\\n```\\n\\n:::\\n\\nYou\\'ll see that a search is now a method that originates from the collection object.»\\n[2] «:::\\n\\n:::note What you will be able to do using the Web bundle\\nOnly Read operations powered by GraphQL. :::\\n\\n\\n\\n### Collections-first approach\\n\\nThe other big change is that the `collections` client focuses on individual collections for interaction. This means that you will no longer need to specify the collection name in every request. Instead, you will create an object for each collection that you want to interact with, and then use that object for all subsequent requests. For example, take the following syntax for performing a simple request to retrieve a few objects from the database:\\n\\n:::note Classic TypeScript syntax\\n\\n```typescript\\nconst response = await client.graphql\\n  .get()\\n  .withClassName(\\'Article\\')\\n  .withLimit(2)\\n  .withFields(\\'title body url\\')\\n  .do();\\n```\\n\\n:::\\n\\nBecomes:\\n\\n:::info `Collections` client syntax\\n\\n```typescript\\nconst articles = client.collection.get(\\'Article\\');\\nconst response = articles.query.fetchObjects({\\n  limit: 2,\\n  returnProperties: [\\'title\\',\\'body\\',\\'url\\']\\n});\\n```\\n\\n:::\\n\\nYou\\'ll see that a search is now a method that originates from the collection object.»\\n[3] «This can be a hurdle when you are working with less often-used options, trying new APIs, or when you are new to Weaviate. import codeCompletionImgUrl from \\'./img/code-completion.gif\\';\\n\\n<img src={codeCompletionImgUrl} alt=\"Code completion and type hints are added in the new Python client\" width=\"85%\"/>\\n\\n\\nTypes are introduced for the data objects as well at creation time, as well as when retrieving them from the database. This means that you can access the properties of the data object directly. So syntax that is currently like this:\\n\\n:::note Classic client syntax\\n```typescript\\nresponse.data?.Get?.Article?.[0].title  // Get the `title` property of the first object\\nresponse.data?.Get?.Article?.[0][\\'_additional\\']?.id  // Get the ID of the first object\\nresponse.data?.Get?.Article?.[0][\\'_additional\\']?.generate?.singleResult  // Get the generated text from a `singlePrompt` request\\nresponse.data?.Get?.Article?.[0][\\'_additional\\']?.generate.groupedResult  // Get the generated text from a `groupedTask` request\\nresponse.data?.Get?.Article?.[0][\\'_additional\\']?.creationTimeUnix // Get the timestamp when the object was created\\n```\\nLook at all those `?` operators and the necessary `[\\'_additional\\']` index due to the `_` in the key name!\\n\\n:::\\n\\nBecomes:\\n\\n:::info `Collections` client syntax\\n\\n```typescript\\nresponse.objects[0].properties.title  // Get the `title` property of the first object\\nresponse.objects[0].uuid  // Get the ID of the first object\\nresponse.objects[0].generated  // Get the generated text from a `singlePrompt` request\\nresponse.generated  // Get the generated text from a `groupedTask` request\\nresponse.metadata?.creationTime // Get the creation time as a native JS Date value\\n```\\n\\n:::\\n\\nWe think that these changes will reduce errors, increase productivity, and make the code easier to read and understand. ### Separated Node and Web Versions\\n\\nFor a long time, we\\'ve used a single client to interact with Weaviate from both the Web and Node.js.»\\n[4] «### Simplified methods\\n\\nStandalone methods with parameters now replace the builder pattern (`with_` methods) for queries. So what used to be a chain of methods like this:\\n\\n:::note Classic client syntax\\n\\n```python\\nresponse = (\\n    client.query.get(\\n        class_name=\"JeopardyQuestion\",\\n        properties=[\"question\", \"answer\"]\\n    )\\n    .with_near_text({\"concepts\": [\"the space race\"]})\\n    .with_generate(\\n        grouped_task=\"Write a haiku about these facts!\",\\n    )\\n    .with_limit(2)\\n    .do()\\n)\\n```\\n\\n:::\\n\\nBecomes:\\n\\n:::info `Collections` client syntax\\n\\n```python\\nquestions = client.collection.get(\"JeopardyQuestion\")\\nresponse = questions.generate.near_text(\\n    query=\"the space race\",\\n    limit=2,\\n    grouped_task=\"Write a haiku about these facts!\"\\n)\\n```\\n\\n:::\\n\\n### Property/metadata return defaults\\n\\nYou might have noticed that above examples do not specify properties to be returned!\\n\\nWe have changed the default behavior to return most properties and metadata such as the object ID, creation time, vector search distance and so on. We think this will make it easier to get started, and for production use-cases where you want to optimize the response size, you can still specify the properties you want to return. ### Batch import typing\\n\\nBatch object, now called `insert_many`, also gets a refresh with the introduction of a `DataObject` class. This class is used to define the properties of the object to be inserted, and is then passed to the `insert_many` method.»\\n[5] «### Simplified methods\\n\\nStandalone methods with parameters now replace the builder pattern (`.with` methods) for queries. So what used to be a chain of methods like this:\\n\\n:::note Classic client syntax\\n\\n```typescript\\ngeneratePrompt = \\'Convert this quiz question: {question} and answer: {answer} into a trivia tweet.\\';\\n\\nresult = await client.graphql\\n  .get()\\n  .withClassName(\\'JeopardyQuestion\\')\\n  .withGenerate({\\n    singlePrompt: generatePrompt,\\n  })\\n  .withNearText({\\n    concepts: [\\'World history\\'],\\n  })\\n  .withFields(\\'round _additional { distance }\\')\\n  .withLimit(2)\\n  .do();\\n```\\n\\n:::\\n\\nBecomes:\\n\\n:::info `Collections` client syntax\\n\\n```typescript\\nconst generatePrompt = `Convert this quiz question: {question} and answer: {answer} into a trivia tweet.`;\\n\\nconst myCollection = client.collections.get(\\'JeopardyQuestion\\');\\nconst result = await myCollection.generate.nearText([\\'World history\\'], {\\n  singlePrompt: generatePrompt\\n}, {\\n  limit: 2,\\n  returnMetadata: [\\'distance\\']\\n\\n})\\n```\\n\\n:::\\n\\n### Property/metadata return defaults\\n\\nYou might have noticed that the above examples do not specify properties to be returned!\\n\\nWe have changed the default behavior to return all non-reference properties, e.g. `text` and `object` types, and the IDs of the objects. We think this will make it easier to get started, and for production use cases where you want to optimize the response size, you can still specify the properties you want to return. To request additional metadata and any cross-references, you can use the `returnMetadata` and `returnReferences` options for each of the search queries. ### Batch import typing\\n\\nBatch object, now called `insertMany()`, also gets a refresh with the introduction of a `DataObject` class.»\\n\\nQuestion: What are the syntax differences between the classic client and the collections client for accessing data object properties in the described programming environment?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to identify and compare the syntax examples provided for both the classic client and the collections client when accessing data object properties. By examining the context, we can determine the specific differences in syntax between the two clients.\\n\\nAnswer: The syntax differences between the classic client and the collections client for accessing data object properties are as follows: Classic Client: - Uses a chain of methods with the `with_` prefix, such as `with_limit` and `with_generate`, to specify query parameters. - Requires specifying the collection name in every request. - Uses a builder pattern for queries. - Requires using the `?` operator for optional chaining and the `[\\'_additional\\']` index due to the underscore in the key name. - Accesses properties using bracket notation, such as `response[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'title\\']`. Collections Client: - Uses standalone methods with parameters, such as `fetch_objects` and `generate.near_text`, to perform queries. - Does not require specifying the collection name in every request. Instead, a collection object is created and used for subsequent requests. - Simplifies the syntax by removing the need for the `?` operator and the `[\\'_additional\\']` index. - Accesses properties using dot notation, such as `response.objects[0].properties[\\'title\\']`. - Introduces types for data objects, allowing direct access to their properties. - Returns most properties and metadata by default, including object IDs, creation times, and vector search distances. The collections client aims to improve readability, reduce errors, and increase productivity by simplifying the syntax and introducing standalone methods. It also eliminates the need to specify the collection name in every request, streamlining the code.\\n\\n---\\n\\nContext:\\n[1] «It significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks. 2. It requires much less compute while achieving better performance (<30% of DALLE)\\n3. MM-RAG capable models also generate images much more faithful to the retrieved context - meaning the quality of the generated images is better and grounded in the retrieved context image. 4.»\\n[2] «Are capable of multimodal in-context learning (e.g., image generation from demonstrations) - meaning that we can feed any demonstration images and text so that the model generates an image that follows the visual characteristics of these in-context images. MM-RAG gives us a way to further control the awesome generative power of these new multimodal models to produce more useful results for use in industry. ## MM-RAG Code Demo\\n\\nBelow we provide code that allows you to implement MM-RAG by retrieving from the multimodal Weaviate collection that we set up earlier and then stuffing a `base64` encoded image along with a text prompt to the GPT4-Vision model released recently by OpenAI. We then take the generated description and pass it into DALL-E-3 to recreate the image from the description. The full code for the example below can be found in this [Jupyter notebook](https://github.com/weaviate-tutorials/multimodal-workshop/blob/main/2-multimodal/2-multimedia-rag.ipynb).»\\n[3] «This tiny ambassador of affection sits proudly, almost as if understanding the joy it brings to those around it. With its compact size and affectionate demeanor, this little canine looks ready to dispense unlimited love and puppy kisses on demand. ### Use Text to Reconstruct the Image from DALL-E-3 (Diffusion Model):\\n\\nCurrently, GPT4-V can\\'t produce images so to generate an image from the above description we will use the new DALL-E-3 model instead:\\n\\n<details>\\n  <summary>Using OpenAI DALL-E-3</summary>\\n\\n```python\\nfrom openai import OpenAI\\ndef generate_image_dalee3(prompt):\\n  openai_client = OpenAI()\\n  response_oai = openai_client.images.generate(\\n    model=\"dall-e-3\",\\n    prompt=str(prompt),\\n    size=\"1024x1024\",\\n    quality=\"standard\",\\n    n=1,\\n  )\\n  result = response_oai.data[0].url\\n  print(f\"Generated image url: {result}\")\\n  return result\\nimage_url = generate_image_dalee3(GENERATED_DESCRIPTION)\\n```\\n\\n</details>\\n\\n**Generated Image:**\\n![generated puppy](./img/puppy_dalle.png)\\n\\n## Conclusion\\nIn this blog, we covered how we can extend the concept of RAG to include retrieval from a multimodal knowledge base. We also explained how multimedia can be embedded into a unified vector space and consequently how we can leverage vector databases to power any-to-any search. I hope you found this article useful! I\\'d love to connect on **X** at [@zainhasan6](https://twitter.com/ZainHasan6)!\\n\\nimport WhatNext from \\'/_includes/what-next.mdx\\'\\n\\n<WhatNext />»\\n[4] «In order to train a diffusion model we ask it to remove the noise from the noised images step by step until it recovers something as close as possible to the original image. This process is known as “de-noising”, is illustrated below and, is carried out for each image in the training set with multiple levels of random noise added. Once the diffusion model is trained in this way it becomes an expert at taking images that are less likely to be seen in the dataset (noisy images) and incrementally turning them into something that is more likely to be seen in the training set. By teaching the model to “de-noise” images we have developed a way to alter images to make them more like images from the training set. ![denoising gif](./img/denoise.gif)\\n*[Source](https://yang-song.net/blog/2021/score/)*\\n\\n![denoising images](./img/denoisingimage.png)\\n*[Source](https://huggingface.co/blog/annotated-diffusion)*\\n\\nNow if we take this trained diffusion model and just give it a random static image and run the de-noising process it will transform the static image into an image that resembles images in the training set!\\n\\n![noising denoising images](./img/noising_denoising.png)\\n\\n## How Text Prompts Control the Image Generation Process\\n\\nSo far we have explained the general idea behind how diffusion models can start off from static noise and incrementally alter the pixel values so that the picture all together gains meaning and follows the distribution of the training set.»\\n[5] «---\\ntitle: How A.I. Creates Art - A Gentle Introduction to Diffusion Models\\nslug: how-ai-creates-art\\nauthors: [zain]\\ndate: 2023-01-24\\ntags: [\\'concepts\\']\\nimage: ./img/hero.png\\ndescription: \"Machine learning models can create beautiful and novel images. Learn how Diffusion Models work and how you could make use of them.\"\\n---\\n![How A.I. Creates Art](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nOne of the major developments this past year were the advancements made in machine learning models that can create beautiful and novel images such as the ones below. Though machine learning models with the capability to create images have existed for a while, this previous year we saw a marked improvement in the quality and photo-realism of the images created by these models. ![Perception of the world](./img/perception_of_the_world.jpg)\\n\\nModels like [DALL·E 2](https://openai.com/product/dall-e-2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion) and others which are the technologies underlying many platforms such as Lensa and Midjourney are being used by millions of people and are quickly becoming main stream as people realize their potential. These models not only have the ability to dream up photo-realistic images when prompted with text input but can also modify given images to add details, replace objects or even paint in a given artists style.»\\n\\nQuestion: Which model outperforms DALL-E and CM3 in image and caption generation tasks while requiring less computational power and producing images more faithful to the retrieved context?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: In the provided context, there is a direct comparison made between different models, including DALL-E, CM3, and MM-RAG capable models. To answer the question, we need to understand the performance and capabilities of each model.\\n\\nAnswer: MM-RAG capable models outperform DALL-E and CM3 in image and caption generation tasks. They require less computational power (<30% of DALL-E) while producing higher-quality images that are more faithful to the retrieved context. This is because MM-RAG provides a way to control the generative power of multimodal models, resulting in more useful outcomes for industrial applications.\\n\\n---\\n\\nContext:\\n[1] «| Welcome to your home away from home in the heart of Manhattan! This huge 2 bedroom is located just steps away from Central Park and all the best that New York City has to offer.\\\\n\\\\nThe apartment has been newly renovated with stainless steel appliances, exposed brick, and beautiful hardwood floors. You\\'ll love the convenience of being located in a very safe and convenient area, with a variety of restaurants and stores nearby. The subway is also very close, making it easy to get around the city.\\\\n\\\\nWhether you\\'re here for business or pleasure, this is the perfect place to call home during your stay. Don\\'t miss out on the opportunity to explore all that New York City has to offer from the comfort of this amazing apartment. Book your stay today and start planning your adventure! |\\n| Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\\n[2] «| Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. The space: This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\\n[3] «It is newly renovated with stainless steel appliances, exposed brick, and beautiful hardwood floors. The apartment is located in a very safe and convenient area, with a variety of restaurants and stores nearby. The subway is also very close, making it easy to get around the city. So what are you waiting for? Book your stay today and experience the best of New York City! | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub.»\\n[4] «|\\n  </div>\\n</details>\\n\\n<details>\\n  <summary>\\n    Cohere Generations\\n  </summary>\\n  <div>\\n\\n| Rank | Result |\\n|------|--------|\\n| 1    | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. |\\n| 2    | THE VILLAGE OF HARLEM....NEW YORK !\\\\n\\\\nThis is the Village of Harlem.»\\n[5] «There is also a washer and dryer in the unit as well as a dishwasher. The apartment is located in a very quiet and safe neighbourhood, and is also close to the subway and many bus lines. It is also a short walk to the beautiful Prospect Park. | Welcome to your country space in the city! This beautiful, bright, and quiet 2 bedroom apartment has been completely renovated and features a private entrance, a large kitchen with brand new stainless steel appliances, two bedrooms each with queen size beds, and a full bathroom with a soaking tub. There is also a washer and dryer in the unit as well as a dishwasher.»\\n\\nQuestion: What features does the renovated 2 bedroom New York City apartment offer?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To determine the features of the renovated 2-bedroom apartment in New York City, we need to carefully examine the provided information and identify the key attributes mentioned. We should also pay attention to any unique or standout features that set this apartment apart from others.\\n\\nAnswer: The renovated 2-bedroom apartment in New York City boasts several attractive features. Here is a list of what it offers: - Prime location: Situated in the heart of Manhattan, just a short distance from Central Park, providing easy access to the best attractions of New York City. - Newly renovated: Features stainless steel appliances, exposed brick walls, and beautiful hardwood floors, creating a modern and stylish ambiance. - Convenience and safety: Located in a safe and convenient neighborhood, with a variety of restaurants, stores, and the subway nearby, making daily life and navigation around the city effortless. - Comfort and privacy: Offers a private entrance, providing added privacy and comfort during your stay. - Spacious and well-equipped kitchen: The large kitchen is equipped with brand new stainless steel appliances, perfect for cooking and entertaining. - Comfortable bedrooms: Includes two bedrooms, each furnished with a queen-size bed, ensuring a good night\\'s rest for up to four people. - Full bathroom: Features a soaking tub for a relaxing bath after a long day of exploring the city. - Additional amenities: Depending on the specific unit, some apartments also offer a washer, dryer, and dishwasher for added convenience during extended stays. - Easy transportation: The proximity to the subway and bus lines makes getting around the city a breeze, and it\\'s also within walking distance of Prospect Park for leisure and outdoor activities. This apartment seems to be an excellent choice for anyone seeking a comfortable, convenient, and well-located stay in New York City, whether for business or pleasure.\\n\\n---\\n\\nContext:\\n[1] «---\\ntitle: Fine-Tuning Cohere\\'s Reranker\\nslug: fine-tuning-coheres-reranker\\nauthors: [erika]\\ndate: 2024-02-13\\nimage: ./img/hero.png\\ntags: [\\'how-to\\', \\'search\\']\\ndescription: \"Learn how to fine-tune Cohere\\'s reranker and generate synthetic data using DSPy!\"\\n---\\n\\n![Hero image](./img/hero.png)\\n\\n## Introduction\\n\\nSearch engines and retrieval augmented generation (RAG) applications rely heavily on retrieval accuracy. There are many techniques for achieving highly accurate retrieval. One of the most common patterns we’ve seen is multi-stage search. Typically this is in two stages: 1. Gather the relevant objects with the first-stage retriever 2.»\\n[2] «Furthermore, options like defining filters or metadata that were defined through string parameters in the `Classic` client are now defined through custom Python classes. import nearTextQueryImgUrl from \\'./img/neartext_query_hints.gif\\';\\n\\n<img src={nearTextQueryImgUrl} alt=\"IDE autocompletion in queries have been added\" width=\"85%\"/>\\n\\n### Retrieval augmented generation\\n\\nAs you saw briefly earlier, we have a `.generate` submodule for retrieval augmented generation. The structure of these mirror those of the queries, with additional parameters added for the generation task. For example, this query:\\n\\n```python\\nresponse = questions.query.near_text(\\n    query=\"the space race\",\\n    limit=2,\\n)\\n```\\n\\nCan be converted to a retrieval augmented generation task by switching the submodule to `.generate` and adding the `grouped_task` parameter:\\n\\n```python\\n# highlight-start\\nresponse = questions.generate.near_text(\\n# highlight-end\\n    query=\"the space race\",\\n    limit=2,\\n    # highlight-start\\n    grouped_task=\"Write a haiku about these facts!\"\\n    # highlight-end\\n)\\n```\\n\\n## Now it\\'s your turn\\n\\n![Tell us what you think](https://media.giphy.com/media/g2QPc8eIPrYkTBwC4v/giphy.gif)\\n\\nIf you\\'ve made it this far, you\\'re probably excited to try out the new client. We\\'re soo excited for you to try it out too!\\n\\nWe have prepared a [dedicated page](/developers/weaviate/client-libraries/python) with instructions on how to install and use the new client.»\\n[3] «---\\ntitle: \\'Verba: Building an Open Source, Modular RAG Application\\'\\nslug: verba-open-source-rag-app\\nauthors: [victoria, edward]\\ndate: 2024-03-07\\nimage: ./img/hero.png\\ntags: [\\'how-to\\', \\'concepts\\']\\ndescription: \\'Verba is an open source Retrieval Augmented Generation (RAG) application built using a modular, customizable architecture that makes it easy for anyone to use AI methods to get personalized answers on their own data. \\'\\n---\\n\\n![Hero image](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nimport example from \\'./img/example.mp4\\';\\nimport chunks from \\'./img/chunks.mp4\\';\\nimport ingestion from \\'./img/ingestion.mp4\\';\\n\\nHave you ever wished that you could ask a question and get a personalized, relevant answer without having to dig through pages of search results? That\\'s exactly what Retrieval Augmented Generation (RAG) lets you do. RAG applications search for relevant sources and create a personalized answer, using provided data sources instead of the general knowledge Large Language Models (LLMs) are trained on. But as cool as RAG is, creating a functional application can be overwhelming.»\\n[4] «We won\\'t get into the details of gRPC here, but if you are interested, you can read more about it [here](https://grpc.io/docs/what-is-grpc/introduction/). We will also be writing more about it in the future. ## Object typing in focus\\n\\n> **Highlights: Strong typing in queries, retrieval augmented generation, data operations, ...**\\n\\nWe\\'ve mentioned object typing a few times already, but let\\'s take a closer look at a few more concrete examples. Here you can see the new syntax for collection definition, queries and retrieval augmented generation, as well as types for each of thees tasks. To get started you can import the set of submodules like shown below, and use them as needed.»\\n[5] «However, the modular architecture also allows for customization of each part of the pipeline if you needed to adjust the system to further support different use cases. We have lots of resources already about Verba, including:\\n\\n- ⚙️ The open source GitHub repository: [https://github.com/weaviate/verba](https://github.com/weaviate/verba)\\n- 💻\\xa0A live demo ingested with Weaviate data, to answer your Weaviate-related questions: [https://verba.weaviate.io](https://verba.weaviate.io)\\n- 📹\\xa0An in-depth video of how to use Verba with your own data: [https://youtu.be/IiNDCPwmqF8](https://youtu.be/IiNDCPwmqF8?feature=shared)\\n\\n![What is RAG? Animation](./img/rag_flow.gif)\\n\\n## What is Retrieval Augmented Generation (RAG)? Retrieval Augmented Generation (RAG) is an advanced method to enhance traditional search techniques by using a Large Language Model (LLM) to help identify and summarize answers. Instead of having to manually comb through documents to find an answer, a RAG system allows you to retrieve relevant sources and return a response in natural, conversational language.»\\n\\nQuestion: What is Retrieval Augmented Generation and what are its common applications?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s break down the context to understand Retrieval Augmented Generation (RAG) and its common applications. First, the context mentions that RAG is a technique used in search engines and information retrieval systems. It is described as an advanced method that enhances traditional search techniques by leveraging a Large Language Model (LLM). The main idea behind RAG is to improve the accuracy and relevance of search results by combining information retrieval techniques with the power of LLMs. By using an LLM, RAG systems can identify and summarize answers from relevant sources, providing users with more concise and personalized responses. Now, let\\'s identify the common applications of RAG: - **Search Engines**: RAG is particularly useful for search engines, where the goal is to provide users with the most relevant and accurate results for their queries. By incorporating RAG techniques, search engines can offer more personalized and contextually relevant answers, enhancing the user experience. - **Question Answering Systems**: RAG is ideal for question-answering systems, where the goal is to provide direct answers to user queries. By retrieving relevant sources and using LLMs to generate responses, RAG systems can offer more accurate and contextually rich answers. - **Information Retrieval Applications**: Any application that involves retrieving information from a large corpus of data can benefit from RAG. This includes document retrieval systems, knowledge bases, and content recommendation engines. RAG enables these applications to provide more targeted and relevant results to users. - **Natural Language Processing (NLP) Tasks**: RAG can be applied to various NLP tasks beyond just question answering. It can be used for text summarization, information extraction, sentiment analysis, and language understanding tasks, where the ability to retrieve relevant context is crucial.\\n\\nAnswer: Retrieval Augmented Generation (RAG) is an advanced technique used in search engines and information retrieval systems. It enhances traditional search techniques by leveraging a Large Language Model (LLM) to identify and summarize relevant answers. RAG is particularly useful for search engines, question-answering systems, information retrieval applications, and various Natural Language Processing (NLP) tasks, providing more accurate, personalized, and contextually relevant responses to users.\\n\\n---\\n\\nContext:\\n[1] «More importantly, every tenant has a dedicated high-performance vector index providing query speeds as if the tenant was the only user on your cluster. With more features in the pipeline, such as tenant-specific rate limiting or tenant-specific replication factors, you can customize performance per tenant even further. ### GDPR-compliant and efficient deletes\\nWhen discussing solutions made for many users, our first intuition is to worry about how we onboard and serve them. But deleting them is equally important – both for technical and legal reasons. Take GDPR as an example.»\\n[2] «You can easily run with 50,000+ active shards per node. With just 20 nodes, you can support 1M concurrently active tenants. And with support for inactive tenants (coming soon - read below), there is no limit per node at all. ### Fast and efficient querying\\nYou don’t need to set a filter to restrict a query to a tenant. The simple addition of a tenant key is enough, and Weaviate will find the right tenant’s shard for you.»\\n[3] «From a performance perspective, you would build a giant monolithic vector index with potentially billions of vectors, yet you would only ever query a tiny fraction of it. With a median tenant storing between 1,000 and 100,000 objects, you would typically query less than 0.01% of the index. What a waste of resources. Additionally, dropping many tenants simultaneously would lead to expensive repair operations on the shared index. Resource isolation for tenants was also not possible.»\\n[4] «### Multi Tenancy\\n\\n[Multi tenancy](https://weaviate.io/developers/weaviate/manage-data/multi-tenancy) isolates data in individual tenants to avoid cross querying of search results. Each tenant is a separate shard in Weaviate, giving you high guarantees of isolation between data. This feature is ideal if you have different customers where each wants to load data into your enterprise application, but don’t want cross pollination of results between those tenants. The Weaviate multi tenancy scales to millions of tenants. If you’d like to learn more about our multi tenancy implementation, [read this blog post](https://weaviate.io/blog/multi-tenancy-vector-search) by our CTO, Etienne Dilocker.»\\n[5] «With Weaviate’s rebalancing features (coming soon), you can ensure tenants are distributed across nodes exactly how you want them to – or leave it to Weaviate to do it for you. ### Active and inactive tenants\\nWith traditional search and other vector search solutions, infrastructure is typically sized for the number of objects or vectors that could be served. But why should you pay for expensive compute and memory resources for users who aren’t currently active? Enabled by Weaviate’s strict isolation between tenants, Weaviate allows you to distinguish between active and inactive tenants. A user hasn’t been logged in for an hour?»\\n\\nQuestion: How does the system ensure fast query speeds for individual tenants and what upcoming features will allow further customization of tenant performance?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to focus on the parts of the context that discuss query speeds and tenant performance. We can then identify the current mechanisms in place and the future features that will enhance them.\\n\\nAnswer: The system ensures fast query speeds for individual tenants by providing each tenant with a dedicated high-performance vector index. This isolation means that queries are restricted to a specific tenant\\'s shard, allowing for quick and efficient retrieval of data. Upcoming features that will allow further customization of tenant performance include tenant-specific rate limiting and tenant-specific replication factors. These features will provide finer-grained control over how tenants utilize resources and interact with the system, ultimately resulting in even faster query speeds and improved performance for individual tenants.\\n\\n---\\n\\nContext:\\n[1] «The below `curl` command will back up all classes in **W1**, and call the backup `my-very-first-backup`. ```bash\\ncurl \\\\\\n-X POST \\\\\\n-H \"Content-Type: application/json\" \\\\\\n-d \\'{\\n     \"id\": \"my-very-first-backup\"\\n    }\\' \\\\\\nhttp://localhost:8080/v1/backups/filesystem\\n```\\n\\n:::note The `backup_id` must be unique. The ID value is used to create a subdirectory in the backup location, and attempting to reuse an existing ID will cause Weaviate to throw an error. Delete the existing directory if one already exists. :::\\n\\nNow try running `3_backup` yourself to back up data from **W1**.»\\n[2] «```bash\\nscripts/3_backup.sh\\n```\\n\\nIf you check the contents of the backup directory again, you should see a new directory called `my-very-first-backup` containing the backup data files. Restoring this data can be done with a similarly short piece of code. The `curl` command below will restore our backup:\\n```bash\\ncurl \\\\\\n-X POST \\\\\\n-H \"Content-Type: application/json\" \\\\\\n-d \\'{\\n     \"id\": \"my-very-first-backup\"\\n    }\\' \\\\\\nhttp://localhost:8090/v1/backups/filesystem/my-very-first-backup/restore\\n```\\n\\nTry running `4_restore` yourself to restore the **W1** backup data to **W2**. ```bash\\nscripts/4_restore.sh\\n```\\n\\nNow, check the schemas again for **W1** and **W2**. ```bash\\nscripts/0_query_instances.sh\\n```\\n\\nDo they *both* now contain the same schema?»\\n[3] «For example, the below `curl` command will restore only the `Author` class regardless of whether any other classes have been also included in `my-very-first-backup`. ```bash\\ncurl \\\\\\n-X POST \\\\\\n-H \"Content-Type: application/json\" \\\\\\n-d \\'{\\n     \"id\": \"my-very-first-backup\",\\n     \"include\": [\"Author\"]\\n    }\\' \\\\\\nhttp://localhost:8090/v1/backups/filesystem/my-very-first-backup/restore\\n```\\n\\nDelete everything in **W2** first with `8_delete_w2`, and try out the partial restore with `4a_partial_restore`. ```bash\\nscripts/8_delete_w2.sh\\nscripts/4a_partial_restore.sh\\n```\\n\\nYou should see that **W2** will only contain one class even though its data was restored from a backup that contains multiple classes. The restore function allows you to restore a class as long as the target Weaviate instance does not already contain that class. So if you run another operation to restore the `Book` class to **W2**, it will result in an instance containing both `Author` and `Book` classes.»\\n[4] «For example, you can create a backup called **first_backup** and push it to **GCS**, like this:\\n\\n```js\\nPOST /v1/backups/gcs/\\n{\\n  \"id\": \"first_backup\"\\n}\\n```\\n\\nThen, you can check the backup status by calling:\\n\\n```js\\nGET /v1/backups/gcs/first_backup\\n```\\n\\n### Restore\\nTo restore a backup, you can call:\\n\\n```js\\nPOST /v1/backups/{store}/{backup_id}/restore\\n```\\n\\nSo, using our previous example, you can restore the **first_backup**, like this:\\n\\n```js\\nPOST /v1/backups/gcs/first_backup/restore\\n```\\n\\nYou can also, check the status of an ongoing restoration by calling:\\n\\n```js\\nGET /v1/backups/gcs/first_backup/restore\\n```\\n\\n### Cross-cloud\\nHere is one interesting thing that you might not have noticed. You can use this setup to run Weaviate with one cloud provider but then store and restore backups to/from another cloud provider. So, for example, you can run Weaviate on AWS and use GCS for your backup needs. How cool is that? ### Class backups\\nYou can also create backups for specific classes or select which classes you want to restore.»\\n[5] «### Asynchronous operations\\nIn some cases, Weaviate\\'s response to your `backup` or `restore` request may have  `\"status\":\"STARTED\"`.<br/>\\n*Isn\\'t it interesting that the status was not indicative of a completion?*\\n\\nThat is because Weaviate\\'s backup operation can be initiated and monitored asynchronously. This means that you don\\'t need to maintain a connection to the server for the operation to complete. And you can look in on the status of a restore operation with a command like:\\n```bash\\ncurl http://localhost:8090/v1/backups/filesystem/my-very-first-backup/restore\\n```\\n\\nWeaviate remains available for read and write operations while backup operations are ongoing. And you can poll the endpoint to check its status, without worrying about any potential downtime. Check out `3a_check_backup_status.sh` and `4b_check_restore_status.sh` for examples of how to query **W1** for the backup status, or **W2** for the restore status respectively.»\\n\\nQuestion: How do you create a unique backup called `my-very-first-backup` for classes in W1 using a curl command?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We can refer to the provided context to identify the steps to create a unique backup called \\'my-very-first-backup\\' for classes in W1 using a curl command.\\n\\nAnswer: To create a unique backup called \\'my-very-first-backup\\' for classes in W1, you can use the following curl command: ```bash curl \\\\ -X POST \\\\ -H \"Content-Type: application/json\" \\\\ -d \\'{ \"id\": \"my-very-first-backup\" }\\' \\\\ http://localhost:8080/v1/backups/filesystem ``` This command sends a POST request to the Weaviate instance at \\'http://localhost:8080\\' to create a backup with the unique ID \\'my-very-first-backup\\'. The \\'-X POST\\' option specifies the HTTP method as POST, and the \\'-d\\' option provides the request body, which includes the \\'id\\' field set to the desired backup name. The \\'-H \"Content-Type: application/json\"\\' option sets the content type of the request to JSON. By executing this command, you initiate the process of creating a backup of all classes in W1 with the specified unique name.\\n\\n---\\n\\nContext:\\n[1] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\\nlet them do it for you.\"\\n\\n## Support for Hugging Face Inference API in Weaviate\\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\\n> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\\n[2] «---\\ntitle: Support for Hugging Face Inference API in Weaviate\\nslug: hugging-face-inference-api-in-weaviate\\nauthors: [sebastian]\\ndate: 2022-09-27\\ntags: [\\'integrations\\']\\nimage: ./img/hero.png\\ndescription: \"Running ML Model Inference in production is hard. You can use Weaviate – a vector database – with Hugging Face Inference module to delegate the heavy lifting.\"\\n---\\n![Support for Hugging Face Inference API in Weaviate](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nVector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data – text, image, audio, etc. – into vectors and use that for context-based queries) and many more use cases. All of these use cases require `Machine Learning model inference` – a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) – which is a compute-heavy process.»\\n[3] «Then, you just run the queries, as per usual:\\n```javascript\\nnearText = {\\n    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\\n    \"distance\": 0.6,\\n}\\n\\nresult = (\\n    client.query\\n    .get(\"Notes\", [\\n        \"name\",\\n        \"comment\",\\n        \"_additional {certainty distance} \"])\\n    .with_near_text(nearText)\\n    .do()\\n)\\n```\\n\\n## Summary\\n> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\\nCheck out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.»\\n[4] «Now, that\\'s a lot of new models. 😉\\n\\n#### How this works\\nThe way the module works, Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. You need a `Hugging Face API Token` to use the Hugging Face module. You can [request it here](https://huggingface.co/login?next=%2Fsettings%2Ftokens).»\\n[5] «On the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:\\n\\n```yaml\\nDEFAULT_VECTORIZER_MODULE: text2vec-huggingface\\nENABLE_MODULES: text2vec-huggingface\\n```\\n\\n## How to get started\\n\\n:::note\\nThis article is not meant as a hands-on tutorial. For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). :::\\n\\nThe overall process to use a Hugging Face module with Weaviate is fairly straightforward. ![Recipe for using the Hugging Face module](./img/how-to-get-started-recipe.png)\\nIf this was a cooking class and you were following a recipe.»\\n\\nQuestion: How can I use Hugging Face modules with Weaviate without running the Inference API myself?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We know from the context that Weaviate offers a way to use Hugging Face modules without having to run the Inference API directly. The process is described as straightforward, and we can identify the key steps to follow: - Weaviate includes a Hugging Face module that provides support for Hugging Face Inference. This is available from version 1.15 onwards. - The Hugging Face module delegates the model inference tasks to the Hugging Face Inference service. - To use the module, one needs to set \\'text2vec-huggingface\\' as the default vectorizer in the configuration. - An API token from Hugging Face is also required, which can be obtained from their website. - With these settings in place, Weaviate coordinates the data imports, updates, and queries, communicating with the Hugging Face Inference API on the user\\'s behalf.\\n\\nAnswer: To use Hugging Face modules with Weaviate without running the Inference API yourself, follow these steps: 1. Ensure you are using Weaviate version 1.15 or newer. 2. Set \\'text2vec-huggingface\\' as the default vectorizer in your Weaviate configuration. 3. Obtain a Hugging Face API token from the Hugging Face website. 4. Provide your API token when setting up the Hugging Face module in Weaviate. 5. With these settings, Weaviate will handle the communication with the Hugging Face Inference API, allowing you to focus on your application\\'s requirements.\\n\\n---\\n\\nContext:\\n[1] «---\\ntitle: Better automated testing with Embedded Weaviate\\nslug: automated-testing\\nauthors: [dan]\\ndate: 2023-07-18\\nimage: ./img/hero.png\\ntags: [\\'how-to\\']\\ndescription: \"Learn how to make testing less of a chore with Embedded Weaviate, and other tips for better automated testing.\"\\n\\n---\\n\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\nimport FilteredTextBlock from \\'@site/src/components/Documentation/FilteredTextBlock\\';\\nimport PyCode from \\'!!raw-loader!/_includes/code/automated-testing.py\\';\\nimport TSCode from \\'!!raw-loader!/_includes/code/automated-testing.ts\\';\\nimport { DownloadButton } from \\'/src/theme/Buttons\\';\\n\\n![Automated testing for Weaviate applications](./img/hero.png)\\n\\nAs a software engineer with experience in test automation, I firmly believe in [Test-Driven Development](https://en.wikipedia.org/wiki/Test-driven_development), and more specifically, incorporating [integration testing](https://en.wikipedia.org/wiki/Integration_testing) from the very early stages of developing an application. <!-- truncate -->\\n\\nBut you probably know that writing tests is quite a task in itself, and in many cases running them can also be a chore. For example, the test suite may need to set up and tear down a separate service such as a database, which can be time-consuming and error-prone. I\\'m here to tell you that it doesn\\'t have to be that way. In this article, I\\'ll show you how to make testing easier with [Embedded Weaviate](/developers/weaviate/installation/embedded), and other tips for better automated testing.»\\n[2] «<!-- truncate -->\\n\\n\\n## 1. Install the client library\\n\\nThe Python and TypeScript client libraries support running Weaviate embedded on Linux, and starting with versions 3.21.0 and 1.2.0 respectively, on macOS as well. <Tabs groupId=\"languages\">\\n  <TabItem value=\"py\" label=\"Python\">\\n\\n  ```bash\\n  pip install weaviate-client  --upgrade\\n  ```\\n\\n  </TabItem>\\n\\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\\n\\n  ```bash\\n  npm install weaviate-ts-embedded typescript ts-node jest  # also install support for TypeScript and Jest testing\\n  ```\\n\\n  </TabItem>\\n</Tabs>\\n\\n\\n## 2. Run the code\\n\\n<Tabs groupId=\"languages\">\\n  <TabItem value=\"py\" label=\"Python\">\\n\\n  Save as `embedded.py` and run `python embedded.py`:\\n  <br/>\\n\\n  <FilteredTextBlock\\n    text={PyCode}\\n    startMarker=\"# START 10lines\"\\n    endMarker=\"# END 10lines\"\\n    language=\"py\"\\n  />\\n  </TabItem>\\n\\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\\n\\n  Save as `embedded.ts` and run `node --loader=ts-node/esm embedded.ts`:\\n  <br/>\\n\\n  <FilteredTextBlock\\n    text={TSCode}\\n    startMarker=\"// START 10lines\"\\n    endMarker=\"// END 10lines\"\\n    language=\"js\"\\n  />\\n  </TabItem>\\n</Tabs>\\n\\n\\n## <i class=\"fa-solid fa-screwdriver-wrench\"></i> How does this work? Essentially, what happens behind the scenes is that the client library downloads the server binary, spawns it in a separate process, connects to it, then terminates it on exit.»\\n[3] «With Embedded Weaviate, you can just run the test suite and be done with it. But Embedded Weaviate is not the only way to make testing easier. In the following sections, we\\'ll look at other ways to make testing easier, and how to make the most of Embedded Weaviate. ## Scoping tests\\n\\nWhile you may be familiar with tests and integration tests in general, here are some specific suggestions for Weaviate-powered applications:\\n* **Whether to test search quality**: This depends primarily on the model used for vectorization, such as by a [Weaviate vectorizer module](/developers/weaviate/modules/retriever-vectorizer-modules). We suggest evaluating models separately, but not tested as a part of the application.»\\n[4] «* **Focus on interactions with the inference provider**: Search itself is a core Weaviate functionality that we can trust. So, we suggest any integration tests focus on the interaction with the inference provider. For example,\\n  * is the vectorization model the expected one? * if switching to a different inference provider or model, does the application still function as expected? * **Other common issues to test** include:\\n  * Connection or authentication issues with the inference provider\\n  * Incomplete or incorrect data imports\\n  * Specifying the vector correctly when [bringing your own vectors](/developers/weaviate/starter-guides/custom-vectors)\\n  * Data definition issues, like invalid class names, properties, or data types\\n\\n\\n## Testing with embedded Weaviate\\n\\n### Set up\\n\\n[Embedded Weaviate](/developers/weaviate/installation/embedded) lets us spawn a Weaviate server instance from the client, and automatically tear it down when the client terminates.»\\n[5] «And doing so, you might just discover the hidden gem that is the value provided by adding tests to your applications. ## Testing and Weaviate\\n\\nIn this article, we will focus on [integration tests](https://en.wikipedia.org/wiki/Integration_testing). Integrated testing is an important part of the development process, and especially so for complex applications. Weaviate-based apps usually fall in this category. For one, Weaviate must interact with the application in a variety of ways.»\\n\\nQuestion: How does Embedded Weaviate simplify the setup process for testing the Weaviate application?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We know from the context that Embedded Weaviate is a feature that allows users to spawn a Weaviate server instance directly from the client. This suggests that it simplifies the setup process by eliminating the need for a separate server instance. Let\\'s continue reading to confirm and elaborate on this.\\n\\nAnswer: Embedded Weaviate simplifies the setup process for testing a Weaviate application by allowing users to spawn a Weaviate server instance directly from the client. This eliminates the need for a separate server instance, streamlining the testing process. With Embedded Weaviate, users can run the test suite without the hassle of setting up and tearing down a separate service, making it more convenient and efficient to test Weaviate-based applications.\\n\\n---\\n\\nContext:\\n[1] «You can reach us through:\\n\\n- Join our dedicated Hacktoberfest channel in our [Discourse community forum](https://forum.weaviate.io/), where we\\'re ready to answer your questions. - Join our dedicated #hacktoberfest channel in our [Weaviate Community Slack](https://weaviate.slack.com/) channel to stay connected and receive real-time support. - Join our [newsletter](https://newsletter.weaviate.io/) and follow us on [Twitter](https://twitter.com/weaviate_io) and [Linkedin](https://www.linkedin.com/company/weaviate-io/mycompany/verification/) to receive updates. - Stay up to date with Weaviate\\'s development by exploring the [Weaviate GitHub Repository](https://github.com/weaviate/weaviate). Don’t forget to give us a ⭐️ while you are there!\\n\\n:::info Pro Tip\\nShare your process online and tag us on [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://nl.linkedin.com/company/weaviate-io).»\\n[2] «- 4th October 2023 (20:00-21:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232574048361254) with Zain Hasan, Senior Dev Advocate @ Weaviate\\n- 5th October 2023 (17:00-18:00 UTC+2): [NEW Python `Collections` Client API Preview](https://form.jotform.com/232683153137859) with JP Hwang - Educator @ Weaviate\\n- 18th October 2023 (15:00-16:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232602295283859) with JP Hwang - Educator @ Weaviate\\n\\nExpand your knowledge with these supplementary resources:\\n\\n- [Weaviate YouTube Channel](https://www.youtube.com/@Weaviate) 📺\\n- [Weaviate Blog](https://weaviate.io/blog) 📰\\n- [Weaviate Recipes](https://github.com/weaviate/recipes) 🍳\\n\\n\\n## What\\'s in it for you? The repositories you contribute to participate in [Hacktoberfest](https://hacktoberfest.com/) and are a great opportunity for your first-ever Hacktoberfest PR. We will also assist you throughout the whole process. You might even receive some swag in the end. ## Connect with the Weaviate Community!\\n\\nTo make your Hacktoberfest experience successful, connect with the Weaviate community for collaboration and assistance.»\\n[3] «Share what you build with Weaviate in [Slack](https://weaviate.slack.com/), on our [Forum](https://forum.weaviate.io/), or on socials. ## Embracing Open Source and Sharing Knowledge\\n\\nAs AI accelerated throughout the year with ever-new innovations popping up, so did the community\\'s curiosity to learn and share knowledge in that area. As an open-source solution, **community** is a foundational pillar of Weaviate. ### [Hacktoberfest](https://weaviate.io/blog/hacktoberfest-2023)\\n\\nCelebrating the spirit of **open source**, we participated in our first [Hacktoberfest](https://hacktoberfest.com/) this October, which was organized by our very own **[Leonie Monigatti](https://www.linkedin.com/in/804250ab/)**! This global event, aimed at engineers and machine learning enthusiasts, fosters collaboration and contributions to open-source technology. Participants who had four pull requests (PRs) accepted between October 1 and 31, 2023, earned a unique digital reward and some Weaviate Merch! Contributions varied in scope, ranging from minor, non-coding inputs to more substantial technical improvements.»\\n[4] «Use the hashtag #hacktoberfest2023 for increased visibility. :::\\n\\n\\n## FAQ\\n\\n- **Will this count towards Hacktoberfest?** Yes, it definitely does! If your PR/MR is created between **October 1** and **October 31** (in any time zone, UTC-12 thru UTC+14), we will add the \"HACKTOBERFEST-ACCEPTED\" label to it. - **Where do I get help?** For any questions or assistance, contact us on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels. - **I have a cool contribution idea. Can I still participate?** Awesome! Connect with us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels and we will figure it out.»\\n[5] «---\\ntitle: Hacktoberfest 2023 - Celebrating Open Source with Weaviate\\nslug: hacktoberfest-2023\\nauthors: [leonie]\\ndate: 2023-10-02\\ntags: []\\nimage: ./img/weaviate-hacktoberfest-2023.png\\ndescription: \"Join us in celebrating Hacktoberfest, a month-long celebration of open source!\"\\n---\\nimport hacktober_demo from \\'./img/hacktoberfest_2023_demo.mp4\\';\\n\\n![Celebrating Hacktoberfest 2023 with Weaviate](./img/weaviate-hacktoberfest-2023.png)\\n\\nAt [Weaviate](https://weaviate.io/), we love open source! Join us in celebrating [Hacktoberfest](https://hacktoberfest.com/), a month-long celebration of open source!\\nParticipants with four pull requests (PR) accepted between **October 1 - 31, 2023** will receive a unique digital reward [from Hacktoberfest](https://hacktoberfest.com/participation/). ## The Task\\n\\nWelcome to our demo sprint!\\n\\nIt is aimed at onboarding engineers and machine learning practitioners to open-source. In our [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page, we showcase what you can do with a [vector database](https://weaviate.io/blog/what-is-a-vector-database). Some of these demos have received more love than others recently, and we want to give them a little polish. ![Untitled](./img/weaviate-demos.png)\\n\\nThe gallery contains demos using Weaviate in different states.»\\n\\nQuestion: How can I participate in the Weaviate community\\'s Hacktoberfest event and stay updated on their developments?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We can break down the question into two parts: \"How can I participate in the Weaviate community\\'s Hacktoberfest event?\" and \"How can I stay updated on their developments?\" To participate in the Weaviate community\\'s Hacktoberfest event: - Join their dedicated Hacktoberfest channel on the Weaviate Community Forum (Discourse) or their Slack channel to connect with the community and seek assistance. - Contribute to the Weaviate repositories during October 1-31, 2023, and aim for four accepted pull requests to earn the rewards. - Share your process and what you build with Weaviate on their Slack, Forum, or social media platforms (Twitter, LinkedIn). To stay updated on the Weaviate community\\'s developments: - Follow their newsletter, Twitter, and LinkedIn accounts for updates. - Explore the Weaviate GitHub Repository to stay up to date with the project\\'s development and contribute if you wish. - Utilize their supplementary resources: the Weaviate YouTube Channel, Blog, and Recipes repository.\\n\\nAnswer: To participate in the Weaviate community\\'s Hacktoberfest event, join their dedicated channels on Discourse and Slack, contribute to their repositories during the specified period, and share your process and creations. To stay updated on their developments, follow their newsletter and social media accounts, explore their GitHub repository, and utilize their supplementary resources.\\n\\n---\\n\\nContext:\\n[1] «On the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:\\n\\n```yaml\\nDEFAULT_VECTORIZER_MODULE: text2vec-huggingface\\nENABLE_MODULES: text2vec-huggingface\\n```\\n\\n## How to get started\\n\\n:::note\\nThis article is not meant as a hands-on tutorial. For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). :::\\n\\nThe overall process to use a Hugging Face module with Weaviate is fairly straightforward. ![Recipe for using the Hugging Face module](./img/how-to-get-started-recipe.png)\\nIf this was a cooking class and you were following a recipe.»\\n[2] «Then, you just run the queries, as per usual:\\n```javascript\\nnearText = {\\n    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\\n    \"distance\": 0.6,\\n}\\n\\nresult = (\\n    client.query\\n    .get(\"Notes\", [\\n        \"name\",\\n        \"comment\",\\n        \"_additional {certainty distance} \"])\\n    .with_near_text(nearText)\\n    .do()\\n)\\n```\\n\\n## Summary\\n> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\\nCheck out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.»\\n[3] «You would need the following ingredients:\\n* Raw Data\\n* Hugging Face API token – which you can request from [their website](https://huggingface.co/settings/tokens)\\n* A working Weaviate instance with the `text2vec-huggingface` enabled\\n\\nThen you would follow these steps. ### Step 1 – initial preparation – create schema and select the hf models\\nOnce you have a Weaviate instance up and running. Define your schema (standard stuff – pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class. This is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\\n```javascript\\n{\\n    \"class\": \"Notes\",\\n    \"moduleConfig\": {\\n        \"text2vec-huggingface\": {\\n            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\\n            ...»\\n[4] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\\nlet them do it for you.\"\\n\\n## Support for Hugging Face Inference API in Weaviate\\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\\n> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\\n[5] «Each `text2vec-*` module uses an external API (like `text2vec-openai` or `text2vec-huggingface`) or a local instance like `text2vec-transformers` to produce a vector for each object. Let\\'s try vectorizing data with the `text2vec-cohere` module. We will be using data from `tiny_jeopardy.csv` [available here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain) containing questions from the game show Jeopardy. We\\'ll just use a few (20) questions here, but the [full dataset on Kaggle](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) includes 200k+ questions. Load the data into a Pandas dataframe, then populate Weaviate like this:\\n\\n```python\\nclient.batch.configure(batch_size=100)  # Configure batch\\nwith client.batch as batch:\\n    for i, row in df.iterrows():\\n        properties = {\\n            \"question\": row.Question,\\n            \"answer\": row.Answer\\n        }\\n        batch.add_data_object(properties, \"Question\")\\n```\\n\\nThis should add a series of `Question` objects with text properties like this:\\n\\n```text\\n{\\'question\\': \\'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger\\',\\n \\'answer\\': \"McDonald\\'s\"}\\n```\\n\\nSince we use the `text2vec-cohere` module to vectorize our data, we can query Weaviate to find data objects most similar to any input text.»\\n\\nQuestion: How do you set up a Weaviate instance with the `text2vec-huggingface` module?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We can refer to the provided context, which outlines the process of setting up the Weaviate instance with the \\'text2vec-huggingface\\' module. Firstly, it is mentioned that to use the Hugging Face module in Weaviate open source, one needs to set \\'text2vec-huggingface\\' as the default vectorizer in the configuration file (usually YAML). This is a crucial step to ensure Weaviate uses the correct module for vectorization. Secondly, the context provides a step-by-step guide, comparing the process to following a recipe in a cooking class. It emphasizes the need for specific \\'ingredients\\', which include raw data, a Hugging Face API token, and a working Weaviate instance with the \\'text2vec-huggingface\\' module enabled. Thirdly, the guide instructs users to define their schema, including class names, properties, and data types. This is a standard procedure when working with structured data in Weaviate. As part of the schema definition, users need to specify which Hugging Face model they want to use for each schema class. This is achieved by adding a \\'moduleConfig\\' property to the schema, where the desired model name is provided. Finally, the context mentions that Weaviate \\'v1.15\\' and newer versions include the Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. This implies that ensuring the Weaviate instance is updated to the latest version is essential for accessing the Hugging Face module. With these steps in mind, we can now formulate a response.\\n\\nAnswer: To set up a Weaviate instance with the \\'text2vec-huggingface\\' module, follow these steps: 1. Ensure you have the necessary ingredients: raw data to work with, a Hugging Face API token (obtainable from their website), and a functioning Weaviate instance. 2. Configure your Weaviate instance by setting \\'text2vec-huggingface\\' as the default vectorizer in the configuration file. 3. Define your schema, including class names, properties, and data types. As part of this step, specify the desired Hugging Face model for each schema class by adding a \\'moduleConfig\\' property to the schema definition. 4. Update your Weaviate instance to the latest version (\\'v1.15\\' or newer) to access the built-in Hugging Face module, which facilitates Hugging Face Inference straight from the vector database. By following these steps, you can effectively set up Weaviate to utilize the \\'text2vec-huggingface\\' module for vectorization and model inference tasks.\\n\\n---\\n\\nContext:\\n[1] «`go_memstats_heap_inuse_bytes` should always be present even with an empty schema. ![datadog summary](./img/datadog-summary.png)\\n\\n\\n## Key Metrics\\n\\nBelow are some key Weaviate metrics to monitor. Standard CPU, Disk, Network metrics are also useful as are [Kubernetes\\nevents](https://grafana.com/blog/2023/01/23/how-to-use-kubernetes-events-for-effective-alerting-and-monitoring/). Note that some Weaviate metrics will not appear until an operation has occurred (for instance batch operations). ### Heap Usage\\n\\nFor heap usage, the expectation is the memory will have a standard jagged pattern underload but that memory will drop periodically\\ndue to the Go garbage collection.»\\n[2] «```\\ngo_memstats_heap_inuse_bytes{job=\"weaviate\"}\\n```\\n### Dashboards\\n\\nOne benefit of this approach is that you can now reuse the existing Weaviate Grafana dashboards. Steps to import these dashboards:\\n\\n1\\\\. Download and import the [preexisting dashboards](https://github.com/weaviate/weaviate/tree/master/tools/dev/grafana/dashboards). 2\\\\. If you\\'re using Grafana Cloud hosted Prometheus you will need to patch the dashboards to change the datasource uid to be `grafanacloud-prom` as below.»\\n[3] «If memory is not dropping and is staying very close to the [GOMEMLIMIT](/blog/gomemlimit-a-game-changer-for-high-memory-applications), you may need to increase resources. ```\\ngo_memstats_heap_inuse_bytes\\n```\\n\\n### Batch Latency\\n\\nBatch latency is important as batch operations are the most efficient way to write data to\\nWeaviate. Monitoring this can give an indication if there is a problem with indexing data. This metric has a label `operation` which\\nallows you to see how long objects, vectors, and inverted index sub operations take. If you are using a [vectorizer module](/developers/weaviate/modules/retriever-vectorizer-modules) you will see additional latency due to the overhead of sending data to the module.»\\n[4] «In other words, the more the user imports, the more the heap will grow. That makes it quite unpredictable to know what live heap we will end up with eventually. In other words, Weaviate – like any other memory-intensive Go application – is a perfect candidate to benefit from GOMEMLIMT. It also comes out of the box with [Prometheus-compatible monitoring](/developers/weaviate/configuration/monitoring), so it should be easy to visualize the heap usage. For this experiment, we will use the latest Weaviate version compiled with Go 1.19.»\\n[5] «The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually. All this means that you can also use the schema to tweak Weaviate\\'s vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level.»\\n\\nQuestion: Should the `go_memstats_heap_inuse_bytes` metric be present in Weaviate even if the schema is empty, and what is its expected behavior under load?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to understand the context provided, which discusses Weaviate, a tool for managing and monitoring data. We also need to refer to the specific statements related to the \\'go_memstats_heap_inuse_bytes\\' metric. The context mentions that the \\'go_memstats_heap_inuse_bytes\\' metric should always be present, even with an empty schema. It is listed as one of the key Weaviate metrics to monitor. This indicates that this particular metric is essential for understanding the performance and behavior of Weaviate. Under load, the expected behavior of the \\'go_memstats_heap_inuse_bytes\\' metric is described as a standard jagged pattern. This suggests that memory usage will fluctuate, but it should also drop periodically due to Go garbage collection. This garbage collection process helps to manage memory usage and prevent excessive memory consumption.\\n\\nAnswer: Yes, the \\'go_memstats_heap_inuse_bytes\\' metric should be present in Weaviate even if the schema is empty. Under load, this metric is expected to exhibit a jagged pattern, with periodic drops in memory usage due to Go garbage collection.\\n\\n---\\n\\nContext:\\n[1] «One of the primary reasons is the computational cost associated with processing and storing longer sequences of tokens. The longer the sequence, the more memory and processing power required to operate on it, which can be a significant challenge for even the most powerful computing systems. The relatively long input window of LLMs is what drives the integration with semantic search. For example, we can use this entire blog post as input in addition to questions if we want the LLM to answer questions such as “What are LLM Chains?” However, when we want to give the LLM an entire book or pages of search results, we need more clever techniques to decompose this task. This is where the `CombineDocuments` chain comes to play! Note, that one method is not better than another, and the performance is dependent solely on your application.»\\n[2] «Then we take this new query and hit the Weaviate vector database to get context to answer the question. The ChatVectorDB chain we use has a default value of k = 4 search results, if we use longer search results we will need another CombineDocuments chain here as well! With the 4 search results, we answer the question with this final prompt:\\n\\n```python\\nPrompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer. {context}\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\n```\\n\\nHopefully this was a nice look under the hood of how the ChatVectorDB chain works. Let’s get into how we can use this with Weaviate!\\n\\n### The Code\\n\\nIf this is your first time using Weaviate, please check out the [Quickstart tutorial](/developers/weaviate/quickstart).»\\n[3] «We will look at the [ChatVectorDB chain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db), it lets you build an LLM that stores chat history and retrieves context from Weaviate to help with generation. To begin, the chat history in this chain uses the `stuff` configuration of CombineDocuments. This means we take as much of the chat history as we can fit in our context window and use it for a query reformulation prompt. The prompt is as follows:\\n\\n```python\\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\"\"\"\\n```\\n\\nWe put the chat_history and the latest user input in the curly bracket syntax.»\\n[4] «Further reading [Why Is Vector Search So Fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)\\n\\n## Use Cases of Vector Databases\\n\\nVector databases’ search capabilities can be used in various applications ranging from classical Machine Learning use cases, such as natural language processing, computer vision, and recommender systems, to providing long-term memory to LLMs in modern applications. The **most popular use case of vector search engines is naturally for search**. Because a vector database can help find similar objects, it is predestined for applications where you might want to find similar products, movies, books, songs, etc. That’s why vector search engines are also **used in recommendation systems** as a restated task of search. With the rise of LLMs, **vector databases have already been used to enhance modern Generative AI applications**.»\\n[5] «Since around [80% of all data is unstructured](https://www.forbes.com/sites/forbestechcouncil/2019/01/29/the-80-blind-spot-are-you-ignoring-unstructured-organizational-data/), it is hard to actually search and retrieve insights from most of the data. The Weaviate vector database unlocks the potential of unstructured data. Searching by fuzzy terms and classification of rich data like free text becomes possible with Weaviate. It uses AI-driven indexing and search technologies to enable real-time text processing. With machine learning methods, Weaviate automatically classifies texts.»\\n\\nQuestion: Why are techniques like the `CombineDocuments` chain necessary for processing large texts with LLMs?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s break down the context to identify the key points that explain the need for techniques like the `CombineDocuments` chain. The context mentions that one of the primary challenges of processing large texts with LLMs is the computational cost and memory requirements. As the input sequence length increases, so do the processing power and memory needs, which can be a significant hurdle even for powerful computing systems. The `CombineDocuments` chain is introduced as a solution to this problem. It is a technique used to decompose large texts into manageable segments that can be more easily processed by LLMs. By taking portions of the chat history and user input, the `CombineDocuments` chain reformulates the question into a standalone query that can be answered using a smaller context window. Additionally, the context mentions that vector databases, such as Weaviate, are used in conjunction with LLMs to enhance their capabilities. Vector databases excel at searching and retrieving similar objects, making them useful for applications beyond just search, including recommendation systems and Generative AI. Weaviate, in particular, enables the search and retrieval of insights from unstructured data by using AI-driven indexing and search technologies.\\n\\nAnswer: Techniques like the `CombineDocuments` chain are necessary for processing large texts with LLMs because they help address the computational cost and memory requirements associated with longer input sequences. By decomposing the task and reformulating queries, the `CombineDocuments` chain enables LLMs to handle larger contexts without overwhelming their resource limitations. Additionally, vector databases like Weaviate further enhance the capabilities of LLMs by providing efficient search and retrieval capabilities for unstructured data.\\n\\n---\\n\\nContext:\\n[1] «What does replication get us? A big one is *availability*. With no replication, any node being down will make its data unavailable. But in a Kubernetes setup composed of say, three Weaviate nodes (three Kubernetes “pods”) and a replication factor of three, you can have any one of the three nodes down and still reach consensus. This reflects Weaviate’s leaderless replication architecture, meaning any node can be down without affecting availability at a cluster level as long as the right data is available somewhere.»\\n[2] «<details>\\n  <summary>Optional: Try it yourself (with minikube)</summary>\\n\\nYou can try running a local, multi-node Weaviate cluster with `minikube`, which can conveniently run a local Kubernetes cluster. We note that deploying Weaviate on a cloud provider’s kubernetes service follows a similar process. <br/>\\n\\nFirst, install `minikube` and `helm` for your system by following these guides ([minikube](https://minikube.sigs.k8s.io/docs/start), [helm](https://helm.sh/docs/intro/install)). We also recommend installing `kubectl` ([by following this guide](https://kubernetes.io/docs/tasks/tools/#kubectl)). <br/>\\n\\nOnce minikube is installed, start a three-node minikube cluster by running the following from the shell:\\n\\n```shell\\nminikube start --nodes 3\\n```\\n\\nOnce the nodes have been created, you should be able to interact with them through the `kubectl` command-line tool.»\\n[3] «Full dynamic scalability will be added in a future release. # highlight-start\\nreplicas: 3\\n# highlight-end\\n... ```\\n\\nYou can now deploy Weaviate in this configuration by running:\\n\\n```shell\\nkubectl create namespace weaviate\\n\\nhelm upgrade --install \\\\\\n  \"weaviate\" \\\\\\n  weaviate/weaviate \\\\\\n  --namespace \"weaviate\" \\\\\\n  --values ./values.yaml\\n```\\n\\nThis will deploy the Weaviate clusters. You can check the status of the deployment by running:\\n\\n```shell\\nkubectl get pods -n weaviate\\n```\\n\\nThis should produce an output similar to the following:\\n\\n```shell\\nNAME         READY   STATUS    RESTARTS   AGE\\nweaviate-0   1/1     Running   0          3m00s\\nweaviate-1   1/1     Running   0          2m50s\\nweaviate-2   1/1     Running   0          2m40s\\n```\\n\\nNow, you need to expose the Weaviate service to the outside world - i.e. to your local machine. You can do this by running:\\n\\n```shell\\nminikube service weaviate --namespace weaviate\\n```\\n\\nThis should show an output similar to the following that shows the URL to access the Weaviate cluster:\\n\\n```shell\\n|-----------|----------|-------------|------------------------|\\n| NAMESPACE |   NAME   | TARGET PORT |          URL           |\\n|-----------|----------|-------------|------------------------|\\n| weaviate  | weaviate |             | http://127.0.0.1:54847 |\\n|-----------|----------|-------------|------------------------|\\n```\\n\\nAnd it should also open a browser window showing the list of Weaviate endpoints.»\\n[4] «To see a list of the newly spun up nodes, run:\\n\\n```shell\\nkubectl get nodes -o wide\\n```\\n\\nYou should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\\n\\n```shell\\nNAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\\nminikube       Ready    control-plane   134m   v1.27.3   192.168.49.2   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\\nminikube-m02   Ready    <none>          134m   v1.27.3   192.168.49.3   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\\nminikube-m03   Ready    <none>          133m   v1.27.3   192.168.49.4   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\\n```\\n\\nNow, add the Weaviate helm repository to your local helm configuration by running:\\n\\n```shell\\nhelm repo add weaviate https://weaviate.github.io/weaviate-helm\\n```\\n\\nAnd save the default configuration with:\\n\\n```shell\\nhelm show values weaviate/weaviate > values.yaml\\n```\\n\\nEdit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it. ```yaml\\n... # Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\\n# to cases where no data is imported yet. Scaling down after importing data may\\n# break usability.»\\n[5] «<br/>\\n\\nJust like that, you’ve got a multi-node Weaviate cluster. Remember that when you create a class, you must have replication enabled. You can do this by adding the `replicationConfig` parameter to the collection definition, like so:\\n\\n```json\\n{\\n  \"class\": \"ClassWithReplication\",\\n  \"properties\": [\\n    {\\n      \"name\": \"exampleProperty\",\\n      \"dataType\": [\"text\"]\\n    }\\n  ],\\n  // highlight-start\\n  \"replicationConfig\": {\\n    \"factor\": 3\\n  }\\n  // highlight-end\\n}\\n```\\n\\nAnd when you insert objects into `ClassWithReplication`, they will be replicated across the three nodes. You can verify this by visiting the `/nodes` endpoint, which will verify that each node contains the same number of objects. </details>\\n\\n## Benefits of replication\\n\\nSo, let’s cover the implications of doing this.»\\n\\nQuestion: How can I check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas?\\n\\nReasoning: Let\\'s think step by step in order to answer the question. We can refer to the provided context, which outlines the process of setting up a local, multi-node Weaviate cluster using \\'minikube\\' and \\'helm\\'. First, we need to ensure that we have the necessary tools installed, including \\'minikube\\', \\'helm\\', and \\'kubectl\\'. Then, we can start a three-node minikube cluster using the provided command. To check the status and details of the nodes in the cluster, we can use the \\'kubectl get nodes\\' command, which will provide an overview of the nodes, including their status, roles, ages, and IP addresses. Once we have confirmed that the nodes are up and running, we can proceed with deploying Weaviate. We add the Weaviate helm repository to our local configuration and save the default values to a \\'values.yaml\\' file. We then edit this file to specify three replicas for the Weaviate deployment. Finally, we can deploy Weaviate using the provided \\'helm upgrade\\' command, which will create the Weaviate clusters with the specified number of replicas. By following these steps, we can check the status and details of nodes in the Kubernetes cluster and prepare to deploy Weaviate with three replicas.\\n\\nAnswer: To check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas, follow these steps: 1. Install \\'minikube\\', \\'helm\\', and \\'kubectl\\' on your system. 2. Start a three-node minikube cluster using the command: \\'minikube start --nodes 3\\'. 3. Use \\'kubectl get nodes -o wide\\' to view the status and details of the nodes, including their roles, ages, IP addresses, and more. 4. Add the Weaviate helm repository to your local configuration with \\'helm repo add weaviate https://weaviate.github.io/weaviate-helm\\'. 5. Save the default configuration using \\'helm show values weaviate/weaviate > values.yaml\\'. 6. Edit \\'values.yaml\\' to specify three replicas by changing \\'replicas: 1\\' to \\'replicas: 3\\'. 7. Deploy Weaviate using the \\'helm upgrade\\' command provided in the context. By executing these steps, you can verify the status and details of the nodes in the Kubernetes cluster and set up the deployment of Weaviate with three replicas.\\n\\n---\\n\\nContext:\\n[1] «For more information about this new feature, read this [blog post](/blog/ref2vec-centroid) by Connor Shorten. Weaviate also has a bunch of example use cases on [GitHub](https://github.com/weaviate/weaviate-examples). Find your favorite example, give it a star, and try to recreate it yourself!\\n\\n## Feature Comparison - Library versus Database\\nThe table below summarizes the differences between vector libraries and databases. This is by no means an exhaustive list of features, and not every library or database has the same features. |**Feature**|**Vector Library**|**Vector Database** (Weaviate as an example)|\\n| ----- | ----- | ----- |\\n| Filtering (in combination with Vector Search)| No| Yes|\\n| Updatability (CRUD)| No (some do, e.g. hnswlib)| Yes|\\n| Incremental importing, concurrent reading while importing| No (some do, e.g. hnswlib) | Yes |\\n| Stores objects and vectors | No | Yes|\\n|Speed | Typically faster than full-blown database|  Typically slower than pure library|\\n| Performance optimized for | In-memory similarity search| End2end-callstack, including: vector search, object retrieval from persistent storage, optional inverted index filtering, network requests, etc.»\\n[2] «Vector Indexing Library\\n\\nSimilarly to vector databases, [vector libraries](https://weaviate.io/blog/vector-library-vs-vector-database) also enable fast vector search. However, vector libraries only store vector embeddings of data objects, and they store them in in-memory indexes. This results in two key differences:\\n\\n1. Updatability: The index data is immutable, and thus, no real-time updates are possible. 2.»\\n[3] «This is why we have **Vector Databases** and **Vector Libraries**. They both use the Approximate Nearest Neighbor (ANN) algorithm to search through vectors in a tiny fraction of the time. You can learn more about this topic from [\"Why Vectors Search is so Fast.\"](/blog/why-is-vector-search-so-fast)\\n\\n## The Big Question\\nSo, if both vector databases and vector libraries allow you to efficiently search through your vectors. What are the key differences between them, and why/when should you choose one over the other? ## Vector Libraries\\nVector libraries store vector embeddings in in-memory indexes, in order to perform similarity search.»\\n[4] «### Example Use Cases\\nVector libraries are commonly used for applications that do not have changing data. For example, academic information retrieval benchmarks are designed to test performance on a **static snapshot** of data. When plugging an ANN index into production-ready applications, databases offer many appealing features not found in a library. <img\\n    src={require(\\'./img/vector-library-vs-database.png\\').default}\\n    alt=\"alt\"\\n    style={{ maxWidth: \"75%\" }}\\n/>\\n\\n## Vector Databases\\nOne of the core features that set vector databases apart from libraries is the ability to store and update your data. Vector databases have full **CRUD** (create, read, update, and delete) support that solves the limitations of a vector library.»\\n[5] «---\\ntitle: Vector Library versus Vector Database\\nslug: vector-library-vs-vector-database\\nauthors: [erika]\\ndate: 2022-12-1\\ntags: [\\'concepts\\', \\'search\\']\\nimage: ./img/hero.png\\ndescription: \"Learn more about the differences between vector libraries and vector databases!\"\\n---\\n![Vector library versus Vector Database](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nIn the world of Vector Search, we use **vector embeddings** – generated by **Machine Learning models** – to represent data objects (text, images, audio, etc.). The key idea here is that embeddings that are semantically similar to each other have a smaller distance between them. We can use [vector distance functions](/blog/distance-metrics-in-vector-search) like euclidean distance or cosine distance to determine if these objects are similar to other objects in the collection. However, to do this we need to compare the distances between the query vector and **every** vector in the collection. This type of calculation can be difficult to scale to millions or billions of vectors.»\\n\\nQuestion: What are the differences between vector libraries and vector databases like Weaviate in terms of features such as filtering, updatability, and performance?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: The context provides a comparison between vector libraries and vector databases, highlighting their key differences. By referring to the provided table and the subsequent explanations, we can identify the specific differences in terms of features, updatability, and performance.\\n\\nAnswer: Vector libraries and vector databases like Weaviate differ in several ways: 1. Filtering: Vector databases support filtering in combination with vector search, while vector libraries do not. 2. Updatability: Vector databases offer CRUD (Create, Read, Update, Delete) operations, allowing real-time updates. Vector libraries, on the other hand, have immutable index data, making updates impossible. 3. Incremental Importing: Vector databases support incremental importing and concurrent reading while importing. While some vector libraries may offer this feature (e.g., hnswlib), it is not a standard capability. 4. Storage: Vector databases store both objects and vectors, whereas vector libraries only store vector embeddings. 5. Speed: Vector libraries are typically faster than full-blown databases due to their in-memory nature. Vector databases are optimized for the end-to-end call stack, which includes various operations beyond just vector search. 6. Performance Optimization: Vector libraries are optimized for in-memory similarity search, making them efficient for specific tasks. Vector databases, on the other hand, are optimized for the entire end-to-end call stack, including vector search, object retrieval, filtering, and network requests. In summary, vector libraries are suitable for applications with static data and in-memory similarity search requirements. Vector databases, like Weaviate, are preferred when you need to store objects and vectors together, perform updates, and have additional features like filtering and CRUD operations.\\n\\n---\\n\\nContext:\\n[1] «![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\\n[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.»\\n[3] «---\\ntitle: Using Cross-Encoders as reranker in multistage vector search\\nslug: cross-encoders-as-reranker\\nauthors: [laura]\\ndate: 2022-08-09\\ntags: [\\'search\\']\\nimage: ./img/hero.png\\ndescription: \"Learn about bi-encoder and cross-encoder machine learning models, and why combining them could improve the vector search experience.\"\\n---\\n![Using Cross-Encoders as reranker in multistage vector search](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nSemantic search overcomes the shortcomings of keyword-based search. If you search for \"programming for data scientists\", a keyword search will find \"An intro to programming for data scientists\", but it won\\'t return \"Python for Machine Learning use-cases\". Vector databases can use machine learning to capture the context and semantics of data and search queries. Usually, machine learning models have a tradeoff between high accuracy and speed. The higher the accuracy, the more computationally expensive the model is.»\\n[4] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\\n[5] «With a vector database like [Weaviate](/), you can store and retrieve vectors and data efficiently using Bi-Encoder models to encode data and queries. A search pipeline can then continue with a Cross-Encoder model which reranks a list of retrieved search result candidates. This blog post was inspired by [Nils Reimer\\'s work on Bi-Encoders and Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html). import WhatNext from \\'/_includes/what-next.mdx\\'\\n\\n<WhatNext />»\\n\\nQuestion: Why are Cross-Encoders considered impractical for semantic search applications with large datasets?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s break down the context to understand the impracticality of Cross-Encoders for large-scale semantic search. The context discusses the trade-off between accuracy and efficiency in machine learning models, specifically in the context of semantic search. Semantic search aims to understand the context and semantics of data and queries, and it relies on machine learning models for this task. Cross-Encoders are introduced as highly accurate models for semantic search. However, the issue lies in their computational expense. When performing a search, a Cross-Encoder model needs to process every single data item in combination with the query. This means that for large datasets with thousands or millions of objects, the search process becomes extremely time-consuming and inefficient.\\n\\nAnswer: Cross-Encoders are considered impractical for semantic search applications with large datasets because they are computationally expensive and inefficient. The need to process every data item individually during a search makes it time-consuming and impractical for large-scale datasets.\\n\\n---\\n\\nContext:\\n[1] «With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here. Be the first to experience this great room and write a review! Don\\'t miss out on this amazing opportunity - book now! |\\n| This beautiful two bedroom apartment is located in the Upper East Side of Manhattan, just a short walk away from Central Park. The apartment is situated in the East Harlem neighbourhood, and is surrounded by a variety of restaurants, shops, and other attractions. The apartment is available for rent for a minimum of seven nights, and is priced at $190 per night. The apartment is equipped with all the necessary amenities, and is available for rent for 249 days out of the year.»\\n[2] «| Welcome to this beautiful two bedroom apartment in the Upper East Side of Manhattan! Located in the East Harlem neighbourhood, this apartment is just a short walk away from Central Park and surrounded by a variety of restaurants, shops, and other attractions. Enjoy all the necessary amenities in this apartment, available for rent for a minimum of seven nights at a price of $190 per night. With 249 days available for rent out of the year, this is the perfect place to stay for your next vacation or business trip! Book now and experience the best of Manhattan! |\\n| This cozy private room is located in the Flatbush neighbourhood of Brooklyn. Enjoy the convenience of being in the city while still having access to the peacefulness of the countryside. The room is available for a minimum of one night and is available for booking all year round.»\\n[3] «The location is convenient, with easy access to public transportation and a variety of shops and restaurants nearby. Although there are no reviews yet, you can rest assured that the host is experienced and dedicated to providing a comfortable and enjoyable stay. With availability 365 days a year, this listing is a great option for anyone looking for a long-term stay in Brooklyn. |\\n| 2    | This spacious 2 bedroom apartment is located in the vibrant East Harlem neighbourhood of Manhattan, just a stone\\'s throw away from Central Park. The entire home is available for rent, providing guests with ample space and privacy during their stay.»\\n[4] «With a minimum stay of 7 nights, this listing is perfect for those looking for a longer-term rental option. Although there are currently no reviews for this property, guests can rest assured that they will be staying in a well-maintained and comfortable space. The host has two listings on Airbnb and the availability for this property is 249 days out of the year. Book now to experience the excitement of New York City from the comfort of this spacious apartment. |\\n| 3    | This private room is located in the Flatbush neighborhood of Brooklyn, offering a unique country space in the heart of the city.»\\n[5] «The beautiful Central Park is also a short walk away.\\\\n\\\\nCome and experience the real New York City in the Village of Harlem. I look forward to meeting you and showing you around my home. |\\n| 3    | Huge 2 BR Upper East  Cental Park\\\\n\\\\nHost Sing\\\\n\\\\nEast Harlem\\\\nManhattan\\\\n\\\\nPrice: $190\\\\n\\\\nThis huge 2 bedroom is located in the heart of Manhattan, steps away from Central Park. It is newly renovated with stainless steel appliances, exposed brick, and beautiful hardwood floors. The apartment is located in a very safe and convenient area, with a variety of restaurants and stores nearby.»\\n\\nQuestion: What is the minimum stay and nightly rate for the new two-bedroom apartment rental in East Harlem near Central Park with 249 days of annual availability?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to identify the relevant information from the context. We are looking for a two-bedroom apartment in East Harlem near Central Park with 249 days of annual availability.\\n\\nAnswer: The minimum stay for the new two-bedroom apartment rental in East Harlem near Central Park with 249 days of annual availability is seven nights, and the nightly rate is $190.\\n\\n---\\n\\nContext:\\n[1] «In other words, GOMEMLIMIT is precisely the missing piece that we outlined before. If the live heap is low (e.g., 100MB), we can delay the next GC cycle until the heap has doubled (200MB). But if the heap has grown close to our limit (e.g., 4GB), the GC runs more often to prevent us from ever running OOM. ### Why a soft limit? What is the difference between a soft and a hard limit?»\\n[2] «* We could complete our full import without problems. * The memory usage never exceeded our soft limit of 2.75GiB and, therefore, never our hard limit of 3GiB. * GOMEMLIMIT made the GC more aggressive when less memory was available but kept it pretty relaxed when there was plenty. ## Conclusion: GOMEMLIMIT is a game-changer!\\nWe have covered quite a few points in this article, so let me summarize the key takeaways:\\n* Memory is either allocated on the stack (cheap) or the heap (expensive). Long-lived memory must live on the heap.»\\n[3] «The Go authors explicitly label GOMEMLIMIT a \"soft\" limit. That means that the Go runtime does not guarantee that the memory usage will exceed the limit. Instead, it uses it as a target. The goal is to fail fast in an impossible-to-solve situation: Imagine we set the limit to a value just a few kilobytes larger than the live heap. The GC would have to run constantly.»\\n[4] «With the new parameter, I will invoke Weaviate like this:\\n\\n```\\ndocker run -it \\\\\\n  -m 3GiB \\\\\\n  -e \"GOMEMLIMIT=2750MiB\" \\\\\\n  -e \"GOGC=100\" \\\\\\n  -e \"PERSISTENCE_DATA_PATH=/var/lib/weaviate\" \\\\\\n  -e \"AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true\" \\\\\\n  -e \"PROMETHEUS_MONITORING_ENABLED=true\" \\\\\\n  -e \"CLUSTER_HOSTNAME=node1\" \\\\\\n  -e \"GODEBUG=gctrace=1\" \\\\\\n  -v \"$PWD/data:/var/lib/weaviate\" \\\\\\n  -p \"8080:8080\" \\\\\\n  -p \"2112:2112\" \\\\\\n  weaviate\\n```\\n\\nNow it\\'s time to run the same experiment again… and, I have some good news! The import succeeded. Let\\'s run a quick sanity check to prove that there are indeed 400,000 objects imported:\\n\\n```bash\\ncurl localhost:8080/v1/graphql \\\\\\n  -H \\'content-type: application/json\\' \\\\\\n  -d \\'{\"query\":\"{Aggregate{Example{meta{count}}}}\"}\\'\\n\\n{\"data\":{\"Aggregate\":{\"Example\":[{\"meta\":{\"count\":400000}}]}}}\\n```\\n\\nAs you can see, the imports are all present, let\\'s take a look at the heap usage according to the Prometheus metrics:\\n\\n![GOMEMLIMIT helps to collect garbage before a OOM crash](./img/gomemlimit-in-action.jpg)\\n\\nThe chart shows exactly what we expected:\\n* At no time did the test exceed our hard 3GiB limit (hence no OOM kill)\\n* As it approached the limit the heap hovered around our configured 2750MiB GOMEMLIMIT\\n* After the import finished, the stable long-lived heap dropped to just above 2GiB\\n\\nLet us also take a look at the most recent GC log:\\n\\n```\\ngc 313 @692.771s 2%: 0.36+393+0.13 ms clock,\\n5.7+337/1572/1065+2.1 ms cpu,\\n2214->2272->2039 MB,\\n2296 MB goal, 0 MB stacks, 0 MB globals, 16 P\\n```\\n\\nAgain, this log output backs up our claims from above:\\n* After completing the import, the live heap is just over 2GiB (2039MB)\\n* As we got closer to our soft limit, the heap goal was no longer twice the live heap. Instead, it is barely above the live heap at 2296MB. * The Go runtime has done a conscious trade-off to sacrifice a bit of CPU time to prevent us from running OOM. We can see that the self-reported cost of GC is now at 2%\\n\\n### In summary, how did GOMEMLIMIT save us from OOM?»\\n[5] «A stack allocation is short-lived and typically very cheap. No Garbage Collection is required for a stack allocation as the end of the function is also the end of the variable\\'s lifetime. On the other hand, a heap allocation is long-lived and considerably more expensive. When allocating onto the heap, the runtime must find a contiguous piece of memory where the new variable fits. Additionally, it must be garbage-collected when the variable is no longer used.»\\n\\nQuestion: What is the role of GOMEMLIMIT in garbage collection cycles, and how does it differ when the live heap is low versus when it is near the limit?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to understand the role of GOMEMLIMIT in managing memory usage and its impact on garbage collection cycles. GOMEMLIMIT is a \"soft\" limit introduced in the Go programming language to optimize memory usage and prevent out-of-memory (OOM) situations. It is a target value that guides the garbage collector (GC) but is not strictly enforced. When the live heap is low: - GOMEMLIMIT allows the GC cycle to be delayed. The GC runs less frequently, conserving resources and improving performance. - The GC is more relaxed, as there is plenty of memory available, and it can take its time to collect and manage memory efficiently. When the live heap is near the limit: - GOMEMLIMIT triggers the GC to run more often. It becomes more aggressive to prevent an OOM situation. - The GC tries to keep the memory usage below the soft limit, sacrificing some CPU time if needed. It becomes a conscious trade-off between memory usage and CPU usage. - The goal of the GC is no longer to double the live heap but to stay barely above it, ensuring that memory usage remains within manageable limits.\\n\\nAnswer: GOMEMLIMIT plays a crucial role in optimizing garbage collection cycles by providing a target memory limit. When the live heap is low, GOMEMLIMIT delays GC cycles and allows for a more relaxed collection. However, when the live heap is near the limit, GOMEMLIMIT triggers more frequent and aggressive GC cycles to prevent OOM. This adaptive behavior ensures efficient memory management and prevents potential crashes due to memory constraints.\\n\\n---\\n\\nContext:\\n[1] «In addition to these two models, however, you can use any model from the Hugging Face Hub (or your own) by following [this guide](/developers/weaviate/modules/reader-generator-modules/sum-transformers#use-another-summarization-module-from-hugging-face). Even when looking only at language models that are trained for summarization tasks, there is still a wide range of choices in terms of sheer numbers, which vary in the target domain (e.g. medical, legal, scientific, etc.) and size (number of parameters, i.e. speed). If you have specific needs, we recommend investigating other models. ### Avoid too long an input\\n\\nAll transformer models have a maximum input length size. For example, `bart-large-cnn` has a maximum limit of 1024 tokens, where each token is part of a word (i.e. a few characters).»\\n[2] «This module can shorten a piece of text into a pithy, to-the-point summary by passing the text retrieved from Weaviate to a language model trained specifically for summarization. By using Weaviate to summarize your data, you can reduce the amount of too long; did not read (TL;DR) content in your life and reduce the problem of information overload. The `sum-transformers` module uses the `bart-large-cnn` model by default, with an option for the `pegasus-xsum` model, but any model from Hugging Face Hub can be used. import WhatsNext from \\'/_includes/what-next.mdx\\';\\n\\n<WhatsNext />»\\n[3] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\\nlet them do it for you.\"\\n\\n## Support for Hugging Face Inference API in Weaviate\\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\\n> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\\n[4] «## Overview\\n![Overview](./img/hugging-face-module-overview.png)\\n\\nThe Hugging Face module is quite incredible, for many reasons. ### Public models\\nYou get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case. In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). ### Private models\\nIf you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate.»\\n[5] «At the time of this writing, model cards still rely on **manual tagging**. It may be the case that the developer uploading the model hasn’t filled out these details. If you are new to Hugging Face, please consider annotating your uploaded models by adding a model card - YAML sections in the `README.md`, like this:\\n\\n<img\\n    src={require(\\'./img/how-to-populate-model-card-min.png\\').default}\\n    alt=\"How to populate a Hugging Face model card\"\\n    style={{ maxWidth: \"70%\" }}\\n/>\\n\\n### Private models\\nA large part of the beauty of Weaviate\\'s integration with Hugging Face is that **anyone** can upload their models to Hugging Face and use them in Weaviate\\'s vector database. For example, I am doing research on COVID-19 literature, so I have fine-tuned a model on CORD-19 title to abstract matching and uploaded it to [CShorten/CORD-19-Title-Abstracts](https://huggingface.co/CShorten/CORD-19-Title-Abstracts-1-more-epoch). ### Domain recap\\nAs a quick recap, **Domain** largely describes the high-level notion of **what** the dataset is about.»\\n\\nQuestion: How can I use a different summarization model from the Hugging Face Hub for my project?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer to this question. We first need to identify which part of the context is relevant to the question. In this case, the relevant information is found in the first paragraph, where it mentions that any model from the Hugging Face Hub can be used for summarization tasks by following a specific guide.\\n\\nAnswer: To use a different summarization model from the Hugging Face Hub, you need to follow the guide mentioned in the context. This guide will provide step-by-step instructions on how to utilize a custom model for your specific project. The flexibility to choose from a wide range of models allows you to select one that aligns with your project\\'s target domain and size requirements.\\n\\n---\\n\\nContext:\\n[1] «Hamilton was granted honorary Brazilian citizenship in 2022. </details>\\n\\n<details>\\n\\n  <summary>\\n    The Loch Ness Monster (52% reduction)\\n  </summary>\\n\\n:::note Summarized text\\nThe Loch Ness Monster is said to be a large, long-necked creature. Popular belief in the creature has varied since it was brought to worldwide attention in 1933. Evidence of its existence is disputed, with a number of disputed photographs and sonar readings. The pseudoscience and subculture of cryptozoology has placed particular emphasis on the creature.»\\n[2] «The ability to combine modules also allows for relations between e.g. a text object and a corresponding image object. As this article will show, Weaviate is also not restricted by some of the limitations of popular ANN libraries. ## When do you call a database a database? If you were to ask one hundred engineers what defines a database you will most likely get as many different answers. We might take the following points for granted in the world of SQL and NoSQL databases, yet they are anything but ubiquitous in the context of vector searches.»\\n[3] «Our people and culture team opens up this part of our town hall meetings by asking, “Who is your Weaviate Hero” —> this is how the name for our Champions Program was set. With the Weaviate Hero Program coming to life, we want to expand this internal initiative and also give kudos to people in our Community who help other members learn, grow, and thrive by sharing their knowledge, providing feedback, collaborating, and leading by example. ## The Weaviate Hero Program - in a Nutshell\\n\\nThe Weaviate Hero Program was brought to life to appreciate, recognize & celebrate members of our Community who contribute and engage in ways that:\\n\\n> *“Make our Community a great and safe place where people can learn, make friends, and grow. A place for giving and receiving.”*\\n\\nCommunity members who ultimately develop and share practices that help others in the group thrive. With this global program, we enable members to accelerate their knowledge, skills, and growth in the areas of Vector Databases, Search, and AI-native applications and become thought leaders through their activities and initiatives across the Weaviate Ecosystem.»\\n[4] «Imagine a fictional company called ACME Accounting Group that offers online accounting services that use AI to make accounting easy and fun. The company has over one million customers. Each customer is a company that can have many users and even more documents. Alice, who works for AliceCorp, should never be able to see the accounting information of Bob, who works for BobInc. Therefore, AliceCorp and BobInc are tenants from the perspective of ACME Accounting.»\\n[5] «Know someone who made a difference in your Weaviate journey? Nominate them as a Weaviate Hero! Let\\'s recognize the people who make our community great. 💚\\n\\nNominate your hero [here](https://docs.google.com/forms/d/e/1FAIpQLSePtgvTPZGx2gbCzVdyjitQ0WHrq4gNNZKiaqrQhphmJc3vJA/viewform). As we keep building, learning, and collaborating, we\\'re excited to see the innovations that 2024 will bring. 💫\\n\\n![Weaviate Hero](img/Weaviate-hero_(1).gif)\\n\\nimport WhatNext from \\'/_includes/what-next.mdx\\'\\n\\n<WhatNext />»\\n\\nQuestion: Who was granted honorary Brazilian citizenship in 2022?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to identify the person mentioned in the context who was granted honorary Brazilian citizenship in 2022.\\n\\nAnswer: Hamilton was granted honorary Brazilian citizenship in 2022.\\n\\n---\\n\\nContext:\\n[1] «## Representing long objects\\n\\nOne of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly exceed the 512 token input limit on Deep Transformer Neural Networks. This problem is a large part of what motivates our interest in Hybrid Search techniques that combine the flexibility of Vector Search with the sparse BM25 word counting algorithm well suited for >512 token text sequences. We think Ref2Vec can also help address this challenge. To be clear, Weaviate already offers a solution to represent long documents with Cross-References! As an example, [the Wikipedia Demo](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) breaks the long Wikipedia articles into a (Paragraph, inArticle, Article) data schema in which each Paragraph contains less than 512 tokens of text.»\\n[2] «We can then vectorize this text description using off-the-shelf models from OpenAI, Cohere, HuggingFace, and others to unlock semantic search. We recently presented an example of this idea for [AirBnB listings](https://weaviate.io/blog/generative-feedback-loops-with-llms), translating tabular data about each property’s price, neighborhood, and more into a text description. Huge thanks to Svitlana Smolianova for creating the following animation of the concept. <img\\n    src={require(\\'./img/gen-example.gif\\').default}\\n    alt=\"alt\"\\n    style={{ width: \"100%\" }}\\n/>\\n\\n### Text Chunking\\nSimilarly related to the 512 token length for vectorizing text chunks, we may consider using the Large Language Model to identify good places to cut up text chunks. For example, if we have a list of items, it might not be best practice to separate the list into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.»\\n[3] «This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in “Orchestrating Tuning”. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents.»\\n[4] «When dealing with dimensions ranging from 8 to 128, we found that using AVX-256 instructions with loop unrolling yields the most favorable results. In the past, we conducted experiments with AVX-512 to assess its performance potential. However, the results didn’t show a significant increase in performance, particularly on older Intel processors. This lack of notable improvement could be due to thermal and power constraints in these older processor architectures. Such constraints could lead to [reductions in CPU frequency](https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/), limiting the capacity of AVX-512 instructions to deliver enhanced performance under these conditions.»\\n[5] «| When you want to control the batch size. |\\n| `rate_limit` | The number of objects sent to Weaviate is rate limited (specified as n_objects per minute). | When you want to avoid hitting third-party vectorization API rate limits. |\\n\\n<Tabs groupId=\"languages\">\\n    <TabItem value=\"wcs\" label=\"Dynamic\">\\n        <FilteredTextBlock\\n            text={PythonCode}\\n            startMarker=\"# START BatchDynamic\"\\n            endMarker=\"# END BatchDynamic\"\\n            language=\"py\"\\n        />\\n    </TabItem>\\n    <TabItem value=\"local\" label=\"Fixed size\">\\n        <FilteredTextBlock\\n            text={PythonCode}\\n            startMarker=\"# START BatchFixedSize\"\\n            endMarker=\"# END BatchFixedSize\"\\n            language=\"py\"\\n        />\\n    </TabItem>\\n    <TabItem value=\"embedded\" label=\"Rate limit\">\\n        <FilteredTextBlock\\n            text={PythonCode}\\n            startMarker=\"# START BatchRateLimit\"\\n            endMarker=\"# END BatchRateLimit\"\\n            language=\"py\"\\n        />\\n    </TabItem>\\n</Tabs>\\n\\n#### Error handling in batch operations\\n\\nThe `v4` client introduces additional error handling operations. You can now monitor the number of errors in an ongoing batching operation, or simply review the errors after the operation has completed.»\\n\\nQuestion: What is the strategy for chunking text for vectorization when dealing with a 512 token length limit?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To effectively chunk text for vectorization when faced with a 512 token length constraint, we need to identify optimal breakpoints in the text that ensure each chunk remains within the limit while preserving the integrity of the information.\\n\\nAnswer: The strategy suggested is to use Large Language Models (LLMs) to identify suitable places to divide the text. This ensures that meaningful segments of text remain together, even if they exceed the 512 token limit. Additionally, the text chunking process is an important step in optimizing retrieval and ensuring compliance with LLM token limits.\\n\\n---\\n\\nContext:\\n[1] «You can find other dev container templates [here](https://containers.dev/templates). **Step 2. Test the Setup**\\n\\n1. After setting up the environment we tested that everything worked by:\\nCreate a [dummy endpoint](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-c16fbf0c6f7b90a46b94b36f88893c2d174476088608841f7254afba0e81373d) which will simply respond with a `{“Hello”: “World”}` object when called. ```python\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n@app.get(\"/\")\\ndef read_root():\\n    \"\"\"\\n    Say hello to the world\\n    \"\"\"\\n    return {\"Hello\": \"World\"}\\n```\\n\\n2.»\\n[2] «These functions give ChatGPT the ability to add objects to the Weaviate instance, query and search through objects in the Weaviate instance and lastly delete objects if needed. Upon interacting with ChatGPT while the plugin is enabled it can be instructed to use a particular endpoint via prompt but will also independently decide when to use the appropriate endpoint to complete the response to a query! These endpoints are what extend the functionality of ChatGPT and enable it to interact with the vector database. We developed these three endpoints through test driven development, as such we will display the tests that each endpoint must first pass and then the implementation that satisfies these tests. In preparation to setup the Weaviate instance for these tests we added the following test documents through a fixture:\\n```python\\n@pytest.fixture\\ndef documents(weaviate_client):\\n    docs = [\\n        {\"text\": \"The lion is the king of the jungle\", \"document_id\": \"1\"},\\n        {\"text\": \"The lion is a carnivore\", \"document_id\": \"2\"},\\n        {\"text\": \"The lion is a large animal\", \"document_id\": \"3\"},\\n        {\"text\": \"The capital of France is Paris\", \"document_id\": \"4\"},\\n        {\"text\": \"The capital of Germany is Berlin\", \"document_id\": \"5\"},\\n    ]\\n\\n    for doc in docs:\\n        client.post(\"/upsert\", json=doc)\\n```\\n\\n**Implementing the `/upsert` endpoint:**\\n\\nAfter using the `/upsert` endpoint we mainly want to test that that we got the appropriate status code in addition to checking that the content, id and vector’s were all upserted correctly. Here\\'s the test that carries this out:\\n\\n```python\\ndef test_upsert(weaviate_client):\\n    response = client.post(\"/upsert\", json={\"text\": \"Hello World\", \"document_id\": \"1\"})\\n    assert response.status_code == 200\\n\\n    docs = weaviate_client.data_object.get(with_vector=True)[\"objects\"]\\n    assert len(docs) == 1\\n    assert docs[0][\"properties\"][\"text\"] == \"Hello World\"\\n    assert docs[0][\"properties\"][\"document_id\"] == \"1\"\\n    assert docs[0][\"vector\"] is not None\\n```\\n\\nThe implementation below satisfies all of these requirements and tests above:\\n```python\\n@app.post(\"/upsert\")\\ndef upsert(doc: Document, client=Depends(get_weaviate_client)):\\n    \"\"\"\\n    Insert a document into weaviate\\n    \"\"\"\\n    client.batch.configure(batch_size=100)  # Configure batch\\n    with client.batch as batch:\\n        batch.add_data_object(\\n            data_object=doc.dict(),\\n            class_name=INDEX_NAME,\\n            vector=get_embedding(doc.text),\\n        )\\n\\n    return {\"status\": \"ok\"}\\n```\\n\\nThe `/query` and `/delete` endpoints were developed similarly, if you\\'re interested you can read below!\\n\\n<details>\\n  <summary>See details for /query endpoint implementation.</summary>\\n\\n**Implement the `/query` endpoint:**\\n\\nFor this endpoint we mainly want to check that it returns the right number of objects and that the required document that we were expecting is part of the returned objects.»\\n[3] «Since none of the other services will be accessed outside of Snowflake, I only needed it for the Jupyter service. ```\\nGRANT USAGE ON SERVICE JUPYTER TO ROLE WEAVIATE_ROLE;\\n```\\n\\n### Step 4: Generate Product Data with Mistral\\nNext, I needed to access the Jupyter Notebooks endpoint. You can find your endpoint by running this command:\\n\\n```sql\\nSHOW ENDPOINTS IN SERVICE JUPYTER;\\n```\\n\\nThis showed me the endpoint for Jupyter that I can use to access it. I grabbed the URL for the Jupyter endpoint and pasted it into my browser. Using the Snowflake Login I set up earlier, I logged into the Jupyter Notebooks interface.»\\n[4] «See the example below:\\n\\n![error](./img/error.png)\\n\\n**Take-aways:** ChatGPT doesn’t have hard-coded instructions on when and how to use the plugin endpoints. You need to be very careful about how you describe your plugin and endpoints to ChatGPT so that they can be used as intended! The `openapi.json` spec that FastAPI generates for you is based on how you documented the endpoints in your code i.e. the function’s name, docstring, query description and the field’s description in your pydantic models. The steps used to do this are outside the scope of this blog post, for more details please refer to the FastAPI documentation. In general you want to have complete and comprehensive documentation for your plugin because the documentation is what will allow it to be used correctly!\\n\\nAdditionally you have to be careful when specifying the descriptions, doc strings etc. not to exceed the context length since the plugin description, API requests, and API responses are all inserted into the conversation with ChatGPT.»\\n[5] «<br/>\\n\\nJust like that, you’ve got a multi-node Weaviate cluster. Remember that when you create a class, you must have replication enabled. You can do this by adding the `replicationConfig` parameter to the collection definition, like so:\\n\\n```json\\n{\\n  \"class\": \"ClassWithReplication\",\\n  \"properties\": [\\n    {\\n      \"name\": \"exampleProperty\",\\n      \"dataType\": [\"text\"]\\n    }\\n  ],\\n  // highlight-start\\n  \"replicationConfig\": {\\n    \"factor\": 3\\n  }\\n  // highlight-end\\n}\\n```\\n\\nAnd when you insert objects into `ClassWithReplication`, they will be replicated across the three nodes. You can verify this by visiting the `/nodes` endpoint, which will verify that each node contains the same number of objects. </details>\\n\\n## Benefits of replication\\n\\nSo, let’s cover the implications of doing this.»\\n\\nQuestion: How do you create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed, we can follow these steps: 1. Define the endpoint route: We want our endpoint to be accessible at the root URL \"/\", so we define it using the `@app.get(\"/\")` decorator. 2. Create a function for the endpoint: We need a function that will be executed when the endpoint is accessed. In this case, we want to return `{\"Hello\": \"World\"}`. 3. Add a docstring to the function: Although not necessary for the functionality, adding a docstring helps with documentation and understanding the purpose of the endpoint. 4. Return the desired JSON object: Inside the function, we simply return `{\"Hello\": \"World\"}`.\\n\\nAnswer: Here\\'s the code snippet that creates the dummy endpoint: ```python from fastapi import FastAPI app = FastAPI() @app.get(\"/\") def read_root(): \"\"\" Say hello to the world \"\"\" return {\"Hello\": \"World\"} ``` This code defines a FastAPI app and creates an endpoint at \"/\" using the `@app.get(\"/\")` decorator. The `read_root()` function is executed when the endpoint is accessed, and it returns `{\"Hello\": \"World\"}`. The docstring provides a description of the endpoint\\'s purpose.\\n\\n---\\n\\nContext:\\n[1] «Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\\n\\n![Reduced memory usage](./img/reduced-memory-usage.jpg)\\n\\nAs part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\\n\\nFirst, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs).»\\n[2] «There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\\n\\nNext, we optimized memory allocations for the HNSW (vector) index.»\\n[3] «[Better control over Garbage Collector](#better-control-over-garbage-collector) - with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 1. [Faster imports for ordered data](#faster-imports-for-ordered-data) - by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n)\\n1. [More efficient filtered aggregations](#more-efficient-filtered-aggregations) - thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 1.»\\n[4] «Next, let\\'s see how we might test imports. ### Testing imports\\n\\nOne particularly common issue we see is skipped objects during import due to rate limits from the vectorization provider. So, let\\'s see how we might test that all objects were imported correctly. In this section, we\\'ll import a small subset (100 objects) of the [original Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions). As always, we\\'ll use [batching](/developers/weaviate/manage-data/import) for optimal speed.»\\n[5] «---\\ntitle: Weaviate 1.15 release\\nslug: weaviate-1-15-release\\nauthors: [connor, erika, laura, sebastian]\\ndate: 2022-09-07\\ntags: [\\'release\\']\\nimage: ./img/hero.png\\ndescription: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\\n---\\n![Weaviate 1.15 release](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWe are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\\n\\nIf you like your content brief and to the point, here is the TL;DR of this release:\\n1. [☁️Cloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\\n1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1.»\\n\\nQuestion: What optimization has Weaviate introduced to manage memory usage during parallel data imports?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: The context mentions that Weaviate has introduced new optimizations to reduce RAM usage without compromising performance. Specifically, it talks about a thread pooling optimization that addresses parallel imports.\\n\\nAnswer: Weaviate introduced thread pooling to manage memory usage during parallel data imports. This optimization ensures that the number of parallel imports doesn\\'t exceed the number of CPU cores, reducing memory spikes and providing maximum performance without unnecessary memory usage.\\n\\n---\\n\\nContext:\\n[1] «Similarly to the original Gorilla paper’s use of Abstract Syntax Tree evaluation, we are also considering an n-gram match where we construct keywords for each query such as “bm25”, “query”, “title” and check how many are contained in the generated query. We can also use the finer-grained perplexity metric that measures the log probability of the ground truth tokens at each step of decoding. We are currently using a simple greedy decoding algorithm to sample from the LoRA fine-tuned LlaMA 7B LLM. To ground the evaluation discussion further, let’s take a look at an incorrect query:\\n\\n```graphql\\n{\\n\\tGet {\\n\\t\\tJobListing(\\n\\t\\t\\tbm25: {query: “software”}\\n\\t\\t\\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\\n\\t\\t){\\n\\t\\ttitle\\n\\t\\tdescription\\n\\t\\tisRemote\\n\\t\\tpostedBy {\\n\\t\\t\\tname\\n\\t\\t  }\\n\\t\\t}\\n\\t}\\n}\\n```\\n\\nAlmost there! But unfortunately the missing comma from the `bm25` to `where` query will prevent this query from successfully executing. As discussed we may have other cases where although the syntax is correct and the query executes, it does not achieve what was specified in the natural language command.»\\n[2] «Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate. Only use the API reference to understand the syntax of the request. ##Natural Language Query\\n%s\\n\\n## Schema\\n%s\\n\\n## API reference\\n%s\\n\\n## Answer\\n```graphql\\n“”” % (nlcommand, customSchema, apiReference)\\n```\\n\\n## Quantitative Evaluation\\n\\nHopefully these visualizations of the Gorilla output on unseen test examples provides a solid indication of where the model is at. We are still kicking off the process of rigorous quantitative measurement, here are some of our thoughts on quantitative evaluation so far.»\\n[3] «The GraphQL query editor features:\\n\\n- Syntax highlighting\\n- Intelligent type ahead of fields, arguments, types, and more\\n- Real-time error highlighting and reporting for queries and variables\\n- Automatic query and variables completion\\n\\n\\n![Weaviate Cloud Service GraphQL IDE ](./img/wcs-console-example-query.png)\\n\\n## Control Access to Weaviate - API Key Management\\nTo help make it easy to control access to your vector databases, all Weaviate Cloud instances can generate API keys with full admin (read and write) access. Read-only API keys are also supported for paid clusters. ![API Key Management in Weaviate Cloud Service](./img/wcs-get-key.png)\\n\\n## Next Stop, GA…\\nThe Weaviate Cloud is in public beta. General Availability is planned for release later in 2023, after we’ve addressed feedback from public beta users. If you’d like to try Weaviate Cloud, you can create a [free Weaviate Cloud Sandbox here](https://console.weaviate.cloud/).»\\n[4] «:::\\n\\n:::note What you will be able to do using the Web bundle\\nOnly Read operations powered by GraphQL. :::\\n\\n\\n\\n### Collections-first approach\\n\\nThe other big change is that the `collections` client focuses on individual collections for interaction. This means that you will no longer need to specify the collection name in every request. Instead, you will create an object for each collection that you want to interact with, and then use that object for all subsequent requests. For example, take the following syntax for performing a simple request to retrieve a few objects from the database:\\n\\n:::note Classic TypeScript syntax\\n\\n```typescript\\nconst response = await client.graphql\\n  .get()\\n  .withClassName(\\'Article\\')\\n  .withLimit(2)\\n  .withFields(\\'title body url\\')\\n  .do();\\n```\\n\\n:::\\n\\nBecomes:\\n\\n:::info `Collections` client syntax\\n\\n```typescript\\nconst articles = client.collection.get(\\'Article\\');\\nconst response = articles.query.fetchObjects({\\n  limit: 2,\\n  returnProperties: [\\'title\\',\\'body\\',\\'url\\']\\n});\\n```\\n\\n:::\\n\\nYou\\'ll see that a search is now a method that originates from the collection object.»\\n[5] «This can be done by sending queries like this to Weaviate:\\\\n\\\\n```graphql\\\\n{\\\\n  Get {\\\\n    Product (\\\\n      nearObject: {\\\\n        id: \\\\\"8abc5-4d5...\\\\\" # id for the User object with vector defined by ref2vec-centroid\\\\n      }\\\\n    ) {\\\\n      product_name\\\\n      price\\\\n    }\\\\n  }\\\\n}\\\\n```\\\\n\\\\nThis short query encapsulates the power of Ref2Vec.\"\\n        },\\n        {\\n          \"_additional\": {\\n            \"rerank\": [\\n              {\\n                \"score\": 5.5508026e-06\\n              }\\n            ]\\n          },\\n          \"content\": \"---\\\\ntitle: What is Ref2Vec and why you need it for your recommendation system\\\\nslug: ref2vec-centroid\\\\nauthors: [connor]\\\\ndate: 2022-11-23\\\\ntags: [\\'integrations\\', \\'concepts\\']\\\\nimage: ./img/hero.png\\\\ndescription: \\\\\"Weaviate introduces Ref2Vec, a new module that utilises Cross-References for Recommendation!\\\\\"\\\\n---\\\\n![Ref2vec-centroid](./img/hero.png)\\\\n\\\\n<!-- truncate -->\\\\n\\\\nWeaviate 1.16 introduced the [Ref2Vec](/developers/weaviate/modules/retriever-vectorizer-modules/ref2vec-centroid) module. In this article, we give you an overview of what Ref2Vec is and some examples in which it can add value such as recommendations or representing long objects. ## What is Ref2Vec? The name Ref2Vec is short for reference-to-vector, and it offers the ability to vectorize a data object with its cross-references to other objects. The Ref2Vec module currently holds the name ref2vec-**centroid** because it uses the average, or centroid vector, of the cross-referenced vectors to represent the **referencing** object.\"\\n        },\\n        {\\n          \"_additional\": {\\n            \"rerank\": [\\n              {\\n                \"score\": 4.7478566e-06\\n              }\\n            ]\\n          },\\n          \"content\": \"## More Coming Soon\\\\nWe are very excited about the potential of Ref2Vec, and how it leverages existing symbolic data to augment vector searches in a new way.»\\n\\nQuestion: What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s think step by step to identify the syntax error in the provided GraphQL query example and how it relates to the evaluation of n-gram matches. We\\'ll start by examining the query and identifying any discrepancies from the expected syntax.\\n\\nAnswer: The syntax error in the provided GraphQL query example is the missing comma between the `bm25` and `where` fields. This error would prevent the query from executing successfully and is thus relevant to the evaluation of n-gram matches, as it affects the structure and validity of the query.\\n\\n---\\n\\nContext:\\n[1] «#### Test\\nWe designed an extreme stress test that would represent the \"worst-case\" scenario for recovery. It has multiple very large, independent Write-Ahead Logs that required for recovery. Before, this could take many hours to recover, while now it takes only a few seconds. ### Full changelog\\nThese are few of the many improvements and bug fixes that were included in this release. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list.»\\n[2] «#### Problem\\nIf Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\\n\\n- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\\n- The recovery process was single-threaded.»\\n[3] «#### Solution\\nWe addressed each of the points above individually and improved the overall MTTR substantially:\\n\\n- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.»\\n[4] «To attach to the logs of a specific containers you can run:\\n\\n```bash\\ndocker compose logs -f <container-name>\\n```\\n\\nfor example:\\n\\n```bash\\ndocker compose logs -f weaviate\\n```\\n\\nYou can also omit the -d option and keep your terminal attached to all logs. To see the names and status of each container you can run:\\n\\n```bash\\ndocker compose ps\\n```\\n\\n## How do I know when Weaviate is up and ready? Weaviate implements a readiness check at `GET /v1/.well-known/ready`. It will return a `2xx` HTTP status code once everything is ready. To check for the readiness programmatically you can use `curl` in this simple `bash` loop:\\n\\n```bash\\nuntil curl --fail -s localhost:8080/v1/.well-known/ready; do\\n  sleep 1\\ndone\\n```\\n\\nMake sure to adjust localhost:8080 if you are running on a different bind address.»\\n[5] «We were able to import 200 million objects and more, while the import performance remained constant throughout the process. [See more on github](https://github.com/weaviate/weaviate/pull/1976). ### Drastically improved Mean-Time-To-Recovery (MTTR)\\nWeaviate `1.14` fixes an issue where a crash-recovery could take multiple minutes, or even hours in some extreme cases. It is now a matter of just seconds. So even in the rare event that your instance crashes, it will come back up very quickly.»\\n\\nQuestion: How was the MTTR improved in the system with regards to handling large write-ahead logs?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer by understanding the problem and the solutions implemented. In the given context, the problem is that the recovery process for Write-Ahead Logs (WALs) could be slow, especially with large unflushed memtables and a single-threaded recovery process.\\n\\nAnswer: The MTTR was improved by addressing the following: 1. Deduplication of large WALs: By adding a deduplication process, redundant data in the WALs is removed, reducing the recovery time. 2. Parallel recovery: The recovery process now runs in parallel, allowing multiple places requiring recovery to work independently and simultaneously, improving the speed. 3. Idle memtable flushing: Any memtable idle for 60 seconds or more is flushed, reducing the amount of data that needs to be recovered and ensuring faster recovery. These changes ensure that the recovery process for large WALs is faster and more efficient, improving the overall MTTR of the system.\\n\\n---\\n\\nContext:\\n[1] «---\\ntitle: Weaviate 2023 Recap\\nslug: 2023-recap\\nauthors: [femke]\\ndate: 2023-12-26\\ntags: []\\nimage: ./img/hero.png\\ndescription: \"A reflection on 2023 from team Weaviate!\"\\n---\\n![hero](img/hero.png)\\n\\n<!-- truncate -->\\n\\nIt’s hard to imagine that less than a year ago, so very few people even knew about the concept of vector databases and how AI could benefit from them. Those who did still had many questions about how they worked and whether they could at all be helpful. Meanwhile, curiosity and interest in AI spiked, especially after OpenAI launched ChatGPT. Curiosity has sped up our progress and made more people aware of the opportunities AI offers, transforming our landscape. Let\\'s all take a moment to reflect and appreciate the start of a momentous change in how we can communicate, learn, teach, and collaborate so much faster and more effectively by leveraging AI.»\\n[2] «---\\ntitle: What to expect from Weaviate in 2023\\nslug: what-to-expect-from-weaviate-in-2023\\nauthors: [etienne]\\ndate: 2023-02-14\\ntags: [\\'engineering\\']\\nimage: ./img/hero.png\\ndescription: \"Learn about the six pillars outlining how Weaviate will evolve in 2023.\"\\n---\\n![What to expect from Weaviate in 2023](./img/hero.png)\\n\\nWithout a doubt, 2022 has been the most exciting year for Weaviate so far. The company and the product have grown tremendously, and we are incredibly excited about 2023. Weaviate’s usage numbers are through the roof, and so are your feedback and requests for what you’re still missing from Weaviate. <!-- truncate -->\\n\\nIn this blog post, I will introduce you to the six pillars outlining how Weaviate will get even better in 2023. Weaviate development is highly dynamic – we don’t waterfall-plan for the entire year – but nevertheless, we want to give you the best possible overview of what to expect in the coming year.»\\n[3] «---\\ntitle: Weaviate’s re:Invent 2023 recap\\nslug: reinvent-2023\\nauthors: [ieva, alea, erika]\\ndate: 2023-12-05\\nimage: ./img/hero.png\\ntags: []\\ndescription: \"Recap the first Weaviate visit to Amazon re:Invent in Las Vegas\"\\n---\\n\\n![AWS re:Invent](./img/hero.png)\\n\\nThis November the Weaviate team went to our first ever AWS re:Invent! Over 50,000 people from around the world came together to learn about the latest developments in cloud technology and to share what they’ve been building with the AWS developer community. Let’s dive into our highlight reel!\\n\\n## GenAI is (still) heating up\\n\\nThis year’s re:Invent focus theme was Generative AI and everyone seemed to be talking about it. In the sessions our team attended, we noticed a particular emphasis on [Retrieval Augmented Generation (RAG)](https://weaviate.io/blog/rag-evaluation). Initially when people came to our booth on Monday, roughly 40% of people knew about RAG. By the last day, nearly everyone that talked to us knew what it was because so many different sessions had covered the topic.»\\n[4] «I am proud of you all and highly excited about the future. Thank you all, and let’s make 2023 the most exciting year for Weaviate users so far!\\n\\n<br></br>\\n\\nimport WhatNext from \\'/_includes/what-next.mdx\\'\\n\\n<WhatNext />»\\n[5] «This way, you can make sure that your voice is heard, and we can see what all of you need the most. <br></br>\\n\\n## Conclusion: Proud of how far we’ve come, excited about the future\\nIn the beginning, I mentioned that not just the product but also the company grew significantly last year. I am incredibly proud of what we have achieved – both overall and in the past year. This wouldn’t have been possible without an absolutely fantastic team. Everyone working on Weaviate – whether a full-time employee or open-source contributor – is doing a fantastic job.»\\n\\nQuestion: What is the title of the Weaviate team\\'s reflection on the year 2023?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to find the title of the Weaviate team\\'s reflection on the year 2023. We can look for a blog post or article published by the Weaviate team that recaps the year.\\n\\nAnswer: \"Weaviate 2023 Recap\"\\n\\n---\\n\\nContext:\\n[1] «This will give you an idea of the Weaviate cluster’s performance over the course of this massive import process. Let’s start off with a bird’s eye view of the whole run. As mentioned above, the entire process took approximately 48 hours and on a high level the system performed very well. The graph beneath the text show the number of vectors imported over time:\\n\\n![Number of vectors imported](./img/number-of-vectors-imported.png)\\n\\nAs can be seen the slope, indicating import speed, is almost linear. meaning that we see pretty much no slow down in the import times regardless of the quantity of objects already imported.»\\n[2] «```python\\ndef test_query(documents):\\n    LIMIT = 3\\n    response = client.post(\"/query\", json={\"text\": \"lion\", \"limit\": LIMIT})\\n\\n    results = response.json()\\n\\n    assert len(results) == LIMIT\\n    for result in results:\\n        assert \"lion\" in result[\"document\"][\"text\"]\\n```\\n\\nThe implementation below will take in a query and return a list of retrieved documents and metadata. ```python\\n@app.post(\"/query\", response_model=List[QueryResult])\\ndef query(query: Query, client=Depends(get_weaviate_client)) -> List[Document]:\\n    \"\"\"\\n    Query weaviate for documents\\n    \"\"\"\\n    query_vector = get_embedding(query.text)\\n\\n    results = (\\n        client.query.get(INDEX_NAME, [\"document_id\", \"text\"])\\n        .with_near_vector({\"vector\": query_vector})\\n        .with_limit(query.limit)\\n        .with_additional(\"certainty\")\\n        .do()\\n    )\\n\\n    docs = results[\"data\"][\"Get\"][INDEX_NAME]\\n\\n    return [\\n        QueryResult(\\n            document={\"text\": doc[\"text\"], \"document_id\": doc[\"document_id\"]},\\n            score=doc[\"_additional\"][\"certainty\"],\\n        )\\n        for doc in docs\\n    ]\\n```\\n</details>\\n\\n<details>\\n  <summary>See details for /delete endpoint implementation.</summary>\\n\\n**Implement the `/delete` endpoint:**\\n\\nHere we simply want to check that the response returned correctly and that after removing one object we the number of total objects in the Weaviate instance goes down by one. ```python\\ndef test_delete(documents, weaviate_client):\\n    num_docs_before_delete = weaviate_client.data_object.get()[\"totalResults\"]\\n\\n    response = client.post(\"/delete\", json={\"document_id\": \"3\"})\\n    assert response.status_code == 200\\n\\n    num_docs_after_delete = weaviate_client.data_object.get()[\"totalResults\"]\\n\\n    assert num_docs_after_delete == num_docs_before_delete - 1\\n```\\n\\nAnd the implementation of the endpoint is as follows:\\n\\n```python\\n@app.post(\"/delete\")\\ndef delete(delete_request: DeleteRequest, client=Depends(get_weaviate_client)):\\n    \"\"\"\\n    Delete a document from weaviate\\n    \"\"\"\\n    result = client.batch.delete_objects(\\n        class_name=INDEX_NAME,\\n        where={\\n            \"operator\": \"Equal\",\\n            \"path\": [\"document_id\"],\\n            \"valueText\": delete_request.document_id,\\n        },\\n    )\\n\\n    if result[\"results\"][\"successful\"] == 1:\\n        return {\"status\": \"ok\"}\\n    else:\\n        return {\"status\": \"not found\"}\\n```\\n</details>\\n\\nHere we showed you how our endpoints work, this will be where your plugin will be most unique, depending on what functionality you want implemented you can create corresponding endpoints and test them. ::::tip\\nNotice the docstrings we’ve included with all of our endpoints, these will be very important in the next step!\\n::::\\n\\n**Step 2: Prepare plugin manifest files**\\n\\nThis is where you specify to OpenAI and specifically ChatGPT which endpoints your plugin is exposing, how it can use those endpoints to accomplish specific tasks, what errors to expect if the endpoints are not used correctly and much more! [OpenAI instruction specify](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest) that you need to create two files: the [`openapi.yaml` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/openapi.yaml) and the [`ai-plugin.json` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/ai-plugin.json). As you can see both of these files need to be in the `.well-known` directory which must be mounted into the app as follows in order for ChatGPT to utilize them correctly.»\\n[3] «But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50–100m, the import process would slow down to a walking pace. #### Solution\\nTo address this problem, we changed how the HNSW index grows. We implemented a relative growth pattern, where the HNSW index size increases by either 25% or 25\\'000 objects (whichever is bigger). ![HNSW index growth chart](./img/hnsw-index-growth.jpg)\\n\\n#### Test\\nAfter introducing the relative growth patterns, we\\'ve run a few tests.»\\n[4] «These functions give ChatGPT the ability to add objects to the Weaviate instance, query and search through objects in the Weaviate instance and lastly delete objects if needed. Upon interacting with ChatGPT while the plugin is enabled it can be instructed to use a particular endpoint via prompt but will also independently decide when to use the appropriate endpoint to complete the response to a query! These endpoints are what extend the functionality of ChatGPT and enable it to interact with the vector database. We developed these three endpoints through test driven development, as such we will display the tests that each endpoint must first pass and then the implementation that satisfies these tests. In preparation to setup the Weaviate instance for these tests we added the following test documents through a fixture:\\n```python\\n@pytest.fixture\\ndef documents(weaviate_client):\\n    docs = [\\n        {\"text\": \"The lion is the king of the jungle\", \"document_id\": \"1\"},\\n        {\"text\": \"The lion is a carnivore\", \"document_id\": \"2\"},\\n        {\"text\": \"The lion is a large animal\", \"document_id\": \"3\"},\\n        {\"text\": \"The capital of France is Paris\", \"document_id\": \"4\"},\\n        {\"text\": \"The capital of Germany is Berlin\", \"document_id\": \"5\"},\\n    ]\\n\\n    for doc in docs:\\n        client.post(\"/upsert\", json=doc)\\n```\\n\\n**Implementing the `/upsert` endpoint:**\\n\\nAfter using the `/upsert` endpoint we mainly want to test that that we got the appropriate status code in addition to checking that the content, id and vector’s were all upserted correctly. Here\\'s the test that carries this out:\\n\\n```python\\ndef test_upsert(weaviate_client):\\n    response = client.post(\"/upsert\", json={\"text\": \"Hello World\", \"document_id\": \"1\"})\\n    assert response.status_code == 200\\n\\n    docs = weaviate_client.data_object.get(with_vector=True)[\"objects\"]\\n    assert len(docs) == 1\\n    assert docs[0][\"properties\"][\"text\"] == \"Hello World\"\\n    assert docs[0][\"properties\"][\"document_id\"] == \"1\"\\n    assert docs[0][\"vector\"] is not None\\n```\\n\\nThe implementation below satisfies all of these requirements and tests above:\\n```python\\n@app.post(\"/upsert\")\\ndef upsert(doc: Document, client=Depends(get_weaviate_client)):\\n    \"\"\"\\n    Insert a document into weaviate\\n    \"\"\"\\n    client.batch.configure(batch_size=100)  # Configure batch\\n    with client.batch as batch:\\n        batch.add_data_object(\\n            data_object=doc.dict(),\\n            class_name=INDEX_NAME,\\n            vector=get_embedding(doc.text),\\n        )\\n\\n    return {\"status\": \"ok\"}\\n```\\n\\nThe `/query` and `/delete` endpoints were developed similarly, if you\\'re interested you can read below!\\n\\n<details>\\n  <summary>See details for /query endpoint implementation.</summary>\\n\\n**Implement the `/query` endpoint:**\\n\\nFor this endpoint we mainly want to check that it returns the right number of objects and that the required document that we were expecting is part of the returned objects.»\\n[5] «Partitioning 2. Cleaning, 3. Staging. Partitioning bricks take an unstructured document and extract structured content from it. It takes the document and breaks it down into elements like `Title`, `Abstract`, and `Introduction`.»\\n\\nQuestion: At what number of imported objects did the slow down begin according to the document?\\n\\nReasoning: Let\\'s think step by step in order to Context: The provided text discusses the process of importing data into a Weaviate cluster and the implementation of various endpoints for interacting with the data. It mentions a slowdown in the import process, but does not explicitly state the number of imported objects at which this slowdown occurs. Question: At what number of imported objects did the slowdown begin according to the document? Reasoning: To answer this question, we need to carefully examine the relevant portions of the provided context. The text mentions a slowdown in the import process, but does not provide a specific number of imported objects at which this slowdown starts. However, it gives us a range to work with. The relevant excerpt from the context is: \"[3] «But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50–100m, the import process would slow down to a walking pace.\" Here, the document is stating that at around 25 million objects, the import process started to become noticeably slower. However, it also mentions that the import process further slowed down to a \"walking pace\" between 50 and 100 million objects. Therefore, we can conclude that the slowdown began around 25 million imported objects, but it became more pronounced and significant as the number of objects increased toward 50 million and beyond.\\n\\nAnswer: The slowdown in the import process began around 25 million imported objects, but it became more significant as the number of objects approached 50 million and beyond.\\n\\n---\\n\\nContext:\\n[1] «1**: *Suppose we have vectors $x$ and $y$ represented in their original space. We apply a compression function $C$ to obtain a shorter representation of $x$ ($x\\'$) and $y$ ($y\\'$) on a compressed space but would require a decompression function $C\\'$ from the compressed space into the original space to be able to use the original distance function. In this case we would obtain $x\\'\\'$ and $y\\'\\'$ from $x\\'$ and $y\\'$ respectively and apply the distance on the approximations of the original $x$ and $y$ so $d(x,y)=d(x\\'\\',y\\'\\') + \\\\delta$ where $\\\\delta$ is the distortion added to the distance calculation due of the reconstruction of the original vectors. The compression/decompression mechanisms should be such that the distortion is minimized.*\\n\\n![comp2](./img/image2.jpg)\\n**Fig. 2**: *Suppose we have vectors $x$ and $y$ represented in their original space.»\\n[2] «In Weaviate, floating-point vectors are represented as `float32` arrays, yielding a compression rate of 1:32, which is already commendable. However, there\\'s a significant secondary benefit: bitwise operations now suffice for distance calculations between compressed binary vectors. This calculation involves a straightforward exclusive OR (XOR) operation between two binary arrays, tallying the resulting ones. Moreover, Go offers SIMD-optimized operations for these binary functions, resulting in notably faster computation than with original vectors. But how much faster exactly?»\\n[3] «## Vector Distance Calculations\\n\\nVector databases use vector representations generated from machine learning models to capture the meaning of data. The fact that these vectors are numerical representations of data objects allows us to apply mathematical operations to them, such as calculating the distance between two vector representations to determine their similarity. To calculate the distance between two vectors, you can use several similarity measures, detailed below:\\n\\n\\n![Vector Distance Calculations](./img/vector_distance.png)\\n\\n\\nAs you can see, there are many different similarity measures - cosine, dot product or euclidean (also know as L2) are the most commonly used in practice. To learn more about the different distance metrics, you can continue reading our blog post on [What are Distance Metrics in Vector Search?](/blog/distance-metrics-in-vector-search/)\\n\\nWhen a vector database needs to retrieve semantically similar objects it has to calculate one of these distance metrics to assess how close a query point is to candidate points in the database. In fact, this vector distance calculation needs to be conducted when you build the index as well as anytime you perform any CRUD operation.»\\n[4] «The next natural question is: How do we compress vectors? ## How to Efficiently Compress Vectors\\n\\nThe main idea behind vector compression is to have a “good-enough” representation of the vectors (as opposed to a perfect representation) so they take up less space in memory while still allowing us to calculate the distance between them in a performant and accurate way. Compression could come from different sources. We could, for example, aim to reduce redundant data to store information more efficiently. We could also sacrifice accuracy in the data in favor of space.»\\n[5] «What simple operation does it spend the majority of its time performing? If you guessed **vector distance calculations** … BINGO! 🎉\\n\\nWhile vector databases use many techniques and algorithms to improve performance (including locality graphs, quantization, hash based approaches), at the end of the day, efficient distance calculations between high-dimensional vectors is a requirement for a good vector database. In fact, when profiling Weaviate indexed using HNSW, we find that 40%-60% of the CPU time is spent doing vector distance calculations. So when someone tells us that they can make this quintessential process *much faster* they have our full attention! If you want to learn how to leverage algorithmic and hardware optimizations to make vector search 40% faster keep reading!\\n\\nIn this post we’ll do a technical deep dive into different implementations for vector distance calculations and optimizations enabled by Intel’s new 5th Gen Xeon Processor - Emerald Rapids, parallelization techniques using SIMD with the Intel AVX-256 and Intel AVX-512 instruction sets, loop unrolling and compiler optimizations by transpiling C to Go assembly. We explain how we attained a **~40% QPS speed up at 90% Recall** in Weaviate running on Intel’s new Xeon Processor, Emerald Rapids.»\\n\\nQuestion: What is the impact of vector compression and decompression on the accuracy of distance calculations between the original vectors?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer to this question, we need to understand the impact of compression and decompression on the original vectors and how it affects the accuracy of distance calculations.\\n\\nAnswer: Vector compression and decompression can introduce distortion to the original vectors, and this distortion can affect the accuracy of distance calculations. The compression function $C$ applied to the original vectors $x$ and $y$ results in a shorter representation, but when these compressed vectors are decompressed back into the original space using $C\\'$, there is a slight difference in the reconstructed vectors $x\\'\\'$ and $y\\'\\'$. This distortion, denoted as $\\\\delta$, adds to the original distance calculation, so the new distance between $x$ and $y$ is given by $d(x, y) = d(x\\'\\', y\\'\\') + \\\\delta$. The impact of this distortion on the accuracy of distance calculations depends on the specific compression and decompression functions used and the similarity measure employed to calculate vector distances. In the context of Weaviate, the compression rate of 1:32 for floating-point vectors to `float32` arrays is mentioned, and it is stated that this compression allows for faster computation through bitwise operations. However, the specific impact on accuracy is not explicitly mentioned and would depend on the chosen similarity measure and the characteristics of the data being compressed. In general, vector compression techniques aim to find a balance between reducing storage requirements and maintaining the accuracy of vector operations. The choice of compression algorithm and the acceptable level of distortion depend on the specific use case and the sensitivity of the application to errors in vector calculations.\\n\\n---\\n\\nContext:\\n[1] «It is one of my favorite topics about Go memory management, and I\\'d be happy to write this post. Please let me know. For this one, it\\'s enough to understand that sometimes heap-allocations happen even when we think they shouldn\\'t. That is important to know because those allocations will put pressure on the GC, which is required for an unexpected OOM situation. ## Why would you run OOM – even when there is enough memory available?»\\n[2] «In this article, I will invite you on a journey. We will cover:\\n* How memory allocations work in Go.\\n* When the Garbage Collector runs, and what the implications are. * Why it was far too easy to run out of memory before Go 1.19. * What GOMEMLIMT is and why it can prevent premature OOM kills. * Run an experiment first without and then with GOMEMLIMIT\\n\\n## What is a garbage-collected language?»\\n[3] «However, as the next few paragraphs show, there are some caveats (and a terrific new solution called GOMEMLIMIT). If you aren\\'t careful, you can run OOM even when you shouldn\\'t. But before we dive into this, we need to talk about stack and heap allocations and why something ends up on the heap. ## The stack vs. the heap\\nIn short, there are two ways to allocate memory: On the stack or the heap.»\\n[4] «We found that the data structures relied on dynamic allocations. So, even if we knew that an array would never be longer than 64 elements, the Go runtime could still decide to allocate an array[100] in the background when the array reaches 51 elements. To fix that, we switched to static allocations, and Weaviate instructs the Go runtime to allocate the exact number of elements. This reduced **static** memory usage even when idle. ### Results\\n\\n🎉 Between these two major updates, plus some smaller ones, we saw a **significant reduction in memory usage of 10-30%**🚀.»\\n[5] «* After introducing `GOMEMLIMIT=2750MiB`, our application no longer crashed and could efficiently use the available memory. The import succeeded. Does that mean that GOMEMLIMIT is your *get-out-of-jail-free* card for heap allocations? No. A Go application that gets heavy usage still has to ensure allocation efficiency. Simply setting a GOMEMLIMIT will not make an inefficient application efficient.»\\n\\nQuestion: Why can heap allocations in Go lead to out-of-memory errors even when there appears to be enough memory available?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to understand the memory management system in Go and the role of the Garbage Collector (GC). Go has automatic memory management, which means the Go runtime handles memory allocation and deallocation. Memory allocations can happen on the stack or the heap. The heap is a dynamic memory allocation area, and the stack is a static memory allocation area with a fixed size. When objects are allocated on the heap, the Go runtime keeps track of these allocations and their sizes. The Garbage Collector is then responsible for deallocating unused memory on the heap. However, the GC runs periodically, and if there are a large number of allocations happening in a short period, it can lead to a situation where the available memory is fragmented and not contiguous, even if the total memory available seems sufficient. Additionally, as mentioned in the context, the Go runtime may sometimes allocate more memory than what is apparently necessary. For example, in context [4], it is mentioned that an array of 51 elements was allocated as an array of 100 elements. This can further contribute to the issue of fragmented memory and unexpected out-of-memory (OOM) errors. Reasoning: Let\\'s understand this step by step to provide a comprehensive answer.\\n\\nAnswer: Heap allocations in Go can lead to out-of-memory errors, even when there seems to be enough memory available due to the dynamic nature of memory allocation on the heap and the periodic garbage collection process. The Go runtime may allocate more memory than needed, and if a large number of allocations occur in a short period, it can fragment the available memory. As a result, the GC may not be able to deallocate memory efficiently, leading to unexpected OOM errors. Additionally, the runtime\\'s behavior of over-allocating memory can further contribute to this issue.\\n\\n---\\n\\nContext:\\n[1] «This pillar is all about performance. The first big step will be the move towards a [Native Roaring Bitmap Index](https://github.com/weaviate/weaviate/issues/2511). In the most extreme case, this new index time can speed up filtered vector search [by a factor of 1000](https://twitter.com/etiennedi/status/1621180981519458305). But it doesn’t stop there; we are already thinking about the next steps. Whether you want faster aggregations or new types of specialized indexes, we will ensure you can hit all your p99 latency targets with Weaviate.»\\n[2] «When we generate these symbolic structures like “room_type”, we can then use it in filtered vector search! Filtered Vector Search has recently been made [1000(!) times faster in Weaviate thanks to roaring bitmap indexing](/blog/weaviate-1-18-release#faster-filtering-with-roaring-bitmaps)!\\n\\nAnother powerful idea is the emergence of `Knowledge Graphs 2.0`. We can use LLMs to suggest relations between pieces of content and then connect these nodes together with the relational edges. Check out this example of GraphGPT!\\n\\n## From ChatGPT to AutoGPT and the role of Weaviate\\nAnother title for this section would be “Complex Tasks that run overnight”. The latest imagination inspiring feat of AI has been the rise of AutoGPT! AutoGPT is a loop of prompts in which the LLM breaks a complex task into subtasks, executes them, reflects on the plan, and so on until the complex task has been completed. Generative Feedback Loops are one of the key pieces to this puzzle.»\\n[3] «With the Hybrid-SaaS option, Weaviate will set up and manage your database cluster on a dedicated private cloud system on the cloud platform of your choice or on premises in your data center. ## The Full Power of the Weaviate Vector Database\\nThe Weaviate Cloud is always updated with the latest [Weaviate open source database](https://github.com/weaviate/weaviate) version and functionality. If you’ve been using Weaviate Cloud since its release, you’ve seen it enhanced with new core Weaviate database functionality, such as:\\n\\n- [ChatGPT plug-in](https://weaviate.io/blog/weaviate-retrieval-plugin)\\n- [Bitmap indexing](https://weaviate.io/blog/weaviate-1-18-release#faster-filtering-with-roaring-bitmaps) for faster filtered search\\n- [HNSW-PQ index and data compression](https://weaviate.io/blog/weaviate-1-18-release#hnsw-pq) for greater cost efficiency\\n- [Database replication](https://weaviate.io/blog/weaviate-1-18-release#replication)\\n- [WAND and BM25 search](https://weaviate.io/blog/weaviate-1-18-release#improvements-to-bm25-and-hybrid-search)\\n- [Cursor API](https://weaviate.io/blog/weaviate-1-18-release#cursor-api) for processing query results\\n\\n## Develop More Productively - Interactive Admin & Query Console\\nWe care a lot about the Weaviate user experience, so developer productivity is a top design priority. Hence, we implemented many new features in the Weaviate Cloud Console, which now allows you to administer your Weaviate clusters - create, delete, restore, monitor your clusters and activate high-availability settings. ![Weaviate Cloud Service Console ](./img/wcs-connected-instances.png)\\n\\nThe console also features a GraphQL IDE that allows you to interactively query your Weaviate databases.»\\n[4] «Add any ordered input, for example, 1, 2, 3, 4 and see how the tree stays balanced. ### Results\\nWe\\'ve run a few local tests to paint a better picture of what you could expect. First, we saw that the RB-Tree is a factor of 20 faster than the binary tree when adding objects with sequential keys (just the tree, without anything else). With a full import test, we saw a **3x performance improvement** 🚀. * Weaviate `1.14.1` - import time **~38 minutes**\\n* Weaviate `1.15.0` - import time **~13 minutes** 🔥\\n\\n## More efficient filtered aggregations\\n\\n![More efficient filtered aggregations](./img/filtered-aggregation.png)\\n\\nRecently we\\'ve been working with a customer who was running multiple filtered aggregations on a large dataset.»\\n[5] «---\\ntitle: Weaviate 1.15 release\\nslug: weaviate-1-15-release\\nauthors: [connor, erika, laura, sebastian]\\ndate: 2022-09-07\\ntags: [\\'release\\']\\nimage: ./img/hero.png\\ndescription: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\\n---\\n![Weaviate 1.15 release](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWe are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\\n\\nIf you like your content brief and to the point, here is the TL;DR of this release:\\n1. [☁️Cloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\\n1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1.»\\n\\nQuestion: What is the expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: In the provided context, it is mentioned that the new Native Roaring Bitmap Index can speed up filtered vector search by a factor of 1000. This is specifically stated in the first paragraph of the context.\\n\\nAnswer: Therefore, the expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index is a factor of 1000.\\n\\n---\\n\\nContext:\\n[1] «import GenerativeFeedbackLoopsLoop from \\'/_includes/code/generative.feedback.loops.loop.mdx\\';\\n\\n<GenerativeFeedbackLoopsLoop/>\\n\\nLet\\'s see the resulting ads and the descriptions they were sourced from:\\n\\n\\n<details>\\n  <summary>\\n    OpenAI Generations\\n  </summary>\\n  <div>\\n\\n| Description | Ad |\\n|-------------|----|\\n| This cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn is the perfect place to stay for an extended period of time. Located at 40.68876 latitude and -73.94312 longitude, this room is clean and quiet, and has a minimum stay of 60 nights. With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here. There are no reviews yet, so be the first to experience this great room! | Are you looking for a place to stay for an extended period of time? Look no further than this cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean and quiet, and has a minimum stay of 60 nights.»\\n[2] «We will add `target` as a property to the `Ad` class, like this:\\n\\n```python\\ntarget_property = {\\n   \"dataType\": [\\n       \"text\"\\n   ],\\n   \"name\": \"target\",\\n   \"description\": \"High-level audience target for this ad.\"\\n}\\n\\n\\nclient.schema.property.create(\"Ad\", target_property)\\n```\\n\\n<details>\\n  <summary>\\n    OpenAI Generations\\n  </summary>\\n  <div>\\n\\n\\n| Ad Target | Ad |\\n|-----------|----|\\n| For Young Couples | Are you looking for a cozy place to stay for an extended period of time? Look no further than this private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean, quiet, and perfect for young couples. With a minimum stay of 60 nights and availability of 365 days a year, you can be sure to find a spot here. Plus, you can be the first to experience this great room - there are no reviews yet! Don\\'t miss out on this great opportunity - book your stay today! |\\n| For Elderly Couples | Are you an elderly couple looking for a place to stay for an extended period of time? Look no further than this cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean and quiet, and has a minimum stay of 60 nights.»\\n[3] «With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here. There are no reviews yet, so be the first to experience this great room! Enjoy the peace and quiet of this room and make it your home away from home. |\\n| For Single Travelers | Are you a single traveler looking for a place to stay for an extended period of time? Look no further than this cozy private room in the Bedford-Stuyvesant neighbourhood of Brooklyn! Located at 40.68876 latitude and -73.94312 longitude, this room is clean, quiet, and has a minimum stay of 60 nights. With a calculated host listings count of 1 and availability of 365 days a year, you can be sure to find a spot here.»\\n[4] «| Welcome to this beautiful two bedroom apartment in the Upper East Side of Manhattan! Located in the East Harlem neighbourhood, this apartment is just a short walk away from Central Park and surrounded by a variety of restaurants, shops, and other attractions. Enjoy all the necessary amenities in this apartment, available for rent for a minimum of seven nights at a price of $190 per night. With 249 days available for rent out of the year, this is the perfect place to stay for your next vacation or business trip! Book now and experience the best of Manhattan! |\\n| This cozy private room is located in the Flatbush neighbourhood of Brooklyn. Enjoy the convenience of being in the city while still having access to the peacefulness of the countryside. The room is available for a minimum of one night and is available for booking all year round.»\\n[5] «By emphasizing the use of real-time information stored in Weaviate, we continuously update the language model with the latest data, bypassing the limitation of its encoded knowledge. Ultimately combining the peanut butter of factual information from the database with the jelly of LLM language understanding. ### Generate a description for each Listing\\n\\nimport GenerativeFeedbackLoopsDescription from \\'/_includes/code/generative.feedback.loops.description.mdx\\';\\n\\n<GenerativeFeedbackLoopsDescription/>\\n\\nNow let\\'s test a semantic search through the Listings, each vectorized with the respective LLM-generated `description`:\\n\\nimport GenerativeFeedbackLoopsSearch from \\'/_includes/code/generative.feedback.loops.search.mdx\\';\\n\\n<GenerativeFeedbackLoopsSearch/>\\n\\n<details>\\n  <summary>\\n    OpenAI Generations\\n  </summary>\\n  <div>\\n\\n| Rank | Result |\\n|------|--------|\\n| 1    | This private room in the Bedford-Stuyvesant neighbourhood of Brooklyn is the perfect spot for a long-term stay. The space is clean and quiet, providing a peaceful retreat from the hustle and bustle of the city. With a minimum stay of 60 nights, you\\'ll have plenty of time to explore all that Brooklyn has to offer.»\\n\\nQuestion: What is the minimum stay requirement for the private room available in the Bedford-Stuyvesant neighbourhood of Brooklyn?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To determine the minimum stay requirement for the private room in the Bedford-Stuyvesant neighbourhood of Brooklyn, we need to examine the provided information carefully. We are given details about a private room in this neighbourhood, including its location, amenities, and availability. By focusing on the relevant sentences, we can extract the necessary information to answer the question.\\n\\nAnswer: The minimum stay requirement for the private room in the Bedford-Stuyvesant neighbourhood of Brooklyn is 60 nights. This information is mentioned twice in the context, emphasizing the duration of the minimum stay for potential guests.\\n\\n---\\n\\nContext:\\n[1] «This case is quite similar to our discussion of Multi-Index Routing and we can similarly evaluate generations with a prompt that explains the needs for SQL and Vector Databases and then asks the LLM whether the router made the right decision. We can also use the RAGAS Context Relevance score for the results of the SQL query. <img\\n  src={require(\\'./img/sql-router.png\\').default}\\n  alt=\"SQL Router Query Engine\"\\n  style={{ maxWidth: \"60%\" }}\\n/>\\n\\nConcluding our discussion of “From RAG to Agent Evaluation”, we believe that it is still too early to tell what the common patterns will be for agent use. We have intentionally shown the multi-hop query engine and query router because these are relatively straightforward to understand. Once we add more open-ended planning loops, tool use and the associated evaluation of how well the model can format API requests to the tool, and more meta internal memory management prompts such as the ideas in MemGPT, it is very difficult to provide a general abstraction around how Agents will be evaluated.»\\n[2] «A system decided to break it into the sub questions {sub_question_1} and {sub_question_2}. Does this decomposition of the question make sense?”. We then have two separate RAG evaluations for each of the sub questions, and then an evaluation of whether the LLM was able to combine the answers from each question to answer the original question. As another example of evolving complexity from RAG to Agents, let’s consider Routing Query Engines. The following visual illustrates an agent routing a query to either an SQL or Vector Database query.»\\n[3] «We covered core concepts around vector databases, such as vector embeddings, and discussed that vector databases enable efficient vector search by leveraging ANN algorithms. Additionally, we explored the tool landscape around vector databases and discussed the advantages of vector databases over traditional and vector-capable databases and vector libraries. import WhatNext from  \\'/_includes/what-next.mdx\\'\\n\\n<WhatNext />»\\n[4] «We don’t just want the items with the most semantic similarity to the query, but also those that are less than $100. LlamaIndex’s [Query Engine](https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html#building-multi-document-agents) presents really nice abstractions to leverage this concept of LLM Query Understanding. In the following illustration, the LLM adds the filter, where “animal” = “dog” to facilitate searching for information about Goldendoodles. This is done by prompting the LLM with information about the data schema and the syntax for formatting structured vector searches in Weaviate. <img\\n    src={require(\\'./img/self-query.gif\\').default}\\n    alt=\"alt\"\\n    style={{ width: \"100%\" }}\\n/>\\n\\n## Index Construction\\nLarge Language Models can also completely change the way we index data for search engines, resulting in better search quality down the line.»\\n[5] «### Vector Database vs. Vector-Capable Database (SQL and NoSQL)\\n\\nToday, many existing databases have already enabled vector support and vector search. However, they usually don’t index the vector embeddings, which makes the vector search slow. Thus, an advantage of AI-native vector databases over vector-capable databases is their efficiency in vector search due to vector indexing. ### Vector Database vs.»\\n\\nQuestion: What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases as discussed in the document?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To evaluate agents in the context of Multi-Index Routing and SQL and Vector Databases, we need to consider the following: - The complexity of the evaluation increases when more open-ended planning loops, tool use, and meta internal memory management prompts are introduced. Therefore, it is challenging to provide a general abstraction for agent evaluation. - The document mentions evaluating generations with a prompt that explains the needs for SQL and Vector Databases and then asking the LLM whether the router made the correct decision. - The RAGAS Context Relevance score can be used for the results of the SQL query to assess the agent\\'s performance. - Another aspect to consider is the decomposition of questions and the ability of the LLM to combine answers from sub-questions to answer the original question. - The visualization provided in the document illustrates an agent routing a query to either an SQL or a Vector Database query, showing the potential for LLMs to understand and facilitate structured searches efficiently.\\n\\nAnswer: The document discusses that it is too early to determine the standard patterns for agent evaluation as the field is constantly evolving. However, some key considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases include: - Using prompts to explain the needs for SQL and Vector Databases and then assessing the LLM\\'s decision-making. - Employing the RAGAS Context Relevance score for evaluating the results of the SQL query. - Considering the decomposition of questions into sub-questions and the LLM\\'s ability to combine answers. - Visualizing agent query routing to either SQL or Vector Databases and understanding the efficiency of vector searches. - Evaluating the agent\\'s performance in open-ended planning loops, tool use, and meta internal memory management prompts.\\n\\n---\\n\\nContext:\\n[1] «Note, how it doesn’t even mention the Cohere model, as all of that is done through configuration. ```python\\ndef semantic_serch(query):\\n   client = _setup.connect_to_weaviate()\\n\\n   nearText = {\\n       \"concepts\": [query],\\n       \"distance\": 0.7,\\n   }\\n\\n   properties = [\\n       \"name\", \"content\", \"enName\", \"link\", \"lang\",\\n       \"_additional {certainty distance}\"\\n   ]\\n\\n   result = (\\n       client.query\\n       .get(\"Document\", properties)\\n       .with_near_text(nearText)\\n       .with_limit(5)\\n       .do()\\n   )\\n\\n   return result[\\'data\\'][\\'Get\\'][\\'Document\\']\\n```\\n## How does this work\\nIt is also important to understand how Weaviate and Cohere communicate with each other. Weaviate fully automates communication with Cohere’s AI Platform. There are two key scenarios when Weaviate will communicate with Cohere:\\n\\n* Data import and modifications – to vectorize data objects\\n* Query time – to vectorize the query\\n\\n### Data import + modifications\\nWhen you import (or update) data into Weaviate, Weaviate will:\\n1. Receive the data object\\n1.»\\n[2] «But until now, getting outputs from Cohere into Weaviate required a few extra steps. This included sending requests to Cohere to vectorize text objects, retrieving the output vectors, and then importing it  into Weaviate along with the matching objects. ### Text2vec-cohere\\nOver the last few weeks, we’ve partnered with Cohere to create a new Weaviate module – [text2vec-cohere](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) – it allows you to use Cohere’s LLMs to run complex NLP queries straight from Weaviate. You can learn all about [text2vec-cohere from the docs](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere). And there is a **lot** that you can do with Cohere, including some very new, exciting developments.»\\n[3] «Send the relevant properties for vectorization to Cohere\\n1. Store the data object and index the vector in Weaviate\\n\\n![Weaviate Import with Cohere](./img/cohere-import.png)\\n\\nThis allows Weaviate to efficiently store and query your data. ### Query\\nWhen you query Weaviate, Weaviate will:\\n1. Receive the query\\n1. Send the query to Cohere to vectorize it\\n1.»\\n[4] «The `Blogs` collection, will have two properties: `content` and `synthetic_query`. ```python\\nschema = {\\n   \"classes\": [\\n       {\\n           \"class\": \"Blogs\",\\n           \"description\": \"Weaviate blogs\",\\n           \"vectorizer\": \"text2vec-cohere\",\\n           \"properties\": [\\n               {\\n                   \"name\": \"content\",\\n                   \"dataType\": [\"text\"],\\n                   \"description\": \"Content from the blogs.\",\\n               },\\n               {\\n                   \"name\": \"synthetic_query\",\\n                   \"dataType\": [\"text\"],\\n                   \"description\": \"Synthetic query generated from a LM.\"\\n               }\\n           ]\\n       }\\n   ]\\n}\\n\\nclient.schema.create(schema)\\n```\\n\\nIf you need to reset your schema and delete objects in a collection, run:\\n`client.schema.delete_all()` or `client.schema.delete_class(\"Blogs\")`\\n\\n#### Synthetic Queries\\n\\nWe are using DSPy’s signature and chain-of-thought module to generate synthetic queries using Cohere’s `command-nightly` model. The signature implements the prompt that: 1. Describes the task, 2. Gives an example of the `input` and `output` fields and 3.»\\n[5] «Capturing these variations is important for this specific use case and it’s also universally applicable to other use cases involving user reviews. But how did we accomplish this with Weaviate? The process is surprisingly straightforward. We created `Product` objects embedded with their reviews, summaries, and other metadata, to enable semantic search. To optimize performance, we vectorize specific fields using the `gpt-3.5-turbo` model and leave out fields that do not require vectorization.»\\n\\nQuestion: How does Weaviate process and vectorize queries using Cohere?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s break down the process step by step to understand how Weaviate utilizes Cohere for processing and vectorizing queries.\\n\\nAnswer: Weaviate, a powerful vector search engine, integrates with Cohere\\'s AI Platform to enhance its capabilities. Here\\'s how Weaviate processes and vectorizes queries using Cohere: 1. **Query Receipt**: When a query is received by Weaviate, it recognizes the need to vectorize the query text. 2. **Communication with Cohere**: Weaviate sends the query to Cohere\\'s AI Platform for vectorization. This communication is automated, thanks to the text2vec-cohere module, a collaboration between Weaviate and Cohere. 3. **Vectorization by Cohere**: Cohere\\'s large language models, such as the `command-nightly` model, perform complex natural language processing on the query. This includes understanding the query intent, generating chain-of-thought responses, and producing output vectors that represent the query\\'s semantic meaning. 4. **Query Execution**: Once the query is vectorized by Cohere, Weaviate uses this vector to search for relevant data objects in its database. It compares the query vector with the vectors of stored data objects to find the most similar matches. 5. **Result Retrieval**: Weaviate retrieves the data objects with vectors closest to the query vector and returns them as the query results. This ensures that the results are contextually relevant to the user\\'s query. By leveraging Cohere\\'s advanced language models, Weaviate can process and understand queries more effectively, enabling it to provide accurate and semantically meaningful search results. This integration enhances Weaviate\\'s capabilities in handling complex NLP queries and improves the overall user experience.\\n\\n---\\n\\nContext:\\n[1] «Typically this is in the context of recommendation in which we have metadata about users, as well as the documents or items. So for example, say we have features that describe a Users looking for Movies such as:\\n\\nUser Features - (Age, Gender, Location, Occupation, Preferences)\\nMovie Features - (Release Year, Genre, Box Office, Duration). So together, the Metadata ranker takes as input something like: [Age, Gender, Location, Occupation, Preferences, Release year, Genre, Box Office, Duration] and predicts a score of how much this User will like the movie. We can fix the User features and rotate in each Document to get a score for each of the candidate movies (retrieved with something like ref2vec) to rank with. In addition to vectors, Weaviate also enables storing metadata features about objects such as `price`, or `color`.»\\n[2] «They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.»\\n[3] «However, most of the LLM APIs don’t actually give us these probabilities. Further, this is probably pretty slow. We will keep an eye on it, but it doesn’t seem like the next step to take for now. ## Metadata Rankers\\nWhereas I would describe Cross-Encoders as `content-based` re-ranking, I would say Metadata rankers are `context-based` re-rankers. Metadata rankers describe using symbolic features to rank relevance.»\\n[4] «We can send these features to wherever our Metadata Ranker is hosted and get those scores back to Weaviate to sort our search results. This is also closely related to another category of ranking methods that use models like XGBoost to combine features, as well as say the bm25 score, vector distance, and maybe even the cross encoder score as well. This is a pretty interesting technique when you additionally factor in multiple properties. For example, we could have a bm25, vector, and cross encoders for the `title`, as well as `content` properties and use a learned model to combine these into a final ranking score. I recently came across a paper titled “Injecting the BM25 Score as Text Improves BERT-Based Re-rankers” published in ECIR 2023.»\\n[5] «As described in our [previous article](https://weaviate.io/blog/ranking-models-for-better-search), re-ranking models are new to the scene of zero-shot generalization. The story of re-rankers has mostly been tabular user features combined with tabular product or item features, fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt. Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score. This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.»\\n\\nQuestion: How does the Metadata ranker predict which movies a user will like based on their personal features and movie characteristics?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to understand how the Metadata Ranker utilizes user and movie data to make predictions. We can break down the process into the following steps: Step 1: Understand User Features: The Metadata Ranker takes into account various user features such as age, gender, location, occupation, and preferences. These features provide a personalized profile of the user, allowing the system to tailor movie recommendations accordingly. Step 2: Analyze Movie Metadata: For each movie, the ranker considers metadata such as release year, genre, box office performance, and duration. This information helps match movies to the user\\'s tastes and preferences. Step 3: Combine User and Movie Data: By integrating user features with movie metadata, the Metadata Ranker can make context-aware predictions. It predicts a score or ranking for each movie, indicating how much the user is likely to enjoy it. Step 4: Rank and Recommend: Finally, the movies are ranked based on the predicted scores, and the system can recommend movies with the highest predicted relevance or appeal to the user.\\n\\nAnswer: The Metadata Ranker utilizes user features (e.g., age, gender, preferences) and movie characteristics (e.g., genre, release year) to predict a score indicating how much a user will like a particular movie. By combining these features, the ranker can provide personalized movie recommendations tailored to the user\\'s tastes and interests.\\n\\n---\\n\\nContext:\\n[1] «Please boost relevance based on recency and if the Author is “Connor Shorten”. ```\\n\\nEach search result then comes packaged with their associated metadata in a key-value array. This offers the additional benefit of allowing business practitioners to easily swap out the ranking logic. This also holds the benefit of dramatically increasing the interpretability of recommendation systems, since LLMs can easily be prompted to provide an explanation of the ranking in addition to the ranking itself. ## Search Result Compression\\n\\nTraditionally, search results are presented to human users as a long list of relevant websites or passages.»\\n[2] «---\\ntitle: Distance Metrics in Vector Search\\nslug: distance-metrics-in-vector-search\\nauthors: [erika]\\ndate: 2023-08-15\\nimage: ./img/hero.png\\ntags: [\\'concepts\\']\\ndescription: \"Learn about why you need distance metrics in vector search and the metrics implemented in Weaviate (Cosine, Dot Product, L2-Squared, Manhattan, and Hamming).\"\\n\\n---\\n\\n![Distance Metrics in Vector Search](img/hero.png)\\n\\n<!-- truncate -->\\n\\n[Vector databases](/blog/what-is-a-vector-database) - like\\xa0[Weaviate](/developers/weaviate)\\xa0- use\\xa0**machine learning models**\\xa0to analyze data and\\xa0**calculate vector embeddings**. The vector embeddings are\\xa0**stored together with the data**\\xa0in a database, and later are used to query the data. In a nutshell, a vector embedding is an array of numbers, that is used to describe an object. For example, strawberries could have a vector\\xa0`[3, 0, 1]`\\xa0– more likely the array would be a lot longer than that. *Note*, the meaning of each value in the array, depends on what machine learning model we use to generate them.»\\n[3] «It specifies various options, such as the batch size, Weaviate host, and API key for authentication. One of the key features of this integration is its seamless interaction with Confluent\\'s Schema Registry. The integration automatically extracts the schema ID from the message\\'s value and queries the Schema Registry to retrieve the associated schema. This schema is then used to deserialize the rest of the data, ensuring that the data is accurately streamed into Weaviate. ### Write The Streaming DataFrame\\nWith your Spark environment initialized and your data stream configured, you\\'re now ready to start the actual data streaming process from Confluent Cloud to Weaviate.»\\n[4] «How, you ask? Having redundancy is key to achieving this. And such redundancy, or replication, has been available for a while in Weaviate for production use. In fact, configuring replication is actually easy and simple, and using it can lead to huge benefits. In this post, we will explain the benefits of replication and show you how to configure it.»\\n[5] «They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.»\\n\\nQuestion: What are the benefits of packaging search results with associated metadata in a key-value array?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, let\\'s break down the relevant information from the context and analyze the benefits mentioned.\\n\\nAnswer: There are two main benefits mentioned in the context. Firstly, it allows business practitioners to easily swap out the ranking logic, providing flexibility and control over the ranking algorithm used. Secondly, it dramatically increases the interpretability of recommendation systems. By having the metadata packaged with the search results, LLMs can provide explanations for the ranking, making the system more transparent and understandable to users. This added interpretability can increase user trust and satisfaction.\\n\\n---\\n\\nContext:\\n[1] «<div align=\"center\">\\n\\n![\"Bob talks about feedback loops\"](img/bob-gen-ai.jpg)\\n##### Our CEO Bob talking about the next level of vector database capabilities - Generative Feedback Loops\\n\\n</div>\\n\\nWe also got tons of great feedback on our open-source RAG app, Verba, which we used to demo Weaviate. It’s something you can use to quickly build RAG apps that fit your use case. Check out [our intro video](https://youtu.be/IiNDCPwmqF8?si=cZxfsnY6vW75c4BD) or dive right in to [our GitHub repo](https://youtu.be/IiNDCPwmqF8?si=cZxfsnY6vW75c4BD)!\\n\\n<div align=\"center\">\\n\\n![\"Adam demos Verba\"](img/adam-verba.jpg)\\n##### Adam from our Developer Growth team demoing Verba, powered by Weaviate!\\n\\n</div>\\n\\nIt was great hearing about the different use cases and industries in which folks are building AI apps – these included chatbots and search applications for things like customer support, document search, image search, and even video search. A common pattern we saw was that organizations of all sizes are evaluating vector database platforms and want guidance on how to find the right database for them. One of our recommendations is to look for a vector database purpose-built for AI.»\\n[2] «---\\ntitle: \\'Enterprise Use Cases of Weaviate Vector Database\\'\\nslug: enterprise-use-cases-weaviate\\nauthors: [adam]\\ndate: 2024-03-12\\nimage: ./img/hero.png\\ntags: [\\'concepts\\', \\'application\\']\\ndescription: \\'Explore enterprise use cases heavily used by our customers adopting generative AI features, search capabilities, and RAG with Weaviate vector database.\\'\\n---\\n# Enterprise Use Cases for Weaviate\\n\\n![Hero image](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nimport verba_overview_video from \\'./img/verba_overview_video.mp4\\';\\nimport ragsend from \\'./img/ragsend.mp4\\';\\nimport generative_feedback_loops from \\'./img/generative_feedback_loops.mp4\\';\\n\\nThe world was taken by storm by OpenAI ChatGPT in November of 2022. The interesting thing about this is that the technology that powers ChatGPT and related language models has actually been around for some time before then and was originally discussed in [this scientific paper](https://arxiv.org/abs/1706.03762v1), released by Google, in 2017. It was also around this time that we, at Weaviate, began exploring the representations of data in different formats that carried semantic meaning to these transformer models, as described by Google’s paper. If you check out the [Weaviate GitHub repository](https://github.com/weaviate/), you’ll see that we began leveraging semantic relationships of vector embeddings as early as the publication of “Attention is All You Need” from Google. In 2019, SeMi Technologies was born, later renamed to Weaviate, and since then we’ve been building and iterating on a highly performant, open source, AI Native vector database that’s closely coupled with language models of your choosing.»\\n[3] «---\\ntitle: \\'Hurricane: Writing Blog Posts with Generative Feedback Loops\\'\\nslug: hurricane-generative-feedback-loops\\nauthors: [connor]\\ndate: 2024-03-19\\nimage: ./img/hero.png\\ntags: [\\'how-to\\', \\'concepts\\']\\ndescription: \\'Hurricane is a web application to demonstrate Generative Feedback Loops with blog posts.\\'\\n---\\n\\n![HERO image](./img/hero.png)\\n\\nimport demo from \\'./img/Hurricane-Demo.mp4\\';\\nimport phoenix from \\'./img/Phoenix-Demo.mp4\\';\\n\\nBack in May 2023, we published an initial vision for [Generative Feedback Loops](/blog/generative-feedback-loops-with-llms), which we have defined to describe saving the outputs of generative AI systems back into databases for future use. We have since extended our experiments from AirBnB listings to [Amazon products](https://medium.com/@iamleonie/recreating-amazons-new-generative-ai-feature-product-review-summaries-50640e40872a), even running every component of a Generative Feedback Loop within [Snowpark Container Services](/blog/generative-feedback-loop-with-weaviate-and-spcs#implementing-a-generative-feedback-loop-with-weaviate). Both of these applications illustrate the ability of generative AI to enhance our data. With the emergence of DSPy, we are now entering a new era of control with generative AI systems and are able to create even more compelling examples of Generative Feedback Loops. When we initially published our predictions for Generative Feedback Loops, we presented Ben Thompson’s [Aggregation Theory](https://stratechery.com/2015/aggregation-theory/) to describe some of the impacts of this technology.»\\n[4] «A generative feedback loop first leverages RAG or generative search in order to yield a set of results based on some analytics and usually the prompt contains some relevant data for your specific user and their past behaviors. From there, you embed the the LLM output back into the vector database to apply search over it again, and this query could contain questions that the prior generation will have some insight over. If you’d like to learn more about this, we’ve authored [a blog post](https://weaviate.io/blog/generative-feedback-loops-with-llms) about how you could leverage generative feedback loops to create targeted ads. <figure>\\n  <video width=\"100%\" autoplay loop muted controls>\\n    <source src={generative_feedback_loops} type=\"video/mp4\" />\\n    Your browser does not support the video tag. </video>\\n  <figcaption>Apply a generative feedback loop to Weaviate</figcaption>\\n</figure>\\n\\n\\n### Ecommerce Recommendations and Multi Modal Search\\n\\nWithin ecommerce, the applications of vector database can be used to create recommendation systems, image search systems, and general search systems.»\\n[5] «## When Do Generative Feedback Loops Run? Having illustrated the general concept of Generative Feedback Loops, where we use data from the database to supplement the factual knowledge of generative models and then write the generated outputs back to the database for future use, let’s discuss when to kick this off. ## Data Ingestion\\nData Ingestion is a shorthand reference to importing data into the database. For example, every time a new User is added to the AirBnB database, we may want to kick off a Generative Feedback Loop of retrieving information about a listing and this new user and save the new ads targeted to this user. On the other hand, we may want to kick this off when a new Listing is added, writing ads for each of our users!\\n\\nAs another example, imagine that we have a collection of podcast episodes.»\\n\\nQuestion: Who is the CEO that discussed the next level of vector database capabilities related to Generative Feedback Loops?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We can identify the relevant information from the context, which mentions the CEO discussing the next level of vector database capabilities.\\n\\nAnswer: Bob is the CEO who discussed the next level of vector database capabilities related to Generative Feedback Loops.\\n\\n---\\n\\nContext:\\n[1] «It specifies various options, such as the batch size, Weaviate host, and API key for authentication. One of the key features of this integration is its seamless interaction with Confluent\\'s Schema Registry. The integration automatically extracts the schema ID from the message\\'s value and queries the Schema Registry to retrieve the associated schema. This schema is then used to deserialize the rest of the data, ensuring that the data is accurately streamed into Weaviate. ### Write The Streaming DataFrame\\nWith your Spark environment initialized and your data stream configured, you\\'re now ready to start the actual data streaming process from Confluent Cloud to Weaviate.»\\n[2] «**Automatic Deserialization**: Thanks to the integration with Confluent\\'s Schema Registry, the data in the Kafka message has been automatically deserialized. Each key in the message\\'s data becomes its own property in the corresponding Weaviate object. For example, **`city`**, **`first_name`**, and **`last_name`** are all individual properties in Weaviate, making the data immediately usable for your AI applications. ## Closing CTAs\\n\\nNot yet a Weaviate customer? Kickstart your AI journey with a [free sandbox environment](https://console.weaviate.cloud/) in Weaviate Cloud today - no credit card required.»\\n[3] «It specifies the necessary JAR packages, including Avro and Kafka SQL libraries, to ensure compatibility and functionality with Confluent Cloud. Additionally, a custom JAR file, `confluent-connector_2.12-3.4.0_0.0.1.jar`, is included to establish the connection between Spark and Weaviate. By running this script, you initialize a Spark session with all the required configurations, laying the groundwork for streaming data from Confluent Cloud to Weaviate Cloud. ### Create A Schema In Weaviate\\n\\nBefore you can start streaming data into Weaviate, you\\'ll need to create a schema to define the structure of the Kafka messages you\\'ll be storing. Here\\'s how to do it:\\n\\n```python\\nwith open(\"../src/it/resources/schema.json\", \"r\") as f:\\n    weaviate_schema = json.load(f)\\n\\nclient.schema.create_class(weaviate_schema)\\n```\\n\\nThis code snippet reads [a JSON file](https://github.com/weaviate/confluent-connector/blob/main/src/it/resources/schema.json) containing the schema definition and uses the Weaviate client\\'s **`create_class`** method to create the schema in Weaviate.»\\n[4] «This sets the stage for the data you\\'ll be streaming from Confluent Cloud. ### Create A Streaming DataFrame To Stream A Topic From Confluent Cloud\\n\\nAfter initializing your Spark environment, the next step is to configure the Kafka data stream that you\\'ll be pulling from Confluent Cloud. Here\\'s how to set it up:\\n\\n:::caution\\nIn this example, we\\'ve already set up a topic on Confluent Cloud that receives data from the DataGen source, specifically utilizing the clickstream user template. :::\\n\\n```python\\nclickstreamDF = (\\n    spark.readStream.format(\"kafka\")\\n    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\\n    .option(\"subscribe\", confluentTopicName)\\n    .option(\"startingOffsets\", \"latest\")\\n    .option(\"kafka.security.protocol\", \"SASL_SSL\")\\n    .option(\\n        \"kafka.sasl.jaas.config\",\\n        \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\'{}\\' password=\\'{}\\';\".format(\\n            confluentApiKey, confluentSecret\\n        ),\\n    )\\n    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\n    .option(\"failOnDataLoss\", \"false\")\\n    .option(\"name\", \"clickStreamReadFromConfluent\")\\n    .load()\\n)\\n```\\n\\nThis code snippet creates a streaming DataFrame named **`clickstreamDF`** in Spark, configured to read from your Kafka topic in Confluent Cloud. It specifies various options like the bootstrap servers, topic name, and security protocols, ensuring a secure and efficient data stream.»\\n[5] «**Kafka Topic in Confluent Cloud:** The journey begins in Confluent Cloud, where you create a fully managed Kafka topic that holds the data you wish to stream into Weaviate. 2. **Spark Cluster with confluent-connector Library:** Next, you\\'ll spin up a Spark cluster loaded with our specialized Confluent-Connector library. This cluster acts as the data processing engine that facilitates the data flow between Confluent Cloud and Weaviate. 3.»\\n\\nQuestion: How does Weaviate handle Kafka message data with Confluent\\'s Schema Registry integration?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to understand how Weaviate interacts with Kafka message data and utilizes Confluent\\'s Schema Registry integration. By breaking down the context, we can identify the steps involved in the process.\\n\\nAnswer: Weaviate, in conjunction with Confluent\\'s Schema Registry integration, handles Kafka message data by automatically extracting the schema ID from the message\\'s value. This schema ID is used to query the Schema Registry and retrieve the associated schema. The integration then deserializes the data in the Kafka message, mapping each key to its own property in the corresponding Weaviate object. This automatic deserialization ensures that data from Kafka is seamlessly transformed into a format that Weaviate can utilize for AI applications.\\n\\n---\\n\\nContext:\\n[1] «A dataset labeled for Question Answering is very different from Image Classification, or even Duplicate Question Detection. ### Task Benchmarks\\nTwo excellent benchmarks that collect Supervised Learning tasks to evaluate Sentence Transformers are Knowledge Intensive Language Tasks ([KILT](https://ai.facebook.com/tools/kilt/)) and Benchmarking Information Retrieval ([BEIR](https://arxiv.org/abs/2104.08663)). **KILT** uses the same domain for all tasks, Wikipedia. Each task is labeled with a different task: Slot Filling, Question Answering, Dialogue, Fact Checking, or Entity Linking. ![KILT Benchmark](./img/KILT-min-correct.png)\\n\\n**BEIR** tests many different tasks: Fact Checking, Citation Prediction, Duplicate Question Retrieval, Argument Retrieval and more.»\\n[2] «The model must learn an intermediate function of entailment. These intermediate functions must be learned for Natural Language Inference, Fact Verification, Question Answering, and so on more so than tasks like aligning English to French translations or detecting Question paraphrases. ### Question Answering vs Fact Verification\\nThese two high-level categories are a good starting place for taxonomizing these datasets. However, there are still noticeable differences within each category. For example, what makes **Question Answering** different from **Fact Verification** for evaluating the quality of a Sentence Transformer?»\\n[3] «---\\ntitle: Weaviate 1.2 release - transformer models\\nslug: weaviate-1-2-transformer-models\\nauthors: [etienne]\\ndate: 2021-03-30\\ntags: [\\'release\\']\\nimage: ./img/hero.png\\n# canonical-url: https://medium.com/semi-technologies/weaviate-version-1-2-x-now-supports-transformer-models-4a12d858cce3\\n# canonical-name: Medium\\ndescription: \"Weaviate v1.2 introduced support for transformers (DistilBERT, BERT, RoBERTa, Sentence-BERT, etc) to vectorize and semantically search through your data.\"\\n---\\n![Weaviate 1.2 release - transformer models](./img/hero.png)\\n\\nIn the v1.0 release of Weaviate ([docs](/developers/weaviate/) — [GitHub](https://github.com/weaviate/weaviate)) we introduced the concept of [modules](/developers/weaviate/concepts/modules). Weaviate modules are used to extend the vector database with vectorizers or functionality that can be used to query your dataset. With the release of Weaviate v1.2, we have introduced the use of transformers ([DistilBERT](https://arxiv.org/abs/1910.01108), [BERT](https://github.com/google-research/bert), [RoBERTa](https://arxiv.org/abs/1907.11692), Sentence-[BERT](https://arxiv.org/abs/1908.10084), etc) to vectorize and semantically search through your data. <!-- truncate -->\\n\\n### Weaviate v1.2 introduction video\\n\\n<div className=\"youtube\">\\n    <iframe src=\"//www.youtube.com/embed/S4lXPPZvGPQ\" frameBorder=\"0\" allowFullScreen></iframe>\\n</div>\\n\\n## What are transformers? A [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) (e.g., [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))) is a deep learning model that is used for NLP tasks.»\\n[4] «The model trained to classify cats in the Paintbrush illustration will not perform as well as the model trained to classify cats in Photorealistic images, if the final use case is Photorealistic images!\\n\\nDomain differences are very common in Natural Language Processing (NLP), such as the difference between Legal Contracts, Financial Statements, Biomedical Scientific Papers, Wikipedia, or Reddit Conversations to give a few examples. ### Color-coded details\\nFor every model, Hugging Face displays a list of important **color-coded** details, such as:\\n* Blue - the **dataset** it was trained on\\n* Green - the **language** of the dataset\\n* White or Purple - **additional details** about the model\\n\\nSo, if we look at two Deep Learning models, we can see that [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large) was trained on **stsb_multi_mt**, which is a **French** dataset. ![Camembert Hugging Face Model Card](./img/camembert-model-card-min.png)\\n\\nWhile [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) was trained on **several datasets** in **English**. ![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)\\n\\nTo put it as bluntly as possible, what makes `dangvantuan/sentence-camembert-large` better at French sentence embeddings than `sentence-transformers/all-MiniLM-L6-v2` is that… it was trained on **French** sentences! There are many examples like this, models trained on **biomedical text**, **legal documents**, or **Spanish** are generally going to perform better when tested on that domain compared to models that haven\\'t been explicitly trained for the domain. Note that these tags are a part of Hugging Face’s **model cards**, an impressive effort to continue advancing the organization of Machine Learning models.»\\n[5] «---\\ntitle: How to choose a Sentence Transformer from Hugging Face\\nslug: how-to-choose-a-sentence-transformer-from-hugging-face\\nauthors: [connor]\\ndate: 2022-10-04\\ntags: [\\'integrations\\']\\nimage: ./img/hero.png\\ndescription: \"Learn about the various Sentence Transformers from Hugging Face!\"\\n---\\n![How to choose a Sentence Transformer from Hugging Face](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n[Weaviate](/developers/weaviate/) has recently unveiled a new module which allows users to easily [integrate models from Hugging Face](/blog/hugging-face-inference-api-in-weaviate) to vectorize their data and incoming queries. At the time of this writing, there are [over 700 models](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) that can be easily plugged into Weaviate. You may ask: **Why are there so many models and how do they differ?**<br/>\\nAnd more importantly: **How to choose a Sentence Transformer for Semantic Search?**\\n\\nThere are too many models to summarize in one flowchart. So instead, we will describe factors that differentiate these models and give you tools to **choose the perfect model for your use case**. <img\\n    src={require(\\'./img/huggingface-sentence-transformers.png\\').default}\\n    alt=\"Weaviate Sentence Transformers\"\\n    style={{ maxWidth: \"70%\" }}\\n/>\\n\\n## Differences in Deep Learning models\\n\\nNot too long ago, Deep Learning models were typically differentiated based on architectural decisions.»\\n\\nQuestion: What are the tasks included in the KILT benchmark for evaluating Sentence Transformers?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: To answer this question, we need to refer to the section in the context that mentions the KILT benchmark. The relevant sentence states: \"Each task is labeled with a different task: Slot Filling, Question Answering, Dialogue, Fact Checking, or Entity Linking.\"\\n\\nAnswer: The tasks included in the KILT benchmark for evaluating Sentence Transformers are Slot Filling, Question Answering, Dialogue, Fact Checking, and Entity Linking.\\n\\n---\\n\\nContext:\\n[1] «---\\ntitle: How to choose a Sentence Transformer from Hugging Face\\nslug: how-to-choose-a-sentence-transformer-from-hugging-face\\nauthors: [connor]\\ndate: 2022-10-04\\ntags: [\\'integrations\\']\\nimage: ./img/hero.png\\ndescription: \"Learn about the various Sentence Transformers from Hugging Face!\"\\n---\\n![How to choose a Sentence Transformer from Hugging Face](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n[Weaviate](/developers/weaviate/) has recently unveiled a new module which allows users to easily [integrate models from Hugging Face](/blog/hugging-face-inference-api-in-weaviate) to vectorize their data and incoming queries. At the time of this writing, there are [over 700 models](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) that can be easily plugged into Weaviate. You may ask: **Why are there so many models and how do they differ?**<br/>\\nAnd more importantly: **How to choose a Sentence Transformer for Semantic Search?**\\n\\nThere are too many models to summarize in one flowchart. So instead, we will describe factors that differentiate these models and give you tools to **choose the perfect model for your use case**. <img\\n    src={require(\\'./img/huggingface-sentence-transformers.png\\').default}\\n    alt=\"Weaviate Sentence Transformers\"\\n    style={{ maxWidth: \"70%\" }}\\n/>\\n\\n## Differences in Deep Learning models\\n\\nNot too long ago, Deep Learning models were typically differentiated based on architectural decisions.»\\n[2] «## Overview\\n![Overview](./img/hugging-face-module-overview.png)\\n\\nThe Hugging Face module is quite incredible, for many reasons. ### Public models\\nYou get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case. In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). ### Private models\\nIf you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate.»\\n[3] «The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service. > \"Running model inference in production is hard,\\nlet them do it for you.\"\\n\\n## Support for Hugging Face Inference API in Weaviate\\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\\n> Both modules are enabled by default in the [Weaviate Cloud](/pricing).»\\n[4] «Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Storing and using vector embeddings with a Weaviate vector database\\n\\nFor this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules.»\\n[5] «The model trained to classify cats in the Paintbrush illustration will not perform as well as the model trained to classify cats in Photorealistic images, if the final use case is Photorealistic images!\\n\\nDomain differences are very common in Natural Language Processing (NLP), such as the difference between Legal Contracts, Financial Statements, Biomedical Scientific Papers, Wikipedia, or Reddit Conversations to give a few examples. ### Color-coded details\\nFor every model, Hugging Face displays a list of important **color-coded** details, such as:\\n* Blue - the **dataset** it was trained on\\n* Green - the **language** of the dataset\\n* White or Purple - **additional details** about the model\\n\\nSo, if we look at two Deep Learning models, we can see that [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large) was trained on **stsb_multi_mt**, which is a **French** dataset. ![Camembert Hugging Face Model Card](./img/camembert-model-card-min.png)\\n\\nWhile [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) was trained on **several datasets** in **English**. ![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)\\n\\nTo put it as bluntly as possible, what makes `dangvantuan/sentence-camembert-large` better at French sentence embeddings than `sentence-transformers/all-MiniLM-L6-v2` is that… it was trained on **French** sentences! There are many examples like this, models trained on **biomedical text**, **legal documents**, or **Spanish** are generally going to perform better when tested on that domain compared to models that haven\\'t been explicitly trained for the domain. Note that these tags are a part of Hugging Face’s **model cards**, an impressive effort to continue advancing the organization of Machine Learning models.»\\n\\nQuestion: How do I choose the right Sentence Transformer model from Hugging Face for Semantic Search?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We know that there are a vast number of Sentence Transformer models available from Hugging Face, and that the Weaviate module provides access to these models for semantic search. The context provides some guidelines on how to choose the right model. Firstly, it is important to understand the differences in the models. The models can vary based on architectural decisions, the datasets they were trained on, and the language of the dataset. The specific domain the model is intended for is also a critical factor. For example, a model trained on French sentences will perform better for French sentence embeddings than a model trained on English sentences. The Hugging Face model cards provide color-coded details about each model, including the dataset, language, and additional information. These details can be used to select a model that aligns with the specific requirements of the semantic search use case.\\n\\nAnswer: To choose the right Sentence Transformer model from Hugging Face for Semantic Search, consider the following factors: - Language: Select a model that has been trained on a dataset in the same language as your semantic search use case. - Domain: Choose a model that is tailored to the specific domain of your data, such as legal, financial, biomedical, or general text. - Dataset: Review the Hugging Face model cards to understand the datasets the models were trained on and select one that aligns with your data. - Architectural differences: While not discussed in detail in the context, architectural decisions can also impact the performance of the model. Consider the specific architecture of each model and how it aligns with your requirements. By considering these factors and utilizing the information provided in the Hugging Face model cards, you can select the most suitable Sentence Transformer model for your semantic search application.\\n\\n---\\n\\nContext:\\n[1] «---\\ntitle: Weaviate introduces an all-new business model for vector search\\nslug: weaviate-cloud-services\\nauthors: [bob]\\ndate: 2022-09-01\\ntags: [\\'release\\']\\nimage: ./img/hero.png\\ndescription: \"Press Release: Pay-as-you-grow comes to Vector Search.\"\\n---\\n![Weaviate introduces an all-new business model for vector search](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n**Amsterdam NL - September 1, 2022**\\n\\nWeaviate, the leader in open-source vector database technology, announces an [entirely new pricing model](/pricing) for its Weaviate Cloud. Inspired by \"pay-as-you-grow\" pricing used in cloud storage, Weaviate has introduced a new business model that makes it easier and more affordable for enterprises of any scale to take advantage of its Weaviate vector database. Beginning today, users have the option of paying a usage-based rate for search on a per-dimension basis. Pricing begins at $0.05 per million dimensions. (No, that\\'s not a typo; the rate is five cents per million.)\\n\\n> \"At this point, as we\\'ve reached 1,500,000 downloads, a lot of people know and love our open-source software, but they want us to run it for them,\" says Weaviate\\'s co-founder Bob van Luijt.\" So, we\\'ve created a ‘NoOps\\' option that allows them to pay only for what they use on our optimized Weaviate Cloud.\"\\n\\nIn addition to remarkably convenient access to the latest vector database capabilities, Weaviate Cloud customers get dedicated customer support on a private Slack channel.»\\n[2] «---\\ntitle: Make Real-Time AI a Reality with Weaviate + Confluent\\nslug: confluent-and-weaviate\\nauthors: [shukri]\\ndate: 2023-09-26\\ntags: [\\'integrations\\', \\'how-to\\']\\nimage: ./img/hero.png\\ndescription: \"Learn how to build an application using Weaviate and Confluent\"\\n---\\nimport confluentVideo from \\'./img/weaviate-confluent.mp4\\';\\n\\n<video width=\"100%\" autoplay loop controls>\\n  <source src={confluentVideo} type=\"video/mp4\" />\\nYour browser does not support the video tag. </video>\\n\\n<!-- truncate -->\\n\\nToday, we’re excited to announce our new integration with [Confluent Cloud](https://www.confluent.io/confluent-cloud/?utm_campaign=tm.pmm_cd.2023_partner_cwc_weaviate_generic&utm_source=weaviate&utm_medium=partnerref). Weaviate users now have simple access to data streams from across their entire business to build a real-time, contextual, and trustworthy knowledge base fueling their AI applications. Confluent’s data streaming platform bridges the legacy and modern data stack, providing a continuous supply of AI-ready data for development of sophisticated customer experiences with constant awareness of what’s happening in the world and their business right now. > “With our new integration with Confluent Cloud, Weaviate is taking a giant leap forward in empowering businesses to build AI applications that are not just smart, but also real-time and context-aware.»\\n[3] «### Data Privacy and Compliance\\n\\nOf course, with any enterprise tool, compliance and data privacy are top of mind for any product adoption. As an open source vector database, we support many deployment models from being self-hostable in your own infrastructure using docker or kubernetes, to being deployed into your own VPC in AWS or GCP via the marketplaces, or if you’re compliance requirements allows it, you can also deploy on Weaviate Cloud which will reduce your overhead of managing your Weaviate clusters as well. You can learn more about our security standards [here](https://weaviate.io/security). ## Application of GenAI and Weaviate in Business Use Cases\\n\\nEnough with features, what are some applications of Weaviate and GenAI in the enterprise? Across many of our customers, there are a few common architectural patterns in the applications they are building.»\\n[4] «---\\nlayout: post\\ntitle: What if you could understand your unstructured data? slug: understand-your-unstructured-data\\nauthors: [laura]\\ndate: 2021-01-20\\ntags: [\\'concepts\\']\\nimage: ./img/hero.png\\n# canonical-url: https://medium.com/semi-technologies/what-if-you-could-understand-your-unstructured-data-92f0861e016\\n# canonical-name: Medium\\ndescription: \"Learn how the AI-first vector database Weaviate unlocks the potential of unstructured data and why this is important.\"\\n---\\n![What if you could understand your unstructured data?](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n## Intro\\nThese days, more and more organizations are adopting a data-driven culture. Business processes and customer experience benefit from good data collection, management and analysis. But in order to really benefit from available data, it is essential to also understand the unstructured data, like free text in PDF documents, emails, invoices or voice transcriptions. Unstructured data is especially hard to index, manage and understand.»\\n[5] «The model is deep learning neural network with many layers and weights distributed across the entire network representing the probablistic nature of your dataset. ![neural-network](./img/neural-network.png)\\n\\nThrough this simple input output system, we can leverage these foundational large language models to generate new content based on natural language prompts. And because they contain massive amounts of data, we don’t necessarily have to train custom neural networks to leverage their capabilities in real world business use cases. Beyond incredible innovation in drug discovery and chip design, these systems allow for the complete automation, or human in the loop based systems, to augment business to consumer experiences or increase enterprise output. ### Vector Databases\\n\\nVector databases are a fairly new class of database that is specifically designed to store and look up vector embeddings.»\\n\\nQuestion: What are the new capabilities available to Weaviate users for managing their business data?\\n\\nReasoning: Let\\'s think step by step in order to answer the question. We know from the context that Weaviate has introduced a new business model and pricing structure, but we also need to consider the integration with Confluent Cloud and the mention of data privacy and compliance.\\n\\nAnswer: Weaviate users now have access to new capabilities that enable them to manage their business data more effectively. Firstly, Weaviate introduced a \"pay-as-you-grow\" pricing model, making it more affordable for enterprises of any scale to adopt its vector database technology. This new business model allows users to pay a usage-based rate for search on a per-dimension basis, starting at $0.05 per million dimensions. Additionally, the integration with Confluent Cloud provides Weaviate users with access to data streams from across their entire business. This enables them to build a real-time, contextual, and trustworthy knowledge base that fuels their AI applications. By bridging the legacy and modern data stack, Confluent\\'s data streaming platform ensures a continuous supply of AI-ready data, helping enterprises develop sophisticated, context-aware AI applications. Furthermore, Weaviate supports various deployment models to address data privacy and compliance concerns. Enterprises can self-host Weaviate in their infrastructure using Docker or Kubernetes, deploy it into their own VPC in AWS or GCP marketplaces, or opt for Weaviate Cloud if their compliance requirements allow it.\\n\\n---\\n\\nContext:\\n[1] «The following image presents a high-level illustration of chunking text. ![chunking](img/chunk.png)\\n\\n### Retrieval \\nThere are four major knobs to tune in Retrieval: Embedding models, Hybrid search weighting, whether to use AutoCut, and Re-ranker models. Most RAG developers may instantly jump to tuning the embedding model used, such as OpenAI, Cohere, Voyager, Jina AI, Sentence Transformers, and many others! Developers also need to consider the dimensionality of the models and how it affects the PQ compression. The next key decision is how to weight the aggregation of sparse and dense retrieval methods in Hybrid Search. The weighting is based on the `alpha` parameter.»\\n[2] «* **Index Construction:** LLMs can transform information to facilitate building search indexes. This can range from summarizing long content to extracting structured data, identifying text chunks, managing document updates, or transformation structured data to text for vectorization. * **LLMs in Re-Ranking:** Ranking models, distinctly from Retrieval, explicitly takes the query and/or a user description, as well as each candidate document as input to the neural network to output a fine-grained score of relevance. LLMs can now do this off-the-shelf without any extra training. Quite interestingly, we can prompt the LLMs to rank with symbolic preferences such as “price” or “recency”, in addition to the unstructured text content.»\\n[3] «---\\ntitle: Running Large Language Models Privately - privateGPT and Beyond\\nslug: private-LLM\\nauthors: zain\\ndate: 2023-05-30\\nimage: ./img/hero.png\\ntags: [\\'concepts\\',\\'how-to\\']\\ndescription: \"A discussion on data privacy and privacy-preserving machine learning for LLMs\"\\n\\n---\\n\\n![Private LLMs](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nLarge Language Models (LLM’s) have revolutionized how we access and consume information, shifting the pendulum from a search engine market that was predominantly retrieval-based (where we asked for source documents containing concepts relevant to our search query), to one now that is growingly memory-based and performs generative search (where we ask LLMs to generate answers to questions based on their knowledge of and training on massive datasets). More importantly, what’s different this time is that with the advent of widely accessible products like ChatGPT that have exposed the underlying technology of these models to the wider consumer market, we’re seeing these models revolutionize how we work, learn and interact on a scale never seen before. This wide-scale adoption of LLMs makes the concerns and challenges around privacy and data security paramount, and ones that each organization needs to address. In this blog post we will explore some of the different potential approaches organizations can take to ensure robust data privacy while harnessing the power of these LLMs.\\n\\n## Understanding the Privacy Challenge\\n\\nLLMs are typically trained on vast amounts of data to develop a statistical understanding of human language patterns. If you’d like to get an introductory explanation of how these models work, please read our previous blogpost: [How LLMs Work](/blog/what-are-llms).»\\n[4] «As described in our [previous article](https://weaviate.io/blog/ranking-models-for-better-search), re-ranking models are new to the scene of zero-shot generalization. The story of re-rankers has mostly been tabular user features combined with tabular product or item features, fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt. Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score. This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.»\\n[5] «In the future, we expect further optionality ablating the capacity of the model due to inherent trade-offs of performance and latency that may make sense for some applications but not others. Discovering jointly which capacity re-ranker is needed and how many retrieved results to re-rank is another challenge for tuning the knobs in retrieval. This is also one of the lowest hanging fruit opportunities for fine-tuning custom models in the RAG stack, which we will discuss further in “Tuning Orchestration”. Another interesting knob to tune is Multi-Index Search. Similar to our discussion on chunking, this is a tricky one that may involve structural changes to the database.»\\n\\nQuestion: How will Long Context LLMs impact re-ranker models?\\n\\nReasoning: Let\\'s think step by step in order to Answer: The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question.\\n\\nAnswer:\\x1b[32mReasoning: Let\\'s think step by step in order to produce the answer. The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question.\\n\\nAnswer: The provided context does not discuss the impact of Long Context LLMs on re-ranker models. Therefore, I cannot answer your question. \\n\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_pro_1_5.inspect_history(n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cohere_fix)",
   "language": "python",
   "name": "cohere_fix_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
