{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/semantic-kernel/Chatbot_RAG_Weaviate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - A Chatbot that uses RAG with Weaviate\n",
    "\n",
    "Implements a simple workflow of constant retrieve then generate chains. Using `semantic kernel` to help with prompt engineering and orchestrating calls to LLMs and Weaviate as knowledgebase from which to retreive semantically relevant context.\n",
    "\n",
    "This chatbot not only answers using relevant retreived content but also cites sources and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install semantic-kernel==0.3.0.dev0\n",
    "#!pip install weaviate-client\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS-specific notes:\n",
    "* if you run into SSL errors when connecting to OpenAI on macOS, see this issue for a [potential solution](https://github.com/microsoft/semantic-kernel/issues/627#issuecomment-1580912248)\n",
    "* on Windows, you may need to run Docker Desktop as administrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import os\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAITextCompletion,\n",
    "    OpenAITextEmbedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.memory.weaviate import weaviate_memory_store\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True, override=True)\n",
    "\n",
    "# Using Docker\n",
    "config = weaviate_memory_store.WeaviateConfig(url=\"http://localhost:8080\")\n",
    "\n",
    "store = weaviate_memory_store.WeaviateMemoryStore(config=config)\n",
    "store.client.schema.delete_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we register the memory store to the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': <semantic_kernel.orchestration.sk_function.SKFunction at 0x10d5946d0>,\n",
       " 'save': <semantic_kernel.orchestration.sk_function.SKFunction at 0x12b032d50>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "\n",
    "kernel.add_text_completion_service(\n",
    "    \"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key, org_id)\n",
    ")\n",
    "kernel.add_text_embedding_generation_service(\n",
    "    \"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id)\n",
    ")\n",
    "\n",
    "kernel.register_memory_store(memory_store=store)\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Documents to Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some initial memories \"About Me\". We can add memories to our weaviate memory store by using `save_information_async`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION = \"AboutMe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for persisting a memory to Weaviate\n",
    "import uuid\n",
    "\n",
    "async def populate_memory(kernel: sk.Kernel) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await kernel.memory.save_information_async(COLLECTION, \n",
    "                                               id = str(uuid.uuid4()), \n",
    "                                               text = 'When I turned 5 my parents gifted me goldfish for my birthday')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'I love datascience')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'I have a black nissan sentra')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'my favourite food is popcorn')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'I like to take long walks.')\n",
    "    print(\"Sucessfully populated memories!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully populated memories!\n"
     ]
    }
   ],
   "source": [
    "await populate_memory(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreived document: When I turned 5 my parents gifted me goldfish for my birthday\n"
     ]
    }
   ],
   "source": [
    "#Conduct semantic search\n",
    "\n",
    "result = await kernel.memory.search_async(COLLECTION, 'Do I have a pet?')\n",
    "print(f\"Retreived document: {result[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can retrieve `k` closest neighbours to a query as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love datascience - Relevance: 0.894\n",
      "my favourite food is popcorn - Relevance: 0.889\n",
      "I like to take long walks. - Relevance: 0.879\n"
     ]
    }
   ],
   "source": [
    "result2 = await kernel.memory.search_async(COLLECTION, 'passion', limit=3)\n",
    "\n",
    "for res in result2: print(f\"{res.text} - Relevance: {res.relevance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a setup a prompt that will allow the LLM to use relevant context retreived from Weaviate to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_RAG(kernel: sk.Kernel) -> Tuple[sk.SKFunctionBase, sk.SKContext]:\n",
    "    sk_prompt = \"\"\"\n",
    "    You are a friendly and talkative AI.\n",
    "    \n",
    "    Answer to the user question: {{$user_input}} \n",
    "    \n",
    "    You can, but don't have to, use relevant information provided here: {{$retreived_context}} \n",
    "    \n",
    "    If you are not sure of the answer say \"I am not sure.\"\n",
    "    \"\"\".strip()\n",
    "\n",
    "    rag_func = kernel.create_semantic_function(sk_prompt, max_tokens=200, temperature=0.8)\n",
    "\n",
    "    context = kernel.create_new_context()\n",
    "\n",
    "    #Need chat history now added to kernel context \n",
    "    context[\"chat_history\"] = \"\"\n",
    "    context[\"retreived_context\"] = \"\"\n",
    "\n",
    "    return rag_func, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def RAG(kernel: sk.Kernel, rag_func: sk.SKFunctionBase, context: sk.SKContext) -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "        context[\"user_input\"] = user_input\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    context[\"retreived_context\"] = ''\n",
    "    \n",
    "    #Retrieve\n",
    "    result = await kernel.memory.search_async(COLLECTION,context[\"user_input\"], limit=5, min_relevance_score=0.5)\n",
    "    \n",
    "    for res in result:\n",
    "        context[\"retreived_context\"] += (res.text + '. \\n')\n",
    "    \n",
    "    #Then generate\n",
    "    answer = await kernel.run_async(rag_func, input_vars=context.variables)\n",
    "    \n",
    "    context[\"chat_history\"] += f\"\\nUser:> {user_input}\\nChatBot:> {answer}\\n\"\n",
    "\n",
    "    print(f\"\\n\\u001b[34mChatBot:> {answer}\\u001b[0m \\n\\n\\033[1;32m Source: {context['retreived_context']}\\u001b[0m \\n\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up a RAG chat (with memory!)\n",
      "Begin chatting (type 'exit' to exit):\n",
      "\n",
      "User:> What pets did I have when I was younger?\n",
      "\n",
      "\u001b[34mChatBot:>  It appears that you were gifted goldfish for your 5th birthday. Do you remember if you had any other pets?\u001b[0m \n",
      "\n",
      "\u001b[1;32m Source: When I turned 5 my parents gifted me goldfish for my birthday. \n",
      "I like to take long walks.. \n",
      "my favourite food is popcorn. \n",
      "I have a black nissan sentra. \n",
      "I love datascience. \n",
      "\u001b[0m \n",
      "\n",
      "User:> What are some places I would enjoy traveling to in my car?\n",
      "\n",
      "\u001b[34mChatBot:> \n",
      "\n",
      "There are plenty of great places you can enjoy traveling to in your car. Some ideas include road trips to national parks, scenic drives along the coast, or even taking a drive to explore a nearby city. Depending on your budget, you could also look for unique attractions like drive-in movie theaters, specialty shops, and restaurants. If you're interested in data science, consider taking a road trip to explore the tech scene in Silicon Valley or explore the birthplace of data science in New York. You can also stop for some popcorn along the way as a treat for yourself.\u001b[0m \n",
      "\n",
      "\u001b[1;32m Source: I like to take long walks.. \n",
      "I have a black nissan sentra. \n",
      "my favourite food is popcorn. \n",
      "I love datascience. \n",
      "When I turned 5 my parents gifted me goldfish for my birthday. \n",
      "\u001b[0m \n",
      "\n",
      "User:> exit\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up a RAG chat (with memory!)\")\n",
    "rag_func, context = await setup_RAG(kernel)\n",
    "\n",
    "print(\"Begin chatting (type 'exit' to exit):\\n\")\n",
    "chatting = True\n",
    "while chatting:\n",
    "    chatting = await RAG(kernel, rag_func, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User:> What pets did I have when I was younger?\n",
      "ChatBot:>  It appears that you were gifted goldfish for your 5th birthday. Do you remember if you had any other pets?\n",
      "\n",
      "User:> What are some places I would enjoy traveling to in my car?\n",
      "ChatBot:> \n",
      "\n",
      "There are plenty of great places you can enjoy traveling to in your car. Some ideas include road trips to national parks, scenic drives along the coast, or even taking a drive to explore a nearby city. Depending on your budget, you could also look for unique attractions like drive-in movie theaters, specialty shops, and restaurants. If you're interested in data science, consider taking a road trip to explore the tech scene in Silicon Valley or explore the birthplace of data science in New York. You can also stop for some popcorn along the way as a treat for yourself.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The chathistory can be obtained from the context.\n",
    "print(context.variables.get('chat_history')[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
