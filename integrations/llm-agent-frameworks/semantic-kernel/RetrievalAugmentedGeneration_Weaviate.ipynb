{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/semantic-kernel/RetrievalAugmentedGeneration_Weaviate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Retrieval Augmented Generation using `Weaviate` and `SK`\n",
    "\n",
    "Implements a simple workflow of retrieve then generate. Using `semantic kernel` to help with prompt engineering and orchestrating interactions with LLMs and Weaviate as a knowledge-base from which to retreive semantically relevant context. \n",
    "\n",
    "This allows us to not only control what is being generated but also cite sources along with the relevance of those sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install semantic-kernel\n",
    "#!pip install weaviate-client\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS-specific notes:\n",
    "* if you run into SSL errors when connecting to OpenAI on macOS, see this issue for a [potential solution](https://github.com/microsoft/semantic-kernel/issues/627#issuecomment-1580912248)\n",
    "* on Windows, you may need to run Docker Desktop as administrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import os\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAITextCompletion,\n",
    "    OpenAITextEmbedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.memory.weaviate import weaviate_memory_store\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Using a locally deployed instance of Weaviate \n",
    "config = weaviate_memory_store.WeaviateConfig(url=\"http://localhost:8080\")\n",
    "\n",
    "store = weaviate_memory_store.WeaviateMemoryStore(config=config)\n",
    "\n",
    "#THE LINE BELOW WILL DELETE ALL DATA IN YOUR SCHEMA!\n",
    "store.client.schema.delete_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we register the memory store to the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': <semantic_kernel.orchestration.sk_function.SKFunction at 0x12affab10>,\n",
       " 'save': <semantic_kernel.orchestration.sk_function.SKFunction at 0x12affbe10>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "\n",
    "kernel.add_text_completion_service(\n",
    "    \"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key, org_id)\n",
    ")\n",
    "kernel.add_text_embedding_generation_service(\n",
    "    \"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id)\n",
    ")\n",
    "\n",
    "kernel.register_memory_store(memory_store=store)\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First with no RAG, just Generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "That depends on you! Do you have any pets?\n"
     ]
    }
   ],
   "source": [
    "# Create a reusable function with one input parameter\n",
    "\n",
    "prompt = \"You are a friendly and talkative AI. Answer {{$input}}\"\n",
    "\n",
    "answer = kernel.create_semantic_function(prompt)\n",
    "\n",
    "print(answer(\"Do I have pets?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add some prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a friendly and talkative AI. Answer {{$input}} or say \n",
    "            'I don't know.' if you don't have an answer.\n",
    "         \"\"\"\n",
    "\n",
    "answer2 = kernel.create_semantic_function(prompt)\n",
    "\n",
    "print(answer2(\"Do I have pets?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually adding memories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some initial memories \"About Me\". We can add memories to our weaviate memory store by using `save_information_async`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION = \"AboutMe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for persisting a memory to Weaviate\n",
    "import uuid\n",
    "\n",
    "async def populate_memory(kernel: sk.Kernel) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await kernel.memory.save_information_async(COLLECTION, \n",
    "                                               id = str(uuid.uuid4()), \n",
    "                                               text = 'When I turned 5 my parents gifted me goldfish for my birthday')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'I love datascience')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'I have a black nissan sentra')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'my favourite food is popcorn')\n",
    "    \n",
    "    await kernel.memory.save_information_async(COLLECTION,\n",
    "                                              id = str(uuid.uuid4()),\n",
    "                                              text = 'I like to take long walks.')\n",
    "    print(\"Sucessfully populated memories!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully populated memories!\n"
     ]
    }
   ],
   "source": [
    "await populate_memory(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreived document: When I turned 5 my parents gifted me goldfish for my birthday\n"
     ]
    }
   ],
   "source": [
    "#Conduct semantic search\n",
    "\n",
    "result = await kernel.memory.search_async(COLLECTION, 'Do I have a pet?')\n",
    "print(f\"Retreived document: {result[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.891325980424881"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_RAG(kernel: sk.Kernel) -> Tuple[sk.SKFunctionBase, sk.SKContext]:\n",
    "    sk_prompt = \"\"\"\n",
    "    You are a friendly and talkative AI.\n",
    "    \n",
    "    Answer the {{$user_input}}. You may use information provided in {{$retreived_context}} it may be useful. \n",
    "    Say 'I don't know.' if you don't have an answer. \n",
    "    \"\"\".strip()\n",
    "\n",
    "    rag_func = kernel.create_semantic_function(sk_prompt, max_tokens=200, temperature=0.8)\n",
    "\n",
    "    context = kernel.create_new_context()\n",
    "\n",
    "    return rag_func, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def RAG(kernel: sk.Kernel, rag_func: sk.SKFunctionBase, context: sk.SKContext) -> bool:\n",
    "\n",
    "    user_input = input(\"User:> \")\n",
    "    context[\"user_input\"] = user_input\n",
    "\n",
    "    #Retrieve\n",
    "    result = await kernel.memory.search_async(COLLECTION,context[\"user_input\"])\n",
    "        \n",
    "    # Can use more ie. upto k retrieved documents by: kernel.memory.search_async(COLLECTION,limit=k,context[\"user_input\"])\n",
    "    if result[0].text:\n",
    "        context[\"retreived_context\"] = result[0].text\n",
    "    \n",
    "    #Then generate\n",
    "    answer = await kernel.run_async(rag_func, input_vars=context.variables)\n",
    "\n",
    "    print(f\"\\n\\u001b[34mChatBot:> {answer}\\u001b[0m \\n\\n\\033[1;32m Source: {context['retreived_context']}\\n Relevance: {result[0].relevance}\\u001b[0m \\n\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A powered by RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Q&A with memory!\n",
      "Ask a question (type 'exit' to exit):\n",
      "\n",
      "User:> What did my parents get me?\n",
      "\n",
      "\u001b[34mChatBot:> \n",
      "\n",
      "It sounds like your parents got you a goldfish for your birthday when you were 5!\u001b[0m \n",
      "\n",
      "\u001b[1;32m Source: When I turned 5 my parents gifted me goldfish for my birthday\n",
      " Relevance: 0.9213910102844238\u001b[0m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Setup Q&A with memory!\")\n",
    "rag_func, context = await setup_RAG(kernel)\n",
    "\n",
    "print(\"Ask a question (type 'exit' to exit):\\n\")\n",
    "answer = await RAG(kernel, rag_func, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
