{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/agentic-rag-benchmark/vanilla-rag-vs-agentic-rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RAG versus Agentic RAG\n",
    "\n",
    "## === Final Score ===\n",
    "\n",
    "### `Agentic RAG`: 34 Wins\n",
    "### `Vanilla RAG`: 11 Wins\n",
    "\n",
    "## ==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will compare Vanilla RAG with Agentic RAG on the task of answering questions about Weaviate.\n",
    "\n",
    "Both systems are connected to a Weaviate Database instance containing chunks of Weaviate's blog posts. These blog posts can help answer questions such as: \"How does BM25 work?\", \"What was released in Weaviate 1.27?\", or \"What is Retrieval-Augmented Generation?\", to give a few examples.\n",
    "\n",
    "We use an **LLM-as-Judge** to determine which answer to each question is better, the Vanilla RAG answer or the Agentic RAG answer. \n",
    "\n",
    "Both systems, including the LLM Judge, use the **GPT-4o** Large Language Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Data into Weaviate Cloud\n",
    "\n",
    "The following code cells illustrate a fairly standard process of loading markdown files from disk, chunking them into 500 token units, and importing the chunks into Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/warnings.py:133: DeprecationWarning: Dep005: You are using weaviate-client version 2.5.1.dev2434+g7589c2f. The latest version is 4.9.3.\n",
      "            Consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import weaviate.classes.config as wvcc\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
    "    }\n",
    ")\n",
    "\n",
    "print(weaviate_client.is_ready())\n",
    "\n",
    "# Create Schema\n",
    "if weaviate_client.collections.exists(\"WeaviateBlogChunk\"):\n",
    "    weaviate_client.collections.delete(\"WeaviateBlogChunk\") \n",
    "\n",
    "collection = weaviate_client.collections.create(\n",
    "    name=\"WeaviateBlogChunk\",\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_openai(\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ),\n",
    "    properties=[\n",
    "            wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"author\", data_type=wvcc.DataType.TEXT),\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = './blog'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Visualization\n",
    "\n",
    "The markdown from the blog posts are processed into 500 token chunks.\n",
    "\n",
    "To gain more intuition for what this looks like, here is a visualizaton of the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: 'Accelerating Vector Search up to +40% with Intel’s latest Xeon CPU - Emerald Rapids'\n",
      "slug: intel\n",
      "authors: [zain, asdine, john]\n",
      "date: 2024-03-26\n",
      "image: ./img/hero.png\n",
      "tags: ['engineering', 'research']\n",
      "description: 'Boosting Weaviate using SIMD-AVX512, Loop Unrolling and Compiler Optimizations'\n",
      "---\n",
      "\n",
      "![HERO image](./img/hero.png)\n",
      "\n",
      "**Overview of Key Sections:**\n",
      "- [**Vector Distance Calculations**](#vector-distance-calculations) Different vector distance metrics popularly used in Weaviate. - [**Implementations of Distance Calculations in Weaviate**](#vector-distance-implementations) Improvements under the hood for implementation of Dot product and L2 distance metrics. - [**Intel’s 5th Gen Intel Xeon Processor, Emerald Rapids**](#enter-intel-emerald-rapids)  More on Intel's new 5th Gen Xeon processor. - [**Benchmarking Performance**](#lets-talk-numbers) Performance numbers on microbenchmarks along with simulated real-world usage scenarios. What’s the most important calculation a vector database needs to do over and over again?\n"
     ]
    }
   ],
   "source": [
    "print(blog_chunks[0]) # 1 500 Token Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "\n",
    "blogs = weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
    "\n",
    "for idx, blog_chunk in enumerate(blog_chunks):\n",
    "    upload = blogs.data.insert(\n",
    "        properties={\n",
    "            \"content\": blog_chunk\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Vanilla RAG and Agentic RAG Systems\n",
    "\n",
    "The following code cells implement the Vanilla RAG and Agentic RAG systems.\n",
    "\n",
    "We also reuse the same class to implement the LLM-as-Judge.\n",
    "\n",
    "There are 4 key things to note here:\n",
    "\n",
    "1. Basic connection and conventional `generate` API\n",
    "\n",
    "2. Structured Outputs `generate`\n",
    "\n",
    "3. Add Weaviate Search as a Tool\n",
    "\n",
    "4. Function Calling Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools model used for OpenAI Function Calling API\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Literal\n",
    "\n",
    "class ParameterProperty(BaseModel):\n",
    "    type: str\n",
    "    description: str\n",
    "    enum: Optional[list[str]] = None\n",
    "\n",
    "\n",
    "class Parameters(BaseModel):\n",
    "    type: Literal[\"object\"]\n",
    "    properties: dict[str, ParameterProperty]\n",
    "    required: Optional[list[str]]\n",
    "\n",
    "\n",
    "class Function(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: Parameters\n",
    "\n",
    "\n",
    "class Tool(BaseModel):\n",
    "    type: Literal[\"function\"]\n",
    "    function: Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class LM_System():\n",
    "    def __init__(\n",
    "            self,\n",
    "            weaviate_client: weaviate.WeaviateClient,\n",
    "            model_name: str,\n",
    "            model_provider: str,\n",
    "            api_key: str\n",
    "    ):\n",
    "        self.weaviate_client = weaviate_client\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model_provider = model_provider\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        match self.model_provider:\n",
    "            case \"openai\":\n",
    "                from openai import OpenAI\n",
    "                self.client = OpenAI(api_key=self.api_key)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported model_provider: {model_provider}\")\n",
    "\n",
    "    def execute_tool(\n",
    "            self, \n",
    "            tool_name: str, \n",
    "            tool_arguments: dict\n",
    "        ) -> str:\n",
    "        match tool_name:\n",
    "            case \"search_blogs\":\n",
    "                return self.search_blogs(**tool_arguments)\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid tool_name: {tool_name}\")\n",
    "\n",
    "    def search_blogs(\n",
    "            self, \n",
    "            search_query: str\n",
    "        ) -> str:\n",
    "        search_collection = self.weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
    "        results = search_collection.query.hybrid(\n",
    "            query=search_query,\n",
    "            limit=5\n",
    "        )\n",
    "        stringified_response = \"\"\n",
    "        for idx, o in enumerate(results.objects):\n",
    "            stringified_response += f\"Search Result: {idx+1}:\\n\"\n",
    "            for prop in o.properties:\n",
    "                stringified_response += f\"{prop}:{o.properties[prop]}\"\n",
    "            stringified_response += \"\\n\"\n",
    "        \n",
    "        return stringified_response\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            prompt: str\n",
    "    ) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant. Use the supplied tools to assist the user.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def generate_with_output_model(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            output_model: BaseModel\n",
    "    ):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant. Use the supplied tools to assist the user.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        response = self.client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            response_format=output_model\n",
    "        )\n",
    "        parsed_response = response.choices[0].message.parsed\n",
    "        parsed_response = parsed_response.json()\n",
    "        return parsed_response\n",
    "        \n",
    "    def vanilla_rag(\n",
    "            self,\n",
    "            search_query: str\n",
    "    ) -> str:\n",
    "        context = self.search_blogs(search_query=search_query)\n",
    "        vanilla_rag_prompt = f\"\"\"Assess the context and answer the question.\n",
    "\n",
    "        [[ question ]]\n",
    "        {search_query}\n",
    "\n",
    "        [[ context ]]\n",
    "        {context}\n",
    "\n",
    "        [[ answer ]]\"\"\"\n",
    "        answer = self.generate(prompt=vanilla_rag_prompt)\n",
    "        return answer\n",
    "\n",
    "    def generate_with_function_calling_loop(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            tools: list[Tool]\n",
    "    ) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Use the supplied tools to assist the user.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        calls, call_budget = 0, 20\n",
    "    \n",
    "        # Initial call to get first response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        ).choices[0]\n",
    "\n",
    "        while calls < call_budget:\n",
    "            message = response.message\n",
    "            \n",
    "            if not message.tool_calls:\n",
    "                return message.content\n",
    "            \n",
    "            # Add assistant message with tool calls\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": message.content if message.content else None,\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tool_call.id,\n",
    "                        \"type\": \"function\", \n",
    "                        \"function\": {\n",
    "                            \"name\": tool_call.function.name,\n",
    "                            \"arguments\": tool_call.function.arguments\n",
    "                        }\n",
    "                    } for tool_call in message.tool_calls\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            # Handle parallel function calls\n",
    "            for tool_call in message.tool_calls:\n",
    "                function_response = self.execute_tool(\n",
    "                    tool_name=tool_call.function.name,\n",
    "                    tool_arguments=json.loads(tool_call.function.arguments)\n",
    "                )\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": function_response,\n",
    "                    \"tool_call_id\": tool_call.id\n",
    "                })\n",
    "            \n",
    "            # Get next response\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                tools=tools\n",
    "            ).choices[0]\n",
    "            \n",
    "            calls += 1\n",
    "        \n",
    "        return \"Exceeded maximum number of function calls\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "lm_service = LM_System(\n",
    "    weaviate_client=weaviate_client,\n",
    "    model_name=\"gpt-4o\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "print(lm_service.generate(\"say hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output Demo\n",
    "\n",
    "The Structured Output makes it so the Language Model can only output either `hello how are you` or `Hello!`\n",
    "\n",
    "We use this for our LLM-as-Judge to determine which System produces the better answer to a technical question about Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"greeting\":\"Hello!\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:1138: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class StructuredHello(BaseModel):\n",
    "    greeting: Literal[\"hello how are you\", \"Hello!\"]\n",
    "\n",
    "print(lm_service.generate_with_output_model(\n",
    "    prompt=\"say hello\",\n",
    "    output_model=StructuredHello)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Result: 1:\n",
      "content:Here are some key differences between Vamana and HNSW:\n",
      "\n",
      "### Vamana indexing - in short:\n",
      "* Build a random graph. * Optimize the graph, so it only connects vectors close to each other. * Modify the graph by removing some short connections and adding some long-range edges to speed up the traversal of the graph. ### HNSW indexing - in short:\n",
      "* Build a hierarchy of layers to speed up the traversal of the nearest neighbor graph. * In this graph, the top layers contain only long-range edges.author:None\n",
      "Search Result: 2:\n",
      "content:Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.author:None\n",
      "Search Result: 3:\n",
      "content:But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50–100m, the import process would slow down to a walking pace. #### Solution\n",
      "To address this problem, we changed how the HNSW index grows. We implemented a relative growth pattern, where the HNSW index size increases by either 25% or 25'000 objects (whichever is bigger). ![HNSW index growth chart](./img/hnsw-index-growth.jpg)\n",
      "\n",
      "#### Test\n",
      "After introducing the relative growth patterns, we've run a few tests.author:None\n",
      "Search Result: 4:\n",
      "content:Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.author:None\n",
      "Search Result: 5:\n",
      "content:At the time of writing this article in early 2021, the first vector index type that's supported is HNSW. By choosing this particular type, one of the limitations is already overcome: HNSW supports querying while inserting. This is a good basis for mutability, but it's not all. Existing HNSW libraries fall short of full CRUD-support. Updating is not possible at all and deleting is only mimicked by marking an object as deleted without cleaning it up.author:None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lm_service.search_blogs(\"Hnsw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vanilla RAG` Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HNSW stands for Hierarchical Navigable Small World. It is a data structure and algorithm used for approximate nearest neighbor search in high-dimensional spaces. HNSW builds a hierarchy of graph layers to expedite the traversal and search process. In this structure, the top layers contain only long-range edges, which allow quicker navigation across the graph. HNSW is optimized for in-memory access and can support querying simultaneously with data insertion, although it faces challenges with full CRUD operations. Its primary advantage is the efficient traversal of the nearest neighbor graph, facilitated by its hierarchical representation.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_service.vanilla_rag(\n",
    "    search_query=\"What is HNSW?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Agentic RAG` Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [Tool(\n",
    "    type=\"function\",\n",
    "    function=Function(\n",
    "        name=\"search_blogs\",\n",
    "        description=\"Search a Vector Database containing blog posts information about Weaviate.\",\n",
    "        parameters=Parameters(\n",
    "            type=\"object\",\n",
    "            properties={\n",
    "                \"search_query\": ParameterProperty(\n",
    "                    type=\"string\",\n",
    "                    description=\"The natural language query to search for in the database\"\n",
    "                )\n",
    "            },\n",
    "            required=[\"search_query\"]\n",
    "        )\n",
    "    )\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HNSW, which stands for Hierarchical Navigable Small World, is a method used for creating a nearest neighbor graph with a hierarchy of layers that speeds up the traversal process. Here's a brief overview based on the search results:\\n\\n1. **Indexing Method**: HNSW creates a hierarchical graph to facilitate efficient nearest neighbor search. The top layers of this hierarchy contain longer-range edges which help in faster traversal of the graph. This structure accelerates the process of finding the nearest neighbors.\\n\\n2. **Growth Pattern**: To handle large volumes of data without significant slowdown, the HNSW index grows relatively, meaning its size increases by a fixed percentage or number of objects, whichever is larger.\\n\\n3. **Mutability**: HNSW supports querying during the insertion process, enabling more dynamic use cases. However, there are limitations in the context of full CRUD operations, as updating indices directly is not well supported.\\n\\n4. **Performance Considerations**: HNSW is optimized for high-throughput scenarios, particularly when used in-memory. Storing the index on disk or using other non-in-memory methods can greatly reduce performance.\\n\\n5. **Graph Traversal**: The hierarchical structure used in HNSW can be likened to planning international travel, where the traversal utilizes a mix of long-range and short-range connections to efficiently navigate to the desired point or solution.\\n\\nIn summary, HNSW is a powerful indexing method for nearest neighbor search that balances speed, scalability, and memory usage, particularly optimized for scenarios requiring in-memory computations.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_service.generate_with_function_calling_loop(\n",
    "    prompt=\"What is HNSW?\",\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate Agentic RAG vs. Vanilla RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Winner(BaseModel):\n",
    "    rationale: str\n",
    "    winner: Literal[\"vanilla rag\", \"agentic rag\"]\n",
    "\n",
    "class RAGEvalModel(BaseModel):\n",
    "    query: str\n",
    "    response: str\n",
    "    win: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"weaviate/WeaviateBlogRAG-0-0-0\")[\"train\"] # Please leave a heart if you find this dataset useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the role of the Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search?\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging run 1 of 45:\n",
      "\u001b[1;32mQuery: What is the role of the Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search?\n",
      "\u001b[0m\n",
      "\u001b[1;32mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The Binary Independence Model (BIM) plays a significant role in the BM25 algorithm, which is used in Weaviate's hybrid search. BM25 builds on the Term-Frequency Inverse-Document Frequency (TF-IDF) scoring method by incorporating the Binary Independence Model from the IDF calculation. This model assumes that the presence or absence of a term in a document is independent of the presence or absence of other terms. It is used to weigh the uniqueness of each keyword in the query relative to the collection of texts. Additionally, BM25 adds a normalization penalty that considers a document's length relative to the average length of all documents in the database. This adjustment helps in providing more relevant search results.\n",
      "\u001b[1;32m\n",
      "Agentic RAG Response:\n",
      "\u001b[0m\n",
      "The role of the Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search is based on its contribution to the Inverse Document Frequency (IDF) calculation. BM25 builds upon the traditional TF-IDF model by incorporating the Binary Independence Model, which helps in determining the score of a document-query pair. This involves weighing the uniqueness of each keyword in the query relative to the collection of texts. Additionally, BM25 includes normalization penalties that account for document length compared to the average length of documents in the database. These aspects help in providing an effective keyword scoring method as part of Weaviate's hybrid search, combining it with vector search for optimal results.\n",
      "\u001b[1;32m\n",
      "Judged Winner to be:\u001b[0m\n",
      "agentic rag\n",
      "\u001b[1;32mWith Rationale:\u001b[0m\n",
      "The responses from both systems describe the role of the Binary Independence Model (BIM) in the BM25 algorithm. However, the response from the agentic rag system provides more context about how BM25 functions within Weaviate's hybrid search by highlighting that it combines keyword scoring with vector search for optimal results. This additional information about the hybrid nature and the combination with vector search adds more value to the user’s understanding of the system's operation. Therefore, the agentic rag system offers a slightly better response due to the extra context provided.\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 0 wins\n",
      "Agentic RAG: 1 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 0 wins\n",
      "Agentic RAG: 2 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 1 wins\n",
      "Agentic RAG: 2 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 2 wins\n",
      "Agentic RAG: 2 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 2 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 3 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 4 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 5 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 6 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 7 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 8 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 9 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 10 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 11 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 12 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 13 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 14 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 4 wins\n",
      "Agentic RAG: 14 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 5 wins\n",
      "Agentic RAG: 14 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 6 wins\n",
      "Agentic RAG: 14 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 6 wins\n",
      "Agentic RAG: 15 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 7 wins\n",
      "Agentic RAG: 15 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 7 wins\n",
      "Agentic RAG: 16 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 7 wins\n",
      "Agentic RAG: 17 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 17 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 18 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 19 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 20 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 21 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 22 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 23 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 23 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 24 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 25 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 26 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 27 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 28 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 29 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 30 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 31 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 31 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 32 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 33 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 34 wins\n",
      "\n",
      "\u001b[96m\n",
      "Scoreboard:\u001b[0m\n",
      "Vanilla RAG: 11 wins\n",
      "Agentic RAG: 34 wins\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load queries\n",
    "\n",
    "compare_system_responses = \"\"\"Assess the responses from two systems and determine which one had the better response:\n",
    "\n",
    "[[ answer from vanilla rag system ]]\n",
    "{vanilla_rag_response}\n",
    "\n",
    "[[ answer from agentic rag system ]]\n",
    "{agentic_rag_response}\n",
    "\n",
    "[[ winning system ]]\n",
    "\"\"\"\n",
    "\n",
    "vanilla_rag_scores, agentic_rag_scores = [], []\n",
    "vanilla_rag_wins = 0\n",
    "agentic_rag_wins = 0\n",
    "\n",
    "for idx, row in enumerate(ds):\n",
    "    if idx == 0:\n",
    "        print(f\"Logging run {idx+1} of {len(ds)}:\")\n",
    "    query = row[\"query\"]\n",
    "    if idx == 0:\n",
    "        print(f\"\\033[1;32mQuery: {query}\\n\\033[0m\")\n",
    "    \n",
    "    vanilla_rag_response = lm_service.vanilla_rag(\n",
    "        search_query=query\n",
    "    )\n",
    "    if idx == 0:\n",
    "        print(\"\\033[1;32mVanilla RAG Response:\\n\\033[0m\")\n",
    "        print(vanilla_rag_response)\n",
    "    \n",
    "    agentic_rag_response = lm_service.generate_with_function_calling_loop(\n",
    "        prompt=query,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    if idx == 0:\n",
    "        print(\"\\033[1;32m\\nAgentic RAG Response:\\n\\033[0m\")\n",
    "        print(agentic_rag_response)\n",
    "    \n",
    "    formatted_compare_system_responses = compare_system_responses.format(\n",
    "        vanilla_rag_response=vanilla_rag_response,\n",
    "        agentic_rag_response=agentic_rag_response\n",
    "    )\n",
    "    \n",
    "    winner = lm_service.generate_with_output_model(\n",
    "        prompt=formatted_compare_system_responses,\n",
    "        output_model=Winner\n",
    "    )\n",
    "\n",
    "    winner = json.loads(winner)\n",
    "\n",
    "    if idx == 0:\n",
    "        print(\"\\033[1;32m\\nJudged Winner to be:\\033[0m\")\n",
    "        print(winner[\"winner\"])\n",
    "        print(\"\\033[1;32mWith Rationale:\\033[0m\")\n",
    "        print(winner[\"rationale\"])\n",
    "\n",
    "    winner = winner[\"winner\"]\n",
    "    \n",
    "    if winner == \"vanilla rag\":\n",
    "        vanilla_rag_wins += 1\n",
    "    else:\n",
    "        agentic_rag_wins += 1\n",
    "        \n",
    "    print(\"\\033[96m\\nScoreboard:\\033[0m\")\n",
    "    print(f\"Vanilla RAG: {vanilla_rag_wins} wins\")\n",
    "    print(f\"Agentic RAG: {agentic_rag_wins} wins\\n\")\n",
    "\n",
    "    # Save results\n",
    "    vanilla_rag_scores.append(RAGEvalModel(\n",
    "        query=query,\n",
    "        response=vanilla_rag_response,\n",
    "        win=(winner == \"vanilla rag\")\n",
    "    ))\n",
    "    \n",
    "    agentic_rag_scores.append(RAGEvalModel(\n",
    "        query=query,\n",
    "        response=agentic_rag_response, \n",
    "        win=(winner == \"agentic rag\")\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Win Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic RAG win rate: 75.56%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Agentic RAG win rate: {agentic_rag_wins / 45 * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
