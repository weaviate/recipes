{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a882a2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/langchain/LCEL/RAG-with-LangChain-LCEL-and-DSPy.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d24a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cshorten/Desktop/DSPy-local/cohere_fix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello! How can I help you today? \\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "gemini_flash = dspy.Google(model=\"gemini-1.5-flash-latest\", api_key=google_api_key)\n",
    "\n",
    "import weaviate\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client, k=10)\n",
    "\n",
    "dspy.settings.configure(lm=gemini_flash, rm=retriever_model)\n",
    "gemini_flash(\"say hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f5a82",
   "metadata": {},
   "source": [
    "# Load Dataset (Questions derived from Weaviate's Blog Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5123191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "gold_answers = []\n",
    "queries = []\n",
    "\n",
    "for row in dataset:\n",
    "    gold_answers.append(row[\"gold_answer\"])\n",
    "    queries.append(row[\"query\"])\n",
    "    \n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "trainset, devset, testset = data[:25], data[25:35], data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaebbea",
   "metadata": {},
   "source": [
    "# Metric to Assess Response Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c95891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypedEvaluator(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a system's answer to a question according to a given criterion.\"\"\"\n",
    "    \n",
    "    criterion: str = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    question: str = dspy.InputField(desc=\"The question asked to the system.\")\n",
    "    ground_truth_answer: str = dspy.InputField(desc=\"An expert written Ground Truth Answer to the question.\")\n",
    "    predicted_answer: str = dspy.InputField(desc=\"The system's answer to the question.\")\n",
    "    rating: float = dspy.OutputField(desc=\"A float rating between 1 and 5. IMPORTANT!! ONLY OUTPUT THE RATING!!\")\n",
    "\n",
    "\n",
    "def MetricWrapper(gold, pred, trace=None):\n",
    "    alignment_criterion = \"How aligned is the predicted_answer with the ground_truth?\"\n",
    "    return dspy.TypedPredictor(TypedEvaluator)(criterion=alignment_criterion,\n",
    "                                          question=gold.question,\n",
    "                                          ground_truth_answer=gold.gold_answer,\n",
    "                                          predicted_answer=pred.answer).rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d9fbf",
   "metadata": {},
   "source": [
    "# LCEL RAG Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20969bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c1ab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! \\n\\nHow can I help you today? \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-9ff74818-3165-4276-9a9c-a3cc33802c81-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=google_api_key)\n",
    "\n",
    "llm.invoke(\"say hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e18567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From LangChain, import standard modules for prompting.\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Given the context: {context}. What is the answer to the question: `{question}`? IMPORTANT! ONLY OUTPUT THE ANSWER!\")\n",
    "\n",
    "# This is how you'd normally build a chain with LCEL. This chain does retrieval then generation (RAG).\n",
    "retrieve = lambda x: dspy.Retrieve(k=5)(x[\"question\"]).passages\n",
    "vanilla_chain = RunnablePassthrough.assign(context=retrieve) | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fbbcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vanilla_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef1b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From DSPy, import the modules that know how to interact with LangChain LCEL.\n",
    "from dspy.predict.langchain import LangChainPredict, LangChainModule\n",
    "\n",
    "# This is how to wrap it so it behaves like a DSPy program.\n",
    "# Just Replace every pattern like `prompt | llm` with `LangChainPredict(prompt, llm)`.\n",
    "zeroshot_chain = RunnablePassthrough.assign(context=retrieve) | LangChainPredict(prompt, llm) | StrOutputParser()\n",
    "zeroshot_chain = LangChainModule(zeroshot_chain)  # then wrap the chain in a DSPy module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c01986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dspy.predict.langchain.LangChainModule"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zeroshot_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9090cfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "You are a processor for prompts. I will give you a prompt template (Python f-string) for an arbitrary task for other LMs.\n",
      "Your job is to prepare three modular pieces: (i) any essential task instructions or guidelines, (ii) a list of variable names for inputs, (iv) the variable name for output.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Template:\n",
      "```\n",
      "\n",
      "${template}\n",
      "\n",
      "```\n",
      "\n",
      "Let's now prepare three modular pieces.\n",
      "\n",
      "Essential Instructions: ${essential_instructions}\n",
      "\n",
      "Input Keys: comma-separated list of valid variable names\n",
      "\n",
      "Output Key: a valid variable name\n",
      "\n",
      "---\n",
      "\n",
      "Template:\n",
      "```\n",
      "\n",
      "Given the context: {context}. What is the answer to the question: `{question}`? IMPORTANT! ONLY OUTPUT THE ANSWER!\n",
      "\n",
      "```\n",
      "\n",
      "Let's now prepare three modular pieces.\n",
      "\n",
      "Essential Instructions:\u001b[32mEssential Instructions: The context should be a string containing relevant information. The question should be a string asking a question about the context. The output should be a string containing only the answer to the question.\n",
      "\n",
      "Input Keys: context, question\n",
      "\n",
      "Output Key: answer \n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nYou are a processor for prompts. I will give you a prompt template (Python f-string) for an arbitrary task for other LMs.\\nYour job is to prepare three modular pieces: (i) any essential task instructions or guidelines, (ii) a list of variable names for inputs, (iv) the variable name for output.\\n\\n---\\n\\nFollow the following format.\\n\\nTemplate:\\n```\\n\\n${template}\\n\\n```\\n\\nLet's now prepare three modular pieces.\\n\\nEssential Instructions: ${essential_instructions}\\n\\nInput Keys: comma-separated list of valid variable names\\n\\nOutput Key: a valid variable name\\n\\n---\\n\\nTemplate:\\n```\\n\\nGiven the context: {context}. What is the answer to the question: `{question}`? IMPORTANT! ONLY OUTPUT THE ANSWER!\\n\\n```\\n\\nLet's now prepare three modular pieces.\\n\\nEssential Instructions:\\x1b[32mEssential Instructions: The context should be a string containing relevant information. The question should be a string asking a question about the context. The output should be a string containing only the answer to the question.\\n\\nInput Keys: context, question\\n\\nOutput Key: answer \\n\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a7b08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Lock Striping is a pattern that solves race conditions, that can occur during parallel data imports, by using a fixed number of locks. It assigns objects to a specific lock based on their UUID, ensuring that objects with the same UUID are not processed concurrently while allowing objects with different UUIDs to be processed in parallel. \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Lock Striping?\"\n",
    "\n",
    "zeroshot_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa6b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▎                                      | 3/25 [00:09<01:12,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "optimizer = BootstrapFewShot(metric=MetricWrapper,\n",
    "                            max_bootstrapped_demos=3)\n",
    "\n",
    "optimized_chain = optimizer.compile(zeroshot_chain, teacher=zeroshot_chain, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cd72879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Lock Striping is a solution for race conditions in database design, particularly when importing data in parallel streams. It uses a fixed number of locks to ensure objects with the same UUID are never processed concurrently, preventing data duplication without sacrificing import performance. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9c97e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dspy.predict.langchain.LangChainModule"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimized_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4314019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimized_chain.chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1609e",
   "metadata": {},
   "source": [
    "# Understanding LCEL in the LangChain Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f58df",
   "metadata": {},
   "source": [
    "![alt text](./langchain_stack.png \"Title Text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cohere_fix)",
   "language": "python",
   "name": "cohere_fix_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
