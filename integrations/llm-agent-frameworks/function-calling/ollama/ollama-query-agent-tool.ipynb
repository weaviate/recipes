{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/function-calling/ollama/ollama-query-agent-tool.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate Query Agent with Ollama\n",
    "\n",
    "This notebook will show you how to define the Weaviate Query Agent as a tool through Ollama.\n",
    "\n",
    "### Requirements\n",
    "1. Weaviate Cloud instance (WCD): The Weaviate Query Agent is only accessible through WCD at the moment. You can create a serverless cluster or a free 14-day sandbox [here](https://console.weaviate.cloud/).\n",
    "1. Install the Ollama with `pip install ollama`. We're using version `0.4.7` at the time of writing this notebook.\n",
    "1. Install the Weaviate Agents package with `pip install weaviate-agents`\n",
    "1. You'll need a Weaviate cluster with data. If you don't have one, check out [this notebook](integrations/Weaviate-Import-Example.ipynb) to import the Weaviate Blogs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate_agents.query import QueryAgent\n",
    "import os\n",
    "import json\n",
    "\n",
    "import ollama\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WEAVIATE_URL\"] = \"\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Query Agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_query_agent_request(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a query to the database and get the response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question or query to search for in the database. This can be any natural language question related to the content stored in the database.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the database containing relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    # connect to your Weaviate Cloud instance\n",
    "    weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=os.getenv(\"WEAVIATE_URL\"), \n",
    "        auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    "        headers={ # add the API key to the model provider from your Weaviate collection, for example `headers={\"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")}` \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # connect the query agent to your Weaviate collection(s)\n",
    "    query_agent = QueryAgent(\n",
    "        client=weaviate_client,\n",
    "        collections=[\"Blogs\"] \n",
    "    )\n",
    "    return query_agent.run(query).final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agent_tool_schema = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'send_query_agent_request',\n",
    "        'description': 'Send a query to the Weaviate database and get relevant information from blog posts.',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'query': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The search query to find information in the blog posts database.',\n",
    "                },\n",
    "            },\n",
    "            'required': ['query'],\n",
    "        },\n",
    "    },\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function calling loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_mapping = {\n",
    "    \"send_query_agent_request\": send_query_agent_request\n",
    "}\n",
    "\n",
    "def ollama_generation_with_weaviate_agent(user_message: str,\n",
    "                                          model_name: str = \"llama3.1\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response using Ollama with access to the Weaviate Query Agent.\n",
    "    \n",
    "    Args:\n",
    "        user_message (str): The user's query or message\n",
    "        model_name (str): The name of the Ollama model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: The final response after potential tool execution\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    }]\n",
    "    \n",
    "    # First call to decide if tool should be used\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        tools=query_agent_tool_schema\n",
    "    )\n",
    "    \n",
    "    # Add model response to message history\n",
    "    messages.append(response[\"message\"])\n",
    "    \n",
    "    # Check if the model decided to use a tool\n",
    "    if response[\"message\"].get(\"tool_calls\"):\n",
    "        print(\"Model decided to use tool...\")\n",
    "        \n",
    "        for tool in response[\"message\"][\"tool_calls\"]:\n",
    "            function_name = tool[\"function\"][\"name\"]\n",
    "            \n",
    "            if function_name in tool_mapping:\n",
    "                # Parse arguments - handle different possible formats\n",
    "                try:\n",
    "                    # If arguments is a JSON string\n",
    "                    if isinstance(tool[\"function\"][\"arguments\"], str):\n",
    "                        args = json.loads(tool[\"function\"][\"arguments\"])\n",
    "                        query = args.get(\"query\", \"\")\n",
    "                    # If arguments is already parsed as a dict\n",
    "                    elif isinstance(tool[\"function\"][\"arguments\"], dict):\n",
    "                        query = tool[\"function\"][\"arguments\"].get(\"query\", \"\")\n",
    "                    else:\n",
    "                        query = \"\"\n",
    "                    \n",
    "                    print(f\"Querying Weaviate with: {query}\")\n",
    "                    \n",
    "                    # Call the function with the query\n",
    "                    function_to_call = tool_mapping[function_name]\n",
    "                    function_response = function_to_call(query)\n",
    "                    \n",
    "                    # Make sure response is a string\n",
    "                    if not isinstance(function_response, str):\n",
    "                        if isinstance(function_response, dict) and 'final_answer' in function_response:\n",
    "                            function_response = function_response['final_answer']\n",
    "                        else:\n",
    "                            function_response = json.dumps(function_response)\n",
    "                    \n",
    "                    # Add the tool response to messages\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": function_response,\n",
    "                        \"name\": function_name  # Include the function name\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error executing tool: {str(e)}\"\n",
    "                    print(error_message)\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": error_message,\n",
    "                        \"name\": function_name\n",
    "                    })\n",
    "        \n",
    "        # Get final response with tool results\n",
    "        final_response = ollama.chat(model=model_name, messages=messages)\n",
    "        return final_response[\"message\"][\"content\"]\n",
    "    else:\n",
    "        # If no tool was used, return the direct response\n",
    "        return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model decided to use tool...\n",
      "Querying Weaviate with: docker deployment\n",
      "To deploy Weaviate with Docker, you'll need to follow these steps:\n",
      "\n",
      "1. **Pull the Weaviate Docker image**: You can pull the official Weaviate Docker image from Docker Hub using:\n",
      "   ```bash\n",
      "   docker pull weaviate/weaviate:latest\n",
      "   ```\n",
      "\n",
      "2. **Run Weaviate in a container**:\n",
      "   If you're on Linux, use `docker run` to start Weaviate in a new container. For example, to expose the default port (8087) and mount a local directory (`/path/to/your/data`) as a volume inside the container, you can use the following command:\n",
      "\n",
      "   ```bash\n",
      "   docker run -p 8080:8080 -v /path/to/your/data:/data weaviate/weaviate:latest --memory 4g --indexing 2 --storage 1 --port 8080\n",
      "   ```\n",
      "\n",
      "3. **Verify Weaviate**:\n",
      "   After running the command, you can verify that Weaviate is up and running by accessing `http://localhost:8080` in your browser. The default admin credentials are username \"admin\" and password \"password\".\n",
      "\n",
      "4. **Stop or remove containers as needed**: Remember to stop or delete the container when you're finished using it to avoid unnecessary resource consumption.\n",
      "\n",
      "By following these steps, you'll have Weaviate running on Docker on your system. Make sure to replace `/path/to/your/data` with an actual path where you want to mount the data volume. This setup is ideal for development and testing purposes.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "response = ollama_generation_with_weaviate_agent(\n",
    "    user_message=\"How do I deploy Weaviate with Docker?\",\n",
    "    model_name=\"llama3.1\"  # or whichever model you have in Ollama\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
