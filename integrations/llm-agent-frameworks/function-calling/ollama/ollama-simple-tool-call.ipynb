{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5587641f",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/function-calling/ollama/ollama-simple-tool-call.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1890708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Weaviate\n",
    "import weaviate\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a0dbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Blogs collection\n",
    "\n",
    "blogs = weaviate_client.collections.get(\"WeaviateBlogChunk\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a06aa7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Result 1:\n",
      "content:Let's explore how PQ works. ## Product Quantization\n",
      "![ann](./img/Ann.png)\n",
      "\n",
      "If you already know the details behind how Product Quantization works feel free to skip this section!\n",
      "\n",
      "The main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently.author:None\n",
      "Search Result 2:\n",
      "content:### Product Quantization\n",
      "While async indexing helped with importing, [product quantization](/developers/weaviate/concepts/vector-quantization#product-quantization) (PQ) was important for alleviating the memory footprint. PQ is a technique used to compress the size of vector embeddings by representing the vectors as centroid ids. The centroid ids are calculated by clustering each segment. We enabled PQ to reduce the memory requirements without slowing down the ingestion. You can either [manually](/developers/weaviate/configuration/pq-compression#manually-configure-pq) enable PQ or have it [automatically triggered](/developers/weaviate/configuration/pq-compression#configure-autopq) based on the number of vectors.author:None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = blogs.query.hybrid(\"Product Quantization\", limit=2)\n",
    "\n",
    "stringified_response = \"\"\n",
    "\n",
    "for idx, o in enumerate(response.objects):\n",
    "    stringified_response += f\"Search Result {idx+1}:\\n\"\n",
    "    for prop in o.properties:\n",
    "        stringified_response+=f\"{prop}:{o.properties[prop]}\"\n",
    "    stringified_response += \"\\n\"\n",
    "\n",
    "print(stringified_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e24e200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hybrid search function\n",
    "\n",
    "def get_search_results(query: str) -> str:\n",
    "    \"\"\"Sends a query to Weaviate's Hybrid Search. Parases the response into a {k}:{v} string.\"\"\"\n",
    "    \n",
    "    response = blogs.query.hybrid(query, limit=5)\n",
    "    \n",
    "    stringified_response = \"\"\n",
    "    for idx, o in enumerate(response.objects):\n",
    "        stringified_response += f\"Search Result: {idx+1}:\\n\"\n",
    "        for prop in o.properties:\n",
    "            stringified_response += f\"{prop}:{o.properties[prop]}\"\n",
    "        stringified_response += \"\\n\"\n",
    "    \n",
    "    return stringified_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd85feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from typing import List, Dict\n",
    "\n",
    "tools_schema=[{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_search_results',\n",
    "        'description': 'Get search results for a provided query.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'query': {\n",
    "              'type': 'string',\n",
    "              'description': 'The search query.',\n",
    "            },\n",
    "          },\n",
    "          'required': ['query'],\n",
    "        },\n",
    "    },\n",
    "}]\n",
    "\n",
    "tool_mapping = {\n",
    "    \"get_search_results\": get_search_results\n",
    "}\n",
    "\n",
    "def ollama_generation_with_tools(user_message: str,\n",
    "                                 tools_schema: List, tool_mapping: Dict,\n",
    "                                 model_name: str = \"llama3.1\") -> str:\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    }]\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        tools=tools_schema\n",
    "    )\n",
    "    if not response[\"message\"].get(\"tool_calls\"):\n",
    "        return response[\"message\"][\"content\"]\n",
    "    else:\n",
    "        for tool in response[\"message\"][\"tool_calls\"]:\n",
    "            function_to_call = tool_mapping[tool[\"function\"][\"name\"]]\n",
    "            print(f\"Calling function {function_to_call}...\")\n",
    "            function_response = function_to_call(tool[\"function\"][\"arguments\"][\"query\"])\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": function_response,\n",
    "            })\n",
    "    \n",
    "    final_response = ollama.chat(model=model_name, messages=messages)\n",
    "    return final_response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aa44f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function <function get_search_results at 0x114749a20>...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the search results provided, it appears that HNSW (Hierarchical Navigable Small World) is different from DiskANN in the following ways:\\n\\n1. **Implementation**: While both Vamana and HNSW are ANN algorithms used for vector indexing, Vamana is an implementation of DiskANN, whereas HNSW has its own highly optimized version implemented by Weaviate.\\n2. **Performance**: Both Vamana (DiskANN) and the Weaviate-implemented HNSW achieve comparable in-memory results, indicating that both are effective for vector search tasks.\\n3. **Challenges**: The implementation of DiskANN on disk-based vectors poses several challenges, including achieving good performance while maintaining excellent database UX.\\n\\nIn summary, while Vamana (DiskANN) and Weaviate's HNSW share similarities as ANN algorithms, they differ in their implementation, with HNSW having its own optimized version.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_generation_with_tools(\"How is HNSW different from DiskANN?\",\n",
    "                            tools_schema=tools_schema, tool_mapping=tool_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weaviate-agents",
   "language": "python",
   "name": "weaviate-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
