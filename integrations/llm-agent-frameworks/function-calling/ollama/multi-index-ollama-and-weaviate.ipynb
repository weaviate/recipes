{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a9c1dd",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/function-calling/ollama/multi-index-ollama-and-weaviate.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34c986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Weaviate\n",
    "import weaviate\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c598f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_collection = weaviate_client.collections.get(\"Code\")\n",
    "docs_collection = weaviate_client.collections.get(\"Docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542b13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hybrid search function\n",
    "\n",
    "def code_search(query: str) -> str:\n",
    "    \"\"\"Sends a query to Weaviate's Hybrid Search. Parases the response into a {k}:{v} string.\"\"\"\n",
    "    \n",
    "    response = code_collection.query.hybrid(query, limit=5)\n",
    "    \n",
    "    stringified_response = \"\"\n",
    "    for idx, o in enumerate(response.objects):\n",
    "        stringified_response += f\"Search Result: {idx+1}:\\n\"\n",
    "        for prop in o.properties:\n",
    "            stringified_response += f\"{prop}:{o.properties[prop]}\"\n",
    "        stringified_response += \"\\n\"\n",
    "    \n",
    "    return stringified_response\n",
    "\n",
    "def docs_search(query: str) -> str:\n",
    "    \"\"\"Sends a query to Weaviate's Hybrid Search. Parases the response into a {k}:{v} string.\"\"\"\n",
    "    \n",
    "    response = docs_collection.query.hybrid(query, limit=5)\n",
    "    \n",
    "    stringified_response = \"\"\n",
    "    for idx, o in enumerate(response.objects):\n",
    "        stringified_response += f\"Search Result: {idx+1}:\\n\"\n",
    "        for prop in o.properties:\n",
    "            stringified_response += f\"{prop}:{o.properties[prop]}\"\n",
    "        stringified_response += \"\\n\"\n",
    "    \n",
    "    return stringified_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a092589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from typing import List, Dict\n",
    "\n",
    "tools_schema=[\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'code_search',\n",
    "            'description': \"Imagine having a super-powered magnifying glass that lets you instantly pinpoint specific lines of code within a vast software project, like finding a needle in a digital haystack. code_search does just that. It enables you to search directly within the codebase using keywords, phrases, or even code snippets themselves, making it an invaluable tool for developers, support teams, and anyone needing to quickly navigate and understand the inner workings of the software.\",\n",
    "            'parameters': {\n",
    "              'type': 'object',\n",
    "              'properties': {\n",
    "                'query': {\n",
    "                  'type': 'string',\n",
    "                  'description': 'The search query.',\n",
    "                },\n",
    "              },\n",
    "              'required': ['query'],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'docs_search',\n",
    "            'description': \"Think of it as having a knowledgeable software expert available at your fingertips, ready to answer any questions you have about how to use the software. documentation_search makes that possible. It lets you search through the human-readable documentation associated with the code, providing clear explanations, instructions, and examples to help you learn and use the software effectively. Whether you're a beginner or a seasoned user, documentation_search is your go-to resource for getting the most out of the software.\",\n",
    "            'parameters': {\n",
    "              'type': 'object',\n",
    "              'properties': {\n",
    "                'query': {\n",
    "                  'type': 'string',\n",
    "                  'description': 'The search query.',\n",
    "                },\n",
    "              },\n",
    "              'required': ['query'],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "tool_mapping = {\n",
    "    \"code_search\": code_search,\n",
    "    \"docs_search\": docs_search\n",
    "}\n",
    "\n",
    "def ollama_generation_with_tools(user_message: str,\n",
    "                                 tools_schema: List, tool_mapping: Dict,\n",
    "                                 model_name: str = \"llama3.1\") -> str:\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    }]\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        tools=tools_schema\n",
    "    )\n",
    "    if not response[\"message\"].get(\"tool_calls\"):\n",
    "        return response[\"message\"][\"content\"]\n",
    "    else:\n",
    "        for tool in response[\"message\"][\"tool_calls\"]:\n",
    "            selected_function = tool[\"function\"][\"name\"]\n",
    "            function_to_call = tool_mapping[selected_function]\n",
    "            print(f\"Calling function \\033[92m{selected_function}\\033[0m...\")\n",
    "            function_response = function_to_call(tool[\"function\"][\"arguments\"][\"query\"])\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": function_response,\n",
    "            })\n",
    "    \n",
    "    final_response = ollama.chat(model=model_name, messages=messages)\n",
    "    return final_response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75e4fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function \u001b[92mdocs_search\u001b[0m...\n",
      "\n",
      "Based on the provided code snippets, I will create a RAG program using DSPy and Weaviate.\n",
      "\n",
      "```python\n",
      "import dspy\n",
      "from weaviate import Client\n",
      "from dspy.teleprompt import BootstrapFewShot\n",
      "from dspy.evaluate.evaluate import Evaluate\n",
      "\n",
      "# Initialize Weaviate client\n",
      "client = Client(\"http://localhost:8080\")\n",
      "\n",
      "# Define validation logic\n",
      "def validate_context_and_answer(example, pred, trace=None):\n",
      "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
      "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
      "    return answer_EM and answer_PM\n",
      "\n",
      "# Set up a basic teleprompter\n",
      "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
      "\n",
      "# Define retrieval module\n",
      "class Retrieve(dspy.Module):\n",
      "    def __init__(self, k=3):\n",
      "        super().__init__()\n",
      "        self.k = k\n",
      "    \n",
      "    def forward(self, question):\n",
      "        query = client.query(\n",
      "            \"Get\",\n",
      "            vector={\n",
      "                \"vector\": {\n",
      "                    \"value\": question,\n",
      "                    \"dimension\": 512\n",
      "                }\n",
      "            },\n",
      "            filter=\"\",\n",
      "            include_vector=True\n",
      "        )\n",
      "        return query\n",
      "\n",
      "# Define predictor module\n",
      "class GenerateAnswer(dspy.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "    \n",
      "    def forward(self, context):\n",
      "        # Use Weaviate to retrieve relevant passages\n",
      "        passages = []\n",
      "        for passage in context:\n",
      "            query = client.query(\n",
      "                \"Get\",\n",
      "                filter=f\"vector contains_all ['{passage}']\",\n",
      "                include_vector=True\n",
      "            )\n",
      "            passages.append(query)\n",
      "        \n",
      "        # Generate answer based on retrieved passages\n",
      "        answer = \"\"\n",
      "        for passage in passages:\n",
      "            answer += passage[\"value\"][\"text\"]\n",
      "        \n",
      "        return dspy.Prediction(context=context, answer=answer)\n",
      "\n",
      "# Define RAG module\n",
      "class RAG(dspy.Module):\n",
      "    def __init__(self, num_passages=3):\n",
      "        super().__init__()\n",
      "        self.retrieve = Retrieve(k=num_passages)\n",
      "        self.generate_answer = GenerateAnswer()\n",
      "    \n",
      "    def forward(self, question):\n",
      "        context = self.retrieve(question).passages\n",
      "        prediction = self.generate_answer(context=context, question=question)\n",
      "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
      "\n",
      "# Compile RAG program\n",
      "teleprompter = LabeledFewShot()\n",
      "compiled_rag = teleprompter.compile(student=RAG(), trainset=[(\"Question\", \"Passage\")])\n",
      "```\n",
      "\n",
      "This code defines a RAG program using DSPy and Weaviate. The `RAG` module uses the `Retrieve` and `GenerateAnswer` modules to retrieve relevant passages from Weaviate and generate an answer based on those passages. The `compiled_rag` variable holds the compiled RAG program, which can be used for inference.\n",
      "\n",
      "Note that this code assumes you have a Weaviate instance running at `http://localhost:8080`. You may need to modify the URL to match your actual Weaviate setup. Additionally, this is just an example and you should adjust it according to your specific use case and requirements.\n"
     ]
    }
   ],
   "source": [
    "answer = ollama_generation_with_tools(\"Can you create a RAG program with DSPy and Weaviate?\",\n",
    "                            tools_schema=tools_schema, tool_mapping=tool_mapping)\n",
    "\n",
    "print(f\"\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f82ec00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function \u001b[92mcode_search\u001b[0m...\n",
      "\n",
      "Based on the search results, it appears that the Avatar optimizer is a class or module in a Python package that is used to optimize some process or algorithm. The exact details are not provided in the search results, but here are some key points that can be inferred:\n",
      "\n",
      "1. **Avatar class**: There is a class named `Avatar` that takes several parameters in its constructor, including `signature`, `tools`, `max_iters`, and `verbose`. This suggests that the Avatar optimizer has various parameters that can be adjusted to control its behavior.\n",
      "2. **Tool management**: The Avatar class seems to have some tool management functionality, as it creates a \"Finish\" tool and appends it to a list of tools.\n",
      "3. **Actor creation**: The Avatar class creates an actor (presumably an instance of some predictive model) based on the input fields and signature. This actor is then cloned for later use.\n",
      "4. **Verbose mode**: The Avatar optimizer can be run in verbose mode, which suggests that it may provide detailed output or logging during its execution.\n",
      "\n",
      "Without more context or information about the specific problem domain or application, it's difficult to provide a more detailed explanation of what the Avatar optimizer does. However, based on these search results, it appears to be some kind of optimization framework or tool that can be used in various contexts.\n"
     ]
    }
   ],
   "source": [
    "answer = ollama_generation_with_tools(\"What is the Avatar optimizer?\",\n",
    "                            tools_schema=tools_schema, tool_mapping=tool_mapping)\n",
    "\n",
    "print(f\"\\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (weaviate-agents)",
   "language": "python",
   "name": "dspy-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
