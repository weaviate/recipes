{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate Query Agent with Baseten\n",
    "\n",
    "This notebook will show you how to define the Weaviate Query Agent as a tool with Baseten.\n",
    "\n",
    "### Requirements\n",
    "1. Weaviate Cloud instance (WCD): The Weaviate Query Agent is only accessible through WCD at the moment. You can create a serverless cluster or a free 14-day sandbox [here](https://console.weaviate.cloud/).\n",
    "1. Install the Weaviate Agents package with `pip install weaviate-agents`\n",
    "1. You'll need a Weaviate cluster with data. If you don't have one, check out [this notebook](integrations/Weaviate-Import-Example.ipynb) to import the Weaviate Blogs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions for Baseten\n",
    "\n",
    "This notebook uses the Llama 3 TensorRT Engine from Baseten, the following reference illustrates how to set it up:\n",
    "\n",
    "As a quick TLDR, you need to run:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade truss\n",
    "truss init llama-3-1-8b-trt-llm\n",
    "cd llama-3-1-8b-trt-llm\n",
    "rm model/model.py\n",
    "```\n",
    "\n",
    "You will now see the following compute config:\n",
    "\n",
    "```yaml\n",
    "model_name: Llama 3.1 8B Engine\n",
    "resources:\n",
    "  accelerator: A100\n",
    "secrets:\n",
    "  hf_access_token: \"set token in baseten workspace\"\n",
    "trt_llm:\n",
    "  build:\n",
    "    base_model: llama\n",
    "    checkpoint_repository:\n",
    "      repo: meta-llama/Llama-3.1-8B-Instruct\n",
    "      source: HF\n",
    "    max_seq_len: 8192\n",
    "```\n",
    "\n",
    "Deploy on Baseten with:\n",
    "\n",
    "```bash\n",
    "truss push --publish --trusted\n",
    "```\n",
    "\n",
    "Reference: https://docs.baseten.co/performance/examples/llama-trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate_agents.query import QueryAgent\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WEAVIATE_URL\"] = \"\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_ENDPOINT\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Query Agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_query_agent_request(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a query to the database and get the response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question or query to search for in the database. This can be any natural language question related to the content stored in the database.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the database containing relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    # connect to your Weaviate Cloud instance\n",
    "    weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=os.getenv(\"WEAVIATE_URL\"),\n",
    "        auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    "        headers={\"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")}, # add the API key to the model provider from your Weaviate collection\n",
    "    )\n",
    "\n",
    "    # connect the query agent to your Weaviate collection(s)\n",
    "    query_agent = QueryAgent(\n",
    "        client=weaviate_client,\n",
    "        collections=[\"Blogs\"]\n",
    "    )\n",
    "    return query_agent.run(query).final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_json = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"send_query_agent_request\",\n",
    "      \"description\": \"Send a query to the database and get the response.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The question or query to search for in the database. This can be any natural language question related to the content stored in the database.\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"query\"\n",
    "        ],\n",
    "        \"additionalProperties\": false\n",
    "      },\n",
    "      \"strict\": true\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Parse the JSON string to use it as a Python object\n",
    "tools = json.loads(tools_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function calling loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_assistant(message, chat_history=None):\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Step 1: Get user message\n",
    "    print(f\"Question:\\n{message}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize messages\n",
    "    if chat_history:\n",
    "        messages = chat_history + [{\"role\": \"user\", \"content\": message}]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that can access external functions. The responses from these function calls will be appended to this dialogue. Please provide responses based on the information from these function calls.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    \n",
    "    # Function to detect and extract tool calls from response\n",
    "    def extract_tool_calls(text):\n",
    "        # Try to parse as JSON first\n",
    "        try:\n",
    "            data = json.loads(text)\n",
    "            # Check if it's a list of tool calls or a single tool call\n",
    "            if isinstance(data, list) and len(data) > 0 and \"name\" in data[0]:\n",
    "                return data\n",
    "            elif isinstance(data, dict) and \"name\" in data and \"parameters\" in data:\n",
    "                return [data]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # Try to extract JSON-like tool calls using regex\n",
    "        try:\n",
    "            # Look for JSON objects that have \"name\" and \"parameters\" fields\n",
    "            pattern = r'{\"name\":.*?\"parameters\":.*?}'\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                tools = []\n",
    "                for match in matches:\n",
    "                    try:\n",
    "                        tool = json.loads(match)\n",
    "                        if \"name\" in tool and \"parameters\" in tool:\n",
    "                            tools.append(tool)\n",
    "                    except:\n",
    "                        pass\n",
    "                if tools:\n",
    "                    return tools\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Initial call to the model\n",
    "    try:\n",
    "        payload = {\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools,  # Assuming tools is defined elsewhere\n",
    "            \"tool_choice\": \"auto\",\n",
    "        }\n",
    "        \n",
    "        resp = requests.post(\n",
    "            os.environ.get('BASETEN_ENDPOINT'),\n",
    "            headers={\"Authorization\": f\"Api-Key {os.environ.get('BASETEN_API_KEY')}\"},\n",
    "            json=payload,\n",
    "        )\n",
    "        \n",
    "        if resp.status_code != 200:\n",
    "            print(f\"API Error: Status code {resp.status_code}\")\n",
    "            print(resp.text)\n",
    "            return messages\n",
    "            \n",
    "        # Process the response\n",
    "        response_text = resp.content.decode('utf-8', errors='replace')\n",
    "        tool_calls = extract_tool_calls(response_text)\n",
    "        \n",
    "        if tool_calls:\n",
    "            # This is a tool call response\n",
    "            print(\"Tool plan:\")\n",
    "            print(\"I need to use a tool to help answer your question.\", \"\\n\")\n",
    "            \n",
    "            # Add assistant message with tool calls\n",
    "            assistant_message = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"I need to use a tool to help answer your question.\",\n",
    "                \"tool_calls\": []\n",
    "            }\n",
    "            \n",
    "            # Process each tool call\n",
    "            for i, tool_call in enumerate(tool_calls):\n",
    "                call_id = f\"call_{i}\"\n",
    "                \n",
    "                # Handle parameters that might be a string\n",
    "                params = tool_call[\"parameters\"]\n",
    "                if isinstance(params, str):\n",
    "                    try:\n",
    "                        params = json.loads(params)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                tool_call_obj = {\n",
    "                    \"id\": call_id,\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_call[\"name\"],\n",
    "                        \"arguments\": json.dumps(params) if isinstance(params, dict) else params\n",
    "                    }\n",
    "                }\n",
    "                assistant_message[\"tool_calls\"].append(tool_call_obj)\n",
    "                \n",
    "                # Print tool call info\n",
    "                print(f\"Tool name: {tool_call['name']} | Parameters: {params}\")\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Add assistant message to conversation history\n",
    "            messages.append(assistant_message)\n",
    "            \n",
    "            # Execute each tool call\n",
    "            for tool_call in assistant_message[\"tool_calls\"]:\n",
    "                function_name = tool_call[\"function\"][\"name\"]\n",
    "                args_str = tool_call[\"function\"][\"arguments\"]\n",
    "                \n",
    "                # Parse arguments\n",
    "                if isinstance(args_str, str):\n",
    "                    try:\n",
    "                        function_args = json.loads(args_str)\n",
    "                    except:\n",
    "                        function_args = {\"query\": args_str}\n",
    "                else:\n",
    "                    function_args = args_str\n",
    "                \n",
    "                try:\n",
    "                    # Call the actual function (assumed to be defined elsewhere)\n",
    "                    function_response = globals()[function_name](function_args.get(\"query\"))\n",
    "                    \n",
    "                    # Add tool response to messages\n",
    "                    tool_message = {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call[\"id\"],\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": str(function_response)\n",
    "                    }\n",
    "                    messages.append(tool_message)\n",
    "                    \n",
    "                    print(f\"Executed tool {function_name} successfully\")\n",
    "                    \n",
    "                    # Since we have a tool response, we'll just format it nicely as the final response\n",
    "                    # This avoids making another API call that might fail\n",
    "                    final_content = f\"Based on the information I found:\\n\\n{function_response}\"\n",
    "                    \n",
    "                    final_message = {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": final_content\n",
    "                    }\n",
    "                    \n",
    "                    messages.append(final_message)\n",
    "                    \n",
    "                    print(\"Final response (based on tool output):\")\n",
    "                    print(final_content)\n",
    "                    print(\"=\"*50)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error executing tool {function_name}: {str(e)}\"\n",
    "                    print(error_msg)\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call[\"id\"],\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": f\"Error: {str(e)}\"\n",
    "                    })\n",
    "                    \n",
    "                    # In case of error, add a simple response\n",
    "                    messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": \"I encountered an error while trying to find information for you.\"\n",
    "                    })\n",
    "            \n",
    "        else:\n",
    "            # This is a direct response with no tool calls\n",
    "            clean_text = re.sub(r'<\\|.*?\\|>', '', response_text).strip()\n",
    "            \n",
    "            if not clean_text:\n",
    "                clean_text = \"I'm not sure how to answer that question.\"\n",
    "            \n",
    "            assistant_message = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": clean_text\n",
    "            }\n",
    "            \n",
    "            messages.append(assistant_message)\n",
    "            \n",
    "            print(\"Direct response:\")\n",
    "            print(clean_text)\n",
    "            print(\"=\"*50)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in request: {str(e)}\")\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"I encountered an error while processing your request: {str(e)}\"\n",
    "        })\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "How do I run Weaviate with Docker?\n",
      "==================================================\n",
      "Tool plan:\n",
      "I need to use a tool to help answer your question. \n",
      "\n",
      "Tool name: send_query_agent_request | Parameters: {'query': 'How to run Weaviate with Docker?'}\n",
      "==================================================\n",
      "Executed tool send_query_agent_request successfully\n",
      "Final response (based on tool output):\n",
      "Based on the information I found:\n",
      "\n",
      "To run Weaviate with Docker, you will need to follow these steps:\n",
      "\n",
      "1. **Install Docker and Docker Compose**: Ensure that you have both the `docker` and `docker-compose` CLI tools installed on your system. Installation processes may differ depending on your operating system, with specific installation guides available for [Mac](https://docs.docker.com/desktop/install/mac-install/), [Windows](https://docs.docker.com/desktop/install/windows-install/), and [Ubuntu Linux](https://docs.docker.com/engine/install/ubuntu/).\n",
      "\n",
      "2. **Obtain the Docker Compose File**: You can either use a pre-prepared Docker Compose file or create a custom one using the [Weaviate configuration tool](https://www.weaviate.io/developers/weaviate/installation/docker-compose#configurator). The file should include all necessary configurations including any additional modules if required.\n",
      "\n",
      "3. **Run Docker Compose**: Navigate to the directory where your Docker Compose file, named `docker-compose.yml`, is stored. To start Weaviate, run the command:\n",
      "   ```bash\n",
      "   docker-compose up -d\n",
      "   ```\n",
      "   The `-d` flag runs the containers in detached mode, meaning they operate in the background.\n",
      "\n",
      "4. **Check Readiness**: To confirm Weaviate is running, you can perform a readiness check by making an HTTP GET request to the endpoint `GET /v1/.well-known/ready` using a tool like `curl`. If the service is ready, it should return a 2xx HTTP status code.\n",
      "\n",
      "These steps should help you successfully run Weaviate in a Docker environment. If you plan to eventually move to a stable production environment, consider using Kubernetes alongside the official Weaviate Helm chart for better management and scalability.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "chat_history = run_assistant(\"How do I run Weaviate with Docker?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful assistant that can access external functions. The responses from these function calls will be appended to this dialogue. Please provide responses based on the information from these function calls.'},\n",
       " {'role': 'user', 'content': 'How do I run Weaviate with Docker?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I need to use a tool to help answer your question.',\n",
       "  'tool_calls': [{'id': 'call_0',\n",
       "    'type': 'function',\n",
       "    'function': {'name': 'send_query_agent_request',\n",
       "     'arguments': '{\"query\": \"How to run Weaviate with Docker?\"}'}}]},\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'call_0',\n",
       "  'name': 'send_query_agent_request',\n",
       "  'content': 'To run Weaviate with Docker, you will need to follow these steps:\\n\\n1. **Install Docker and Docker Compose**: Ensure that you have both the `docker` and `docker-compose` CLI tools installed on your system. Installation processes may differ depending on your operating system, with specific installation guides available for [Mac](https://docs.docker.com/desktop/install/mac-install/), [Windows](https://docs.docker.com/desktop/install/windows-install/), and [Ubuntu Linux](https://docs.docker.com/engine/install/ubuntu/).\\n\\n2. **Obtain the Docker Compose File**: You can either use a pre-prepared Docker Compose file or create a custom one using the [Weaviate configuration tool](https://www.weaviate.io/developers/weaviate/installation/docker-compose#configurator). The file should include all necessary configurations including any additional modules if required.\\n\\n3. **Run Docker Compose**: Navigate to the directory where your Docker Compose file, named `docker-compose.yml`, is stored. To start Weaviate, run the command:\\n   ```bash\\n   docker-compose up -d\\n   ```\\n   The `-d` flag runs the containers in detached mode, meaning they operate in the background.\\n\\n4. **Check Readiness**: To confirm Weaviate is running, you can perform a readiness check by making an HTTP GET request to the endpoint `GET /v1/.well-known/ready` using a tool like `curl`. If the service is ready, it should return a 2xx HTTP status code.\\n\\nThese steps should help you successfully run Weaviate in a Docker environment. If you plan to eventually move to a stable production environment, consider using Kubernetes alongside the official Weaviate Helm chart for better management and scalability.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Based on the information I found:\\n\\nTo run Weaviate with Docker, you will need to follow these steps:\\n\\n1. **Install Docker and Docker Compose**: Ensure that you have both the `docker` and `docker-compose` CLI tools installed on your system. Installation processes may differ depending on your operating system, with specific installation guides available for [Mac](https://docs.docker.com/desktop/install/mac-install/), [Windows](https://docs.docker.com/desktop/install/windows-install/), and [Ubuntu Linux](https://docs.docker.com/engine/install/ubuntu/).\\n\\n2. **Obtain the Docker Compose File**: You can either use a pre-prepared Docker Compose file or create a custom one using the [Weaviate configuration tool](https://www.weaviate.io/developers/weaviate/installation/docker-compose#configurator). The file should include all necessary configurations including any additional modules if required.\\n\\n3. **Run Docker Compose**: Navigate to the directory where your Docker Compose file, named `docker-compose.yml`, is stored. To start Weaviate, run the command:\\n   ```bash\\n   docker-compose up -d\\n   ```\\n   The `-d` flag runs the containers in detached mode, meaning they operate in the background.\\n\\n4. **Check Readiness**: To confirm Weaviate is running, you can perform a readiness check by making an HTTP GET request to the endpoint `GET /v1/.well-known/ready` using a tool like `curl`. If the service is ready, it should return a 2xx HTTP status code.\\n\\nThese steps should help you successfully run Weaviate in a Docker environment. If you plan to eventually move to a stable production environment, consider using Kubernetes alongside the official Weaviate Helm chart for better management and scalability.'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
