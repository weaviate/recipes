{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate Query Agent with Together AI\n",
    "\n",
    "This notebook will show you how to define the Weaviate Query Agent as a tool through Together AI.\n",
    "\n",
    "### Requirements\n",
    "1. Weaviate Cloud instance (WCD): The Weaviate Query Agent is only accessible through WCD at the moment. You can create a serverless cluster or a free 14-day sandbox [here](https://console.weaviate.cloud/).\n",
    "2. Together AI API key: You can grab an API key [here](https://api.together.xyz/).\n",
    "3. Install the Weaviate Agents package with `pip install weaviate-agents`\n",
    "4. You'll need a Weaviate cluster with data. If you don't have one, check out [this notebook](integrations/Weaviate-Import-Example.ipynb) to import the Weaviate Blogs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate_agents.query import QueryAgent\n",
    "import os\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WEAVIATE_URL\"] = \"\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = \"\"\n",
    "os.environ[\"TOGETHER_AI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Query Agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_query_agent_request(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a query to the database and get the response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question or query to search for in the database. This can be any natural language question related to the content stored in the database.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the database containing relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    # connect to your Weaviate Cloud instance\n",
    "    weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=os.getenv(\"WEAVIATE_URL\"), \n",
    "        auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    "        headers={ # add the API key to the model provider from your Weaviate collection, for example `headers={\"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\"}` \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # connect the query agent to your Weaviate collection(s)\n",
    "    query_agent = QueryAgent(\n",
    "        client=weaviate_client,\n",
    "        collections=[\"Blogs\"] \n",
    "    )\n",
    "    return query_agent.run(query).final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To run Weaviate with Docker, you should follow these steps:\\n\\n1. **Install Docker and Docker Compose**: Ensure that both Docker and Docker Compose are installed on your machine. You can find the installation guides for different operating systems like Mac, Windows, and Ubuntu Linux in the Docker documentation ([Docker for Mac](https://docs.docker.com/desktop/install/mac-install/), [Docker for Windows](https://docs.docker.com/desktop/install/windows-install/), and [Docker for Ubuntu](https://docs.docker.com/engine/install/ubuntu/)).\\n\\n2. **Obtain the Docker Compose File**: You can obtain a pre-configured `docker-compose.yml` file using the Weaviate configuration tool available on their website. Alternatively, you can use one of the example files from the Weaviate documentation.\\n\\n3. **Run the Docker Compose File**: Navigate to the directory where your `docker-compose.yml` file is located. Ensure it is named correctly as `docker-compose.yml`. Start the Weaviate setup by running the following command:\\n\\n   ```bash\\n   docker compose up -d\\n   ```\\n\\n   The `-d` option will run your containers in detached mode, meaning you won't see the logs in your terminal unless you specifically attach to them.\\n\\n4. **Verify Readiness**: To check if Weaviate is up and running, use the readiness check provided by Weaviate. You can programmatically check this by running a simple `bash` loop that checks on a specific endpoint:\\n\\n   ```bash\\n   until curl --fail -s localhost:8080/v1/.well-known/ready; do\\n     sleep 1\\n   done\\n   ```\\n\\n   Adjust `localhost:8080` if you're running on a different bind address.\\n\\nThese steps will get Weaviate running on Docker for development and evaluation purposes. For a production environment, it is recommended to use a more robust setup like Kubernetes with Helm.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "\n",
    "send_query_agent_request(\"How do I run Weaviate with Docker?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Together AI and define Query Agent tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url = \"https://api.together.xyz/v1\",\n",
    "    api_key = os.getenv(\"TOGETHER_AI_API_KEY\") \n",
    ")\n",
    "\n",
    "tools = [\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"send_query_agent_request\",\n",
    "      \"description\": \"Send a query to the database and get the response\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Any question or query to search for in the database\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"query\"]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling loop and query time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "How do I run Weaviate with Docker?\n",
      "==================================================\n",
      "Tool calls:\n",
      "Tool name: send_query_agent_request | Parameters: {'query': 'How do I run Weaviate with Docker?'}\n",
      "==================================================\n",
      "Final response:\n",
      " To run Weaviate with Docker, follow these steps:\n",
      "\n",
      "1. Install Docker and Docker Compose on your machine.\n",
      "2. Obtain the Docker Compose file using the Weaviate configuration tool.\n",
      "3. Run Docker Compose to start the containers in the background.\n",
      "4. Check Weaviate's readiness using a cURL command.\n",
      "\n",
      "By following these steps, you can successfully set up Weaviate using Docker for local development or experimentation.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def run_assistant(message, chat_history=None):\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Step 1: Get user message\n",
    "    print(f\"Question:\\n{message}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize messages with system prompt and user query\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that can access external functions. The responses from these function calls will be appended to this dialogue. Please provide responses based on the information from these function calls.\"},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    \n",
    "    # Add chat history if available\n",
    "    if chat_history:\n",
    "        messages = chat_history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Step 2: Generate tool calls (if any)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", # Together AI supports other models as well\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Get the assistant's message\n",
    "    assistant_message = response.choices[0].message\n",
    "    messages.append(assistant_message.model_dump())\n",
    "    \n",
    "    # Process tool calls if any\n",
    "    while assistant_message.tool_calls:\n",
    "        tool_calls = assistant_message.tool_calls\n",
    "        \n",
    "        if assistant_message.content:\n",
    "            print(\"Tool plan:\")\n",
    "            print(assistant_message.content, \"\\n\")\n",
    "        \n",
    "        print(\"Tool calls:\")\n",
    "        for call in tool_calls:\n",
    "            function_name = call.function.name\n",
    "            function_args = json.loads(call.function.arguments)\n",
    "            print(f\"Tool name: {function_name} | Parameters: {function_args}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Step 3: Execute tools and get results\n",
    "        tool_results = []\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            try:\n",
    "                if function_name == \"send_query_agent_request\":\n",
    "                    function_response = send_query_agent_request(function_args.get(\"query\"))\n",
    "                    # Add the function response to messages\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": str(function_response)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing tool {function_name}: {str(e)}\")\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": f\"Error: {str(e)}\"\n",
    "                })\n",
    "        \n",
    "        # Step 4: Get the next response from the model\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        messages.append(assistant_message.model_dump())\n",
    "    \n",
    "    # Print final response\n",
    "    print(\"Final response:\")\n",
    "    print(assistant_message.content)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Example usage\n",
    "chat_history = run_assistant(\"How do I run Weaviate with Docker?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
