{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/function-calling/composio/agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to integrate Composioâ€™s Gmail tool with Weaviate to create an agent that will automatically respond to Gmail messages by searching a Weaviate collection for relevant information and using that information to draft replies. A common use case for this might be to automate email support using an agent that has access to your docs/FAQs. Composio allows an AI agent/your app to easily connect to your user's apps like Gmail, Slack, Trello etc. Paired together with Weaviate to store, manage and query data, more personalized and context-aware agents can be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "pip install composio-core langchain-openai langchain-weaviate\n",
    "pip install langchain-community langchain pypdf python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composio Setup\n",
    "\n",
    "In the command line, run - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composio add gmail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the link from your command line to set up an integration between your gmail account and Composio.\n",
    "\n",
    "Also run -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composio triggers enable gmail_new_gmail_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command enables a trigger for when a new mail is received in gmail. When a new email arrives on the connected account, the trigger provides data about the mail like the sender, mail content etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI api key\n",
    "\n",
    "This tutorial uses an embedding model and LLM from OpenAI, for which you will need an API key set as an evironment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\t#load environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we connect to a Weaviate instance. Here, we're using Weaviate Embedded. This embedded instance will stay alive for as long as the parent application is running. For a more persistent instance, you can look at the docs here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, os\n",
    "\n",
    "client = weaviate.connect_to_embedded(\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\"),  # Replace with your API key\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Client is ready?\", client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Weaviate Collection\n",
    "\n",
    "Let's create a collection to store our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import classes as wvc\n",
    "\n",
    "dataCollection = client.collections.create(\n",
    "    name=\"GenerativeAI\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_openai(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a collection, we can import whatever data we want - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "loader = PyPDFLoader(\"Generative_artificial_intelligence.pdf\", extract_images=False)\n",
    "docs = loader.load_and_split(text_splitter)\n",
    "WeaviateVectorStore.from_documents(docs, embeddings, client=client, index_name=\"GenerativeAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the data, we can get our collection - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(\"GenerativeAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we initialize Composio's tools for Gmail - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composio_langchain import Action, ComposioToolSet\n",
    "\n",
    "toolset = ComposioToolSet() #Initialize Composio's Toolset\n",
    "\n",
    "replyTool = toolset.get_actions(\n",
    "    actions=[\n",
    "        Action.GMAIL_REPLY_TO_THREAD, # Reply to a gmail thread\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GMAIL_REPLY_TO_THREAD` action represents a function that the LLM can call. Composio provides these actions with optimized JSON schemas which makes it very easy to integrate external tools with LLMs/agentic workflows.\n",
    "\n",
    "We are also going to define a tool that queries our Weaviate collection based on the question asked by the user -  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "# Function to search Weaviate Collection\n",
    "\n",
    "def search_collection(query: str) -> str:\n",
    "    \"\"\"Searches the Weaviate collection for user query and returns the results.\"\"\"\n",
    "\n",
    "    response = collection.query.hybrid(query=query, limit=3)\n",
    "\n",
    "    stringified_response = \"\"\n",
    "    for idx, o in enumerate(response.objects):\n",
    "        stringified_response += f\"Search Result: {idx+1}:\\n\"\n",
    "        for prop in o.properties:\n",
    "            stringified_response += f\"{prop}:{o.properties[prop]}\"\n",
    "        stringified_response += \"\\n\"\n",
    "    \n",
    "    return stringified_response\n",
    "\n",
    "# Create a Structured tool from the above function\n",
    "\n",
    "searchTool = StructuredTool.from_function(\n",
    "    func=search_collection,\n",
    "    name=\"search_collection\",\n",
    "    description=\"Searches the Weaviate collection for the given query and returns the results.\",\n",
    ")\n",
    "\n",
    "tools: list[StructuredTool] = replyTool + [searchTool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\t\t\n",
    "\n",
    "# The prompt can be customized to fit the needs of your app\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an AI email assistant that can write and reply to emails. You have to use the search_document tool to search the Weaviate collection for the user's query. When the user asks you a question, use the search_document tool to search for the query in the Weaviate collection and then answer the question using the search results. Send the answer back to the user in an email.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "# Create an instance of AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a listener for the trigger we created above - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the listener\n",
    "listener = toolset.create_trigger_listener()\n",
    "\n",
    "# Attach listener to trigger and provide callback function that is executed when the trigger receives new data\n",
    "@listener.callback(filters={\"trigger_name\": \"gmail_new_gmail_message\"})\n",
    "def on_new_gmail_message(event) -> None:\n",
    "    try:\n",
    "        print(\"data received - \", event)\n",
    "\n",
    "\t\t\t\t# Extract the relevant information from the event\n",
    "        headers = event.originalPayload[\"payload\"][\"headers\"],\n",
    "        sender = headers[16][\"value\"],\n",
    "        query = event.originalPayload[\"snippet\"],\n",
    "        thread_id = event.originalPayload[\"threadId\"],\n",
    "\n",
    "        res = agent_executor.invoke({\"input\": f\"This is the query you have to respond to: {query}. It's from {sender} and the threadId is {thread_id}.\"})\n",
    "        print(res)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "print(\"Listener started!\")\n",
    "listener.listen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Listener started!\n",
    "\n",
    "data received - appName='gmail' payload={'threadId': '1916dc9e35a3401p', 'messageId': '1916dc9e35a3401p', 'messageTimestamp': '2024-08-20T03:16:59Z', 'messageText': 'I wanted to know how generative ai works for art. Can i generate images\\r\\nfrom gen ai models?\\r\\n', ...}\n",
    "\n",
    "> Entering new AgentExecutor chain...\n",
    "\n",
    "Invoking: `search_document` with `{'query': 'how generative ai works for art and generating images'}`\n",
    "\n",
    "\n",
    "Search Result: 1:\n",
    "text:OpenAI Codex.\n",
    "Producing high-quality visual art is a prominent\n",
    "application of generative AI.[45] Generative AI\n",
    "systems trained on sets of images with text\n",
    "captions include Imagen, DALL-E, Midjourney, Adobe  Firefly, Stable Diffusion and others (see Artificial\n",
    "intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generationpage:2.0source:Generative_artificial_intelligence.pdf\n",
    "Search Result: 2:\n",
    "text:ThÃ©Ã¢tre D'opÃ©ra Spatial, an image\n",
    "generated with Midjourney\n",
    "Generative artificial intelligence\n",
    "Generative artificial intelligence (generative AI, GenAI,[1]\n",
    "or GAI) is artificial intelligence capable of generating text,\n",
    "images, videos, or other data using generative models,[2]\n",
    "often in respons e to prompts.[3][4] Generative AI models\n",
    "learn the patterns and structure of their input training data andpage:0.0source:Generative_artificial_intelligence.pdf\n",
    "Search Result: 3:\n",
    "text:artistic works. By the early 1970s , Harold Cohen was creating and exhibiting generative AI works created\n",
    "by AARON, the computer program Cohen created to generate paintings.[26]\n",
    "Markov chains have long been used to model natural langua ges since their development by Russian\n",
    "mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic inpage:1.0source:Generative_artificial_intelligence.pdf\n",
    "\n",
    "Invoking: `gmail_reply_to_thread` with `{'thread_id': '1916dc9e35a3401p', 'message_body': 'Dear John,\\n\\nThank you for your inquiry about generative AI and its application in art. \\n\\nGenerative AI works by utilizing models that can generate text, images, videos, or other data based on training data. In the context of art, it involves systems trained on large datasets of images paired with text captions. Some popular generative AI models for image generation include DALL-E, Midjourney, and Stable Diffusion. These models learn the patterns and structures from the training data, allowing them to create new and unique images in response to prompts.\\n\\nSo yes, you can definitely generate images using generative AI models!\\n\\nBest regards,\\n[Your Name]', ...}\n",
    "\n",
    "[2024-08-20 08:48:08,645][INFO] Executing action: GMAIL_REPLY_TO_THREAD\n",
    "{'execution_details': {'executed': True}, 'response_data': {'id': '1916dcac6c9424e9', 'threadId': '1916dc9e35a3401p', 'labelIds': ['SENT']}}I have replied to John's inquiry about how generative AI works for art and the possibility of generating images using generative AI models. If you need any further assistance, feel free to ask!\n",
    "\n",
    "> Finished chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect with Composio and learn more\n",
    "\n",
    "If you encounter any problems, please let us know at our [Discord](https://discord.com/invite/cNruWaAhQk).\n",
    "\n",
    "Check out [Composio's documentation](https://docs.composio.dev/introduction/intro/overview) to learn more about how to use and integrate various tools for different usecases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
