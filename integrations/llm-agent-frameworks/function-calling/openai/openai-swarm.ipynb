{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes.config as wc\n",
    "import os\n",
    "import re\n",
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/weaviate/warnings.py:130: DeprecationWarning: Dep005: You are using weaviate-client version 4.5.5. The latest version is 4.9.0.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n",
      "/var/folders/sp/z75p2kwx2777ymvvhrzx20hw0000gn/T/ipykernel_67081/1799340039.py:1: ResourceWarning: unclosed <ssl.SSLSocket fd=80, family=2, type=1, proto=0, laddr=('10.0.0.14', 65463), raddr=('35.201.124.182', 443)>\n",
      "  weaviate_client = weaviate.connect_to_wcs(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "weaviate_client = weaviate.connect_to_wcs(\n",
    "    cluster_url=CLUSTER_URL, \n",
    "    auth_credentials=weaviate.auth.AuthApiKey(AUTH_KEY),\n",
    "      headers={ \"X-OpenAI-Api-Key\": OPENAI_KEY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.collections.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created collection: Blogs.\n"
     ]
    }
   ],
   "source": [
    "weaviate_client.collections.create(\n",
    "    name=\"WeaviateBlogs\",\n",
    "\n",
    "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai( # specify the vectorizer and model\n",
    "        model=\"text-embedding-3-large\",\n",
    "    ),\n",
    "    generative_config=wc.Configure.Generative.openai(\n",
    "        model = \"gpt-4\"\n",
    "    ),\n",
    "\n",
    "    properties=[\n",
    "            wc.Property(name=\"content\", data_type=wc.DataType.TEXT)\n",
    "      ]\n",
    ")\n",
    "\n",
    "print(\"Successfully created collection: Blogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Blogs into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = '../../data'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: ChatGPT for Generative Search\\nslug: generative-search\\nauthors: [zain, erika, connor]\\ndate: 2023-02-07\\ntags: [\\'search\\', \\'integrations\\']\\nimage: ./img/hero.png\\ndescription: \"Learn how you can customize Large Language Models prompt responses to your own data by leveraging vector databases.\"\\n---\\n![ChatGPT for Generative Search](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWhen OpenAI launched ChatGPT at the end of 2022, more than one million people had tried the model in just a week and that trend has only continued with monthly active users for the chatbot service reaching over 100 Million, quicker than any service before, as reported by [Reuters](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/) and [Yahoo Finance](https://finance.yahoo.com/news/chatgpt-on-track-to-surpass-100-million-users-faster-than-tiktok-or-instagram-ubs-214423357.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAFCTz2vosCcjWFstJGkvduTSNZJrxULx8EHwbTE8mF7EV-hAlWvmMe59ex94LHlkB40zlUMUPshv5Ggq1GxyY9oDQxtoLcc0GV2E-v-0DeGuZi7dtEJT9MZF5NvUe20V64ZCVNziFtJdWUL_AAxMFoCGFxT1duBiaPbfzwkjbyNQ). It wouldn’t be hyperbole to say that NLP and Generative Large Language Models (LLMs) have taken the world by storm. Though this was not the first AI chatbot that has been released to the public, what really surprised people about this particular service was the breadth and depth of knowledge it had and its ability to articulate that knowledge with human-like responses. Aside from this, the generative aspect of this model is also quite apparent as it can hallucinate situations and dream up vivid details to fill in descriptions when prompted to do so. This gives the chatbot service somewhat of a human-like “creativity” - which is what adds a wow factor to the user experience!\\n\\nGenerative LLMs like ChatGPT\\'s GPT-3 (Chat Generative Pre-trained Transformer) are trained on a huge corpora of open data from the internet - since the majority of general human knowledge is archived and accessible via the Internet, these models have a lot of training material to learn from.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = weaviate_client.collections.get(\"WeaviateBlogs\")\n",
    "\n",
    "for idx, blog_chunk in enumerate(blog_chunks):\n",
    "    upload = blogs.data.insert(\n",
    "        properties={\n",
    "            \"content\": blog_chunk\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1153\n"
     ]
    }
   ],
   "source": [
    "collection = weaviate_client.collections.get(\"WeaviateBlogs\")\n",
    "response = collection.aggregate.over_all(total_count=True)\n",
    "\n",
    "print(response.total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from swarm import Swarm, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_client = Swarm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_weaviate(search_query: str) -> str:\n",
    "    \"\"\"Searches the Weaviate Collection with Hybrid Search.\"\"\"\n",
    "    \n",
    "    response = collection.query.hybrid(\n",
    "        query=search_query,\n",
    "        limit=3,\n",
    "        alpha=0.5,\n",
    "        return_properties=[\"content\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_agent_a():\n",
    "    return agent_a\n",
    "\n",
    "def transfer_to_agent_b():\n",
    "    return agent_b\n",
    "\n",
    "def transfer_to_agent_c():\n",
    "    return agent_c\n",
    "\n",
    "agent_a = Agent(\n",
    "    name=\"Lead Marketer\",\n",
    "    model= \"gpt-4o\",\n",
    "    instructions=\"You are an expert in marketing and rely on experts from your company to sell new products.\",\n",
    "    functions=[transfer_to_agent_b, transfer_to_agent_c],\n",
    ")\n",
    "\n",
    "agent_b = Agent(\n",
    "    name=\"Database Expert\",\n",
    "    model= \"gpt-4o\",\n",
    "    instructions=\"You are a world class database engineer with an extensive knowledge in indexes, multi-tenancy, and more.\",\n",
    "    functions=[transfer_to_agent_a, search_weaviate]\n",
    ")\n",
    "\n",
    "agent_c = Agent(\n",
    "    name=\"Search Expert\",\n",
    "    model= \"gpt-4o\",\n",
    "    instructions=\"You are an expert in the field of information retrieval. You know everything there is to know about bm25, vector search, hybrid search, and all applications building off of search.\",\n",
    "    functions=[transfer_to_agent_a, search_weaviate]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For promoting the new hybrid search feature, we should emphasize its innovative approach of combining keyword and vector search capabilities. Below is a suggested content piece that highlights the benefits of this feature, along with relevant context:\n",
      "\n",
      "**Introducing Hybrid Search: The Future of Intelligent Searching**\n",
      "\n",
      "In today's rapidly evolving digital landscape, accessing the right information efficiently is crucial to success. Whether you're looking for specific documents, data insights, or the latest industry trends, the challenge always lies in sifting through vast amounts of data quickly and accurately. Enter our groundbreaking Hybrid Search feature, a game-changer in the realm of information retrieval.\n",
      "\n",
      "**What is Hybrid Search?**\n",
      "\n",
      "Hybrid Search is a unique blend of traditional keyword search and advanced vector search technologies. By merging these two powerful methodologies, Hybrid Search offers unprecedented accuracy and relevance, ensuring users can find exactly what they’re looking for in record time.\n",
      "\n",
      "1. **Keyword Search:** Our tried-and-true keyword search function allows users to type in specific terms or phrases. It’s perfect for straightforward, well-defined queries where you know exactly what you're looking for.\n",
      "   \n",
      "2. **Vector Search:** Vector search, on the other hand, uses machine learning models to understand the context and semantics of your query. It searches based on intent, even if your keywords don’t exactly match the document content.\n",
      "\n",
      "**Why Choose Hybrid Search?**\n",
      "\n",
      "- **Enhanced Precision:** By combining keyword and vector search, hybrid search can pinpoint the most relevant results with higher precision.\n",
      "  \n",
      "- **Semantic Understanding:** Gain insights from data even when queries are not straightforward, thanks to the intelligent understanding of context through vector search.\n",
      "  \n",
      "- **Efficient and Quick:** Reduces the time spent on searching by delivering results that are accurate and relevant in a single query.\n",
      "  \n",
      "- **User-Friendly Experience:** Enjoy the simplicity of keyword search with the powerful accuracy of vector search, all within an easy-to-use interface.\n",
      "\n",
      "**Unlock New Possibilities**\n",
      "\n",
      "With Hybrid Search, you are equipped to delve deeper into your data repositories, reveal hidden insights, and leverage information like never before. It's not just about finding information; it's about finding the right information swiftly and seamlessly.\n",
      "\n",
      "**Learn More**\n",
      "\n",
      "Explore how Hybrid Search can transform your search experience, making it more intelligent, intuitive, and efficient. Join us for a live demonstration on [Date] and see firsthand how this innovative technology is setting the new standard for search functionalities.\n",
      "\n",
      "By painting a clear picture of its advantages and technological advancement, this promotional content aims to capture the interest of potential users and encourage them to explore the benefits of Hybrid Search.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "My company is releasing a new feature called hybrid search, which combines keyword and vector search.\n",
    "\n",
    "Come up with a piece on content that promotes the hybrid search feature. Please include relevant context that is in the database.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = swarm_client.run(\n",
    "    agent=agent_a,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "print(response.messages[-1][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
