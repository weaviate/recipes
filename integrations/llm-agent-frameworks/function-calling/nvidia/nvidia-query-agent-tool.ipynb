{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/function-calling/nvidia/nvidia-query-agent-tool.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate Query Agent with NVIDIA NIM\n",
    "\n",
    "This notebook will show you how to define the Weaviate Query Agent as a tool through NVIDIA NIM.\n",
    "\n",
    "### Requirements\n",
    "1. Weaviate Cloud instance (WCD): The Weaviate Query Agent is only accessible through WCD at the moment. You can create a serverless cluster or a free 14-day sandbox [here](https://console.weaviate.cloud/).\n",
    "1. NVIDIA NIM API key. Grab one [here](https://build.nvidia.com/models).\n",
    "1. Install the Weaviate Agents package with `pip install weaviate-agents`\n",
    "1. You'll need a Weaviate cluster with data. If you don't have one, check out [this notebook](integrations/Weaviate-Import-Example.ipynb) to import the Weaviate Blogs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate_agents.query import QueryAgent\n",
    "import os\n",
    "import json\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WEAVIATE_URL\"] = \"\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = \"\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Query Agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_query_agent_request(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a query to the database and get the response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question or query to search for in the database. This can be any natural language question related to the content stored in the database.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the database containing relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    # connect to your Weaviate Cloud instance\n",
    "    weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=os.getenv(\"WEAVIATE_URL\"), \n",
    "        auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    "        headers={ # add the API key to the model provider from your Weaviate collection, for example `headers={\"X-Nvidia-Api-Key\": os.getenv(\"NVIDIA_API_KEY\")}` \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # connect the query agent to your Weaviate collection(s)\n",
    "    query_agent = QueryAgent(\n",
    "        client=weaviate_client,\n",
    "        collections=[\"Blogs\"] \n",
    "    )\n",
    "    return query_agent.run(query).final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tool JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agent_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"send_query_agent_request\",\n",
    "        \"description\": \"Send a query to the database and get the response.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Any question or query to search for in the database\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to NVIDIA client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = os.getenv(\"NVIDIA_API_KEY\"),\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"meta/llama-3.1-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function calling loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_calling_loop(query, tool_definition, query_function, model_name=\"meta/llama-3.1-70b-instruct\"):\n",
    "    \"\"\"\n",
    "    Execute a function calling loop with a specified tool and query function.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query to process\n",
    "        tool_definition (dict): The JSON definition of the tool to be used for function calling\n",
    "        query_function (callable): The function to execute when the tool is called\n",
    "        model_name (str): The LLM model to use for function calling\n",
    "        \n",
    "    Returns:\n",
    "        str: The final response from the model\n",
    "    \"\"\"\n",
    "    # Initialize messages with user query\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    # First call - model decides whether to use the tool\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        tools=[tool_definition],\n",
    "        tool_choice=\"auto\",\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    assistant_message = chat_response.choices[0].message\n",
    "    messages.append(assistant_message)\n",
    "    \n",
    "    if assistant_message.tool_calls and len(assistant_message.tool_calls) > 0:\n",
    "        tool_call = assistant_message.tool_calls[0]\n",
    "        tool_name = tool_call.function.name\n",
    "        \n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        print(f\"Tool called: {tool_name}\")\n",
    "        print(f\"Arguments: {args}\")\n",
    "        \n",
    "        # Execute the query function with the provided arguments\n",
    "        if tool_name == tool_definition[\"function\"][\"name\"]:\n",
    "            # For a QueryAgent tool, we expect a 'query' parameter\n",
    "            if 'query' in args:\n",
    "                result = query_function(args['query'])\n",
    "            else:\n",
    "                # Handle other possible argument structures\n",
    "                result = query_function(**args)\n",
    "            \n",
    "            # Ensure the result is a string\n",
    "            if not isinstance(result, str):\n",
    "                if isinstance(result, dict) and 'final_answer' in result:\n",
    "                    result_str = result['final_answer']\n",
    "                else:\n",
    "                    result_str = json.dumps(result)\n",
    "            else:\n",
    "                result_str = result\n",
    "            \n",
    "            # Add tool response to messages\n",
    "            messages.append({\n",
    "                \"role\": \"tool\", \n",
    "                \"content\": result_str, \n",
    "                \"tool_call_id\": tool_call.id, \n",
    "                \"name\": tool_name\n",
    "            })\n",
    "            \n",
    "            # Second call - model generates final response based on tool output\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                tools=[tool_definition],\n",
    "                tool_choice=\"auto\",\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            assistant_message = chat_response.choices[0].message\n",
    "    \n",
    "    # Print and return the final response\n",
    "    if assistant_message.content:\n",
    "        print(f\"\\nFinal response: {assistant_message.content}\")\n",
    "        return assistant_message.content\n",
    "    else:\n",
    "        print(\"\\nNo text response from assistant.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool called: send_query_agent_request\n",
      "Arguments: {'query': 'What is Docker?'}\n",
      "\n",
      "Final response: Docker is a set of platform-as-a-service products that use operating-system-level virtualization to deliver software in packages called containers. Containers are a method of virtualization of the operating system to isolate applications into separate environments.\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a Jupyter notebook cell\n",
    "response = function_calling_loop(\n",
    "    query=\"What is Docker?\", \n",
    "    tool_definition=query_agent_tool,\n",
    "    query_function=send_query_agent_request\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
