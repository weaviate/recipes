{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/llamaindex/simple-query-engine/simple-query-engine.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U weaviate-client llama-index llama-index-vector-stores-weaviate llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import weaviate\n",
    "from weaviate import classes as wvc\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, Settings\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "# global\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Lets set the OPENAI key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-key\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_embedded(\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# client = weaviate.connect_to_local(\n",
    "#     headers={\n",
    "#         \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(\"BlogPost\")\n",
    "\n",
    "collection = client.collections.create(\n",
    "    name=\"BlogPost\",\n",
    "    description=\"Blog post from the Weaviate website\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_openai(),\n",
    "    generative_config=wvc.config.Configure.Generative.openai(model=\"gpt-3.5-turbo\"),\n",
    "    properties=[\n",
    "        wvc.config.Property(name=\"content\", data_type=wvc.config.DataType.TEXT)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('../data-loaders-episode1/data').load_data()\n",
    "\n",
    "# chunk up the blog posts into nodes \n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 9\n",
      "Node ID: 99fbe400-8ca8-4ed3-8ada-13bb9c428967\n",
      "Text: title: What is Ref2Vec and why you need it for your\n",
      "recommendation system  Weaviate 1.16 introduced the\n",
      "[Ref2Vec](/developers/weaviate/modules/retriever-vectorizer-\n",
      "modules/ref2vec-centroid) module. In this article, we give you an\n",
      "overview of what Ref2Vec is and some examples in which it can add\n",
      "value such as recommendations or representing long ...\n"
     ]
    }
   ],
   "source": [
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(\"Number of nodes:\", len(nodes))\n",
    "print(nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's name our index properly as BlogPost, as we will need it later.\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client, index_name=\"BlogPost\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intersection between LLMs and search lies in the challenge of finding suitable representations for long objects, particularly text documents that exceed the input limit of Deep Transformer Neural Networks. This motivates the interest in Hybrid Search techniques that combine Vector Search with algorithms like BM25, which is well-suited for handling text sequences longer than 512 tokens. Ref2Vec can play a role in addressing this challenge by helping to represent long documents effectively in search applications.\n"
     ]
    }
   ],
   "source": [
    "# now, at any time, we can get our vector store and query it.\n",
    "\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client, index_name=\"BlogPost\"\n",
    ")\n",
    "\n",
    "loaded_index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "\n",
    "query_engine = loaded_index.as_query_engine()\n",
    "response = query_engine.query(\"What is the intersection between LLMs and search?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
