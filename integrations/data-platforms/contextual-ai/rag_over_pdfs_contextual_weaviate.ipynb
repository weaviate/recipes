{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/data-platforms/contextual-ai/rag_over_pdfs_contextual_weaviate.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performing RAG over PDFs with Weaviate and Contextual AI Parser\n",
        "## A recipe üßë‚Äçüç≥ üîç üíö\n",
        "\n",
        "By Jinash Rouniyar, DevRel @ Contextual AI\n",
        "\n",
        "**Last updated:** October 2025\n",
        "\n",
        "**Versions used:**\n",
        "- Weaviate version `1.25.3`\n",
        "- Weaviate Python client `4.9.4`\n",
        "- Contextual AI client `latest`\n",
        "- OpenAI API (for embeddings and generation)\n",
        "\n",
        "This is a code recipe that uses [Weaviate](https://weaviate.io/) to perform RAG over PDF documents parsed by [Contextual AI Parser](https://docs.contextual.ai/api-reference/parse/parse-file).\n",
        "\n",
        "In this notebook, we accomplish the following:\n",
        "* Parse two distinct document types using Contextual AI Parser: research papers and table-rich documents\n",
        "* Extract structured markdown with document hierarchy preservation and advanced table extraction\n",
        "* Generate text embeddings with OpenAI\n",
        "* Perform multi-modal RAG using [Weaviate](https://weaviate.io/developers/weaviate/search/generative)\n",
        "\n",
        "To run this notebook, you'll need:\n",
        "* A [Contextual AI API key](https://docs.contextual.ai/user-guides/beginner-guide#get-your-api-key) - for document parsing and content extraction\n",
        "* An [OpenAI API key](https://platform.openai.com/docs/quickstart) - for text embeddings and generative responses\n",
        "\n",
        "Note: This notebook can be run on any environment with internet access, as Contextual AI handles the parsing on their cloud infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Contextual AI client and Weaviate client\n",
        "\n",
        "Note: If Colab prompts you to restart the session after running the cell below, click \"restart\" and proceed with running the rest of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install --upgrade --quiet contextual-client\n",
        "%pip install -U weaviate-client==\"4.9.4\"\n",
        "%pip install rich\n",
        "%pip install requests\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "# Suppress Weaviate client logs\n",
        "logging.getLogger(\"weaviate\").setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Part 1: Contextual AI Parser\n",
        "\n",
        "Contextual AI Parser is a cloud-based document parsing service that excels at extracting structured information from PDFs, DOC/DOCX, and PPT/PPTX files. It provides high-quality markdown extraction with document hierarchy preservation, making it ideal for RAG applications.\n",
        "\n",
        "The parser handles complex documents with images, tables, and hierarchical structures, providing multiple output formats including:\n",
        "- `markdown-document`: Single concatenated markdown output\n",
        "- `markdown-per-page`: Page-by-page markdown output\n",
        "- `blocks-per-page`: Structured JSON with document hierarchy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we'll use Contextual AI Parser to extract structured content from two different types of documents:\n",
        "\n",
        "1. **Research Paper**: \"Attention is All You Need\" - A seminal transformer architecture paper\n",
        "2. **Table-Rich Document**: OmniDocBench dataset documentation with large tables\n",
        "\n",
        "This demonstrates Contextual AI's capabilities with different document types and structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Documents to parse with Contextual AI\n",
        "documents = [\n",
        "    {\n",
        "        \"url\": \"https://arxiv.org/pdf/1706.03762\",\n",
        "        \"title\": \"Attention Is All You Need\",\n",
        "        \"type\": \"research_paper\",\n",
        "        \"description\": \"Seminal transformer architecture paper that introduced self-attention mechanisms\"\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/03-standalone-api/04-parse/data/omnidocbench-text.pdf\",\n",
        "        \"title\": \"OmniDocBench Dataset Documentation\", \n",
        "        \"type\": \"table_rich_document\",\n",
        "        \"description\": \"Dataset documentation with large tables demonstrating table extraction capabilities\"\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Keys Setup üîë\n",
        "\n",
        "We'll be using the Contextual AI API for parsing documents and OpenAI API for both generating text embeddings and for the generative model in our RAG pipeline. The code below dynamically fetches your API keys based on whether you're running this notebook in Google Colab or as a regular Jupyter notebook.\n",
        "\n",
        "If you're running this notebook in Google Colab, make sure you [add](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75) your API keys as secrets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API key variable names\n",
        "contextual_api_key_var = \"CONTEXTUAL_API_KEY\"  # Replace with the name of your secret/env var\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"  # Replace with the name of your secret/env var\n",
        "\n",
        "# Fetch API keys\n",
        "try:\n",
        "    # If running in Colab, fetch API keys from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    contextual_api_key = userdata.get(contextual_api_key_var)\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise ValueError(f\"Secret '{contextual_api_key_var}' not found in Colab secrets.\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API keys from environment variables\n",
        "    import os\n",
        "    contextual_api_key = os.getenv(contextual_api_key_var)\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{contextual_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "\n",
        "print(\"API keys configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download and parse PDFs using Contextual AI Parser\n",
        "\n",
        "Here we use Contextual AI's Python SDK to parse a batch of PDFs. The result is structured markdown content with document hierarchy that we can use for text extraction and chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from contextual import ContextualAI\n",
        "from time import sleep\n",
        "import os\n",
        "\n",
        "# Setup Contextual AI client\n",
        "client = ContextualAI(api_key=contextual_api_key)\n",
        "\n",
        "# Create directory for downloaded PDFs\n",
        "os.makedirs(\"pdfs\", exist_ok=True)\n",
        "\n",
        "# Download PDFs and submit parse jobs\n",
        "job_data = []\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Downloading and submitting parse job for: {doc['title']}\")\n",
        "    print(f\"Type: {doc['type']} - {doc['description']}\")\n",
        "    \n",
        "    # Download PDF\n",
        "    file_path = f\"pdfs/{doc['type']}_{i}.pdf\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(requests.get(doc['url']).content)\n",
        "    \n",
        "    # Configure parsing parameters based on document type\n",
        "    if doc['type'] == \"research_paper\":\n",
        "        # For research papers, focus on hierarchy and figures\n",
        "        parse_config = {\n",
        "            \"parse_mode\": \"standard\",\n",
        "            \"figure_caption_mode\": \"concise\",\n",
        "            \"enable_document_hierarchy\": True,\n",
        "            \"page_range\": \"0-5\"  # Parse first 6 pages\n",
        "        }\n",
        "    else:  # table_rich_document\n",
        "        # For table-rich documents, enable table splitting\n",
        "        parse_config = {\n",
        "            \"parse_mode\": \"standard\",\n",
        "            \"enable_split_tables\": True,\n",
        "            \"max_split_table_cells\": 100,\n",
        "        }\n",
        "    \n",
        "    # Submit parse job\n",
        "    with open(file_path, \"rb\") as fp:\n",
        "        response = client.parse.create(\n",
        "            raw_file=fp,\n",
        "            **parse_config\n",
        "        )\n",
        "    \n",
        "    job_data.append({\n",
        "        \"job_id\": response.job_id,\n",
        "        \"file_path\": file_path,\n",
        "        \"document\": doc\n",
        "    })\n",
        "    print(f\"Submitted job {response.job_id} for {doc['title']}\")\n",
        "\n",
        "print(f\"\\nSubmitted {len(job_data)} parse jobs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor parse job status and retrieve results\n",
        "\n",
        "We'll monitor all parse jobs and retrieve the results once they're completed. Contextual AI provides structured markdown with document hierarchy information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor all parse jobs\n",
        "completed_jobs = set()\n",
        "\n",
        "while len(completed_jobs) < len(job_data):\n",
        "    for i, job_info in enumerate(job_data):\n",
        "        job_id = job_info[\"job_id\"]\n",
        "        if job_id not in completed_jobs:\n",
        "            status = client.parse.job_status(job_id)\n",
        "            doc_title = job_info[\"document\"][\"title\"]\n",
        "            doc_type = job_info[\"document\"][\"type\"]\n",
        "            print(f\"Job {i+1}/{len(job_data)} ({doc_title} - {doc_type}): {status.status}\")\n",
        "            \n",
        "            if status.status == \"completed\":\n",
        "                completed_jobs.add(job_id)\n",
        "            elif status.status == \"failed\":\n",
        "                print(f\"Job failed for {doc_title}\")\n",
        "                completed_jobs.add(job_id)  # Add to completed to avoid infinite loop\n",
        "    \n",
        "    if len(completed_jobs) < len(job_data):\n",
        "        print(\"\\nWaiting for remaining jobs to complete...\")\n",
        "        sleep(30)\n",
        "\n",
        "print(\"\\nAll parse jobs completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieve and process parsed content\n",
        "\n",
        "We'll retrieve the parsed results and process them into chunks suitable for vector search. Contextual AI provides excellent document structure preservation, which we'll leverage for better RAG performance.\n",
        "\n",
        "**Key Feature**: Contextual AI preserves document hierarchy through `parent_ids`, allowing us to maintain section relationships and provide richer context to our RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve results and process into chunks\n",
        "texts, titles, sources, doc_types = [], [], [], []\n",
        "\n",
        "for job_info in job_data:\n",
        "    job_id = job_info[\"job_id\"]\n",
        "    document = job_info[\"document\"]\n",
        "    \n",
        "    if job_id in completed_jobs:\n",
        "        try:\n",
        "            print(f\"Processing {document['title']} ({document['type']})\")\n",
        "            \n",
        "            # Get results with blocks-per-page for hierarchical information\n",
        "            results = client.parse.job_results(\n",
        "                job_id, \n",
        "                output_types=['blocks-per-page']\n",
        "            )\n",
        "            \n",
        "            print(f\"  - {len(results.pages)} pages parsed\")\n",
        "            \n",
        "            # Create hash table for parent content lookup\n",
        "            hash_table = {}\n",
        "            for page in results.pages:\n",
        "                for block in page.blocks:\n",
        "                    hash_table[block.id] = block.markdown\n",
        "            \n",
        "            # Process blocks with hierarchy context\n",
        "            for page in results.pages:\n",
        "                for block in page.blocks:\n",
        "                    # Filter blocks based on document type and content quality\n",
        "                    if (block.type in ['text', 'heading', 'table'] and \n",
        "                        len(block.markdown.strip()) > 30):\n",
        "                        \n",
        "                        # Add hierarchy context if available\n",
        "                        context_text = block.markdown\n",
        "                        \n",
        "                        if hasattr(block, 'parent_ids') and block.parent_ids:\n",
        "                            parent_content = \"\\n\".join([\n",
        "                                hash_table.get(parent_id, \"\") \n",
        "                                for parent_id in block.parent_ids\n",
        "                            ])\n",
        "                            if parent_content.strip():\n",
        "                                context_text = f\"{parent_content}\\n\\n{block.markdown}\"\n",
        "                        \n",
        "                        # Add document metadata as context\n",
        "                        full_text = f\"Document: {document['title']}\\nType: {document['type']}\\n\\n{context_text}\"\n",
        "                        \n",
        "                        texts.append(full_text)\n",
        "                        titles.append(document['title'])\n",
        "                        sources.append(f\"Page {page.index + 1}\")\n",
        "                        doc_types.append(document['type'])\n",
        "                        \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {document['title']}: {e}\")\n",
        "\n",
        "print(f\"\\nProcessed {len(texts)} chunks from {len(set(titles))} documents\")\n",
        "print(f\"Document types: {', '.join(set(doc_types))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíö Part 2: Weaviate\n",
        "### Connect to your Weaviate cluster and create a collection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Connect to your running Weaviate cluster (choose one option)**\n",
        "1. [Weaviate Cloud](https://console.weaviate.cloud/)\n",
        "2. [Embedded Weaviate](https://weaviate.io/developers/weaviate/installation/embedded)\n",
        "3. [Other options](https://weaviate.io/developers/weaviate/installation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Option 1: Weaviate Cloud**\n",
        "You can create a free 14-day free sandbox on [WCD](https://console.weaviate.cloud/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Weaviate Cloud\n",
        "\n",
        "import weaviate\n",
        "from weaviate.auth import Auth\n",
        "from getpass import getpass\n",
        "\n",
        "if \"WEAVIATE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"WEAVIATE_API_KEY\"] = getpass(\"Weaviate API Key\")\n",
        "if \"WEAVIATE_URL\" not in os.environ:\n",
        "  os.environ[\"WEAVIATE_URL\"] = getpass(\"Weaviate URL\")\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key\")\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=os.environ.get(\"WEAVIATE_URL\"),\n",
        "    auth_credentials=Auth.api_key(os.environ.get(\"WEAVIATE_API_KEY\")),\n",
        "    headers={\"X-OpenAI-Api-Key\": (os.environ.get(\"OPENAI_API_KEY\"))}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Option 2: Weaviate Embedded**\n",
        "Embedded Weaviate](https://weaviate.io/developers/weaviate/installation/embedded) allows you to spin up a Weaviate instance directly from your application code, without having to use a Docker container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import weaviate\n",
        "\n",
        "# # Connect to Weaviate embedded\n",
        "# client_weaviate = weaviate.connect_to_embedded(\n",
        "#     headers={\n",
        "#         \"X-OpenAI-Api-Key\": openai_api_key\n",
        "#     }\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create your collection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import weaviate.classes.config as wc\n",
        "from weaviate.classes.config import Property, DataType\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"contextual_parser\"\n",
        "\n",
        "# Delete the collection if it already exists\n",
        "if (client_weaviate.collections.exists(collection_name)):\n",
        "    client_weaviate.collections.delete(collection_name)\n",
        "\n",
        "# Create the collection\n",
        "collection = client_weaviate.collections.create(\n",
        "    name=collection_name,\n",
        "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(\n",
        "        model=\"text-embedding-3-large\",                           # Specify your embedding model here\n",
        "    ),\n",
        "\n",
        "    # Enable generative model from OpenAI\n",
        "    generative_config=wc.Configure.Generative.openai(\n",
        "    model=\"gpt-4o\"                                                # Specify your generative model for RAG here\n",
        "    ),\n",
        "\n",
        "    # Define properties of metadata\n",
        "    properties=[\n",
        "        wc.Property(\n",
        "            name=\"text\",\n",
        "            data_type=wc.DataType.TEXT\n",
        "        ),\n",
        "        wc.Property(\n",
        "            name=\"title\",\n",
        "            data_type=wc.DataType.TEXT,\n",
        "            skip_vectorization=True\n",
        "        ),\n",
        "        wc.Property(\n",
        "            name=\"source\",\n",
        "            data_type=wc.DataType.TEXT,\n",
        "            skip_vectorization=True\n",
        "        ),\n",
        "        wc.Property(\n",
        "            name=\"document_type\",\n",
        "            data_type=wc.DataType.TEXT,\n",
        "            skip_vectorization=True\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrangle data into an acceptable format for Weaviate\n",
        "\n",
        "Transform our data from lists to a list of dictionaries for insertion into our Weaviate collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the data object\n",
        "data = []\n",
        "\n",
        "# Create a dictionary for each row by iterating through the corresponding lists\n",
        "for text, title, source, doc_type in zip(texts, titles, sources, doc_types):\n",
        "    data_point = {\n",
        "        \"text\": text,\n",
        "        \"title\": title,\n",
        "        \"source\": source,\n",
        "        \"document_type\": doc_type,\n",
        "    }\n",
        "    data.append(data_point)\n",
        "\n",
        "print(f\"Prepared {len(data)} chunks for insertion into Weaviate\")\n",
        "print(f\"Chunks by document type:\")\n",
        "for doc_type in set(doc_types):\n",
        "    count = doc_types.count(doc_type)\n",
        "    print(f\"  - {doc_type}: {count} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insert data into Weaviate and generate embeddings\n",
        "\n",
        "Embeddings will be generated upon insertion to our Weaviate collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert text chunks and metadata into vector DB collection\n",
        "response = collection.data.insert_many(\n",
        "    data\n",
        ")\n",
        "\n",
        "if (response.has_errors):\n",
        "    print(response.errors)\n",
        "else:\n",
        "    print(\"Insert complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Query the data\n",
        "\n",
        "Here, we perform a simple similarity search to return the most similar embedded chunks to our search query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weaviate.classes.query import MetadataQuery\n",
        "\n",
        "# Example 1: Search for transformer-related content\n",
        "print(\"=== Searching for Transformer Architecture ===\")\n",
        "response = collection.query.near_text(\n",
        "    query=\"transformer architecture attention mechanism\",\n",
        "    limit=3,\n",
        "    return_metadata=MetadataQuery(distance=True),\n",
        "    return_properties=[\"text\", \"title\", \"source\", \"document_type\"]\n",
        ")\n",
        "\n",
        "for i, obj in enumerate(response.objects):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(f\"Title: {obj.properties['title']}\")\n",
        "    print(f\"Type: {obj.properties['document_type']}\")\n",
        "    print(f\"Source: {obj.properties['source']}\")\n",
        "    print(f\"Similarity: {1 - obj.metadata.distance:.3f}\")\n",
        "    print(f\"Text preview: {obj.properties['text'][:200]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Example 2: Search for table-related content\n",
        "print(\"\\n=== Searching for Table/Data Content ===\")\n",
        "response = collection.query.near_text(\n",
        "    query=\"dataset table benchmark performance metrics\",\n",
        "    limit=3,\n",
        "    return_metadata=MetadataQuery(distance=True),\n",
        "    return_properties=[\"text\", \"title\", \"source\", \"document_type\"]\n",
        ")\n",
        "\n",
        "for i, obj in enumerate(response.objects):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(f\"Title: {obj.properties['title']}\")\n",
        "    print(f\"Type: {obj.properties['document_type']}\")\n",
        "    print(f\"Source: {obj.properties['source']}\")\n",
        "    print(f\"Similarity: {1 - obj.metadata.distance:.3f}\")\n",
        "    print(f\"Text preview: {obj.properties['text'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform RAG on parsed articles\n",
        "\n",
        "Weaviate's `generate` module allows you to perform RAG over your embedded data without having to use a separate framework.\n",
        "\n",
        "We specify a prompt that includes the field we want to search through in the database (in this case it's `text`), a query that includes our search term, and the number of retrieved results to use in the generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "\n",
        "# Example 1: RAG on Transformer Architecture\n",
        "print(\"=== RAG Query: Transformer Architecture ===\")\n",
        "prompt = \"Explain how {text} works, using only the retrieved context.\"\n",
        "query = \"transformer attention mechanism\"\n",
        "\n",
        "response = collection.generate.near_text(\n",
        "    query=query,\n",
        "    limit=4,\n",
        "    grouped_task=prompt,\n",
        "    return_properties=[\"text\", \"title\", \"source\", \"document_type\"]\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "console.print(Panel(f\"{prompt}\".replace(\"{text}\", query), title=\"Prompt\", border_style=\"bold red\"))\n",
        "console.print(Panel(response.generated, title=\"Generated Content\", border_style=\"bold green\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: RAG on Dataset/Benchmark Information\n",
        "print(\"\\n=== RAG Query: Dataset and Benchmark Information ===\")\n",
        "prompt = \"What information does the retrieved context provide about {text}?\"\n",
        "query = \"dataset benchmark performance evaluation\"\n",
        "\n",
        "response = collection.generate.near_text(\n",
        "    query=query,\n",
        "    limit=4,\n",
        "    grouped_task=prompt,\n",
        "    return_properties=[\"text\", \"title\", \"source\", \"document_type\"]\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "console.print(Panel(f\"{prompt}\".replace(\"{text}\", query), title=\"Prompt\", border_style=\"bold red\"))\n",
        "console.print(Panel(response.generated, title=\"Generated Content\", border_style=\"bold green\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Cross-document comparison\n",
        "print(\"\\n=== RAG Query: Cross-Document Analysis ===\")\n",
        "prompt = \"Compare and contrast the information about {text} from different document types in the retrieved context.\"\n",
        "query = \"attention mechanisms neural networks\"\n",
        "\n",
        "response = collection.generate.near_text(\n",
        "    query=query,\n",
        "    limit=4,\n",
        "    grouped_task=prompt,\n",
        "    return_properties=[\"text\", \"title\", \"source\", \"document_type\"]\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "console.print(Panel(f\"{prompt}\".replace(\"{text}\", query), title=\"Prompt\", border_style=\"bold red\"))\n",
        "console.print(Panel(response.generated, title=\"Generated Content\", border_style=\"bold green\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a unique RAG pipeline using Contextual AI Parser and Weaviate with two distinct document types:\n",
        "\n",
        "### What We Demonstrated:\n",
        "1. **Research Paper Parsing**: \"Attention is All You Need\" with document hierarchy preservation\n",
        "2. **Table-Rich Document Parsing**: OmniDocBench dataset with advanced table extraction\n",
        "3. **Multi-format RAG**: Semantic search across different document types\n",
        "4. **Contextual Intelligence**: Leveraging document structure for better retrieval\n",
        "\n",
        "### Contextual AI Parser Advantages:\n",
        "- **Cloud-based processing**: No local GPU/compute requirements\n",
        "- **Document hierarchy preservation**: Maintains section relationships and structure\n",
        "- **Advanced table handling**: Smart table splitting with header propagation\n",
        "- **Multiple output formats**: Blocks, markdown, and structured JSON\n",
        "- **Production-ready**: Scalable cloud service with enterprise features\n",
        "\n",
        "### Key Differentiators from Other Parsers:\n",
        "- **Hierarchical context**: Parent-child relationships preserved in chunks\n",
        "- **Table intelligence**: Large tables automatically split with context preservation\n",
        "- **Document type awareness**: Different parsing strategies for different content types\n",
        "- **Rich metadata**: Document structure information enhances RAG quality\n",
        "\n",
        "### Weaviate Integration Benefits:\n",
        "- **Multi-modal search**: Query across different document types simultaneously\n",
        "- **Metadata filtering**: Filter by document type, source, and other attributes\n",
        "- **Generative AI**: Built-in RAG with context-aware generation\n",
        "- **Scalability**: From embedded development to cloud production\n",
        "\n",
        "### Next Steps for Enhancement:\n",
        "* Implement document-level metadata for better source attribution\n",
        "* Add hybrid search combining keyword and semantic search\n",
        "* Experiment with different chunking strategies for each document type\n",
        "* End-to-end RAG agents via [Contextual AI](https://docs.contextual.ai/user-guides/beginner-guide)\n",
        "* Read more about Agentic RAG from [Weaviate](https://weaviate.io/blog/what-is-agentic-rag)\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to get started?** This notebook provides a complete, production-ready example of integrating Contextual AI Parser with Weaviate for sophisticated RAG applications. The combination of Contextual AI's advanced parsing capabilities and Weaviate's powerful vector search and generation features creates a robust foundation for document-based AI applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup: Close Weaviate connection\n",
        "client_weaviate.close()\n",
        "\n",
        "# Optional: Clean up downloaded PDFs\n",
        "import shutil\n",
        "if os.path.exists(\"pdfs\"):\n",
        "    shutil.rmtree(\"pdfs\")\n",
        "    print(\"Cleaned up downloaded PDFs\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
