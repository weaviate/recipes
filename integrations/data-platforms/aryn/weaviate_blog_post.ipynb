{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c47f4-f33e-4fdc-8c9a-b3d6b78eac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453829db-30af-4e7b-adfa-564b8e5e854d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will walk through how to prepare and load data into Weaviate using Sycamore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02514c7-6a82-4cc5-80d0-35cfc6889aa5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "To run this notebook, you should complete the following setup tasks. \n",
    "\n",
    "- **Sycamore**. The Sycamore library can be installed using `pip` with the command `pip install sycamore-ai`. We recommend you install this in a virtual environment to isolate its dependencies.\n",
    "- **ArynPartitioner**. This notebook utilizes the Aryn Partitioning Service. This provides an endpoint that integrates with Sycamore for partitioning PDFs. You can sign up for a free API key at [https://www.aryn.ai/get-started](https://www.aryn.ai/get-started). Once you have gotten an API key, export it by setting the `ARYN_API_KEY` environment variable. You can read about other options to specify your API key [here](https://sycamore.readthedocs.io/en/stable/aryn_cloud/aryn_partitioning_service.html). Alternatively, you can run the partitioning step locally, as described below. \n",
    "- **Poppler**. Some of Sycamore's PDF processing routines depend on the `poppler` package being available. This can be installed with your platform's native package manager. For example, on Mac with Homebrew, you can install it with `brew install poppler` and on Debian Linux and it's derivatives you can use `sudo apt install poppler-utils`. More information about Poppler can be found [here](https://poppler.freedesktop.org/).\n",
    "- **OpenAI**. The `SummarizeImages` transform makes use of OpenAI to compute text summaries of images. To make use of OpenAI, you need an OpenAI API key, which you can get from [here](https://platform.openai.com). This notebook assumes you have set the `OPENAI_API_KEY` environment variable set to your key, though we show below how to set the key directly as well. \n",
    "- **Weaviate**. Weaviate should be accessible via localhost on port 8080 for HTTP and port 50051 for gRPC. To support embedding of queries, you should have the `sentence-transformers-all-MiniLM-L6-v2` model setup. You can find a sample Docker compose file to set this up [here](https://github.com/aryn-ai/sycamore/blob/main/apps/weaviate/compose.yml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397ff6-f443-4d41-9593-09cb9d070ff4",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "The first step is read both files into a DocSet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4427013-3274-4868-995c-7df724e7818f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sycamore\n",
    "\n",
    "paths = [\"../data/\"] \n",
    "\n",
    "context = sycamore.init()\n",
    "ds = context.read.binary(paths=paths, binary_format=\"pdf\")\n",
    "\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1515f-6c21-426d-81e1-f7b27799350f",
   "metadata": {},
   "source": [
    "Next, we run the ArynPartitioner to segment the documents. We show an example of how one page is partitioned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a74f5e-215c-4435-80c6-af2a85d7ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "from sycamore.utils.pdf_utils import show_pages\n",
    "\n",
    "# Make sure that your Aryn token is available in the environment variable\n",
    "# ARYN_API_KEY\n",
    "partitioner = ArynPartitioner(\n",
    "    extract_table_structure=True, \n",
    "    extract_images=True)\n",
    "\n",
    "# Alternatively, you can uncomment the following to run the ArynPartitioner\n",
    "# locally. This works best if you have a NVIDIA GPU. \n",
    "# partitioner = ArynPartitioner(\n",
    "#     extract_table_structure=True, \n",
    "#     extract_images=True,\n",
    "#     use_partitioning_service=False)\n",
    "\n",
    "ds = ds.partition(partitioner=partitioner)\n",
    "\n",
    "# The show_pages utility displays a subset of pages with their bounding \n",
    "# boxes after partitioning. This can be useful for understanding and \n",
    "# debugging the output of the ArynPartitioner. \n",
    "show_pages(ds, limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76249a3e-c0c9-4485-8ba3-3b47e7e6326a",
   "metadata": {},
   "source": [
    "At this point we have split the papers into elements, and we can look at the output. Here we look at section headers from the first paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19a3ab-95e8-4ade-bb76-dfe2d3920d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ds.filter(lambda doc: doc.properties['path'].endswith(\"paper01.pdf\"))\\\n",
    "         .filter_elements(lambda el: el.type == \"Section-header\" and el.text_representation is not None)\\\n",
    "         .take_all()\n",
    "\n",
    "for d in docs:\n",
    "    for e in d.elements:\n",
    "        print(e.text_representation.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb6ad7-9228-4bef-8d30-4220259fd799",
   "metadata": {},
   "source": [
    "You can see that the section headers were correctly extracted, though a few of the table titles were also identified as section headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9967bd5-c58e-4f23-a1ef-6366cd9d79a8",
   "metadata": {},
   "source": [
    "## Entity Extraction and Summarization\n",
    "In addition to basic partitioning, Sycamore makes it easy to augment your documents with metadata to improve retrieval. For example, the following code extracts the title, authors, and abstract of each paper in the DocSet and saves it as metadata associated with the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b51f9-bb5b-4e3d-a236-88360c7b4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.llms import OpenAI, OpenAIModels\n",
    "from sycamore.transforms.extract_schema import OpenAIPropertyExtractor\n",
    "\n",
    "# Specifies a schema name and type that direct the LLM what properties to extract.\n",
    "schema_name = \"PaperInfo\"\n",
    "schema = {\n",
    "\t\"title\": \"string\",\n",
    "\t\"authors\": \"list[string]\",\n",
    "\t\"abstract\": \"string\"\n",
    "}\n",
    "\n",
    "openai = OpenAI(OpenAIModels.GPT_4O) # Reads the OPENAI_API_KEY env var\n",
    "\n",
    "# Extract the properties and add them under a special key \"entity\" in the \n",
    "# document properties. By default this sends the first 10 elements of the \n",
    "# of the Document the LLM. \n",
    "ds = ds.extract_properties(OpenAIPropertyExtractor(\n",
    "   llm=openai,\n",
    "   schema_name=schema_name,\n",
    "   schema=schema))\n",
    "\n",
    "ds.show(show_elements=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872216b7-ec38-40de-8133-52de2c83c29e",
   "metadata": {},
   "source": [
    "The following code summarizes each image in the documents using GPT-4o. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf08c8-a9e4-4100-aff8-fe2bf45f65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.transforms.summarize_images import OpenAIImageSummarizer, SummarizeImages\n",
    "\n",
    "ds = ds.transform(SummarizeImages)\n",
    "\n",
    "# By default the SummarizeImages transform will use GPT-4o and pick up\n",
    "# credentials from the OPENAI_API_KEY environment variables. You\n",
    "# can use a custom LLM like the following. \n",
    "#\n",
    "# summarizer = OpenAIImageSummarizer(openai_model=openai)\n",
    "# ds = ds.transform(SummarizeImages, summarizer=summarizer)\n",
    "\n",
    "# Display only the image elements from each document. \n",
    "ds.filter_elements(lambda e: e.type == 'Image')\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4318759-ed46-497a-8c38-4aba8299bd0e",
   "metadata": {},
   "source": [
    "## Writing to Weaviate\n",
    "The final step is to write the records to Weaviate. The following code configures the Weaviate client assuming that it runs locally, though you can adjust this to point to any Weaviate endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7262478-2622-4a85-9d56-138cfce80309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.client import AdditionalConfig, ConnectionParams\n",
    "from weaviate.config import Timeout\n",
    "from weaviate.collections.classes.config import Configure\n",
    "from weaviate.classes.config import ReferenceProperty\n",
    "\n",
    "collection = \"WeaviateSycamoreDemoCollection\"\n",
    "wv_client_args = {\n",
    "    \"connection_params\": ConnectionParams.from_params(\n",
    "        http_host=\"localhost\",\n",
    "        http_port=8080,\n",
    "        http_secure=False,\n",
    "        grpc_host=\"localhost\",\n",
    "        grpc_port=50051,\n",
    "        grpc_secure=False,\n",
    "    ),\n",
    "    \"additional_config\": AdditionalConfig(timeout=Timeout(init=2, query=45, insert=300)),\n",
    "}\n",
    "collection_config_params = {\n",
    "    \"name\": collection,\n",
    "    \"description\": \"A collection to demo data-prep with Sycamore\",\n",
    "\n",
    "    # Sycamore can be used to embed document chunks before writing to Weaviate, so this is primarily to \n",
    "    # ensure that queries are embedded using the correct model in Weaviate. If you don't need to embed\n",
    "    # queries or can do so externally, you can change the vectorizer_config to None. \n",
    "    \"vectorizer_config\": [Configure.NamedVectors.text2vec_transformers(name=\"embedding\", source_properties=['text_representation'])],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bd2d5-9ed3-4f1f-86e2-2de0af827f71",
   "metadata": {},
   "source": [
    "Next, we write the data out from Sycamore into Weaviate. The following code does a few things: (1) It associates the \"path\" and \"entity\" properties from the top-level documents with each element to simplify queries, (2) it breaks each document into chunks and creates embeddings for each chunk, and (3) it writes the chunks to Weaviate using the configuration defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da88ec8-9b33-4880-99f7-82d99687b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.transforms.embed import SentenceTransformerEmbedder\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "ds.spread_properties([\"path\", \"entity\"])\\\n",
    "  .explode()\\\n",
    "  .embed(embedder=SentenceTransformerEmbedder(\n",
    "      model_name=model_name, batch_size=1000))\\\n",
    "  .write.weaviate(\n",
    "      wv_client_args=wv_client_args,\n",
    "      collection_name=collection,\n",
    "      collection_config=collection_config_params,\n",
    "      flatten_properties=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434095a-00f7-47e5-9d25-15c0c205b27e",
   "metadata": {},
   "source": [
    "## Querying with Weaviate\n",
    "Once the data has been loaded into Weaviate, you can query with the standard client. The following shows an example of a query that uses both a hybrid search and filters to find images aobut skin cancer image classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae1c30-3efa-45f7-aa86-f3ccb7a9a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "wcl = weaviate.connect_to_local()\n",
    "demo = wcl.collections.get(collection)\n",
    "\n",
    "# Utility method for formatting the output in an easily readable way.\n",
    "def print_search_result(sr):\n",
    "    for obj in sr.objects:\n",
    "        print(\"=\" * 80)\n",
    "        for p in obj.properties:\n",
    "            print(f\"{p: <30}| {obj.properties[p]}\")\n",
    "\n",
    "# Specify the properties to return in the vector search.\n",
    "get_props = [\n",
    "\t\"text_representation\",\n",
    "\t\"type\",\n",
    "\t\"properties__path\",\n",
    "\t\"properties__page_number\",\n",
    "\t\"properties__entity__authors\",\n",
    "\t\"properties__entity__title\"\n",
    "]\n",
    "\n",
    "# Do a hybrid search query with a filter for Image elements.\n",
    "print_search_result(demo.query.hybrid(\n",
    "    query=\"Applications of deep learning to skin cancer image classification.\",\n",
    "    query_properties=[\"text_representation\"],\n",
    "    target_vector=\"embedding\",\n",
    "    return_properties=get_props,\n",
    "    filters=Filter.by_property(\"type\").equal(\"Image\"),\n",
    "    limit=2\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
