{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/integrations/compute-infrastructure/replicate-llama2/notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the quick notebook on using Llama 2 ðŸ¦™ ðŸš¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To begin, make sure you:\n",
    "1. pip install weaviate-client\n",
    "2. pip install llama-index\n",
    "3. pip install replicate \n",
    "\n",
    "### You will also need to have an access key for:\n",
    "1. [OpenAI](https://openai.com/)\n",
    "2. [Replicate](https://replicate.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  -U weaviate-client llama-index llama-index-vector-stores-weaviate llama-index-llms-replicate replicate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_embedded(\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection was created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"build_git_commit\":\"ab0312d5d\",\"build_go_version\":\"go1.23.1\",\"build_image_tag\":\"localhost\",\"build_wv_version\":\"1.26.6\",\"level\":\"warning\",\"msg\":\"prop len tracker file /Users/dudanogueira/.local/share/weaviate/blogpost/EOnZvpRaS761/proplengths does not exist, creating new tracker\",\"time\":\"2024-12-17T17:22:21-03:00\"}\n",
      "{\"action\":\"hnsw_prefill_cache_async\",\"build_git_commit\":\"ab0312d5d\",\"build_go_version\":\"go1.23.1\",\"build_image_tag\":\"localhost\",\"build_wv_version\":\"1.26.6\",\"level\":\"info\",\"msg\":\"not waiting for vector cache prefill, running in background\",\"time\":\"2024-12-17T17:22:21-03:00\",\"wait_for_cache_prefill\":false}\n",
      "{\"build_git_commit\":\"ab0312d5d\",\"build_go_version\":\"go1.23.1\",\"build_image_tag\":\"localhost\",\"build_wv_version\":\"1.26.6\",\"level\":\"info\",\"msg\":\"Created shard blogpost_EOnZvpRaS761 in 1.662125ms\",\"time\":\"2024-12-17T17:22:21-03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"build_git_commit\":\"ab0312d5d\",\"build_go_version\":\"go1.23.1\",\"build_image_tag\":\"localhost\",\"build_wv_version\":\"1.26.6\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-12-17T17:22:21-03:00\",\"took\":51292}\n"
     ]
    }
   ],
   "source": [
    "from weaviate import classes as wvc\n",
    "# clean slate\n",
    "client.collections.delete(\"BlogPost\")\n",
    "\n",
    "collection = client.collections.create(\n",
    "    name=\"BlogPost\",\n",
    "    description=\"Blog post from the Weaviate website.\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_openai(\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ),\n",
    "    properties=[\n",
    "        wvc.config.Property(\n",
    "            name=\"text\", description=\"Content from the blog post\", data_type=wvc.config.DataType.TEXT\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Collection was created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load the blogs in using the reader\n",
    "blogs = SimpleDirectoryReader('./data').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"BlogPost\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(blogs, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define our Replicate API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"<YOUR REPLICATE API KEY HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref2Vec is a module that allows for the vectorization of a data object based on its cross-references to other objects. It derives a centroid vector from the cross-referenced vectors to represent the referencing object. This approach enables the characterization of the referencing object through its relationships and actions, which can be refined over time. In building recommendation systems, Ref2Vec helps by representing a user's interests through a graph of cross-references from the user to objects they have interacted with. This unique representation of each user's preferences can then be used to rank search results based on relevance to the user's interests, enhancing the quality of recommendations provided.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is ref2vec? How does this feature help with building recommendation systems?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role-based access control (RBAC), enhanced async indexing improvements, native Japanese language support, conflict resolution improvements, new Japanese `kagome_ja` tokenizer for keyword/hybrid search, groundwork for Keyword & Hybrid search improvements with BlockMax WAND, Voyage AI Multimodal model support, and Weaviate Embeddings are new in Weaviate 1.28.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is new in Weaviate 1.28?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
