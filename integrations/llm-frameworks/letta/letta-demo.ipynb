{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build Agents with Weaviate and Letta!\n",
    "\n",
    "This repository will illustrate the core concepts behind Letta, a new startup built from the success of MemGPT from researchers at UC Berkeley.\n",
    "\n",
    "Letta helps you build Agents with **meta memory**.\n",
    "\n",
    "In addition to the prompt sent to an LLM, the LLM will also be provided with a short description of it's internal knowledge about the user and their interests derived from previous chats.\n",
    "\n",
    "There are 3 parts to this notebook:\n",
    "\n",
    "## 1. Getting Started with Letta Agents\n",
    "\n",
    "An illustration of the basic setup requirements and a simple example of an Agent with internal memory.\n",
    "\n",
    "## 2. Upload Weaviate Search Tool to Letta\n",
    "\n",
    "Wrap Weaviate search in a Python function and upload it to the Letta Tool server.\n",
    "\n",
    "## 3. Letta Agent with Weaviate\n",
    "\n",
    "Construct a Letta Agent that chats with Weaviate's blog posts indexed in Weaviate. Observe how the Letta Agent develops an internal model of the chat user and the blog posts through these interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"architectural-visual.png\" alt=\"visual\" width=\"30%\" height=\"auto\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started with Letta Agents\n",
    "\n",
    "### Primer: Install Letta Server\n",
    "\n",
    "`$pip install -U letta`\n",
    "\n",
    "Start local server:\n",
    "\n",
    "`$letta server`\n",
    "\n",
    "### You can then create a Letta Agent in the GUI\n",
    "\n",
    "We will also see how to do this with Python APIs in the notebook.\n",
    "\n",
    "<img src=\"letta-create-agent.png\" alt=\"Create Letta Agent GUI\" width=\"60%\" height=\"auto\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Connect to Letta Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta import create_client \n",
    "\n",
    "# connect to the letta server\n",
    "letta_client = create_client(base_url=\"http://localhost:8283\")\n",
    "\n",
    "# get this from your local GUI\n",
    "my_letta_agent_id = \"agent-7abd9578-7e64-4416-ba71-5a3f908d0bea\"\n",
    "\n",
    "# send a message to the agent\n",
    "response = letta_client.send_message(\n",
    "    agent_id=my_letta_agent_id, \n",
    "    role=\"user\", \n",
    "    message=\"How are you? Can you tell me about how HNSW works?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Monologue:\n",
      "Responding to the user's request for information on HNSW. Given his background, I'm providing a concise yet in-depth description. Now waiting for his follow-up.\n",
      "\n",
      "Message Sent:\n",
      "{\n",
      "  \"message\": \"I'm doing great, thank you! As for HNSW, Hierarchical Navigable Small World graphs, it's a state-of-the-art method for nearest neighbor search in high dimensional spaces. The main idea is that during the indexing stage, it builds a graph with a small-world property that allows for quick traversal. The graph's vertices are the data items, and each vertex has a set of connections on different layers - the lowest layer connects to the nearest items, while the higher layers have connections that span larger distances. This hierarchy significantly accelerates the search process because it can quickly navigate to the right area in earlier, higher layers, then fine-tune on lower layers. Would you like a more technical deep dive, perhaps focusing on the insertion algorithm or the search algorithm?\"\n",
      "}\n",
      "\n",
      "Function Return Status:\n",
      "Status: success\n",
      "Time: {\n",
      "  \"status\": \"OK\",\n",
      "  \"message\": \"None\",\n",
      "  \"time\": \"2024-10-27 10:00:52 PM EDT-0400\"\n",
      "}\n",
      "\n",
      "Usage Statistics:\n",
      "Completion tokens: 279\n",
      "Prompt tokens: 2375\n",
      "Total tokens: 2654\n",
      "Step count: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the internal monologue\n",
    "print(\"Internal Monologue:\")\n",
    "print(response.messages[0].internal_monologue)\n",
    "print()\n",
    "\n",
    "# Print the message sent\n",
    "print(\"Message Sent:\")\n",
    "print(response.messages[1].function_call.arguments)\n",
    "print()\n",
    "\n",
    "# Print the function return status\n",
    "print(\"Function Return Status:\")\n",
    "print(f\"Status: {response.messages[2].status}\")\n",
    "print(f\"Time: {response.messages[2].function_return}\")\n",
    "print()\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"Usage Statistics:\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\") \n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Step count: {response.usage.step_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Monologue:\n",
      "Explaining Product Quantization. The user seems to be interested in database management systems, so focusing on memory efficiency.\n",
      "\n",
      "Message Sent:\n",
      "{\n",
      "  \"message\": \"Product Quantization (PQ) is a technique for high-dimensional vectors compression that notably boosts the efficiency of nearest neighbor searches. It works by dividing the high-dimensional vector space into smaller, manageable subspaces. Then, it independently quantizes each subspace into 'codewords' derived from a limited 'codebook'. When a vector is stored, it's mapped to a combination of these codewords from the different subspaces. The high-dimensional vector retrieval now becomes retrieving the local codewords and assembling them. Thus, instead of storing the high-dimensional vector in full, we store the compact indices referencing the codebook. This reduces the storage requisite dramatically without gravely compromising the search precision. Essentially, PQ trades a bit of accuracy for the benefit of significant memory efficiency.\"\n",
      "}\n",
      "\n",
      "Function Return Status:\n",
      "Status: success\n",
      "Time: {\n",
      "  \"status\": \"OK\",\n",
      "  \"message\": \"None\",\n",
      "  \"time\": \"2024-10-27 10:01:40 PM EDT-0400\"\n",
      "}\n",
      "\n",
      "Usage Statistics:\n",
      "Completion tokens: 200\n",
      "Prompt tokens: 2697\n",
      "Total tokens: 2897\n",
      "Step count: 1\n"
     ]
    }
   ],
   "source": [
    "response = letta_client.send_message(\n",
    "    agent_id=my_letta_agent_id, \n",
    "    role=\"user\", \n",
    "    message=\"Please explain how Product Quantization saves the memory needed to store vector embeddings in a database management system.\"\n",
    ")\n",
    "\n",
    "# Print the internal monologue\n",
    "print(\"Internal Monologue:\")\n",
    "print(response.messages[0].internal_monologue)\n",
    "print()\n",
    "\n",
    "# Print the message sent\n",
    "print(\"Message Sent:\")\n",
    "print(response.messages[1].function_call.arguments)\n",
    "print()\n",
    "\n",
    "# Print the function return status\n",
    "print(\"Function Return Status:\")\n",
    "print(f\"Status: {response.messages[2].status}\")\n",
    "print(f\"Time: {response.messages[2].function_return}\")\n",
    "print()\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"Usage Statistics:\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\") \n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Step count: {response.usage.step_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"id\": \"message-0b60401b-8b3d-4349-86c6-69c666b6b17f\",\n",
      "            \"date\": \"2024-10-28T02:03:16+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"Again Chad is asking about his database interests. Let's review the conversation history once more to ensure we're not missing anything.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-0b60401b-8b3d-4349-86c6-69c666b6b17f\",\n",
      "            \"date\": \"2024-10-28T02:03:16+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"conversation_search\",\n",
      "                \"arguments\": \"{\\n  \\\"query\\\": \\\"database\\\",\\n  \\\"request_heartbeat\\\": true\\n}\",\n",
      "                \"function_call_id\": \"call_kuc4xYiVcX3NHPRwoLOF1qud\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-6a86ac68-c571-4bf4-8b42-81a07bb05b96\",\n",
      "            \"date\": \"2024-10-28T02:03:16+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"Showing 5 of 5 results (page 0/0): [\\\\n  \\\\\\\"timestamp: 2024-10-28 02:01:31.242887, user - {\\\\\\\\n  \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user_message\\\\\\\\\\\\\\\",\\\\\\\\n  \\\\\\\\\\\\\\\"message\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Please explain how Product Quantization saves the memory needed to store vector embeddings in a database management system.\\\\\\\\\\\\\\\",\\\\\\\\n  \\\\\\\\\\\\\\\"time\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2024-10-27 10:01:31 PM EDT-0400\\\\\\\\\\\\\\\"\\\\\\\\n}\\\\\\\",\\\\n  \\\\\\\"timestamp: 2024-10-28 02:01:40.219156, assistant - Explaining Product Quantization. The user seems to be interested in database management systems, so focusing on memory efficiency.\\\\\\\",\\\\n  \\\\\\\"timestamp: 2024-10-28 02:02:24.052080, user - {\\\\\\\\n  \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user_message\\\\\\\\\\\\\\\",\\\\\\\\n  \\\\\\\\\\\\\\\"message\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Can you tell me what kind of database topics I am the most interested in?\\\\\\\\\\\\\\\",\\\\\\\\n  \\\\\\\\\\\\\\\"time\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2024-10-27 10:02:24 PM EDT-0400\\\\\\\\\\\\\\\"\\\\\\\\n}\\\\\\\",\\\\n  \\\\\\\"timestamp: 2024-10-28 02:02:27.123174, assistant - I am going to scan our conversation history and see what specific topics related to databases Chad has shown interest in.\\\\\\\",\\\\n  \\\\\\\"timestamp: 2024-10-28 02:02:32.437664, assistant - Providing information on Chad's interests in databases based on our chat history. Asking for more input to better understand his interests.\\\\\\\"\\\\n]\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:03:16 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_kuc4xYiVcX3NHPRwoLOF1qud\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-f27009f2-c6a2-4a25-89ba-aa868a64b867\",\n",
      "            \"date\": \"2024-10-28T02:03:23+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"Based on our conversations, Chad is interested in how database systems manage storage of high-dimensional vector embeddings. I've noticed he's heavily invested in memory optimization techniques like Product Quantization.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-f27009f2-c6a2-4a25-89ba-aa868a64b867\",\n",
      "            \"date\": \"2024-10-28T02:03:23+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"send_message\",\n",
      "                \"arguments\": \"{\\n  \\\"message\\\": \\\"Reviewing our conversation so far, it appears that you have a keen interest in understanding the memory optimization in database management systems, particularly how high-dimensional vector embeddings are stored. You've shown interest in Product Quantization, a technique to save memory while maintaining efficient nearest neighbor searches. If there are other aspects of database systems you're interested in, please don't hesitate to let me know!\\\"\\n}\",\n",
      "                \"function_call_id\": \"call_i5cEkttbYdHXMpFke1rxc2Xx\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-2b12a08f-5b7b-44eb-be59-79600e6f5a79\",\n",
      "            \"date\": \"2024-10-28T02:03:23+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"None\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:03:23 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_i5cEkttbYdHXMpFke1rxc2Xx\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 191,\n",
      "        \"prompt_tokens\": 9162,\n",
      "        \"total_tokens\": 9353,\n",
      "        \"step_count\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = letta_client.send_message(\n",
    "    agent_id=my_letta_agent_id, \n",
    "    role=\"user\", \n",
    "    message=\"Can you tell me what kind of database topics I am the most interested in?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Upload Weaviate Tool to Letta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-foobar\"\n",
    "os.environ[\"WEAVIATE_URL\"] = \"https://weaviate-foobar.c0.us-east1.gcp.weaviate.cloud\"\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = \"weaviate-foobar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/8dp_379x15d8zz4ppsjthdw40000gn/T/ipykernel_77675/2823592656.py:6: ResourceWarning: unclosed <ssl.SSLSocket fd=81, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('10.0.0.119', 50331), raddr=('35.201.124.182', 443)>\n",
      "  weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "\n",
    "# Connect to Weaviate Cloud\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.environ[\"WEAVIATE_URL\"],\n",
    "    auth_credentials=Auth.api_key(os.environ[\"WEAVIATE_API_KEY\"]),\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(weaviate_client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_weaviate_collection(\n",
    "    self,\n",
    "    search_query: str,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    This tool queries an external database collection\n",
    "    named by the parameter {collection_name} to find the most semantically similar items to the query.\n",
    "\n",
    "    Args: \n",
    "        collection_name (str): The name of the database collection\n",
    "        search_query (str): The search query\n",
    "\n",
    "    Returns: \n",
    "        search_results (str): The results from the search engine.\n",
    "    \"\"\"\n",
    "    import weaviate\n",
    "    from weaviate.classes.init import Auth\n",
    "    import os\n",
    "    weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=os.environ[\"WEAVIATE_URL\"],\n",
    "        auth_credentials=Auth.api_key(os.environ[\"WEAVIATE_API_KEY\"]),\n",
    "        headers={\n",
    "            \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "        }\n",
    "    )\n",
    "    # ToDo, need to figure out how to dynamically create these collections\n",
    "    weaviate_collection = weaviate_client.collections.get(\"WeaviateBlogs\")\n",
    "    query_result = weaviate_collection.query.hybrid(\n",
    "        query=search_query,\n",
    "        alpha=0.5,\n",
    "        limit=5\n",
    "    )\n",
    "    weaviate_client.close()\n",
    "    results = query_result.objects\n",
    "    formatted_results = \"\\n\".join(\n",
    "        [f\"[Search Result {i+1}] {str(result.properties)}\" for i, result in enumerate(results)]\n",
    "    )\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results demo:\n",
      "[Search Result 1] {'content': 'Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.'}\n",
      "[Search Result 2] {'content': '---\\ntitle: HNSW+PQ - Exploring ANN algorithms Part 2.1\\nslug: ann-algorithms-hnsw-pq\\nauthors: [abdel]\\ndate: 2023-03-14\\ntags: [\\'research\\']\\nimage: ./img/hero.png\\ndescription: \"Implementing HNSW + Product Quantization (PQ) vector compression in Weaviate.\"\\n---\\n![HNSW+PQ - Exploring ANN algorithms Part 2.1](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWeaviate is already a very performant and robust [vector database](https://weaviate.io/blog/what-is-a-vector-database) and with the recent release of  v1.18 we are now bringing vector compression algorithms to Weaviate users everywhere. The main goal of this new feature is to offer similar performance at a fraction of the memory requirements and cost. In this blog we expand on the details behind this delicate balance between recall performance and memory management. In our previous blog [Vamana vs. HNSW - Exploring ANN algorithms Part 1](/blog/ann-algorithms-vamana-vs-hnsw), we explained the challenges and benefits of the Vamana and HNSW indexing algorithms.'}\n",
      "[Search Result 3] {'content': 'This feature allows HNSW to work directly with compressed vectors. This means using Product Quantization to compress vectors and calculate distances. As mentioned before, we could still store the complete representation of the vectors on disk and use them to correct the distances as we explore nodes during querying. For the time being, we are implementing only HNSW+PQ which means we do no correction to the distances. In the future we will explore adding such a correction and see the implications in recall and latency since we will have more accurate distances but also much more disk reads.'}\n",
      "[Search Result 4] {'content': 'Here are some key differences between Vamana and HNSW:\\n\\n### Vamana indexing - in short:\\n* Build a random graph. * Optimize the graph, so it only connects vectors close to each other. * Modify the graph by removing some short connections and adding some long-range edges to speed up the traversal of the graph. ### HNSW indexing - in short:\\n* Build a hierarchy of layers to speed up the traversal of the nearest neighbor graph. * In this graph, the top layers contain only long-range edges.'}\n",
      "[Search Result 5] {'content': \"Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. ðŸ¤”\\n\\n### Weaviate\\n[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). ðŸ™‚\\n\\nThe HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.\"}\n"
     ]
    }
   ],
   "source": [
    "results_demo = search_weaviate_collection(\n",
    "    None,  # Remove self since this is just a function, not a method\n",
    "    \"What is HNSW?\"\n",
    ");\n",
    "\n",
    "print(\"\\033[92mResults demo:\\033[0m\");\n",
    "print(results_demo);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_search_tool = letta_client.create_tool(\n",
    "    search_weaviate_collection,\n",
    "    tags=[\"weaviate\", \"search\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(description=None, source_type='python', module=None, user_id='user-00000000-0000-4000-8000-000000000000', id='tool-0571e78e-c70a-4874-82d1-bdc77f3f0745', name='search_weaviate_collection', tags=['weaviate', 'search'], source_code='def search_weaviate_collection(\\n    self,\\n    search_query: str,\\n):\\n    \"\"\"\\n    This tool queries an external database collection\\n    named by the parameter `collection_name` to find the most semantically similar items to the query.\\n\\n    Args: \\n        collection_name (str): The name of the database collection\\n        search_query (str): The search query\\n\\n    Returns: \\n        search_results (str): The results from the search engine.\\n    \"\"\"\\n    import weaviate\\n    from weaviate.classes.init import Auth\\n    import os\\n    weaviate_client = weaviate.connect_to_weaviate_cloud(\\n        cluster_url=os.environ[\"WEAVIATE_URL\"],\\n        auth_credentials=Auth.api_key(os.environ[\"WEAVIATE_API_KEY\"]),\\n        headers={\\n            \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"]\\n        }\\n    )\\n    # ToDo, need to figure out how to dynamically create these collections\\n    weaviate_collection = weaviate_client.collections.get(\"WeaviateBlogs\")\\n    query_result = weaviate_collection.query.hybrid(\\n        query=search_query,\\n        alpha=0.5,\\n        limit=5\\n    )\\n    weaviate_client.close()\\n    results = query_result.objects\\n    formatted_results = \"\\\\n\".join(\\n        [f\"[Search Result {i+1}] {str(result.properties)}\" for i, result in enumerate(results)]\\n    )\\n    return formatted_results\\n', json_schema={'name': 'search_weaviate_collection', 'description': 'This tool queries an external database collection', 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': 'The search query'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['search_query', 'request_heartbeat']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id=None, id='tool-1794219b-4d95-4ca1-ba4f-ee00a2ff6ec5', name='conversation_search_date', tags=['base', 'letta-base'], source_code='def conversation_search_date(self: Agent, start_date: str, end_date: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using a date range.\\n\\n    Args:\\n        start_date (str): The start of the date range to search, in the format \\'YYYY-MM-DD\\'.\\n        end_date (str): The end of the date range to search, in the format \\'YYYY-MM-DD\\'.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    results, total = self.persistence_manager.recall_memory.date_search(start_date, end_date, count=count, start=page * count)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(results) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [f\"timestamp: {d[\\'timestamp\\']}, {d[\\'message\\'][\\'role\\']} - {d[\\'message\\'][\\'content\\']}\" for d in results]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n', json_schema={'name': 'conversation_search_date', 'description': 'Search prior conversation history using a date range.', 'parameters': {'type': 'object', 'properties': {'start_date': {'type': 'string', 'description': \"The start of the date range to search, in the format 'YYYY-MM-DD'.\"}, 'end_date': {'type': 'string', 'description': \"The end of the date range to search, in the format 'YYYY-MM-DD'.\"}, 'page': {'type': 'integer', 'description': 'Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['start_date', 'end_date', 'request_heartbeat']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id='user-00000000-0000-4000-8000-000000000000', id='tool-28a89a20-38b7-428a-9c26-aea969cfc1b5', name='core_memory_append', tags=['memory', 'letta-base'], source_code='def core_memory_append(self: \"Agent\", name: str, content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Append to the contents of core memory.\\n\\n    Args:\\n        name (str): Section of the memory to be edited (persona or human).\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(self.memory.get_block(name).value)\\n    new_value = current_value + \"\\\\n\" + str(content)\\n    self.memory.update_block_value(name=name, value=new_value)\\n    return None\\n', json_schema={'name': 'core_memory_append', 'description': 'Append to the contents of core memory.', 'parameters': {'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'Section of the memory to be edited (persona or human).'}, 'content': {'type': 'string', 'description': 'Content to write to the memory. All unicode (including emojis) are supported.'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['name', 'content', 'request_heartbeat']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id=None, id='tool-68be4f21-a417-439d-ae45-af5a46c9cf2a', name='pause_heartbeats', tags=['base', 'letta-base'], source_code='def pause_heartbeats(self: Agent, minutes: int) -> Optional[str]:\\n    import datetime\\n\\n    from letta.constants import MAX_PAUSE_HEARTBEATS\\n\\n    minutes = min(MAX_PAUSE_HEARTBEATS, minutes)\\n\\n    # Record the current time\\n    self.pause_heartbeats_start = datetime.datetime.now(datetime.timezone.utc)\\n    # And record how long the pause should go for\\n    self.pause_heartbeats_minutes = int(minutes)\\n\\n    return f\"Pausing timed heartbeats for {minutes} min\"\\n', json_schema={'name': 'pause_heartbeats', 'description': 'Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.', 'parameters': {'type': 'object', 'properties': {'minutes': {'type': 'integer', 'description': 'Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).'}}, 'required': ['minutes']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id=None, id='tool-7f946292-4485-47d1-a251-37512943e279', name='send_message', tags=['base', 'letta-base'], source_code='def send_message(self: Agent, message: str) -> Optional[str]:\\n    \"\"\"\\n    Sends a message to the human user.\\n\\n    Args:\\n        message (str): Message contents. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\\n    return None\\n', json_schema={'name': 'send_message', 'description': 'Sends a message to the human user.', 'parameters': {'type': 'object', 'properties': {'message': {'type': 'string', 'description': 'Message contents. All unicode (including emojis) are supported.'}}, 'required': ['message']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id=None, id='tool-9cfc8f7d-f514-4672-b799-89c6472e0c3d', name='conversation_search', tags=['base', 'letta-base'], source_code='def conversation_search(self: Agent, query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search prior conversation history using case-insensitive string matching.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    results, total = self.persistence_manager.recall_memory.text_search(query, count=count, start=page * count)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(results) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [f\"timestamp: {d[\\'timestamp\\']}, {d[\\'message\\'][\\'role\\']} - {d[\\'message\\'][\\'content\\']}\" for d in results]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n', json_schema={'name': 'conversation_search', 'description': 'Search prior conversation history using case-insensitive string matching.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'String to search for.'}, 'page': {'type': 'integer', 'description': 'Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['query', 'request_heartbeat']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id='user-00000000-0000-4000-8000-000000000000', id='tool-9f404993-5c72-4124-b06a-b0acde1631fe', name='core_memory_replace', tags=['memory', 'letta-base'], source_code='def core_memory_replace(self: \"Agent\", name: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\\n    \"\"\"\\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\\n\\n    Args:\\n        name (str): Section of the memory to be edited (persona or human).\\n        old_content (str): String to replace. Must be an exact match.\\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    current_value = str(self.memory.get_block(name).value)\\n    if old_content not in current_value:\\n        raise ValueError(f\"Old content \\'{old_content}\\' not found in memory block \\'{name}\\'\")\\n    new_value = current_value.replace(str(old_content), str(new_content))\\n    self.memory.update_block_value(name=name, value=new_value)\\n    return None\\n', json_schema={'name': 'core_memory_replace', 'description': 'Replace the contents of core memory. To delete memories, use an empty string for new_content.', 'parameters': {'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'Section of the memory to be edited (persona or human).'}, 'old_content': {'type': 'string', 'description': 'String to replace. Must be an exact match.'}, 'new_content': {'type': 'string', 'description': 'Content to write to the memory. All unicode (including emojis) are supported.'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['name', 'old_content', 'new_content', 'request_heartbeat']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id=None, id='tool-b4c7217c-ad09-4c03-8dd6-c2328d523aed', name='archival_memory_search', tags=['base', 'letta-base'], source_code='def archival_memory_search(self: Agent, query: str, page: Optional[int] = 0) -> Optional[str]:\\n    \"\"\"\\n    Search archival memory using semantic (embedding-based) search.\\n\\n    Args:\\n        query (str): String to search for.\\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\\n\\n    Returns:\\n        str: Query result string\\n    \"\"\"\\n    import math\\n\\n    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    from letta.utils import json_dumps\\n\\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\\n        page = 0\\n    try:\\n        page = int(page)\\n    except:\\n        raise ValueError(f\"\\'page\\' argument must be an integer\")\\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\\n    results, total = self.persistence_manager.archival_memory.search(query, count=count, start=page * count)\\n    num_pages = math.ceil(total / count) - 1  # 0 index\\n    if len(results) == 0:\\n        results_str = f\"No results found.\"\\n    else:\\n        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\\n        results_formatted = [f\"timestamp: {d[\\'timestamp\\']}, memory: {d[\\'content\\']}\" for d in results]\\n        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\\n    return results_str\\n', json_schema={'name': 'archival_memory_search', 'description': 'Search archival memory using semantic (embedding-based) search.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'String to search for.'}, 'page': {'type': 'integer', 'description': 'Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['query', 'request_heartbeat']}}),\n",
       " Tool(description=None, source_type='python', module=None, user_id=None, id='tool-ca4d5d7e-6c75-4462-960f-280c1b2c41c8', name='archival_memory_insert', tags=['base', 'letta-base'], source_code='def archival_memory_insert(self: Agent, content: str) -> Optional[str]:\\n    \"\"\"\\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\\n\\n    Args:\\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\\n\\n    Returns:\\n        Optional[str]: None is always returned as this function does not produce a response.\\n    \"\"\"\\n    self.persistence_manager.archival_memory.insert(content)\\n    return None\\n', json_schema={'name': 'archival_memory_insert', 'description': 'Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.', 'parameters': {'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to write to the memory. All unicode (including emojis) are supported.'}, 'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['content', 'request_heartbeat']}})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letta_client.list_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the available tools on the Letta Server by going to \"Tool Builder\"\n",
    "\n",
    "<img src=\"letta-tools.png\" alt=\"Letta Tool Viewer\" width=\"60%\" height=\"auto\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even update the functions in your tools on the GUI!\n",
    "\n",
    "<img src=\"letta-update-tools.png\" alt=\"Edit Letta Tools\" width=\"40%\" height=\"auto\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Letta Agent with Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta import LLMConfig, EmbeddingConfig\n",
    "# set default llm config for agents\n",
    "letta_client.set_default_llm_config(\n",
    "    LLMConfig.default_config(model_name=\"letta\")\n",
    ")\n",
    "\n",
    "# set default embedding config for agents\n",
    "letta_client.set_default_embedding_config(\n",
    "    EmbeddingConfig.default_config(model_name=\"letta\")\n",
    ")\n",
    "\n",
    "agent_state = letta_client.create_agent(\n",
    "    name=\"newest-weaviate-blogs-agent\", \n",
    "    tools=[weaviate_search_tool.name], \n",
    ")\n",
    "\n",
    "new_agent_id = agent_state.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"id\": \"message-88154604-59aa-4fd0-9d1e-807f8f47cbd6\",\n",
      "            \"date\": \"2024-10-28T02:13:33+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"The user is looking for information about Weaviate's Vector Indexing features. Let's perform a search in the external database to fetch the most accurate response.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-88154604-59aa-4fd0-9d1e-807f8f47cbd6\",\n",
      "            \"date\": \"2024-10-28T02:13:33+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"search_weaviate_collection\",\n",
      "                \"arguments\": \"{\\n  \\\"search_query\\\": \\\"Weaviate Vector Indexing features\\\",\\n  \\\"request_heartbeat\\\": true\\n}\",\n",
      "                \"function_call_id\": \"call_d7Tu2SMdFWwZ0XzefTAsr6hd\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-0f6ba847-8183-4640-95ab-8a238ccbc32e\",\n",
      "            \"date\": \"2024-10-28T02:13:35+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"[Search Result 1] {'content': 'Speaking of Cloud, arguably the easiest way to spin up a new use case with Weaviate is through the [Weaviate Cloud Services](/pricing). <br></br>\\\\\\\\n\\\\\\\\n### New Vector Indexes\\\\\\\\n![vector indexes](./img/vector-indexes.png)\\\\\\\\n\\\\\\\\nLast year we gave you a sneak peek into our [Vector Indexing Research](/blog/ann-algorithms-vamana-vs-hnsw), and this year you will be able to try out new vector indexes for yourself. Since the beginning, Weaviate has supported vector indexing with [HNSW](/developers/weaviate/concepts/vector-index), which leads to [best-in-class query times](/developers/weaviate/benchmarks/ann). But not every use case requires single-digit millisecond latencies. Instead, some prefer cost-effectiveness.'}\\\\n[Search Result 2] {'content': 'Thus, an advantage of AI-native vector databases over vector-capable databases is their efficiency in vector search due to vector indexing. ### Vector Database vs. Vector Indexing Library\\\\\\\\n\\\\\\\\nSimilarly to vector databases, [vector libraries](https://weaviate.io/blog/vector-library-vs-vector-database) also enable fast vector search. However, vector libraries only store vector embeddings of data objects, and they store them in in-memory indexes. This results in two key differences:\\\\\\\\n\\\\\\\\n1.'}\\\\n[Search Result 3] {'content': '### Indexing Knobs\\\\\\\\nFor the sake of designing RAG systems, the most important indexing knob looks like vector compression settings. Launched in March 2023, Weaviate 1.18 introduced Product Quantization (PQ). PQ is a vector compression algorithm that groups contiguous segments of a vector, clusters their values across the collection, and then reduces the precision with centroids. For example, a contiguous segment of 4 32-bit floats requires 16 bytes to represent, a segment length of 4 with 8 centroids results in only needing 1 byte, a 16:1 memory reduction. Recent advances in PQ Rescoring help significantly with recall loss from this compression, but is still an important consideration with very high levels of compression.'}\\\\n[Search Result 4] {'content': \\\\\\\"Weaviate was built to combine the speed and capabilities of ANN algorithms with the features of a database such as backups, real-time queries, persistence, and replication (part of the v1.17 release). Weaviate can be accessed through GraphQL, REST, and client libraries in multiple programming languages. ### Example Use Cases\\\\\\\\n\\\\\\\\nVector databases are great to use for your application if your data is constantly changing. You can use vector search engines for e-commerce recommendations, image search, semantic similarity, and the list goes on. Weaviate just released a new module that introduces a way to represent a user's interests by drawing a graph of cross-references.\\\\\\\"}\\\\n[Search Result 5] {'content': 'Then when a query arrives, Weaviate traverses the index to obtain a good approximated answer to the query in a fraction of the time that a brute-force approach would take. [HNSW](/developers/weaviate/concepts/vector-index#hnsw) is the first production-ready indexing algorithm we implemented in Weaviate. It is a robust and fast algorithm that builds a hierarchical representation of the index **in memory** that could be quickly traversed to find the k nearest neighbors of a query vector. ## Need for disk solutions\\\\\\\\nThere are other challenges to overcome. Databases have grown so fast that even the above-described algorithms will not be enough.'}\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:13:35 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_d7Tu2SMdFWwZ0XzefTAsr6hd\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-403de61e-2759-427d-99b7-6422ecade263\",\n",
      "            \"date\": \"2024-10-28T02:13:47+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"Based on the search results, I summarized the necessary information about Weaviate's Vector Indexing features.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-403de61e-2759-427d-99b7-6422ecade263\",\n",
      "            \"date\": \"2024-10-28T02:13:47+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"send_message\",\n",
      "                \"arguments\": \"{\\n  \\\"message\\\": \\\"Weaviate's Vector Indexing features are designed to provide exceptional efficiency in vector search. Here are few key points I found from my resources:\\\\n\\\\n- Weaviate has always supported vector indexing with [HNSW](/developers/weaviate/concepts/vector-index), leading to [best-in-class query times](/developers/weaviate/benchmarks/ann). Recently, there has been research into new vector indexes.\\\\n\\\\n- They introduced Product Quantization (PQ) in March 2023, a vector compression algorithm that significantly reduces memory requirements.\\\\n\\\\n- HNSW, the first production-ready indexing algorithm implemented in Weaviate, builds a hierarchical representation of the index in memory that can be quickly traversed to find the k nearest neighbors of a query vector.\\\\n\\\\n- Vector databases like Weaviate are excellent when data is constantly changing and are suitable for use-cases like e-commerce recommendations, image search, and semantic similarity.\\\\n\\\\nFor more comprehensive information, I'd recommend checking out the official Weaviate resources.\\\"\\n}\",\n",
      "                \"function_call_id\": \"call_s2ffjPDDL6Qp9s0p8BuooTTY\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-5d45f9bd-5a3e-4e23-b5d8-1e53f72344df\",\n",
      "            \"date\": \"2024-10-28T02:13:47+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"None\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:13:47 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_s2ffjPDDL6Qp9s0p8BuooTTY\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 329,\n",
      "        \"prompt_tokens\": 5975,\n",
      "        \"total_tokens\": 6304,\n",
      "        \"step_count\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = letta_client.send_message(\n",
    "    agent_id=new_agent_id,\n",
    "    role=\"user\",\n",
    "    message=\"Can you please tell me about Weaviate's Vector Indexing features?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"id\": \"message-ba06f6f6-a885-4cb0-b51c-a48acc041f57\",\n",
      "            \"date\": \"2024-10-28T02:13:50+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"The user is curious about how Product Quantization works. Searching for a condensed response in our external database.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-ba06f6f6-a885-4cb0-b51c-a48acc041f57\",\n",
      "            \"date\": \"2024-10-28T02:13:50+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"search_weaviate_collection\",\n",
      "                \"arguments\": \"{\\n  \\\"search_query\\\": \\\"Product Quantization working\\\",\\n  \\\"request_heartbeat\\\": true\\n}\",\n",
      "                \"function_call_id\": \"call_1VvD849HDYYYKC9TOE9iiYzU\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-412f7c1a-38af-4135-8343-629970a639f3\",\n",
      "            \"date\": \"2024-10-28T02:13:51+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"[Search Result 1] {'content': \\\\\\\"Let's explore how PQ works. ## Product Quantization\\\\\\\\n![ann](./img/Ann.png)\\\\\\\\n\\\\\\\\nIf you already know the details behind how Product Quantization works feel free to skip this section!\\\\\\\\n\\\\\\\\nThe main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently.\\\\\\\"}\\\\n[Search Result 2] {'content': '---\\\\\\\\ntitle: How to Reduce Memory Requirements by up to 90%+ using Product Quantization\\\\\\\\nslug: pq-rescoring\\\\\\\\nauthors: [abdel, zain]\\\\\\\\ndate: 2023-09-19\\\\\\\\ntags: [\\\\\\\\'research\\\\\\\\',\\\\\\\\'engineering\\\\\\\\']\\\\\\\\nimage: ./img/hero.png\\\\\\\\ndescription: \\\\\\\"The details behind how you can compress vectors using PQ with little loss of recall!\\\\\\\"\\\\\\\\n---\\\\\\\\n![Product Quanitzation in Weaviate - How to Reduce Memory Requirements by up to 90%+](./img/hero.png)\\\\\\\\n\\\\\\\\n\\\\\\\\n:::info Highlights\\\\\\\\n- [Product Quantization](#product-quantizationpq) - how PQ reduces the memory requirements of running Weaviate\\\\\\\\n- [Improving PQ with Rescoring](#the-problem-with-pq) - why compressing vectors decreases recall and how the rescoring trick can help reduce this\\\\\\\\n- [Experimental results](#experiment-results) - QPS vs. recall, indexing times, and memory savings\\\\\\\\n- [Benchmarking PQ](#benchmarking-pq-with-your-own-data) - scripts for benchmarking PQ with your own data\\\\\\\\n:::\\\\\\\\n\\\\\\\\n-----\\\\\\\\n\\\\\\\\n## ðŸ“šWhat is Product Quantization(PQ)? Product Quantization is a way to compress vectors, allowing users to save on memory requirements. To understand how product quantization (PQ) compression works, imagine every vector you want to store is a unique house address. This address allows you to precisely locate where someone lives including country, state, city, street number, and even down to the house number.'}\\\\n[Search Result 3] {'content': 'If your data follows a different distribution, extending the code is very simple so do not hesitate to contact us. ## Results of the KMeans Vs Tile encoding\\\\\\\\n\\\\\\\\nIn this section we present some results on the comparison of Product Quantization using KMeans vs the Tile encoder. Table 1 shows a comparison of Product Quantization using KMeans and the Tile encoders. We compare the time to calculate distance, the time to fit and encode the data and the recall. We only compare it to KMeans with one dimension per segment since the tile encoder would only support this setting for now.'}\\\\n[Search Result 4] {'content': 'The points in the curve are obtained varying the amount of centroids. The â€œbaseâ€ curve refers to the fixed average time to calculate distances between uncompressed vectors.*\\\\\\\\n\\\\\\\\nAs we should expect, Product Quantization is very useful for saving memory. It comes at a cost though. The most expensive part is fitting the KMeans clustering algorithm. To remedy this we could use a different encoder based on the distribution of the data, however that is a topic we shall save for later!\\\\\\\\n\\\\\\\\nA final word on performance, not all applications require a high recall.'}\\\\n[Search Result 5] {'content': 'This means that if you would run it single threaded, fitting 200,000 vectors and encoding the whole data on Gist with KMeans would have taken nearly 5 hours. In contrast, with the Tile encoder it would be nearly three minutes. Additionally, notice that when Product Quantization is used with HNSW, we will need to compress at some point. This will mean fitting the existing data and encoding existing vectors. It will also need to compress the rest of the incoming vectors as they come.'}\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:13:51 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_1VvD849HDYYYKC9TOE9iiYzU\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-045009ec-0f16-4654-879f-ee9867147e2f\",\n",
      "            \"date\": \"2024-10-28T02:14:03+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"I've summarized the idea of Product Quantization as per the resources. The key is to make it understandable for the user. This explanation should give an overview but further exploration would help in mastering the concept.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-045009ec-0f16-4654-879f-ee9867147e2f\",\n",
      "            \"date\": \"2024-10-28T02:14:03+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"send_message\",\n",
      "                \"arguments\": \"{\\n  \\\"message\\\": \\\"Product Quantization (PQ) is an efficient method for compressing vectors, which significantly reduces memory requirements. Here's a simplified explanation of how it works:\\\\n\\\\n- At a high level, Product Quantization adds the concept of segments to the compression function. It operates by 'chopping up' a vector and handling each segment independently. \\\\n\\\\n- If you have a 128-dimensional vector, for example, you could divide it into 32 segments; each segment would contain 4 dimensions. Once this segmentation is done, each segment is compressed separately. \\\\n\\\\nAs with any form of compression, there is often a trade-off between efficiency and precision. However, methods such as rescoring can help reduce the impact on recall. \\\\n\\\\nFor instance, imagine that every vector you want to store is a unique house address. This address allows you to precisely locate where someone lives including country, state, city, and so forth. PQ effectively 'compresses' this detailâ€”imagine going from a full address to just a zip code. While some precision is lost, the memory savings can be substantial.\\\"\\n}\",\n",
      "                \"function_call_id\": \"call_HqqkyyqN2I5CmlRKODGxZESH\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-4d41e74a-b304-4879-b0ee-d0cb96086801\",\n",
      "            \"date\": \"2024-10-28T02:14:03+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"None\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:14:03 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_HqqkyyqN2I5CmlRKODGxZESH\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 347,\n",
      "        \"prompt_tokens\": 8681,\n",
      "        \"total_tokens\": 9028,\n",
      "        \"step_count\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = letta_client.send_message(\n",
    "    agent_id=new_agent_id,\n",
    "    role=\"user\",\n",
    "    message=\"Can you please tell me about how Product Quantization works?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"id\": \"message-1cde7e9f-e635-4ffd-bc52-6668b73f8445\",\n",
      "            \"date\": \"2024-10-28T02:14:07+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"The user is curious about how Product Quantization works. I will search the external Weaviate collection for a detailed yet concise explanation.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-1cde7e9f-e635-4ffd-bc52-6668b73f8445\",\n",
      "            \"date\": \"2024-10-28T02:14:07+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"search_weaviate_collection\",\n",
      "                \"arguments\": \"{\\n  \\\"search_query\\\": \\\"Product Quantization working\\\",\\n  \\\"request_heartbeat\\\": true\\n}\",\n",
      "                \"function_call_id\": \"call_lwbmxGjbkWdVPNWXjPVhtCb4\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-3472d4c3-0455-4ca4-a862-2b5e0328e013\",\n",
      "            \"date\": \"2024-10-28T02:14:08+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"[Search Result 1] {'content': \\\\\\\"Let's explore how PQ works. ## Product Quantization\\\\\\\\n![ann](./img/Ann.png)\\\\\\\\n\\\\\\\\nIf you already know the details behind how Product Quantization works feel free to skip this section!\\\\\\\\n\\\\\\\\nThe main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently.\\\\\\\"}\\\\n[Search Result 2] {'content': '---\\\\\\\\ntitle: How to Reduce Memory Requirements by up to 90%+ using Product Quantization\\\\\\\\nslug: pq-rescoring\\\\\\\\nauthors: [abdel, zain]\\\\\\\\ndate: 2023-09-19\\\\\\\\ntags: [\\\\\\\\'research\\\\\\\\',\\\\\\\\'engineering\\\\\\\\']\\\\\\\\nimage: ./img/hero.png\\\\\\\\ndescription: \\\\\\\"The details behind how you can compress vectors using PQ with little loss of recall!\\\\\\\"\\\\\\\\n---\\\\\\\\n![Product Quanitzation in Weaviate - How to Reduce Memory Requirements by up to 90%+](./img/hero.png)\\\\\\\\n\\\\\\\\n\\\\\\\\n:::info Highlights\\\\\\\\n- [Product Quantization](#product-quantizationpq) - how PQ reduces the memory requirements of running Weaviate\\\\\\\\n- [Improving PQ with Rescoring](#the-problem-with-pq) - why compressing vectors decreases recall and how the rescoring trick can help reduce this\\\\\\\\n- [Experimental results](#experiment-results) - QPS vs. recall, indexing times, and memory savings\\\\\\\\n- [Benchmarking PQ](#benchmarking-pq-with-your-own-data) - scripts for benchmarking PQ with your own data\\\\\\\\n:::\\\\\\\\n\\\\\\\\n-----\\\\\\\\n\\\\\\\\n## ðŸ“šWhat is Product Quantization(PQ)? Product Quantization is a way to compress vectors, allowing users to save on memory requirements. To understand how product quantization (PQ) compression works, imagine every vector you want to store is a unique house address. This address allows you to precisely locate where someone lives including country, state, city, street number, and even down to the house number.'}\\\\n[Search Result 3] {'content': 'If your data follows a different distribution, extending the code is very simple so do not hesitate to contact us. ## Results of the KMeans Vs Tile encoding\\\\\\\\n\\\\\\\\nIn this section we present some results on the comparison of Product Quantization using KMeans vs the Tile encoder. Table 1 shows a comparison of Product Quantization using KMeans and the Tile encoders. We compare the time to calculate distance, the time to fit and encode the data and the recall. We only compare it to KMeans with one dimension per segment since the tile encoder would only support this setting for now.'}\\\\n[Search Result 4] {'content': 'The points in the curve are obtained varying the amount of centroids. The â€œbaseâ€ curve refers to the fixed average time to calculate distances between uncompressed vectors.*\\\\\\\\n\\\\\\\\nAs we should expect, Product Quantization is very useful for saving memory. It comes at a cost though. The most expensive part is fitting the KMeans clustering algorithm. To remedy this we could use a different encoder based on the distribution of the data, however that is a topic we shall save for later!\\\\\\\\n\\\\\\\\nA final word on performance, not all applications require a high recall.'}\\\\n[Search Result 5] {'content': 'This means that if you would run it single threaded, fitting 200,000 vectors and encoding the whole data on Gist with KMeans would have taken nearly 5 hours. In contrast, with the Tile encoder it would be nearly three minutes. Additionally, notice that when Product Quantization is used with HNSW, we will need to compress at some point. This will mean fitting the existing data and encoding existing vectors. It will also need to compress the rest of the incoming vectors as they come.'}\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:14:08 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_lwbmxGjbkWdVPNWXjPVhtCb4\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-291593db-b6f9-462b-b9e3-edb85e943a1c\",\n",
      "            \"date\": \"2024-10-28T02:14:20+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"The user requested information about Product Quantization. By communicating the fundamentals of PQ from the search results, the user will get a better understanding of how it works.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-291593db-b6f9-462b-b9e3-edb85e943a1c\",\n",
      "            \"date\": \"2024-10-28T02:14:20+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"send_message\",\n",
      "                \"arguments\": \"{\\n  \\\"message\\\": \\\"Product Quantization, or PQ, is a technique used to compress vectors which results in significant savings in memory requirements. Here's an overview of how it works:\\\\n\\\\n- PQ introduces the idea of segments in the compression algorithm. To compress a vector, it gets divided into smaller parts, or segments, which are compressed separately.\\\\n\\\\n- To illustrate, if we're dealing with a 128-dimensional vector, this could be divided into 32 segments, with each segment containing 4 dimensions. Each of these segments is then compressed independently.\\\\n\\\\n- An analogy to understand how PQ works is to consider vectors as unique house addresses. These addresses allow for precision in locating a house, right down to the house number. PQ 'compresses' this, akin to representing the full address information with just a zip code, which reduces precision but leads to substantial memory savings.\\\\n\\\\nWhile PQ is highly efficient for reducing memory requirements, it's worth noting that it typically comes with a slight loss of recall, which can sometimes be counteracted with techniques like rescoring.\\\"\\n}\",\n",
      "                \"function_call_id\": \"call_wGvmKoFe9bQf9amIc1uR5HmZ\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-eb601d9e-1f89-4766-b33c-fb01ae18a508\",\n",
      "            \"date\": \"2024-10-28T02:14:20+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"None\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:14:20 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_wGvmKoFe9bQf9amIc1uR5HmZ\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 332,\n",
      "        \"prompt_tokens\": 11559,\n",
      "        \"total_tokens\": 11891,\n",
      "        \"step_count\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = letta_client.send_message(\n",
    "    agent_id=new_agent_id,\n",
    "    role=\"user\",\n",
    "    message=\"Can you please tell me about how Product Quantization works?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"id\": \"message-621443d9-55d4-4e84-9818-e88987240191\",\n",
      "            \"date\": \"2024-10-28T02:14:35+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"The user wants to recall what he's learned about Weaviate. I'll use the conversation_search function to find relevant parts of our past conversations.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-621443d9-55d4-4e84-9818-e88987240191\",\n",
      "            \"date\": \"2024-10-28T02:14:35+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"conversation_search\",\n",
      "                \"arguments\": \"{\\n  \\\"query\\\": \\\"Weaviate\\\",\\n  \\\"request_heartbeat\\\": true\\n}\",\n",
      "                \"function_call_id\": \"call_nwSAud5X36ty7pCF01Zzo6nV\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-9daaf309-a1a6-4fa0-833c-d5ddd18bf53a\",\n",
      "            \"date\": \"2024-10-28T02:14:35+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"Showing 4 of 4 results (page 0/0): [\\\\n  \\\\\\\"timestamp: 2024-10-28 02:13:29.825804, user - {\\\\\\\\n  \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user_message\\\\\\\\\\\\\\\",\\\\\\\\n  \\\\\\\\\\\\\\\"message\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Can you please tell me about Weaviate's Vector Indexing features?\\\\\\\\\\\\\\\",\\\\\\\\n  \\\\\\\\\\\\\\\"time\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2024-10-27 10:13:29 PM EDT-0400\\\\\\\\\\\\\\\"\\\\\\\\n}\\\\\\\",\\\\n  \\\\\\\"timestamp: 2024-10-28 02:13:33.551586, assistant - The user is looking for information about Weaviate's Vector Indexing features. Let's perform a search in the external database to fetch the most accurate response.\\\\\\\",\\\\n  \\\\\\\"timestamp: 2024-10-28 02:13:47.447362, assistant - Based on the search results, I summarized the necessary information about Weaviate's Vector Indexing features.\\\\\\\",\\\\n  \\\\\\\"timestamp: 2024-10-28 02:14:07.184639, assistant - The user is curious about how Product Quantization works. I will search the external Weaviate collection for a detailed yet concise explanation.\\\\\\\"\\\\n]\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:14:35 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_nwSAud5X36ty7pCF01Zzo6nV\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-b9190195-e775-4e53-829d-662d46382b7a\",\n",
      "            \"date\": \"2024-10-28T02:14:44+00:00\",\n",
      "            \"message_type\": \"internal_monologue\",\n",
      "            \"internal_monologue\": \"I have synthesized information about our previous conversations about Weaviate. This includes discussions about Weaviate's Vector Indexing and Product Quantization. Offering further exploration depending on the user's interest or openness to new topics.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-b9190195-e775-4e53-829d-662d46382b7a\",\n",
      "            \"date\": \"2024-10-28T02:14:44+00:00\",\n",
      "            \"message_type\": \"function_call\",\n",
      "            \"function_call\": {\n",
      "                \"name\": \"send_message\",\n",
      "                \"arguments\": \"{\\n  \\\"message\\\": \\\"From our previous conversations, we 've focused primarily on two aspects of Weaviate:\\\\n\\\\n1. **Weaviate's Vector Indexing features**: Weaviate uses Hyperplane-proximal Nearest Neighbors Search (HNSW) for indexing, allowing for best-in-class query times. It also includes intuitive research into new vector indexes.\\\\n\\\\n2. **Product Quantization (PQ) in Weaviate**: Introduced in March 2023, PQ is a vector compression method that greatly reduces memory usage. It operates by dividing a vector into smaller segments, which are then compressed independently.\\\\n\\\\nWould you like to delve deeper into these topics or explore a new one?\\\"\\n}\",\n",
      "                \"function_call_id\": \"call_wUizZ9Uu3sdf4LMSv97vlKvR\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"message-638afdab-99d3-4b9d-86e6-6c327594bede\",\n",
      "            \"date\": \"2024-10-28T02:14:44+00:00\",\n",
      "            \"message_type\": \"function_return\",\n",
      "            \"function_return\": \"{\\n  \\\"status\\\": \\\"OK\\\",\\n  \\\"message\\\": \\\"None\\\",\\n  \\\"time\\\": \\\"2024-10-27 10:14:44 PM EDT-0400\\\"\\n}\",\n",
      "            \"status\": \"success\",\n",
      "            \"function_call_id\": \"call_wUizZ9Uu3sdf4LMSv97vlKvR\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 270,\n",
      "        \"prompt_tokens\": 13802,\n",
      "        \"total_tokens\": 14072,\n",
      "        \"step_count\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = letta_client.send_message(\n",
    "    agent_id=new_agent_id,\n",
    "    role=\"user\",\n",
    "    message=\"Can you please tell me about what I've been learning about Weaviate?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for checking out our first Weaviate and Letta Notebook!\n",
    "\n",
    "### Keep up with us on X to be notified of future updates!\n",
    "\n",
    "- [Charles Packer](https://x.com/charlespacker) \n",
    "- [Sarah Wooders](https://x.com/sarahwooders)\n",
    "- [Erika Cardenas](https://x.com/ecardenas300)\n",
    "- [Connor Shorten](https://x.com/CShorten30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
