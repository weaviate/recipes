{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789da70a",
   "metadata": {},
   "source": [
    "# Adding Depth to RAG Programs\n",
    "\n",
    "In this tutorial we will be building on the previous work on `Getting Started with RAG`, now adding **Depth**, or additional LLM components, to our RAG program.\n",
    "\n",
    "## RAG Programs\n",
    "\n",
    "We will explore 4 variants of RAG:\n",
    "\n",
    "1. [RAG] \n",
    "\n",
    "Retrieve->`GenerateAnswer`\n",
    "\n",
    "2. [RAGwithSummarizer] \n",
    "\n",
    "Retrieve->`SummarizeContext`->`GenerateAnswer`\n",
    "\n",
    "3. [MultiHopRAG] \n",
    "\n",
    "Loop(`AskQuestion`->Retrieve)->`GenerateAnswer`\n",
    "\n",
    "4. [MultiHopRAGwithSummarizer] \n",
    "\n",
    "Loop(`AskQuestion`->Retrieve->`SummarizeContext`)->`GenerateAnswer`\n",
    "\n",
    "## LLM Metric\n",
    "\n",
    "We will use an LLM metric to evaluate these systems along a weighted combination of (1) how detailed the responses are, (2) how well supported they are by the context, and (3) how well they align with a given ground truth answer to the question.\n",
    "\n",
    "Our LLM metric is itself a 3-layer DSPy program:\n",
    "\n",
    "`SummarizeContext` -> `JudgeAnswer` -> `ParseFloat`\n",
    "\n",
    "The LLM Metric scores answer quality as a combination of:\n",
    "\n",
    "`Detail` + `Faithfulness`*2 + `Alignment`\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "We will explore the `BootstrapFewShot` optimizer to bootstrap 2 synthetic input-output examples for each component in the 4 RAG programs, and then we will let the `BayesianSignatureOptimizer` loose on our most complex program the MultiHopRAGwithSummarizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05754dee",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Development Set\n",
    "\n",
    "| Program                    | Optimizer | Devset Uncompiled | Devset Compiled |\n",
    "|----------------------------|-----------|-------------------|-----------------|\n",
    "| RAG                        | BFS       | 248               | 288             |\n",
    "| RAGwithSummarizer          | BFS       | 288               | 276             |\n",
    "| MultiHopRAG                | BFS       | 240               | 260             |\n",
    "| MultiHopRAGwithSummarizer  | BFS       | 204               | 288             |\n",
    "| MultiHopRAGwithSummarizer  | BSO       | 204               | 300             |\n",
    "\n",
    "### Test Set\n",
    "\n",
    "| Program                    | Optimizer | Testset Uncompiled | Testset Compiled |\n",
    "|----------------------------|-----------|--------------------|------------------|\n",
    "| RAG                        | BFS       | 315                | 350              |\n",
    "| RAGwithSummarizer          | BFS       | 325                | 292              |\n",
    "| MultiHopRAG                | BFS       | 299                | 302              |\n",
    "| MultiHopRAGwithSummarizer  | BFS       | 316                | 341              |\n",
    "| MultiHopRAGwithSummarizer  | BSO       | 316                | 301              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b010ed0",
   "metadata": {},
   "source": [
    "# A Deeper look into the discovered Prompts\n",
    "\n",
    "(From the `BayesianSignatureOptimizer` on `MultiHopRAGwithSummarizer`)\n",
    "\n",
    "| Component                | Token Count |\n",
    "|--------------------------|-------------|\n",
    "| QueryWriter Prompt       | 4,090       |\n",
    "| Summarize Prompt         | 3,209       |\n",
    "| OptimizedAnswer Prompt   | 4,324       |\n",
    "\n",
    "\n",
    "#### Each example has `4` input-output examples of the task, and a task description either kept as is or replaced by an LLM written version if optimal according to the metric.\n",
    "\n",
    "#### Note how you get `Reasoning` input-output traces in the few-shot examples!\n",
    "\n",
    "# The full optimized prompts can be found at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf843a7",
   "metadata": {},
   "source": [
    "# Installation and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45507732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate Retriever and configure LLM\n",
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "import openai\n",
    "\n",
    "gpt_turbo = dspy.OpenAI(model=\"gpt-3.5-turbo\", max_tokens=4000)\n",
    "mistral_ollama = dspy.OllamaLocal(model=\"mistral\", max_tokens=4000, timeout_s=480)\n",
    "\n",
    "weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "dspy.settings.configure(lm=gpt_turbo, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cffd7e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI technologies are used in a wide range of applications, such as speech recognition, image recognition, natural language processing, and autonomous vehicles. AI systems can be designed to perform tasks that typically require human intelligence, and they are becoming increasingly advanced and prevalent in our daily lives.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_turbo(\"What is Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f9f3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. These machines are designed to perform tasks that would normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI systems use various techniques and algorithms, including machine learning, deep learning, natural language processing, and reasoning, to analyze data, identify patterns, make predictions, and solve problems. The ultimate goal of AI is to create intelligent machines that can work and learn autonomously, adapt to new situations, and improve their performance over time.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_ollama(\"What is Artificial Intelligence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1654e7",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b4594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the dataset from a file named 'dataset.json' in the same filesystem, we will use the following approach:\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming 'dataset.json' is in the same directory as this script\n",
    "file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "\n",
    "# Read the dataset from 'dataset.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Initialize empty lists for gold_answers and queries\n",
    "gold_answers = []\n",
    "queries = []\n",
    "\n",
    "# Parse the gold_answers and queries\n",
    "for row in dataset:\n",
    "    gold_answers.append(row[\"gold_answer\"])\n",
    "    queries.append(row[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16253721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "trainset = data[:30]\n",
    "devset = data[30:35] # Small Devset\n",
    "testset = data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6b2a4",
   "metadata": {},
   "source": [
    "# How do you create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259ed925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed, you need to follow these steps:\n",
      "\n",
      "1. Import the FastAPI module: `from fastapi import FastAPI`\n",
      "2. Create an instance of the FastAPI class: `app = FastAPI()`\n",
      "3. Define a route that responds to HTTP GET requests at the root (\"/\") URL. This is done by using the `@app.get(\"/\")` decorator followed by a function that returns the desired message. The function could look like this:\n",
      "```python\n",
      "def read_root():\n",
      "    \"\"\"\n",
      "    Say hello to the world\n",
      "    \"\"\"\n",
      "    return {\"Hello\": \"World\"}\n",
      "```\n",
      "So, the complete code would look like this:\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    \"\"\"\n",
      "    Say hello to the world\n",
      "    \"\"\"\n",
      "    return {\"Hello\": \"World\"}\n",
      "```\n",
      "When this code is run and the application is accessed at its root URL, it will respond with `{\"Hello\": \"World\"}`.\n"
     ]
    }
   ],
   "source": [
    "print(devset[1].gold_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d7816",
   "metadata": {},
   "source": [
    "# What optimization has Weaviate introduced to manage memory usage during parallel data imports?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f217e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate has introduced thread pooling optimization to manage memory usage during parallel data imports. This optimization ensures that the parallelization does not exceed the number of CPU cores, thus providing maximum performance without unnecessary memory usage.\n"
     ]
    }
   ],
   "source": [
    "print(devset[2].gold_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed8f33",
   "metadata": {},
   "source": [
    "# LLM Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d141ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n"
     ]
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a system's answer to a question according to a given criterion.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"The context for answering the question.\")\n",
    "    criterion = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    question = dspy.InputField(desc=\"The question asked to the system.\")\n",
    "    ground_truth_answer = dspy.InputField(desc=\"An expert written Ground Truth Answer to the question.\")\n",
    "    predicted_answer = dspy.InputField(desc=\"The system's answer to the question.\")\n",
    "    rating = dspy.OutputField(desc=\"A rating between 1 and 5. IMPORTANT!! Only output the rating as an `int` and nothing else.\")\n",
    "\n",
    "class RatingParser(dspy.Signature):\n",
    "    \"\"\"Parse the rating from a string.\"\"\"\n",
    "    \n",
    "    raw_rating_response = dspy.InputField(desc=\"The string that contains the rating in it.\")\n",
    "    rating = dspy.OutputField(desc=\"An integer valued rating.\")\n",
    "    \n",
    "class Summarizer(dspy.Signature):\n",
    "    \"\"\"Summarize the information provided in the search results in 5 sentences.\"\"\"\n",
    "    \n",
    "    question = dspy.InputField(desc=\"a question to a search engine\")\n",
    "    context = dspy.InputField(desc=\"context filtered as relevant to the query by a search engine\")\n",
    "    summary = dspy.OutputField(desc=\"a 5 sentence summary of information in the context that would help answer the question.\")\n",
    "\n",
    "class RAGMetricProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.evaluator = dspy.ChainOfThought(Evaluator)\n",
    "        self.rating_parser = dspy.Predict(RatingParser)\n",
    "        self.summarizer = dspy.ChainOfThought(Summarizer)\n",
    "    \n",
    "    def forward(self, gold, pred, trace=None):\n",
    "        # Todo add trace to interface with teleprompters\n",
    "        predicted_answer = pred.answer\n",
    "        question = gold.question\n",
    "        ground_truth_answer = gold.gold_answer\n",
    "        \n",
    "        detail = \"Is the assessed answer detailed?\"\n",
    "        faithful = \"Is the assessed answer factually supported by the context?\"\n",
    "        ground_truth = f\"The Ground Answer Truth to the Question: {question} is given as: \\n \\n {ground_truth_answer} \\n \\n How aligned is this Predicted Answer? {predicted_answer}\"\n",
    "        \n",
    "        # Judgement\n",
    "        with dspy.context(lm=gpt_turbo):\n",
    "            context = dspy.Retrieve(k=10)(question).passages\n",
    "            # Context Summary\n",
    "            context = self.summarizer(question=question, context=context).summary\n",
    "            raw_detail_response = self.evaluator(context=context, \n",
    "                                 criterion=detail,\n",
    "                                 question=question,\n",
    "                                 ground_truth_answer=ground_truth_answer,\n",
    "                                 predicted_answer=predicted_answer).rating\n",
    "            raw_faithful_response = self.evaluator(context=context, \n",
    "                                 criterion=faithful,\n",
    "                                 question=question,\n",
    "                                 ground_truth_answer=ground_truth_answer,\n",
    "                                 predicted_answer=predicted_answer).rating\n",
    "            raw_ground_truth_response = self.evaluator(context=context, \n",
    "                                 criterion=ground_truth,\n",
    "                                 question=question,\n",
    "                                 ground_truth_answer=ground_truth_answer,\n",
    "                                 predicted_answer=predicted_answer).rating\n",
    "        \n",
    "        # Structured Output Parsing\n",
    "        with dspy.context(lm=gpt_turbo):\n",
    "            detail_rating = self.rating_parser(raw_rating_response=raw_detail_response).rating\n",
    "            faithful_rating = self.rating_parser(raw_rating_response=raw_faithful_response).rating\n",
    "            ground_truth_rating = self.rating_parser(raw_rating_response=raw_ground_truth_response).rating\n",
    "        \n",
    "        total = float(detail_rating) + float(faithful_rating)*2 + float(ground_truth_rating)\n",
    "    \n",
    "        return total / 5.0\n",
    "\n",
    "toy_ground_truth_answer = \"\"\"\n",
    "Cross encoders score the relevance of a document to a query. They are commonly used to rerank documents.\n",
    "\"\"\"\n",
    "\n",
    "lgtm_query = \"What do cross encoders do?\"\n",
    "lgtm_example = dspy.Example(question=lgtm_query, gold_answer=toy_ground_truth_answer)\n",
    "\n",
    "\n",
    "# If this is your first time exploring LLM metrics,\n",
    "# I recommend trying the exercise of improving this answer to achieve a higher LLM rating.\n",
    "\n",
    "lgtm_pred = dspy.Example(answer=\"They re-rank documents.\")\n",
    "\n",
    "llm_metric = RAGMetricProgram()\n",
    "llm_metric_rating = llm_metric(lgtm_example, lgtm_pred)\n",
    "print(llm_metric_rating)\n",
    "\n",
    "def MetricWrapper(gold, pred, trace=None):\n",
    "    return llm_metric(gold, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbf255",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521a1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we will re-use this initial signature when defining the other RAG programs,\n",
    "\n",
    "# Copy-paste signature initialization from 1. Getting Started with RAG in DSPy\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"Helpful information for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A detailed answer that is supported by the context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ca6ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        with dspy.context(lm=mistral_ollama):\n",
    "            pred = self.generate_answer(context=context, question=question).answer\n",
    "        return dspy.Prediction(context=context, answer=pred, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a5cff",
   "metadata": {},
   "source": [
    "# LGTM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98686e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGTM test query: What do cross encoders do? \n",
      " \n",
      " \n",
      "Uncompiled Answer: Cross Encoders are a type of ranking model that uses a classification mechanism to determine the similarity between data pairs (query and data object) instead of producing vector embeddings for data. They achieve high accuracy but require more time and computational resources compared to other methods like Metadata Rankers or Score Rankers. To use Cross Encoders, you need to provide a pair of items as input, which can be the query and a data object in the context of search. \n",
      " \n",
      "\n",
      "LLM Metric Rating: 4.0\n"
     ]
    }
   ],
   "source": [
    "uncompiled_Prediction = RAG()(lgtm_query)\n",
    "print(f\"LGTM test query: {lgtm_query} \\n \\n \")\n",
    "print(f\"Uncompiled Answer: {uncompiled_Prediction.answer} \\n \\n\")\n",
    "test_example = dspy.Example(question=lgtm_query, gold_answer=toy_ground_truth_answer)\n",
    "test_pred = uncompiled_Prediction\n",
    "llm_metric_rating = llm_metric(test_example, test_pred)\n",
    "print(f\"LLM Metric Rating: {llm_metric_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722234ee",
   "metadata": {},
   "source": [
    "# Adding Depth to RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70c261",
   "metadata": {},
   "source": [
    "# RAG with Search Result Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7673860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(dspy.Signature):\n",
    "    \"\"\"Please summarize all relevant information in the context.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"Documents determined to be relevant to the question.\")\n",
    "    question = dspy.InputField()\n",
    "    summarized_context = dspy.OutputField(desc=\"A detailed summary of information in the context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ef302bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGwithSummarizer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=10)\n",
    "        self.summarizer = dspy.ChainOfThought(Summarizer)\n",
    "        self.summarizer._compiled = True\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        with dspy.context(llm=gpt_turbo):\n",
    "            summarized_context = self.summarizer(context=context, question=question).summarized_context\n",
    "        with dspy.context(lm=mistral_ollama):\n",
    "            pred = self.generate_answer(context=summarized_context, question=question).answer\n",
    "        return dspy.Prediction(answer=pred, summarized_context=summarized_context, context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16bc7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Answer: Cross Encoders are models that perform content-based re-ranking by comparing the similarity between data pairs using a classification mechanism. They are known for their accuracy but slower speed compared to Bi-Encoders, which is why they are often used in specialized use cases. Pre-trained Cross-Encoder models are available for such purposes, and Large Language Models (LLMs) can also \n",
      " \n",
      "\n",
      "LLM Metric Rating: 3.2\n"
     ]
    }
   ],
   "source": [
    "uncompiled_Prediction = RAGwithSummarizer()(lgtm_query)\n",
    "print(f\"Uncompiled Answer: {uncompiled_Prediction.answer} \\n \\n\")\n",
    "test_example = dspy.Example(question=lgtm_query, gold_answer=toy_ground_truth_answer)\n",
    "test_pred = uncompiled_Prediction\n",
    "llm_metric_rating = llm_metric(test_example, test_pred)\n",
    "print(f\"LLM Metric Rating: {llm_metric_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b4cd9f",
   "metadata": {},
   "source": [
    "# Multi-Hop RAG\n",
    "\n",
    "This implementation is based on the Baleen architecture from Khattab et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e8c9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Contexts produced from previous search queries.\")\n",
    "    question = dspy.InputField(desc=\"The complex question we began with.\")\n",
    "    query = dspy.OutputField(desc=\"A search query that will help answer the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c741807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "class MultiHopRAG(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_question = dspy.ChainOfThought(GenerateSearchQuery)\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context_history = []\n",
    "        queries = []\n",
    "        \n",
    "        # Note to self, discuss how this differs from AutoGPT\n",
    "        context = self.retrieve(question).passages\n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_question(context=context, question=question).query\n",
    "            queries.append(query) # For inspection\n",
    "            passages = self.retrieve(query).passages\n",
    "            context_history.append(passages)\n",
    "            context = deduplicate(context + passages)\n",
    "    \n",
    "        with dspy.context(lm=mistral_ollama):\n",
    "            pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=pred.answer, queries=queries, context_history=context_history, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707180c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Answer: Cross-encoder models are a type of machine learning model used to measure the similarity between two data points, such as two sentences or two images. They differ from Bi-Encoder models in that they take a pair of items as input and output a single score representing their similarity (Figure 3), while Bi-Encoders produce separate embeddings for each item. Cross-encoder models have been shown to achieve higher accuracy than Bi-Encoders when trained on representative datasets. However, due to the need to use the model with every data item during a search in combination with the query, this method is less efficient for large-scale semantic search applications. To address the efficiency issue, we can combine the strengths of both models by \n",
      " \n",
      "\n",
      "LLM Metric Rating: 2.8\n"
     ]
    }
   ],
   "source": [
    "uncompiled_Prediction = MultiHopRAG()(lgtm_query)\n",
    "print(f\"Uncompiled Answer: {uncompiled_Prediction.answer} \\n \\n\")\n",
    "test_example = dspy.Example(question=lgtm_query, gold_answer=toy_ground_truth_answer)\n",
    "test_pred = uncompiled_Prediction\n",
    "llm_metric_rating = llm_metric(test_example, test_pred)\n",
    "print(f\"LLM Metric Rating: {llm_metric_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819d428",
   "metadata": {},
   "source": [
    "# Multi-Hop RAG with Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421e4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHopRAGwithSummarizer(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_question = dspy.ChainOfThought(GenerateSearchQuery)\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.summarizer = dspy.ChainOfThought(Summarizer)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        queries = []\n",
    "        summarized_context_log = []\n",
    "        \n",
    "        context = self.retrieve(question).passages\n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_question(context=context, question=question).query\n",
    "            queries.append(query)\n",
    "            passages = self.retrieve(query).passages\n",
    "            summarized_passages = self.summarizer(question=query, context=passages).summarized_context\n",
    "            summarized_context_log.append(summarized_passages)\n",
    "            context.append(summarized_passages)\n",
    "        with dspy.context(lm=mistral_ollama):\n",
    "            pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer, queries=queries, summarized_context_log=summarized_context_log, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e71bfa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Answer: Cross Encoders are models that utilize pre-trained models to rank the relevance of documents. They take a `(query, document)` pair as input and output a high precision relevance score. These models can also be used as guardrails for generative models to prevent harmful or NSFW content from making it through the search pipeline. Cross Encoders are more accurate than \n",
      " \n",
      "\n",
      "LLM Metric Rating: 3.6\n"
     ]
    }
   ],
   "source": [
    "uncompiled_Prediction = MultiHopRAGwithSummarizer()(lgtm_query)\n",
    "print(f\"Uncompiled Answer: {uncompiled_Prediction.answer} \\n \\n\")\n",
    "test_example = dspy.Example(question=lgtm_query, gold_answer=toy_ground_truth_answer)\n",
    "test_pred = uncompiled_Prediction\n",
    "llm_metric_rating = llm_metric(test_example, test_pred)\n",
    "print(f\"LLM Metric Rating: {llm_metric_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d0f5a",
   "metadata": {},
   "source": [
    "# Collect Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecd78fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Encoders are a type of ranking model used in semantic search applications for content-based re-ranking. They achieve higher accuracy than Bi-Encoders by representing data and queries as vectors in a high-dimensional space and comparing their similarity to rank results. However, they are less efficient due to their time-consuming nature. To address this issue, the combination of Bi-Encoders and Cross-Encoders is used. In this approach, Bi-Encoders are employed for efficient retrieval of candidate results based on a query, followed by Cross-Encoders for accurate reranking of these candidates. This method allows us to benefit from the strengths of both models and handle large scale\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/stanfordnlp/dspy/blob/main/testing/optimizer_tester.py\n",
    "\n",
    "class ProgramWrapper:\n",
    "    def __init__(self, name, program):\n",
    "        self.name = name\n",
    "        self.uncompiled = program()\n",
    "        self.uncomplied_score = 0.0\n",
    "        self.compiled = None\n",
    "        self.compiled_score = 0.0\n",
    "\n",
    "programs = {\n",
    "    \"RAG\": ProgramWrapper(\"RAG\", RAG),\n",
    "    \"RAGwithSummarizer\": ProgramWrapper(\"RAGwithSummarizer\", RAGwithSummarizer),\n",
    "    \"MultiHopRAG\": ProgramWrapper(\"MultiHopRAG\", MultiHopRAG),\n",
    "    \"MultiHopRAGwithSummarizer\": ProgramWrapper(\"MultiHopRAGwithSummarizer\", MultiHopRAGwithSummarizer),\n",
    "}\n",
    "\n",
    "print(programs[\"RAG\"].uncompiled(\"What are cross encoders?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb67122",
   "metadata": {},
   "source": [
    "# LGTM Test (Uncompiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f195c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: Cross Encoders are a type of ranking model that uses a classification mechanism to determine the similarity between data pairs\n",
      "\n",
      "LLM Rating: 2.2\n",
      "\n",
      "RAGwithSummarizer: Cross Encoders are models that perform content-based re-ranking by comparing the similarity between data pairs using a classification mechanism. They are known for their accuracy but slower speed compared to Bi-Encoders, which is why they are often used in specialized use cases. Pre-trained Cross-Encoder models are available for such purposes, and Large Language Models (LLMs) can also\n",
      "\n",
      "LLM Rating: 3.2\n",
      "\n",
      "MultiHopRAG: Cross-encoder models are a type of machine learning model used to measure the similarity between two data points, such as two sentences or two images. They differ from Bi-Encoder models in that they take a pair of items as input and output a single score representing their similarity (Figure 3), while Bi-Encoders produce separate embeddings for each item. Cross-encoder models have been shown to achieve higher accuracy than Bi-Encoders when trained on representative datasets. However, due to the need to use the model with every data item during a search in combination with the query, this method is less efficient for large-scale semantic search applications. To address the efficiency issue, we can combine the strengths of both models by\n",
      "\n",
      "LLM Rating: 2.8\n",
      "\n",
      "MultiHopRAGwithSummarizer: Cross Encoders are models that utilize pre-trained models to rank the relevance of documents. They take a `(query, document)` pair as input and output a high precision relevance score. These models can also be used as guardrails for generative models to prevent harmful or NSFW content from making it through the search pipeline. Cross Encoders are more accurate than\n",
      "\n",
      "LLM Rating: 3.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for program_key in programs.keys():\n",
    "    response = programs[program_key].uncompiled(lgtm_query)\n",
    "    print(f\"{program_key}: {response.answer}\\n\")\n",
    "    metric_pred = llm_metric(test_example, response)\n",
    "    print(f\"LLM Rating: {metric_pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6651c1f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0411b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'gold_answer': 'The strategy for chunking text for vectorization when dealing with a 512 token length limit involves using a Large Language Model to identify suitable places to cut up text chunks. This process, known as \"chunking\", breaks down long documents into smaller sections, each containing an important piece of information. This approach not only helps to stay within the LLMs token limit but also enhances the retrieval of information. It\\'s important to note that the chunking should be done thoughtfully, not just splitting a list of items into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.', 'question': 'What is the strategy for chunking text for vectorization when dealing with a 512 token length limit?'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc98fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating... RAG\n",
      "\n",
      "Average Metric: 12.399999999999999 / 5  (248.0%)\n",
      "Uncomplied score for RAG = 248.0.\n",
      "\n",
      "Evaluating... RAGwithSummarizer\n",
      "\n",
      "Average Metric: 14.399999999999999 / 5  (288.0%)\n",
      "Uncomplied score for RAGwithSummarizer = 288.0.\n",
      "\n",
      "Evaluating... MultiHopRAG\n",
      "\n",
      "Average Metric: 12.0 / 5  (240.0%)\n",
      "Uncomplied score for MultiHopRAG = 240.0.\n",
      "\n",
      "Evaluating... MultiHopRAGwithSummarizer\n",
      "\n",
      "Average Metric: 10.200000000000001 / 5  (204.0%)\n",
      "Uncomplied score for MultiHopRAGwithSummarizer = 204.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=4, display_progress=False)\n",
    "\n",
    "for program_key in programs.keys():\n",
    "    print(f\"Evaluating... {program_key}\\n\")\n",
    "    uncompiled_score = evaluate(programs[program_key].uncompiled, metric=MetricWrapper)\n",
    "    print(f\"Uncomplied score for {program_key} = {uncompiled_score}.\\n\")\n",
    "    programs[program_key].uncompiled_score = uncompiled_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e80a6",
   "metadata": {},
   "source": [
    "# Compile Programs (BootstrapFewShot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "234b5fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling... RAG\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                         | 2/30 [00:51<11:58, 25.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Compiling... RAGwithSummarizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                         | 2/30 [00:33<07:49, 16.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Compiling... MultiHopRAG\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                         | 2/30 [00:41<09:41, 20.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Compiling... MultiHopRAGwithSummarizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                         | 2/30 [01:03<14:45, 31.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "#teacherLM = dspy.OpenAI(model=\"gpt-4\", max_tokens=12_000, model_type='chat')\n",
    "teleprompter = BootstrapFewShot(metric=llm_metric, max_bootstrapped_demos=2, max_rounds=1)\n",
    "\n",
    "for program_key in programs.keys():\n",
    "    print(f\"Compiling... {program_key}\\n\")\n",
    "    compiled_program = teleprompter.compile(programs[program_key].uncompiled, trainset=trainset)\n",
    "    programs[program_key].compiled = compiled_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05ca27d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cross Encoders are machine learning models used for content-based re-ranking of search results. They utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents based on their semantic similarity with a given query. The advantage of using cross encoders is that they can reason about the relevance of results without requiring specialized training for each specific domain or dataset, leading to a more personalized and context-aware search experience.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programs[\"MultiHopRAGwithSummarizer\"].compiled(\"What are cross encoders\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd7a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt_turbo.inspect_history(n=1)\n",
    "#mistral_ollama.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f99fe6",
   "metadata": {},
   "source": [
    "# LGTM Test (Compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7df9e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: Cross encoders are a type of ranking model used for content-based re-ranking that achieve high accuracy within their domain but are more time-consuming compared to other methods like bi-encoders. Instead of producing vector embeddings for data, cross encoders use a classification mechanism for data pairs. The input of the model is always a pair of items, such as two sentences, and it outputs a value between 0 and 1 indicating their similarity. To use cross encoders for search, you need to calculate the similarity between each data item and the search query by passing them as pairs to the model.\n",
      "\n",
      "LLM Rating: 4.0\n",
      "\n",
      "RAGwithSummarizer: Cross encoders are models used for content-based re-ranking. They do not generate vector embeddings for data but rather compare data pairs using a classification mechanism. This approach allows for determining the similarity between data items and a query, resulting in higher accuracy compared to bi-encoders. However, cross encoders can be less efficient for\n",
      "\n",
      "LLM Rating: 2.0\n",
      "\n",
      "MultiHopRAG: Cross encoders are machine learning models that take a pair of items (typically a query and a document) as input and output a relevance score indicating their similarity or relationship. They use pre-trained models, such as those available on Sentence Transformers, for content-based re-ranking of search results. Cross encoders offer advantages like further reasoning about relevance without specialized training and easy generalization to recommendation.\n",
      "\n",
      "LLM Rating: 4.0\n",
      "\n",
      "MultiHopRAGwithSummarizer: Cross encoders are content-based re-ranking models that utilize pre-trained models to rank the relevance of data items or documents by determining the similarity between pairs. They take a pair of items, such as two sentences, as input and output a similarity score between 0 and 1. Unlike other ranking models like metadata rankers or score rankers, cross encoders do not produce vector embeddings for individual data items but instead use a classification mechanism. This makes them suitable for interfacing with search engines like Weaviate to re-rank search results and provide a more personalized and context-aware search experience. Cross encoders offer an advantage over other ranking models as they can reason about the relev\n",
      "\n",
      "LLM Rating: 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for program_key in programs.keys():\n",
    "    response = programs[program_key].compiled(lgtm_query)\n",
    "    print(f\"{program_key}: {response.answer}\\n\")\n",
    "    metric_pred = llm_metric(test_example, response)\n",
    "    print(f\"LLM Rating: {metric_pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c97790",
   "metadata": {},
   "source": [
    "# Evaluate Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13e48c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating... RAG\n",
      "\n",
      "Average Metric: 14.399999999999999 / 5  (288.0%)\n",
      "Complied score for RAG = 288.0.\n",
      "\n",
      "Evaluating... RAGwithSummarizer\n",
      "\n",
      "Average Metric: 13.8 / 5  (276.0%)\n",
      "Complied score for RAGwithSummarizer = 276.0.\n",
      "\n",
      "Evaluating... MultiHopRAG\n",
      "\n",
      "Average Metric: 13.0 / 5  (260.0%)\n",
      "Complied score for MultiHopRAG = 260.0.\n",
      "\n",
      "Evaluating... MultiHopRAGwithSummarizer\n",
      "\n",
      "Average Metric: 14.4 / 5  (288.0%)\n",
      "Complied score for MultiHopRAGwithSummarizer = 288.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=4, display_progress=False)\n",
    "\n",
    "for program_key in programs.keys():\n",
    "    print(f\"Evaluating... {program_key}\\n\")\n",
    "    compiled_score = evaluate(programs[program_key].compiled, metric=MetricWrapper)\n",
    "    print(f\"Complied score for {program_key} = {compiled_score}.\\n\")\n",
    "    programs[program_key].compiled_score = compiled_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d52df",
   "metadata": {},
   "source": [
    "# Optimizing 4-layer programs with the BayesianSignatureOptimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3711682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████         | 4/5 [00:45<00:11, 11.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████         | 4/5 [00:46<00:11, 11.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████         | 4/5 [00:44<00:11, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████         | 4/5 [00:44<00:11, 11.19s/it]\n",
      "[I 2024-03-01 17:32:40,174] A new study created in memory with name: no-name-1603d8be-db64-48b6-bf67-fc547136396f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:142: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n",
      "[I 2024-03-01 17:33:39,365] Trial 0 finished with value: 284.0 and parameters: {'10762695104_predictor_instruction': 1, '10762695104_predictor_demos': 4, '10762694384_predictor_instruction': 3, '10762694384_predictor_demos': 2, '10762694624_predictor_instruction': 0, '10762694624_predictor_demos': 0}. Best is trial 0 with value: 284.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.2 / 5  (284.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Parse the rating from a string.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Raw Rating Response: The string that contains the rating in it.\n",
      "Rating: An integer valued rating.\n",
      "\n",
      "---\n",
      "\n",
      "Raw Rating Response: 5\n",
      "Rating:\u001b[32m 5\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-01 17:34:42,225] Trial 1 finished with value: 300.0 and parameters: {'10762695104_predictor_instruction': 0, '10762695104_predictor_demos': 4, '10762694384_predictor_instruction': 3, '10762694384_predictor_demos': 3, '10762694624_predictor_instruction': 0, '10762694624_predictor_demos': 4}. Best is trial 1 with value: 300.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 5  (300.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Parse the rating from a string.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Raw Rating Response: The string that contains the rating in it.\n",
      "Rating: An integer valued rating.\n",
      "\n",
      "---\n",
      "\n",
      "Raw Rating Response: 5\n",
      "Rating:\u001b[32m 5\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-01 17:35:51,242] Trial 2 finished with value: 300.0 and parameters: {'10762695104_predictor_instruction': 4, '10762695104_predictor_demos': 1, '10762694384_predictor_instruction': 0, '10762694384_predictor_demos': 0, '10762694624_predictor_instruction': 1, '10762694624_predictor_demos': 2}. Best is trial 1 with value: 300.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 5  (300.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Parse the rating from a string.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Raw Rating Response: The string that contains the rating in it.\n",
      "Rating: An integer valued rating.\n",
      "\n",
      "---\n",
      "\n",
      "Raw Rating Response: 5\n",
      "Rating:\u001b[32m 5\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "\n",
    "llm_prompter = dspy.OpenAI(model='gpt-4', max_tokens=4000, model_type='chat')\n",
    "\n",
    "teleprompter = BayesianSignatureOptimizer(task_model = dspy.settings.lm,\n",
    "                                          metric=MetricWrapper,\n",
    "                                          prompt_model=llm_prompter,\n",
    "                                          n=5,\n",
    "                                          verbose=False)\n",
    "\n",
    "kwargs = dict(num_threads=4, display_progress=False)\n",
    "\n",
    "BSO_optimized_MultiHopRAGwithSummarizer = teleprompter.compile(MultiHopRAGwithSummarizer(),\n",
    "                                                              devset=devset,\n",
    "                                                              optuna_trials_num=3,\n",
    "                                                              max_bootstrapped_demos=4,\n",
    "                                                              max_labeled_demos=4,\n",
    "                                                              eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f27bf763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 5  (300.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(BSO_optimized_MultiHopRAGwithSummarizer, metric=MetricWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b50269f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    context=['[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.', 'Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.', '![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.', 'Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They are popular for taking a `(query, document)` pair as input and outputting a high precision relevance score, which can be generalized to recommendation scenarios as well.', 'Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They offer the advantage of further reasoning about relevance without specialized training and can be interfaced with Weaviate to re-rank search results, providing a more personalized and context-aware search experience.'],\n",
       "    answer='Cross encoders are machine learning models that can be used for content-based re-ranking of search results. They utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents based on a given query. These models take a `(query, document)` pair as input and output a high precision relevance score. Cross encoders offer the advantage of further reasoning about relevance without specialized training and can be interfaced with Weaviate to re-rank search results, providing a more personalized and context-aware search experience. They represent an improvement over Bi-Encoders in terms of accuracy but are less efficient due to their requirement for processing every document against',\n",
       "    queries=['\"What are cross encoders in the context of ranking models for content-based re-ranking?\"', '\"Explanation of cross encoders in content-based re-ranking\"'],\n",
       "    summarized_context_log=['Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They are popular for taking a `(query, document)` pair as input and outputting a high precision relevance score, which can be generalized to recommendation scenarios as well.', 'Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They offer the advantage of further reasoning about relevance without specialized training and can be interfaced with Weaviate to re-rank search results, providing a more personalized and context-aware search experience.'],\n",
       "    question='What are cross encoders?'\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSO_optimized_MultiHopRAGwithSummarizer(\"What are cross encoders?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86382b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: Helpful information for answering the question.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: A detailed answer that is supported by the context.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Similarly to the original Gorilla paper’s use of Abstract Syntax Tree evaluation, we are also considering an n-gram match where we construct keywords for each query such as “bm25”, “query”, “title” and check how many are contained in the generated query. We can also use the finer-grained perplexity metric that measures the log probability of the ground truth tokens at each step of decoding. We are currently using a simple greedy decoding algorithm to sample from the LoRA fine-tuned LlaMA 7B LLM. To ground the evaluation discussion further, let’s take a look at an incorrect query:\n",
      "\n",
      "```graphql\n",
      "{\n",
      "\tGet {\n",
      "\t\tJobListing(\n",
      "\t\t\tbm25: {query: “software”}\n",
      "\t\t\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
      "\t\t){\n",
      "\t\ttitle\n",
      "\t\tdescription\n",
      "\t\tisRemote\n",
      "\t\tpostedBy {\n",
      "\t\t\tname\n",
      "\t\t  }\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "```\n",
      "\n",
      "Almost there! But unfortunately the missing comma from the `bm25` to `where` query will prevent this query from successfully executing. As discussed we may have other cases where although the syntax is correct and the query executes, it does not achieve what was specified in the natural language command.»\n",
      "[2] «</video>\n",
      "<figcaption>Use the search bar to generate a GraphQL</figcaption>\n",
      "</figure>\n",
      "\n",
      "To me, it was great to see that we got the feature to work for a couple of examples. Although it’s imperfect and needs some refinement, I think, the feature has exciting potential to improve the general search experience. Here's a example conversion from natural language to GraphQL. <details>\n",
      "  <summary>Natural Language Query</summary>\n",
      "Products for sleep from the Now Foods brand\n",
      "</details>\n",
      "\n",
      "<details>\n",
      "  <summary>GraphQL</summary>\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get{\n",
      "    Product(\n",
      "      nearText: {concepts: [\"sleep\"]}\n",
      "      where: {\n",
      "        path: [\"brand\"],\n",
      "        operator: Equal,\n",
      "        valueString: \"Now Foods\"\n",
      "      }\n",
      "    ) {\n",
      "      name\n",
      "      brand\n",
      "      ingredients\n",
      "      reviews\n",
      "      image\n",
      "      rating\n",
      "      description\n",
      "      summary\n",
      "      effects\n",
      "      _additional {\n",
      "        id\n",
      "        distance\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "</details>\n",
      "\n",
      "Now, let's delve deeper into what happens behind the scenes. Upon receiving the natural language query, we use the [GPT4 Chat Completion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api) to generate a GraphQL query.»\n",
      "[3] «Shown in Step 1, we use a Synthetic Database Schema and an API Reference to prompt GPT-3.5-16K to write the query shown in the reference customized to the schema. In Step 2, we use the Synthetic Database Schema, API Reference, and Synthetic Query from Step 1 to prompt GPT-3.5-16K to write an nlcommand for when a user would want to execute the synthetic query. Then in Step 3, we template the Synthetic NLCommand, Synthetic Database Schema, and API Reference to fine-tune the LlaMA 7B model to output the Synthetic Query produced in Step 1. We then evaluate the Gorilla model with Query Execution, LLM Eval, and N-Gram matching stratgies. We also evaluate how well Weaviate can retrieve the API Reference using the Synthetic NLCommand as input.»\n",
      "[4] «The context includes examples of GraphQL queries generated from natural language queries using GPT-4 Chat Completion API. It also discusses the evaluation of n-gram matches in queries, mentioning a specific syntax error in a GraphQL query example related to missing commas.»\n",
      "[5] «The context includes discussions on evaluating GraphQL queries using n-gram matches and perplexity metrics, converting natural language queries to GraphQL with the GPT4 Chat Completion API, and generating valid GraphQL queries for Weaviate using an API reference and schema.»\n",
      "\n",
      "Question: What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?\n",
      "\n",
      "Reasoning: Let's think step by step in order to identify the syntax error in the GraphQL query example related to the evaluation of n-gram matches. The context mentions a specific syntax error in a GraphQL query, but it doesn't provide the actual query for us to examine directly. However, we can infer some information from the context that might help us understand the nature of the error. First, let's recall that n-gram matching is a method used to evaluate the similarity between strings by looking for contiguous sequences of words or phrases (n-grams) in both strings. In the context, it seems that the GraphQL query example contains a syntax error related to missing commas, which might affect the performance of n-gram matching.\n",
      "\n",
      "Answer: Based on the information provided in the context, the syntax error in the GraphQL query example related to the evaluation of n-gram matches is likely to be missing commas between fields or arguments that are expected to be separated by commas. For instance, if a list of fields or arguments is expected but not properly enclosed in parentheses and separated by commas, n-gram matching might not work correctly due to the incorrect query structure. Example: Instead of `Get{ Product( field1 field2 ) }`, it should be `Get{ Product( field1, field2 ) }`.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «#### Solution\n",
      "We addressed each of the points above individually and improved the overall MTTR substantially:\n",
      "\n",
      "- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.»\n",
      "[2] «#### Problem\n",
      "If Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\n",
      "\n",
      "- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\n",
      "- The recovery process was single-threaded.»\n",
      "[3] «#### Test\n",
      "We designed an extreme stress test that would represent the \"worst-case\" scenario for recovery. It has multiple very large, independent Write-Ahead Logs that required for recovery. Before, this could take many hours to recover, while now it takes only a few seconds. ### Full changelog\n",
      "These are few of the many improvements and bug fixes that were included in this release. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list.»\n",
      "[4] «In Weaviate version 1.14.0, several improvements were made to handle large write-ahead logs and improve the Mean Time to Recovery (MTTR). These improvements include a deduplication process to reduce redundant data in large WALs, running the recovery process in parallel to allow independent recovery, and adding a mechanism to flush idle memtables after 60 seconds to speed up recovery and prevent the need for recovery in many cases. Additionally, the recovery time for extreme stress tests was reduced from hours to just a few seconds.»\n",
      "[5] «In Weaviate version 1.14.0, improvements were made to drastically reduce the Mean Time to Recovery (MTTR) in the event of a crash. The recovery process, which previously could take multiple minutes or even hours, now only takes a few seconds. This improvement was achieved by addressing issues such as unflushed memtables becoming very large and implementing a multi-threaded recovery process. Additionally, the system ensures that no data is lost in the event of a crash by using a Write-Ahead Log (WAL) that is parsed at startup to recover all previously unfinished operations.»\n",
      "\n",
      "Question: How was the MTTR improved in the system with regards to handling large write-ahead logs?\n",
      "\n",
      "Reasoning: Let's think step by step in order to answer the question. In the context provided, we can see that the Mean Time to Recovery (MTTR) was improved in Weaviate version 1.14.0 by addressing issues related to large write-ahead logs. The improvements include: 1. A deduplication process was added to reduce redundant data in large WALs. 2. The recovery process now runs in parallel, allowing independent recovery and reducing the time required for multiple large WALs. 3. A mechanism was added to flush idle memtables after 60 seconds to speed up recovery and prevent the need for recovery in many cases.\n",
      "\n",
      "Answer: In Weaviate version 1.14.0\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «We can then vectorize this text description using off-the-shelf models from OpenAI, Cohere, HuggingFace, and others to unlock semantic search. We recently presented an example of this idea for [AirBnB listings](https://weaviate.io/blog/generative-feedback-loops-with-llms), translating tabular data about each property’s price, neighborhood, and more into a text description. Huge thanks to Svitlana Smolianova for creating the following animation of the concept. <img\n",
      "    src={require('./img/gen-example.gif').default}\n",
      "    alt=\"alt\"\n",
      "    style={{ width: \"100%\" }}\n",
      "/>\n",
      "\n",
      "### Text Chunking\n",
      "Similarly related to the 512 token length for vectorizing text chunks, we may consider using the Large Language Model to identify good places to cut up text chunks. For example, if we have a list of items, it might not be best practice to separate the list into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.»\n",
      "[2] «## Representing long objects\n",
      "\n",
      "One of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly exceed the 512 token input limit on Deep Transformer Neural Networks. This problem is a large part of what motivates our interest in Hybrid Search techniques that combine the flexibility of Vector Search with the sparse BM25 word counting algorithm well suited for >512 token text sequences. We think Ref2Vec can also help address this challenge. To be clear, Weaviate already offers a solution to represent long documents with Cross-References! As an example, [the Wikipedia Demo](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) breaks the long Wikipedia articles into a (Paragraph, inArticle, Article) data schema in which each Paragraph contains less than 512 tokens of text.»\n",
      "[3] «This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in “Orchestrating Tuning”. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents.»\n",
      "[4] «The strategies for chunking text to stay within the 512 token limit for vectorization include breaking long documents into smaller sections, ensuring each chunk contains important information, and utilizing techniques like Ref2Vec and Cross-References in Weaviate to represent long documents effectively.»\n",
      "[5] «The best practices for chunking text for vectorization within a 512 token limit involve using off-the-shelf models for vectorization, chunking data before insertion into Weaviate, and utilizing Hybrid Search techniques like Ref2Vec and Weaviate's Cross-References for representing long documents.»\n",
      "\n",
      "Question: What is the strategy for chunking text for vectorization when dealing with a 512 token length limit?\n",
      "\n",
      "Reasoning: Let's think step by step in order to answer the question. The context discusses the challenge of vectorizing long texts that exceed the 512 token input limit on Deep Transformer Neural Networks. To address this issue, the text suggests several strategies for chunking text before vectorization. These strategies include: 1. Breaking long documents into smaller sections: This involves dividing a large document into smaller parts or chunks that contain less than 512 tokens of text each. 2. Ensuring each chunk contains important information: It's essential to ensure that each chunk contains valuable and meaningful information, as this will enhance the retrieval process and help stay within the LLMs token limit. 3. Utilizing techniques like Ref2Vec and Cross\n",
      "\n",
      "Answer: The strategy for chunking text for vectorization when dealing with a 512 token length limit involves breaking long documents into smaller sections, ensuring each chunk contains important information, and utilizing techniques like Ref2Vec and Cross-References in Weaviate to represent long documents effectively. This approach allows you to maintain the essential context of the document while staying within the token limit for vectorization.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\n",
      "\n",
      "![Reduced memory usage](./img/reduced-memory-usage.jpg)\n",
      "\n",
      "As part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\n",
      "\n",
      "First, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs).»\n",
      "[2] «There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.»\n",
      "[3] «[Better control over Garbage Collector](#better-control-over-garbage-collector) - with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 1. [Faster imports for ordered data](#faster-imports-for-ordered-data) - by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n)\n",
      "1. [More efficient filtered aggregations](#more-efficient-filtered-aggregations) - thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 1.»\n",
      "[4] «Weaviate has implemented memory management optimizations to improve performance during parallel data imports. These optimizations include thread pooling to reduce memory spikes, automatic handling of parallelization to match CPU cores, and memory allocation optimizations for the HNSW index. Other improvements include better control over the garbage collector, faster imports for ordered data, and more efficient filtered aggregations.»\n",
      "[5] «Weaviate has introduced memory management optimizations for parallel data imports, including thread pooling to reduce memory spikes and automatic handling of parallelization to ensure optimal performance without excessive memory usage. Additionally, memory allocation optimizations have been implemented for the HNSW index to reduce memory usage while maintaining performance.»\n",
      "\n",
      "Question: What optimization has Weaviate introduced to manage memory usage during parallel data imports?\n",
      "\n",
      "Reasoning: Let's think step by step in order to answer the question. We are looking for information about memory management optimizations that Weaviate has introduced specifically for parallel data imports.\n",
      "\n",
      "Answer: Based on the context provided, Weaviate has introduced several memory management optimizations for parallel data imports. These optimizations include thread pooling to reduce memory spikes and automatic handling of parallelization to ensure optimal performance without excessive memory usage. Additionally, memory allocation optimizations have been implemented for the HNSW index to reduce memory usage while maintaining performance.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\n",
      "[4] «Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They are popular for taking a `(query, document)` pair as input and outputting a high precision relevance score, which can be generalized to recommendation scenarios as well.»\n",
      "[5] «Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They offer the advantage of further reasoning about relevance without specialized training and can be interfaced with Weaviate to re-rank search results, providing a more personalized and context-aware search experience.»\n",
      "\n",
      "Question: What are cross encoders?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Cross encoders are machine learning models that can be used for content-based re-ranking of search results. They utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents based on a given query. These models take a `(query, document)` pair as input and output a high precision relevance score. Cross encoders offer the advantage of further reasoning about relevance without specialized training and can be interfaced with Weaviate to re-rank search results, providing a more personalized and context-aware search experience. Cross encoders are an improvement over Bi-Encoders in terms of accuracy but are less efficient due to their requirement\n",
      "\n",
      "Answer:\u001b[32m Cross encoders are machine learning models that can be used for content-based re-ranking of search results. They utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents based on a given query. These models take a `(query, document)` pair as input and output a high precision relevance score. Cross encoders offer the advantage of further reasoning about relevance without specialized training and can be interfaced with Weaviate to re-rank search results, providing a more personalized and context-aware search experience. They represent an improvement over Bi-Encoders in terms of accuracy but are less efficient due to their requirement for processing every document against\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mistral_ollama.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ece49",
   "metadata": {},
   "source": [
    "# Alright, we are finished optimizing. Evaluate on Held-out Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "889bcbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating... RAG\n",
      "\n",
      "Average Metric: 47.199999999999996 / 15  (314.7%)\n",
      "Complied score for RAG = 314.67.\n",
      "\n",
      "Evaluating... RAGwithSummarizer\n",
      "\n",
      "Average Metric: 48.800000000000004 / 15  (325.3%)\n",
      "Complied score for RAGwithSummarizer = 325.33.\n",
      "\n",
      "Evaluating... MultiHopRAG\n",
      "\n",
      "Average Metric: 44.800000000000004 / 15  (298.7%)\n",
      "Complied score for MultiHopRAG = 298.67.\n",
      "\n",
      "Evaluating... MultiHopRAGwithSummarizer\n",
      "\n",
      "Average Metric: 47.400000000000006 / 15  (316.0%)\n",
      "Complied score for MultiHopRAGwithSummarizer = 316.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=testset, num_threads=4, display_progress=False)\n",
    "\n",
    "for program_key in programs.keys():\n",
    "    print(f\"Evaluating... {program_key}\\n\")\n",
    "    compiled_score = evaluate(programs[program_key].uncompiled, metric=MetricWrapper)\n",
    "    print(f\"Complied score for {program_key} = {compiled_score}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a779dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating... RAG\n",
      "\n",
      "Average Metric: 52.6 / 15  (350.7%)\n",
      "Complied score for RAG = 350.67.\n",
      "\n",
      "Evaluating... RAGwithSummarizer\n",
      "\n",
      "Average Metric: 43.8 / 15  (292.0%)\n",
      "Complied score for RAGwithSummarizer = 292.0.\n",
      "\n",
      "Evaluating... MultiHopRAG\n",
      "\n",
      "Average Metric: 45.400000000000006 / 15  (302.7%)\n",
      "Complied score for MultiHopRAG = 302.67.\n",
      "\n",
      "Evaluating... MultiHopRAGwithSummarizer\n",
      "\n",
      "Average Metric: 51.2 / 15  (341.3%)\n",
      "Complied score for MultiHopRAGwithSummarizer = 341.33.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=testset, num_threads=4, display_progress=False)\n",
    "\n",
    "for program_key in programs.keys():\n",
    "    print(f\"Evaluating... {program_key}\\n\")\n",
    "    compiled_score = evaluate(programs[program_key].compiled, metric=MetricWrapper)\n",
    "    print(f\"Complied score for {program_key} = {compiled_score}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b8b659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 45.2 / 15  (301.3%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "301.33"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(BSO_optimized_MultiHopRAGwithSummarizer, metric=MetricWrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57618e",
   "metadata": {},
   "source": [
    "# Final LGTM tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03c6bc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    context=['---\\ntitle: What is Ref2Vec and why you need it for your recommendation system\\nslug: ref2vec-centroid\\nauthors: [connor]\\ndate: 2022-11-23\\ntags: [\\'integrations\\', \\'concepts\\']\\nimage: ./img/hero.png\\ndescription: \"Weaviate introduces Ref2Vec, a new module that utilises Cross-References for Recommendation!\"\\n---\\n![Ref2vec-centroid](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWeaviate 1.16 introduced the [Ref2Vec](/developers/weaviate/modules/retriever-vectorizer-modules/ref2vec-centroid) module. In this article, we give you an overview of what Ref2Vec is and some examples in which it can add value such as recommendations or representing long objects. ## What is Ref2Vec? The name Ref2Vec is short for reference-to-vector, and it offers the ability to vectorize a data object with its cross-references to other objects. The Ref2Vec module currently holds the name ref2vec-**centroid** because it uses the average, or centroid vector, of the cross-referenced vectors to represent the **referencing** object.', \"Although all the query does is provide the ID of the User object, Ref2Vec has done the hard work by inferring a centroid vector from the User's references to other vectors. And as the set of references continues to evolve, the Ref2Vec vectors will continue to evolve also, ensuring that the User vector remains up-to-date with their latest interests. Whether your goal is to construct a Home Feed interface for users or to pair with search queries, Ref2Vec provides a strong foundation to enable Recommendation with fairly low overhead. For example, it can achieve personalized re-ranking, also known as a session-based recommendation, without persisting user data over a long sequence of interactions. A new user could have personalization available after a few interactions on the app which will help them quickly settle in and feel at home, helping to overcome what is known as the cold-start problem.\", \"![Cross-reference](./img/Weaviate-Ref2Vec_1.png)\\n\\nRef2Vec gives Weaviate another way to vectorize a class, such as the User class, based on their relationships to other classes. This allows Weaviate to quickly create up-to-date representations of users based on their relationships such as recent interactions. If a user clicks on 3 shoe images on an e-commerce store, it is a safe bet that they want to see more shoes. Ref2Vec captures this intuition by calculating vectors that aggregate each User's interaction with another class. The below animation visualizes a real example of this in e-Commerce images.\", 'Ref2Vec in Weaviate is a module that utilizes cross-references to create user profiles and provide personalized recommendations. It works by inferring centroid vectors from user references, allowing for dynamic updates and personalized recommendations without persisting user data over long sequences of interactions.', 'Ref2Vec in Weaviate is a module that utilizes cross-references to vectorize data objects, representing the referencing object with the average vector of its cross-referenced vectors. This allows Weaviate to provide personalized recommendations based on user preferences and actions, overcoming the cold-start problem and enabling session-based recommendations.'],\n",
       "    answer='Ref2Vec is a module in Weaviate that utilizes cross-references to vectorize data objects, representing the referencing object with the average vector of its cross-referenced vectors. It enables personalized recommendations based on user preferences and actions without persisting user data over long sequences of interactions, helping to overcome the cold-start problem and enable session-based recommendations.',\n",
       "    queries=['\"What is Ref2Vec in Weaviate and how does it work for recommendation systems?\"', '\"What is Ref2Vec in Weaviate and how does it work?\"'],\n",
       "    summarized_context_log=['Ref2Vec in Weaviate is a module that utilizes cross-references to create user profiles and provide personalized recommendations. It works by inferring centroid vectors from user references, allowing for dynamic updates and personalized recommendations without persisting user data over long sequences of interactions.', 'Ref2Vec in Weaviate is a module that utilizes cross-references to vectorize data objects, representing the referencing object with the average vector of its cross-referenced vectors. This allows Weaviate to provide personalized recommendations based on user preferences and actions, overcoming the cold-start problem and enabling session-based recommendations.'],\n",
       "    question='What is ref2vec?'\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSO_optimized_MultiHopRAGwithSummarizer(\"What is ref2vec?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53dee75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    context=['An `alpha` of 0 is pure bm25 and an alpha of 1 is pure vector search. Therefore, the set `alpha` is dependent on your data and application. Another emerging development is the effectiveness of zero-shot re-ranking models. Weaviate currently offers 2 [re-ranking models from Cohere](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/reranker-cohere): `rerank-english-v2.0` and `rerank-multilingual-v2.0`. As evidenced from the name, these models mostly differ because of the training data used and the resulting multilingual capabilities.', '### rankedFusion\\n\\nThe `rankedFusion` algorithm is the original hybrid fusion algorithm that has been available since the launch of hybrid search in Weaviate. In this algorithm, each object is scored according to its position in the results for the given search, starting from the highest score for the top-ranked object and decreasing down the order. The total score is calculated by adding these rank-based scores from the vector and keyword searches. Now, let’s take a look at the newer `relativeScoreFusion` algorithm. ### relativeScoreFusion\\n\\nThe `relativeScoreFusion` algorithm was added in Weaviate version `1.20`.', 'Re-ranking is a necessary step when implementing hybrid search. The alpha parameter dictates the weighting of each algorithm and determines the re-ranking of the results. ## Weaviate UX\\nTo use hybrid search in Weaviate, you only need to confirm that you’re using Weaviate `v1.17` or a later version. You can run the hybrid queries in GraphQL or the other various client programming languages. There are five parameters needed to run the hybrid search query (some are optional):\\n* `hybrid`: shows that you want to use hybrid search\\n* `query`: search query\\n* `alpha`(optional): weighting for each search algorithm (alpha = 0 (sparse), alpha = 1 (dense), alpha = 0.5 (equal weight for sparse and dense))\\n* `vector` (optional): optional to supply your own vector\\n* `score`(optional): additional information on how much the sparse and dense method contributed to the result\\n\\nWith just a few lines of code, you can start using hybrid search.', 'Weaviate introduced the `relativeScoreFusion` algorithm in version `1.20` for hybrid search, along with re-ranking models from Cohere for improved search capabilities.', 'Weaviate has integrated re-ranking models from Cohere, including `rerank-english-v2.0` and `rerank-multilingual-v2.0`, which provide developers with off-the-shelf models for enhancing search capabilities in AI-powered applications.'],\n",
       "    answer='Weaviate introduced re-ranking in version `1.20`.',\n",
       "    queries=['\"Weaviate version re-ranking added\"', '\"Weaviate version re-ranking models Cohere added\"'],\n",
       "    summarized_context_log=['Weaviate introduced the `relativeScoreFusion` algorithm in version `1.20` for hybrid search, along with re-ranking models from Cohere for improved search capabilities.', 'Weaviate has integrated re-ranking models from Cohere, including `rerank-english-v2.0` and `rerank-multilingual-v2.0`, which provide developers with off-the-shelf models for enhancing search capabilities in AI-powered applications.'],\n",
       "    question='What version of Weaviate added re-ranking?'\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSO_optimized_MultiHopRAGwithSummarizer(\"What version of Weaviate added re-ranking?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05013c12",
   "metadata": {},
   "source": [
    "# BSO Optimized `QueryWriter` Prompt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "decc3eb7",
   "metadata": {},
   "source": [
    "Write a search query that will help answer a complex question.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Context: Contexts produced from previous search queries.\n",
    "\n",
    "Question: The complex question we began with.\n",
    "\n",
    "Reasoning: Let's think step by step in order to ${produce the query}. We ...\n",
    "\n",
    "Query: A search query that will help answer the question.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «Similarly to the original Gorilla paper’s use of Abstract Syntax Tree evaluation, we are also considering an n-gram match where we construct keywords for each query such as “bm25”, “query”, “title” and check how many are contained in the generated query. We can also use the finer-grained perplexity metric that measures the log probability of the ground truth tokens at each step of decoding. We are currently using a simple greedy decoding algorithm to sample from the LoRA fine-tuned LlaMA 7B LLM. To ground the evaluation discussion further, let’s take a look at an incorrect query:\n",
    "\n",
    "```graphql\n",
    "{\n",
    "\tGet {\n",
    "\t\tJobListing(\n",
    "\t\t\tbm25: {query: “software”}\n",
    "\t\t\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
    "\t\t){\n",
    "\t\ttitle\n",
    "\t\tdescription\n",
    "\t\tisRemote\n",
    "\t\tpostedBy {\n",
    "\t\t\tname\n",
    "\t\t  }\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "Almost there! But unfortunately the missing comma from the `bm25` to `where` query will prevent this query from successfully executing. As discussed we may have other cases where although the syntax is correct and the query executes, it does not achieve what was specified in the natural language command.»\n",
    "[2] «</video>\n",
    "<figcaption>Use the search bar to generate a GraphQL</figcaption>\n",
    "</figure>\n",
    "\n",
    "To me, it was great to see that we got the feature to work for a couple of examples. Although it’s imperfect and needs some refinement, I think, the feature has exciting potential to improve the general search experience. Here's a example conversion from natural language to GraphQL. <details>\n",
    "  <summary>Natural Language Query</summary>\n",
    "Products for sleep from the Now Foods brand\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>GraphQL</summary>\n",
    "\n",
    "```graphql\n",
    "{\n",
    "  Get{\n",
    "    Product(\n",
    "      nearText: {concepts: [\"sleep\"]}\n",
    "      where: {\n",
    "        path: [\"brand\"],\n",
    "        operator: Equal,\n",
    "        valueString: \"Now Foods\"\n",
    "      }\n",
    "    ) {\n",
    "      name\n",
    "      brand\n",
    "      ingredients\n",
    "      reviews\n",
    "      image\n",
    "      rating\n",
    "      description\n",
    "      summary\n",
    "      effects\n",
    "      _additional {\n",
    "        id\n",
    "        distance\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "</details>\n",
    "\n",
    "Now, let's delve deeper into what happens behind the scenes. Upon receiving the natural language query, we use the [GPT4 Chat Completion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api) to generate a GraphQL query.»\n",
    "[3] «Shown in Step 1, we use a Synthetic Database Schema and an API Reference to prompt GPT-3.5-16K to write the query shown in the reference customized to the schema. In Step 2, we use the Synthetic Database Schema, API Reference, and Synthetic Query from Step 1 to prompt GPT-3.5-16K to write an nlcommand for when a user would want to execute the synthetic query. Then in Step 3, we template the Synthetic NLCommand, Synthetic Database Schema, and API Reference to fine-tune the LlaMA 7B model to output the Synthetic Query produced in Step 1. We then evaluate the Gorilla model with Query Execution, LLM Eval, and N-Gram matching stratgies. We also evaluate how well Weaviate can retrieve the API Reference using the Synthetic NLCommand as input.»\n",
    "[4] «The context includes examples of GraphQL queries generated from natural language queries using GPT-4 Chat Completion API. It also discusses the evaluation of n-gram matches in queries, mentioning a specific syntax error in a GraphQL query example related to missing commas.»\n",
    "[5] «The context includes discussions on evaluating GraphQL queries using n-gram matches and perplexity metrics, converting natural language queries to GraphQL with the GPT4 Chat Completion API, and generating valid GraphQL queries for Weaviate using an API reference and schema.»\n",
    "\n",
    "Question: What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the query. We need to identify the specific syntax error in the provided GraphQL query example related to the evaluation of n-gram matches. This error could be preventing the query from successfully executing or achieving the intended result specified in the natural language command.\n",
    "\n",
    "Query: What is the syntax error in the GraphQL query example provided in the evaluation of n-gram matches?\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «Similarly to the original Gorilla paper’s use of Abstract Syntax Tree evaluation, we are also considering an n-gram match where we construct keywords for each query such as “bm25”, “query”, “title” and check how many are contained in the generated query. We can also use the finer-grained perplexity metric that measures the log probability of the ground truth tokens at each step of decoding. We are currently using a simple greedy decoding algorithm to sample from the LoRA fine-tuned LlaMA 7B LLM. To ground the evaluation discussion further, let’s take a look at an incorrect query:\n",
    "\n",
    "```graphql\n",
    "{\n",
    "\tGet {\n",
    "\t\tJobListing(\n",
    "\t\t\tbm25: {query: “software”}\n",
    "\t\t\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
    "\t\t){\n",
    "\t\ttitle\n",
    "\t\tdescription\n",
    "\t\tisRemote\n",
    "\t\tpostedBy {\n",
    "\t\t\tname\n",
    "\t\t  }\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "Almost there! But unfortunately the missing comma from the `bm25` to `where` query will prevent this query from successfully executing. As discussed we may have other cases where although the syntax is correct and the query executes, it does not achieve what was specified in the natural language command.»\n",
    "[2] «</video>\n",
    "<figcaption>Use the search bar to generate a GraphQL</figcaption>\n",
    "</figure>\n",
    "\n",
    "To me, it was great to see that we got the feature to work for a couple of examples. Although it’s imperfect and needs some refinement, I think, the feature has exciting potential to improve the general search experience. Here's a example conversion from natural language to GraphQL. <details>\n",
    "  <summary>Natural Language Query</summary>\n",
    "Products for sleep from the Now Foods brand\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>GraphQL</summary>\n",
    "\n",
    "```graphql\n",
    "{\n",
    "  Get{\n",
    "    Product(\n",
    "      nearText: {concepts: [\"sleep\"]}\n",
    "      where: {\n",
    "        path: [\"brand\"],\n",
    "        operator: Equal,\n",
    "        valueString: \"Now Foods\"\n",
    "      }\n",
    "    ) {\n",
    "      name\n",
    "      brand\n",
    "      ingredients\n",
    "      reviews\n",
    "      image\n",
    "      rating\n",
    "      description\n",
    "      summary\n",
    "      effects\n",
    "      _additional {\n",
    "        id\n",
    "        distance\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "</details>\n",
    "\n",
    "Now, let's delve deeper into what happens behind the scenes. Upon receiving the natural language query, we use the [GPT4 Chat Completion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api) to generate a GraphQL query.»\n",
    "[3] «Shown in Step 1, we use a Synthetic Database Schema and an API Reference to prompt GPT-3.5-16K to write the query shown in the reference customized to the schema. In Step 2, we use the Synthetic Database Schema, API Reference, and Synthetic Query from Step 1 to prompt GPT-3.5-16K to write an nlcommand for when a user would want to execute the synthetic query. Then in Step 3, we template the Synthetic NLCommand, Synthetic Database Schema, and API Reference to fine-tune the LlaMA 7B model to output the Synthetic Query produced in Step 1. We then evaluate the Gorilla model with Query Execution, LLM Eval, and N-Gram matching stratgies. We also evaluate how well Weaviate can retrieve the API Reference using the Synthetic NLCommand as input.»\n",
    "[4] «The context includes examples of GraphQL queries generated from natural language queries using GPT-4 Chat Completion API. It also discusses the evaluation of n-gram matches in queries, mentioning a specific syntax error in a GraphQL query example related to missing commas.»\n",
    "[5] «The context includes discussions on evaluating GraphQL queries using n-gram matches and perplexity metrics, converting natural language queries to GraphQL with the GPT4 Chat Completion API, and generating valid GraphQL queries for Weaviate using an API reference and schema.»\n",
    "\n",
    "Question: What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the query. We need to identify the specific syntax error in the provided GraphQL query example related to the evaluation of n-gram matches. To do this, we should search for the specific error mentioned in the context, focusing on the missing comma issue in the GraphQL query example.\n",
    "\n",
    "Query: \"GraphQL missing comma syntax error example evaluation n-gram matches\"\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «#### Solution\n",
    "We addressed each of the points above individually and improved the overall MTTR substantially:\n",
    "\n",
    "- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.»\n",
    "[2] «#### Problem\n",
    "If Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\n",
    "\n",
    "- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\n",
    "- The recovery process was single-threaded.»\n",
    "[3] «#### Test\n",
    "We designed an extreme stress test that would represent the \"worst-case\" scenario for recovery. It has multiple very large, independent Write-Ahead Logs that required for recovery. Before, this could take many hours to recover, while now it takes only a few seconds. ### Full changelog\n",
    "These are few of the many improvements and bug fixes that were included in this release. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list.»\n",
    "[4] «In Weaviate version 1.14.0, several improvements were made to handle large write-ahead logs and improve the Mean Time to Recovery (MTTR). These improvements include a deduplication process to reduce redundant data in large WALs, running the recovery process in parallel to allow independent recovery, and adding a mechanism to flush idle memtables after 60 seconds to speed up recovery and prevent the need for recovery in many cases. Additionally, the recovery time for extreme stress tests was reduced from hours to just a few seconds.»\n",
    "[5] «In Weaviate version 1.14.0, improvements were made to drastically reduce the Mean Time to Recovery (MTTR) in the event of a crash. The recovery process, which previously could take multiple minutes or even hours, now only takes a few seconds. This improvement was achieved by addressing issues such as unflushed memtables becoming very large and implementing a multi-threaded recovery process. Additionally, the system ensures that no data is lost in the event of a crash by using a Write-Ahead Log (WAL) that is parsed at startup to recover all previously unfinished operations.»\n",
    "\n",
    "Question: How was the MTTR improved in the system with regards to handling large write-ahead logs?\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the query. We know that the MTTR (Mean Time to Recovery) was improved in the system by implementing a deduplication process, running the recovery process in parallel, and flushing idle memtables. To find more details on how these improvements specifically addressed the issue of handling large write-ahead logs, we can search for specific information related to these changes.\n",
    "\n",
    "Query: What specific improvements were made to the system to handle large write-ahead logs and improve the Mean Time to Recovery (MTTR) in Weaviate version 1.14.0?\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «#### Solution\n",
    "We addressed each of the points above individually and improved the overall MTTR substantially:\n",
    "\n",
    "- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.»\n",
    "[2] «#### Problem\n",
    "If Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\n",
    "\n",
    "- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\n",
    "- The recovery process was single-threaded.»\n",
    "[3] «#### Test\n",
    "We designed an extreme stress test that would represent the \"worst-case\" scenario for recovery. It has multiple very large, independent Write-Ahead Logs that required for recovery. Before, this could take many hours to recover, while now it takes only a few seconds. ### Full changelog\n",
    "These are few of the many improvements and bug fixes that were included in this release. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list.»\n",
    "[4] «In Weaviate version 1.14.0, several improvements were made to handle large write-ahead logs and improve the Mean Time to Recovery (MTTR). These improvements include a deduplication process to reduce redundant data in large WALs, running the recovery process in parallel to allow independent recovery, and adding a mechanism to flush idle memtables after 60 seconds to speed up recovery and prevent the need for recovery in many cases. Additionally, the recovery time for extreme stress tests was reduced from hours to just a few seconds.»\n",
    "[5] «In Weaviate version 1.14.0, improvements were made to drastically reduce the Mean Time to Recovery (MTTR) in the event of a crash. The recovery process, which previously could take multiple minutes or even hours, now only takes a few seconds. This improvement was achieved by addressing issues such as unflushed memtables becoming very large and implementing a multi-threaded recovery process. Additionally, the system ensures that no data is lost in the event of a crash by using a Write-Ahead Log (WAL) that is parsed at startup to recover all previously unfinished operations.»\n",
    "\n",
    "Question: How was the MTTR improved in the system with regards to handling large write-ahead logs?\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the query. We know that the improvements made to the system in version 1.14.0 included a deduplication process, running the recovery process in parallel, and flushing idle memtables after 60 seconds. These changes were aimed at reducing the Mean Time to Recovery (MTTR) for handling large write-ahead logs. To find more specific details on how these improvements impacted the MTTR, we can search for the specific changes made in Weaviate version 1.14.0 related to MTTR and write-ahead logs.\n",
    "\n",
    "Query: \"Mean Time to Recovery improvement in Weaviate version 1.14.0 for handling large write-ahead logs\"\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
    "1. [Metadata Rankers](#metadata-rankers)\n",
    "1. [Score Rankers](#score-rankers)\n",
    "\n",
    "## Cross Encoders\n",
    "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
    "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
    "\n",
    "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
    "\n",
    "*Figure 5 - Multistage search pipeline using Weaviate*\n",
    "\n",
    "## Pre-trained Cross-Encoder models\n",
    "\n",
    "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
    "[3] «![Cross-Encoder](./img/cross-encoder.png)\n",
    "\n",
    "*Figure 3 - Representation of a Cross-Encoder model*\n",
    "\n",
    "\n",
    "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
    "\n",
    "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\n",
    "\n",
    "Question: What are cross encoders?\n",
    "\n",
    "Reasoning: Let's think step by step in order to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e0265",
   "metadata": {},
   "source": [
    "# BSO Optimized `Summarize` Prompt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61b7d75d",
   "metadata": {},
   "source": [
    "Given the context and a question, distill the context into a succinct summary that directly addresses the question, highlighting key points or information related to the problem domain, which is preferably in computer programming or database management. Aim to produce a detailed, comprehensive, and technically accurate summary using domain-specific terminology.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Context: Documents determined to be relevant to the question.\n",
    "\n",
    "Question: ${question}\n",
    "\n",
    "Reasoning: Let's think step by step in order to ${produce the summarized_context}. We ...\n",
    "\n",
    "Context Summary: A detailed summary of information in the context.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «We can then vectorize this text description using off-the-shelf models from OpenAI, Cohere, HuggingFace, and others to unlock semantic search. We recently presented an example of this idea for [AirBnB listings](https://weaviate.io/blog/generative-feedback-loops-with-llms), translating tabular data about each property’s price, neighborhood, and more into a text description. Huge thanks to Svitlana Smolianova for creating the following animation of the concept. <img\n",
    "    src={require('./img/gen-example.gif').default}\n",
    "    alt=\"alt\"\n",
    "    style={{ width: \"100%\" }}\n",
    "/>\n",
    "\n",
    "### Text Chunking\n",
    "Similarly related to the 512 token length for vectorizing text chunks, we may consider using the Large Language Model to identify good places to cut up text chunks. For example, if we have a list of items, it might not be best practice to separate the list into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.»\n",
    "[2] «This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in “Orchestrating Tuning”. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents.»\n",
    "[3] «## Representing long objects\n",
    "\n",
    "One of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly exceed the 512 token input limit on Deep Transformer Neural Networks. This problem is a large part of what motivates our interest in Hybrid Search techniques that combine the flexibility of Vector Search with the sparse BM25 word counting algorithm well suited for >512 token text sequences. We think Ref2Vec can also help address this challenge. To be clear, Weaviate already offers a solution to represent long documents with Cross-References! As an example, [the Wikipedia Demo](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) breaks the long Wikipedia articles into a (Paragraph, inArticle, Article) data schema in which each Paragraph contains less than 512 tokens of text.»\n",
    "\n",
    "Question: What are the strategies for chunking text to stay within the 512 token limit for vectorization?\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the summarized_context. We have information from multiple sources discussing the importance of chunking text to stay within the 512 token limit for vectorization. The strategies mentioned include breaking long documents into smaller sections, ensuring each chunk contains important information, and using techniques like Ref2Vec and Cross-References in Weaviate to represent long documents effectively.\n",
    "\n",
    "Context Summary: The strategies for chunking text to stay within the 512 token limit for vectorization include breaking long documents into smaller sections, ensuring each chunk contains important information, and utilizing techniques like Ref2Vec and Cross-References in Weaviate to represent long documents effectively.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «We can then vectorize this text description using off-the-shelf models from OpenAI, Cohere, HuggingFace, and others to unlock semantic search. We recently presented an example of this idea for [AirBnB listings](https://weaviate.io/blog/generative-feedback-loops-with-llms), translating tabular data about each property’s price, neighborhood, and more into a text description. Huge thanks to Svitlana Smolianova for creating the following animation of the concept. <img\n",
    "    src={require('./img/gen-example.gif').default}\n",
    "    alt=\"alt\"\n",
    "    style={{ width: \"100%\" }}\n",
    "/>\n",
    "\n",
    "### Text Chunking\n",
    "Similarly related to the 512 token length for vectorizing text chunks, we may consider using the Large Language Model to identify good places to cut up text chunks. For example, if we have a list of items, it might not be best practice to separate the list into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.»\n",
    "[2] «This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in “Orchestrating Tuning”. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents.»\n",
    "[3] «## Representing long objects\n",
    "\n",
    "One of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly exceed the 512 token input limit on Deep Transformer Neural Networks. This problem is a large part of what motivates our interest in Hybrid Search techniques that combine the flexibility of Vector Search with the sparse BM25 word counting algorithm well suited for >512 token text sequences. We think Ref2Vec can also help address this challenge. To be clear, Weaviate already offers a solution to represent long documents with Cross-References! As an example, [the Wikipedia Demo](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) breaks the long Wikipedia articles into a (Paragraph, inArticle, Article) data schema in which each Paragraph contains less than 512 tokens of text.»\n",
    "\n",
    "Question: \"Best practices for chunking text for vectorization within a 512 token limit\"\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the summarized_context. We have three key pieces of information related to chunking text for vectorization within a 512 token limit. The first document discusses using off-the-shelf models to vectorize text descriptions and the importance of chunking text to stay within the token limit. The second document emphasizes the importance of chunking data before inserting it into Weaviate for better retrieval and mentions different strategies for parsing documents. The third document highlights the challenge of representing long documents that exceed the 512 token limit and suggests using Hybrid Search techniques like Ref2Vec and Weaviate's Cross-References as solutions.\n",
    "\n",
    "Context Summary: The best practices for chunking text for vectorization within a 512 token limit involve using off-the-shelf models for vectorization, chunking data before insertion into Weaviate, and utilizing Hybrid Search techniques like Ref2Vec and Weaviate's Cross-References for representing long documents.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\n",
    "\n",
    "![Reduced memory usage](./img/reduced-memory-usage.jpg)\n",
    "\n",
    "As part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\n",
    "\n",
    "First, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs).»\n",
    "[2] «There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
    "\n",
    "Next, we optimized memory allocations for the HNSW (vector) index.»\n",
    "[3] «[Better control over Garbage Collector](#better-control-over-garbage-collector) - with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 1. [Faster imports for ordered data](#faster-imports-for-ordered-data) - by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n)\n",
    "1. [More efficient filtered aggregations](#more-efficient-filtered-aggregations) - thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 1.»\n",
    "\n",
    "Question: \"Weaviate memory management optimization parallel data imports\"\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the summarized_context. We have information about memory management optimizations in Weaviate, specifically related to parallel data imports. The optimizations include thread pooling to reduce memory spikes during imports, automatic handling of parallelization to match the number of CPU cores, and memory allocation optimizations for the HNSW index. Additionally, there are improvements in garbage collector control, faster imports for ordered data, and more efficient filtered aggregations.\n",
    "\n",
    "Context Summary: Weaviate has implemented memory management optimizations to improve performance during parallel data imports. These optimizations include thread pooling to reduce memory spikes, automatic handling of parallelization to match CPU cores, and memory allocation optimizations for the HNSW index. Other improvements include better control over the garbage collector, faster imports for ordered data, and more efficient filtered aggregations.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
    "\n",
    "Next, we optimized memory allocations for the HNSW (vector) index.»\n",
    "[2] «Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\n",
    "\n",
    "![Reduced memory usage](./img/reduced-memory-usage.jpg)\n",
    "\n",
    "As part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\n",
    "\n",
    "First, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs).»\n",
    "[3] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\n",
    "\n",
    "Question: What memory management optimizations has Weaviate introduced for parallel data imports, such as thread pooling, automatic handling of parallelization, and memory allocation optimizations for the HNSW index?\n",
    "\n",
    "Reasoning: Let's think step by step in order to produce the summarized context. We will first look at the memory management optimizations introduced for parallel data imports, including thread pooling and automatic handling of parallelization. Then, we will explore the memory allocation optimizations specifically for the HNSW index.\n",
    "\n",
    "Context Summary: Weaviate has introduced memory management optimizations for parallel data imports, including thread pooling to reduce memory spikes and automatic handling of parallelization to ensure optimal performance without excessive memory usage. Additionally, memory allocation optimizations have been implemented for the HNSW index to reduce memory usage while maintaining performance.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
    "1. [Metadata Rankers](#metadata-rankers)\n",
    "1. [Score Rankers](#score-rankers)\n",
    "\n",
    "## Cross Encoders\n",
    "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
    "[2] «These kinds of models are increasingly being used as guardrails for generative models. For example, a harmful or NSFW content detector can prevent these generations from making it through the search pipeline. An interesting idea I recently heard from Eddie Zhou on Jerry Liu’s Llama Index Fireside Chat is the idea of using Natural Language Inference models to prevent hallucination by predicting the entailment or contradiction taking as the [retrieved context, generated output] as input. Because large language models are stochastic models, we can sample several candidate generations and filter them through score rankers like these. ## A Recap of the Ranking Models\n",
    "* **Cross Encoders** are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents.»\n",
    "[3] «As described in our [previous article](https://weaviate.io/blog/ranking-models-for-better-search), re-ranking models are new to the scene of zero-shot generalization. The story of re-rankers has mostly been tabular user features combined with tabular product or item features, fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt. Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score. This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.»\n",
    "\n",
    "Question: \"What are cross encoders in the context of ranking models for content-based re-ranking?\"\n",
    "\n",
    "Reasoning: Let's think step by step in order to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8933e06",
   "metadata": {},
   "source": [
    "# BSO Optimized `Answer` Prompt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cf0117f",
   "metadata": {},
   "source": [
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Context: Helpful information for answering the question.\n",
    "\n",
    "Question: ${question}\n",
    "\n",
    "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
    "\n",
    "Answer: A detailed answer that is supported by the context.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «Similarly to the original Gorilla paper’s use of Abstract Syntax Tree evaluation, we are also considering an n-gram match where we construct keywords for each query such as “bm25”, “query”, “title” and check how many are contained in the generated query. We can also use the finer-grained perplexity metric that measures the log probability of the ground truth tokens at each step of decoding. We are currently using a simple greedy decoding algorithm to sample from the LoRA fine-tuned LlaMA 7B LLM. To ground the evaluation discussion further, let’s take a look at an incorrect query:\n",
    "\n",
    "```graphql\n",
    "{\n",
    "\tGet {\n",
    "\t\tJobListing(\n",
    "\t\t\tbm25: {query: “software”}\n",
    "\t\t\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
    "\t\t){\n",
    "\t\ttitle\n",
    "\t\tdescription\n",
    "\t\tisRemote\n",
    "\t\tpostedBy {\n",
    "\t\t\tname\n",
    "\t\t  }\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "Almost there! But unfortunately the missing comma from the `bm25` to `where` query will prevent this query from successfully executing. As discussed we may have other cases where although the syntax is correct and the query executes, it does not achieve what was specified in the natural language command.»\n",
    "[2] «</video>\n",
    "<figcaption>Use the search bar to generate a GraphQL</figcaption>\n",
    "</figure>\n",
    "\n",
    "To me, it was great to see that we got the feature to work for a couple of examples. Although it’s imperfect and needs some refinement, I think, the feature has exciting potential to improve the general search experience. Here's a example conversion from natural language to GraphQL. <details>\n",
    "  <summary>Natural Language Query</summary>\n",
    "Products for sleep from the Now Foods brand\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>GraphQL</summary>\n",
    "\n",
    "```graphql\n",
    "{\n",
    "  Get{\n",
    "    Product(\n",
    "      nearText: {concepts: [\"sleep\"]}\n",
    "      where: {\n",
    "        path: [\"brand\"],\n",
    "        operator: Equal,\n",
    "        valueString: \"Now Foods\"\n",
    "      }\n",
    "    ) {\n",
    "      name\n",
    "      brand\n",
    "      ingredients\n",
    "      reviews\n",
    "      image\n",
    "      rating\n",
    "      description\n",
    "      summary\n",
    "      effects\n",
    "      _additional {\n",
    "        id\n",
    "        distance\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "</details>\n",
    "\n",
    "Now, let's delve deeper into what happens behind the scenes. Upon receiving the natural language query, we use the [GPT4 Chat Completion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api) to generate a GraphQL query.»\n",
    "[3] «Shown in Step 1, we use a Synthetic Database Schema and an API Reference to prompt GPT-3.5-16K to write the query shown in the reference customized to the schema. In Step 2, we use the Synthetic Database Schema, API Reference, and Synthetic Query from Step 1 to prompt GPT-3.5-16K to write an nlcommand for when a user would want to execute the synthetic query. Then in Step 3, we template the Synthetic NLCommand, Synthetic Database Schema, and API Reference to fine-tune the LlaMA 7B model to output the Synthetic Query produced in Step 1. We then evaluate the Gorilla model with Query Execution, LLM Eval, and N-Gram matching stratgies. We also evaluate how well Weaviate can retrieve the API Reference using the Synthetic NLCommand as input.»\n",
    "[4] «The context includes examples of GraphQL queries generated from natural language queries using GPT-4 Chat Completion API. It also discusses the evaluation of n-gram matches in queries, mentioning a specific syntax error in a GraphQL query example related to missing commas.»\n",
    "[5] «The context includes discussions on evaluating GraphQL queries using n-gram matches and perplexity metrics, converting natural language queries to GraphQL with the GPT4 Chat Completion API, and generating valid GraphQL queries for Weaviate using an API reference and schema.»\n",
    "\n",
    "Question: What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?\n",
    "\n",
    "Reasoning: Let's think step by step in order to identify the syntax error in the GraphQL query example related to the evaluation of n-gram matches. The context mentions a specific syntax error in a GraphQL query, but it doesn't provide the actual query for us to examine directly. However, we can infer some information from the context that might help us understand the nature of the error. First, let's recall that n-gram matching is a method used to evaluate the similarity between strings by looking for contiguous sequences of words or phrases (n-grams) in both strings. In the context, it seems that the GraphQL query example contains a syntax error related to missing commas, which might affect the performance of n-gram matching.\n",
    "\n",
    "Answer: Based on the information provided in the context, the syntax error in the GraphQL query example related to the evaluation of n-gram matches is likely to be missing commas between fields or arguments that are expected to be separated by commas. For instance, if a list of fields or arguments is expected but not properly enclosed in parentheses and separated by commas, n-gram matching might not work correctly due to the incorrect query structure. Example: Instead of `Get{ Product( field1 field2 ) }`, it should be `Get{ Product( field1, field2 ) }`.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «#### Solution\n",
    "We addressed each of the points above individually and improved the overall MTTR substantially:\n",
    "\n",
    "- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.»\n",
    "[2] «#### Problem\n",
    "If Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\n",
    "\n",
    "- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\n",
    "- The recovery process was single-threaded.»\n",
    "[3] «#### Test\n",
    "We designed an extreme stress test that would represent the \"worst-case\" scenario for recovery. It has multiple very large, independent Write-Ahead Logs that required for recovery. Before, this could take many hours to recover, while now it takes only a few seconds. ### Full changelog\n",
    "These are few of the many improvements and bug fixes that were included in this release. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list.»\n",
    "[4] «In Weaviate version 1.14.0, several improvements were made to handle large write-ahead logs and improve the Mean Time to Recovery (MTTR). These improvements include a deduplication process to reduce redundant data in large WALs, running the recovery process in parallel to allow independent recovery, and adding a mechanism to flush idle memtables after 60 seconds to speed up recovery and prevent the need for recovery in many cases. Additionally, the recovery time for extreme stress tests was reduced from hours to just a few seconds.»\n",
    "[5] «In Weaviate version 1.14.0, improvements were made to drastically reduce the Mean Time to Recovery (MTTR) in the event of a crash. The recovery process, which previously could take multiple minutes or even hours, now only takes a few seconds. This improvement was achieved by addressing issues such as unflushed memtables becoming very large and implementing a multi-threaded recovery process. Additionally, the system ensures that no data is lost in the event of a crash by using a Write-Ahead Log (WAL) that is parsed at startup to recover all previously unfinished operations.»\n",
    "\n",
    "Question: How was the MTTR improved in the system with regards to handling large write-ahead logs?\n",
    "\n",
    "Reasoning: Let's think step by step in order to answer the question. In the context provided, we can see that the Mean Time to Recovery (MTTR) was improved in Weaviate version 1.14.0 by addressing issues related to large write-ahead logs. The improvements include: 1. A deduplication process was added to reduce redundant data in large WALs. 2. The recovery process now runs in parallel, allowing independent recovery and reducing the time required for multiple large WALs. 3. A mechanism was added to flush idle memtables after 60 seconds to speed up recovery and prevent the need for recovery in many cases.\n",
    "\n",
    "Answer: In Weaviate version 1.14.0\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «We can then vectorize this text description using off-the-shelf models from OpenAI, Cohere, HuggingFace, and others to unlock semantic search. We recently presented an example of this idea for [AirBnB listings](https://weaviate.io/blog/generative-feedback-loops-with-llms), translating tabular data about each property’s price, neighborhood, and more into a text description. Huge thanks to Svitlana Smolianova for creating the following animation of the concept. <img\n",
    "    src={require('./img/gen-example.gif').default}\n",
    "    alt=\"alt\"\n",
    "    style={{ width: \"100%\" }}\n",
    "/>\n",
    "\n",
    "### Text Chunking\n",
    "Similarly related to the 512 token length for vectorizing text chunks, we may consider using the Large Language Model to identify good places to cut up text chunks. For example, if we have a list of items, it might not be best practice to separate the list into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.»\n",
    "[2] «## Representing long objects\n",
    "\n",
    "One of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly exceed the 512 token input limit on Deep Transformer Neural Networks. This problem is a large part of what motivates our interest in Hybrid Search techniques that combine the flexibility of Vector Search with the sparse BM25 word counting algorithm well suited for >512 token text sequences. We think Ref2Vec can also help address this challenge. To be clear, Weaviate already offers a solution to represent long documents with Cross-References! As an example, [the Wikipedia Demo](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) breaks the long Wikipedia articles into a (Paragraph, inArticle, Article) data schema in which each Paragraph contains less than 512 tokens of text.»\n",
    "[3] «This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in “Orchestrating Tuning”. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents.»\n",
    "[4] «The strategies for chunking text to stay within the 512 token limit for vectorization include breaking long documents into smaller sections, ensuring each chunk contains important information, and utilizing techniques like Ref2Vec and Cross-References in Weaviate to represent long documents effectively.»\n",
    "[5] «The best practices for chunking text for vectorization within a 512 token limit involve using off-the-shelf models for vectorization, chunking data before insertion into Weaviate, and utilizing Hybrid Search techniques like Ref2Vec and Weaviate's Cross-References for representing long documents.»\n",
    "\n",
    "Question: What is the strategy for chunking text for vectorization when dealing with a 512 token length limit?\n",
    "\n",
    "Reasoning: Let's think step by step in order to answer the question. The context discusses the challenge of vectorizing long texts that exceed the 512 token input limit on Deep Transformer Neural Networks. To address this issue, the text suggests several strategies for chunking text before vectorization. These strategies include: 1. Breaking long documents into smaller sections: This involves dividing a large document into smaller parts or chunks that contain less than 512 tokens of text each. 2. Ensuring each chunk contains important information: It's essential to ensure that each chunk contains valuable and meaningful information, as this will enhance the retrieval process and help stay within the LLMs token limit. 3. Utilizing techniques like Ref2Vec and Cross\n",
    "\n",
    "Answer: The strategy for chunking text for vectorization when dealing with a 512 token length limit involves breaking long documents into smaller sections, ensuring each chunk contains important information, and utilizing techniques like Ref2Vec and Cross-References in Weaviate to represent long documents effectively. This approach allows you to maintain the essential context of the document while staying within the token limit for vectorization.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\n",
    "\n",
    "![Reduced memory usage](./img/reduced-memory-usage.jpg)\n",
    "\n",
    "As part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\n",
    "\n",
    "First, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs).»\n",
    "[2] «There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
    "\n",
    "Next, we optimized memory allocations for the HNSW (vector) index.»\n",
    "[3] «[Better control over Garbage Collector](#better-control-over-garbage-collector) - with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 1. [Faster imports for ordered data](#faster-imports-for-ordered-data) - by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n)\n",
    "1. [More efficient filtered aggregations](#more-efficient-filtered-aggregations) - thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 1.»\n",
    "[4] «Weaviate has implemented memory management optimizations to improve performance during parallel data imports. These optimizations include thread pooling to reduce memory spikes, automatic handling of parallelization to match CPU cores, and memory allocation optimizations for the HNSW index. Other improvements include better control over the garbage collector, faster imports for ordered data, and more efficient filtered aggregations.»\n",
    "[5] «Weaviate has introduced memory management optimizations for parallel data imports, including thread pooling to reduce memory spikes and automatic handling of parallelization to ensure optimal performance without excessive memory usage. Additionally, memory allocation optimizations have been implemented for the HNSW index to reduce memory usage while maintaining performance.»\n",
    "\n",
    "Question: What optimization has Weaviate introduced to manage memory usage during parallel data imports?\n",
    "\n",
    "Reasoning: Let's think step by step in order to answer the question. We are looking for information about memory management optimizations that Weaviate has introduced specifically for parallel data imports.\n",
    "\n",
    "Answer: Based on the context provided, Weaviate has introduced several memory management optimizations for parallel data imports. These optimizations include thread pooling to reduce memory spikes and automatic handling of parallelization to ensure optimal performance without excessive memory usage. Additionally, memory allocation optimizations have been implemented for the HNSW index to reduce memory usage while maintaining performance.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
    "1. [Metadata Rankers](#metadata-rankers)\n",
    "1. [Score Rankers](#score-rankers)\n",
    "\n",
    "## Cross Encoders\n",
    "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
    "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
    "\n",
    "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
    "\n",
    "*Figure 5 - Multistage search pipeline using Weaviate*\n",
    "\n",
    "## Pre-trained Cross-Encoder models\n",
    "\n",
    "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
    "[3] «![Cross-Encoder](./img/cross-encoder.png)\n",
    "\n",
    "*Figure 3 - Representation of a Cross-Encoder model*\n",
    "\n",
    "\n",
    "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
    "\n",
    "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\n",
    "[4] «Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They are popular for taking a `(query, document)` pair as input and outputting a high precision relevance score, which can be generalized to recommendation scenarios as well.»\n",
    "[5] «Cross encoders are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents. They offer the advantage of further reasoning about relevance without specialized training and can be interfaced with Weaviate to re-rank search results, providing a more personalized and context-aware search experience.»\n",
    "\n",
    "Question: What are cross encoders?\n",
    "\n",
    "Reasoning: Let's think step by step in order to"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
