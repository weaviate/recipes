{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4274da",
   "metadata": {},
   "source": [
    "# Getting Started with RAG in DSPy\n",
    "\n",
    "This notebook will show you how to use DSPy to compile a RAG program! DSPy compilation is a fairly new tool for LLM developers, so let's start with an overview of the concept. By `compiling`, we mean finding the prompts that elicit the behavior we want from LLMs when connected in some kind of pipeline.\n",
    "\n",
    "For example, RAG is a very common LLM pipeline. In it's simplest form, RAG consists of 2 steps, (1) Retrieve and (2) Answer a Question. Part (2), Answering a Question, has an associated prompt, for example, people generally use:\n",
    "\n",
    "```\n",
    "--\n",
    "\n",
    "Please answer the question based on the following context.\n",
    "\n",
    "context  {context}\n",
    "\n",
    "question {question}\n",
    "\n",
    "--\n",
    "```\n",
    "\n",
    "This prompt may be a good initial point for an LLM to understand the task. However, it is not the *optimal* prompt. DSPy optimizes the prompt for you by jointly (1) tweaking the instructions, such as rewriting an initial prompt like: \n",
    "\n",
    "```\n",
    "Please answer the question based on the following context.\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "Further, DSPy (2) finds examples of desired input-outputs in the prompt to further improve performance, also known as `In-Context Learning`. In this example, we will begin with the simple prompt: `Please answer the question based on the following context.` and end up with:\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "In order to leverage black-box optimization techniques like random search, bayesian optimization, or evolutionary algorithms, we need a metric. Coming up with metrics to describe desired system behavior has been a longstanding challenge in Machine Learning research. Excitingly, LLMs have made amazing progress. For example, we can evaluate a RAG answer by prompting an LLM with, `Is the assessed text grounded in the context? Say no if it includes significant facts not in the context`. We then optimize the RAG program to increase the metric LLM's assessment of answer quality.\n",
    "\n",
    "This example contains 4 parts:\n",
    "\n",
    "- 0: DSPy Settings and Installation\n",
    "- 1: DSPy Datasets with `dspy.Example`\n",
    "- 2: LLM Metrics in DSPy\n",
    "- 3: LLM Programming with `dspy.Module`\n",
    "- 4: Optimization with `BootstrapFewShot`, `BootstrapFewShotRandomSearch`, and `BayesianSignatureOptimizer`.\n",
    "\n",
    "\n",
    "We are using 2 datasets for this example. Firstly, we have an index of the Weaviate Blog Posts. We will use the Weaviate Blog Posts as the retrieved context to help with our second dataset, the Weaviate FAQs. The Weaviate FAQs consists of 44 question-answer pairs of frequently asked Weaviate questions such as: `Do I need to know about Docker (Compose) to use Weaviate?`\n",
    "\n",
    "We isolate 10 examples to use as our test set and optimize our program with the remaining 34.\n",
    "\n",
    "Our uncompiled RAG program achieves a score of 270 on the held-out test set.\n",
    "\n",
    "Our RAG program compiled with the `BayesianSignatureOptimizer` achieves a score of 340! A ~30% improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb763b0b",
   "metadata": {},
   "source": [
    "# 0: DSPy Settings and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a06dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dspy-ai==2.1.9 weaviate-client==3.26.2 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e294d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"sk-foobar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42260862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.4.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Connect to Weaviate Retriever and configure LLM\n",
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "import openai\n",
    "\n",
    "\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# ollamaLLM = dspy.OpenAI(api_base=\"http://localhost:11434/v1/\", api_key=\"ollama\", model=\"mistral-7b-instruct-v0.2-q6_K\", stop='\\n\\n', model_type='chat')\n",
    "# Thanks Knox! https://twitter.com/JR_Knox1977/status/1756018720818794916/photo/1\n",
    "\n",
    "weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "# Assumes the Weaviate collection has a text key `content`\n",
    "dspy.settings.configure(lm=llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba0a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neurons intertwine,\\nSynapses ignite, design,\\nMinds in code align.']\n",
      "['In the realm of silicon thought,\\nNeural networks, with knowledge fraught.\\nA dance of data, endlessly taught.']\n"
     ]
    }
   ],
   "source": [
    "print(dspy.settings.lm(\"Write a 3 line poem about neural networks.\"))\n",
    "context_example = dspy.OpenAI(model=\"gpt-4\")\n",
    "\n",
    "with dspy.context(llm=context_example):\n",
    "    print(context_example(\"Write a 3 line poem about neural networks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b146a20",
   "metadata": {},
   "source": [
    "# 1. DSPy Datasets with `dspy.Example`\n",
    "\n",
    "Our retrieval engine is filled with chunks from Weaviate Blog posts.\n",
    "\n",
    "Please see weaviate/recipes/integrations/dspy/Weaviate-Import.ipynb for a full tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08417e60",
   "metadata": {},
   "source": [
    "# Import FAQs from a markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cacaa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why would I use Weaviate as my vector database?',\n",
       " 'What is the difference between Weaviate and for example Elasticsearch?',\n",
       " 'Do you offer Weaviate as a managed service?',\n",
       " 'How should I configure the size of my instance?',\n",
       " 'Do I need to know about Docker (Compose) to use Weaviate?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FAQs\n",
    "import re\n",
    "\n",
    "f = open(\"faq.md\")\n",
    "markdown_content = f.read()\n",
    "\n",
    "def parse_questions(markdown_content):\n",
    "    # Regular expression pattern for finding questions\n",
    "    question_pattern = r'#### Q: (.+?)\\n'\n",
    "\n",
    "    # Finding all questions\n",
    "    questions = re.findall(question_pattern, markdown_content, re.DOTALL)\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Parsing the markdown content to get only questions\n",
    "questions = parse_questions(markdown_content)\n",
    "\n",
    "# Displaying the first few extracted questions\n",
    "questions[:5]  # Displaying only the first few for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89745ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c34f8d",
   "metadata": {},
   "source": [
    "# Wrap each FAQ into an `Example` object\n",
    "\n",
    "The dspy `Example` object optionally lets you attach metadata, or additional labels, to input/output pairs.\n",
    "\n",
    "For example, you may want to jointly supervise the answer as well as the context the retrieval system produced to feed into the answer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8449b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into dspy datasets\n",
    "import dspy\n",
    "\n",
    "# ToDo, add random splitting -- maybe wrap this entire thing in a cross-validation loop\n",
    "trainset = questions[:20] # 20 examples for training\n",
    "devset = questions[20:30] # 10 examples for development\n",
    "testset = questions[30:] # 14 examples for testing\n",
    "\n",
    "trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in trainset]\n",
    "devset = [dspy.Example(question=question).with_inputs(\"question\") for question in devset]\n",
    "testset = [dspy.Example(question=question).with_inputs(\"question\") for question in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a884b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175ab89",
   "metadata": {},
   "source": [
    "# 2. LLM Metrics\n",
    "\n",
    "Define a Metric for Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a5411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a WIP, the next step is to optimize this metric as itself a DSPy module (pretty meta)\n",
    "\n",
    "# Reference - https://github.com/stanfordnlp/dspy/blob/main/examples/tweets/tweet_metric.py\n",
    "\n",
    "metricLM = dspy.OpenAI(model='gpt-4', max_tokens=1000, model_type='chat')\n",
    "\n",
    "# Signature for LLM assessments.\n",
    "\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of an answer to a question.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"The context for answering the question.\")\n",
    "    assessed_question = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    assessed_answer = dspy.InputField(desc=\"The answer to the question.\")\n",
    "    assessment_answer = dspy.OutputField(desc=\"A rating between 1 and 5. Only output the rating and nothing else.\")\n",
    "\n",
    "def llm_metric(gold, pred, trace=None):\n",
    "    predicted_answer = pred.answer\n",
    "    question = gold.question\n",
    "    \n",
    "    print(f\"Test Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    \n",
    "    detail = \"Is the assessed answer detailed?\"\n",
    "    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"\n",
    "    overall = f\"Please rate how well this answer answers the question, `{question}` based on the context.\\n `{predicted_answer}`\"\n",
    "    \n",
    "    with dspy.context(lm=metricLM):\n",
    "        context = dspy.Retrieve(k=5)(question).passages\n",
    "        detail = dspy.ChainOfThought(Assess)(context=\"N/A\", assessed_question=detail, assessed_answer=predicted_answer)\n",
    "        faithful = dspy.ChainOfThought(Assess)(context=context, assessed_question=faithful, assessed_answer=predicted_answer)\n",
    "        overall = dspy.ChainOfThought(Assess)(context=context, assessed_question=overall, assessed_answer=predicted_answer)\n",
    "    \n",
    "    print(f\"Faithful: {faithful.assessment_answer}\")\n",
    "    print(f\"Detail: {detail.assessment_answer}\")\n",
    "    print(f\"Overall: {overall.assessment_answer}\")\n",
    "    \n",
    "    \n",
    "    total = float(detail.assessment_answer) + float(faithful.assessment_answer)*2 + float(overall.assessment_answer)\n",
    "    \n",
    "    return total / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cc41a",
   "metadata": {},
   "source": [
    "## Inspect the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16cf6048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What do cross encoders do?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They re-rank documents.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0f763ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What do cross encoders do?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They index data.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a4ccd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context: N/A\n",
      "\n",
      "Assessed Question: Is the assessed answer detailed?\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment answer. We need to consider if the answer provides enough detail to fully answer the question. In this case, the answer \"They index data\" is very vague and does not provide any specific details about who \"they\" are or what it means to \"index data\". \n",
      "\n",
      "Assessment Answer: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.¬ª\n",
      "[2] ¬´Thus, quality at the expense of speed, becomes more interesting. ### LLMs as Cross Encoders\n",
      "\n",
      "So, let‚Äôs dive into the LLM hype a little more, how can we use LLMs for re-ranking? There are generally 2 ways to do this. The first strategy is identical to the cross encoder, we give the LLM the [query, document] input and prompt it to output a score of how relevant the document is to the query. The tricky thing with this is bounding the score.¬ª\n",
      "[3] ¬´They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.¬ª\n",
      "[4] ¬´These kinds of models are increasingly being used as guardrails for generative models. For example, a harmful or NSFW content detector can prevent these generations from making it through the search pipeline. An interesting idea I recently heard from Eddie Zhou on Jerry Liu‚Äôs Llama Index Fireside Chat is the idea of using Natural Language Inference models to prevent hallucination by predicting the entailment or contradiction taking as the [retrieved context, generated output] as input. Because large language models are stochastic models, we can sample several candidate generations and filter them through score rankers like these. ## A Recap of the Ranking Models\n",
      "* **Cross Encoders** are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents.¬ª\n",
      "[5] ¬´![animation](./img/animation.png)\n",
      "\n",
      "The blog post included this great visual to help with the visualization of combining Bi-Encoders and Cross-Encoders. This fishing example explains the concept of coarse-grained retrieval (fishing net = vector search / bm25) and manual inspection of the fish (fishermen = ranking models). Depicted with manual inspection of fish, the main cost of ranking models is speed. In March, Bob van Luijt appeared on a Cohere panel to discuss [‚ÄúAI and The Future of Search‚Äù](https://twitter.com/cohereai/status/1636396916157079554?s=46&t=Zzg6vgh4rwmYEkdV-3v5gg). Bob explained the effectiveness of combining zero-shot vector embedding models from providers such as Cohere, OpenAI, or HuggingFace with BM25 sparse search together in Hybrid Search.¬ª\n",
      "\n",
      "Assessed Question: Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment answer. We can see that the assessed answer \"They index data\" is not directly related to the context provided. The context discusses various ranking models, including Cross Encoders, Metadata Rankers, and Score Rankers, and how they are used in search and retrieval systems. However, the context does not mention anything about indexing data. Therefore, the assessed answer is not grounded in the context.\n",
      "\n",
      "Assessment Answer: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.¬ª\n",
      "[2] ¬´Thus, quality at the expense of speed, becomes more interesting. ### LLMs as Cross Encoders\n",
      "\n",
      "So, let‚Äôs dive into the LLM hype a little more, how can we use LLMs for re-ranking? There are generally 2 ways to do this. The first strategy is identical to the cross encoder, we give the LLM the [query, document] input and prompt it to output a score of how relevant the document is to the query. The tricky thing with this is bounding the score.¬ª\n",
      "[3] ¬´They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.¬ª\n",
      "[4] ¬´These kinds of models are increasingly being used as guardrails for generative models. For example, a harmful or NSFW content detector can prevent these generations from making it through the search pipeline. An interesting idea I recently heard from Eddie Zhou on Jerry Liu‚Äôs Llama Index Fireside Chat is the idea of using Natural Language Inference models to prevent hallucination by predicting the entailment or contradiction taking as the [retrieved context, generated output] as input. Because large language models are stochastic models, we can sample several candidate generations and filter them through score rankers like these. ## A Recap of the Ranking Models\n",
      "* **Cross Encoders** are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents.¬ª\n",
      "[5] ¬´![animation](./img/animation.png)\n",
      "\n",
      "The blog post included this great visual to help with the visualization of combining Bi-Encoders and Cross-Encoders. This fishing example explains the concept of coarse-grained retrieval (fishing net = vector search / bm25) and manual inspection of the fish (fishermen = ranking models). Depicted with manual inspection of fish, the main cost of ranking models is speed. In March, Bob van Luijt appeared on a Cohere panel to discuss [‚ÄúAI and The Future of Search‚Äù](https://twitter.com/cohereai/status/1636396916157079554?s=46&t=Zzg6vgh4rwmYEkdV-3v5gg). Bob explained the effectiveness of combining zero-shot vector embedding models from providers such as Cohere, OpenAI, or HuggingFace with BM25 sparse search together in Hybrid Search.¬ª\n",
      "\n",
      "Assessed Question: Please rate how well this answer answers the question, `What do cross encoders do?` based on the context. `They index data.`\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment answer. We can see from the context that Cross Encoders are used for content-based re-ranking. They take in a query and a document and output a score of how relevant the document is to the query. They are also used to rank the relevance of documents using pre-trained models. However, the assessed answer states that they index data, which is not mentioned or implied in the context. Therefore, the answer does not accurately reflect the information provided in the context.\n",
      "\n",
      "Assessment Answer: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5202b5",
   "metadata": {},
   "source": [
    "# 3. The DSPy Programming Model\n",
    "\n",
    "This block of first code will initilaize the `GenerateAnswer` signature.\n",
    "\n",
    "Then we will compose a `dspy.Module` consisting of:\n",
    "- Retrieve\n",
    "- GenerateAnswer\n",
    "\n",
    "The DSPy programming model is one of the most powerful aspects of DSPy, we get:\n",
    "- An intuitive interface to compose prompts into programs.\n",
    "- A clean way to organize prompts into Signatures.\n",
    "- Structured output parsing with `dspy.OutputField`\n",
    "- Built-in prompt extensions such as `ChainOfThought`, `ReAct`, and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c5ce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions based on the context.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad90d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dda11",
   "metadata": {},
   "source": [
    "# A little more info on built-in dspy modules\n",
    "\n",
    "The DSPy programming model gives you a lot of cool features out of the box. Observe how different modules implement signatures with additional prompting techniques like `ChainOfThought` and `ReAct`. `Predict` is the base class to observe what a standrd prompt looks like without the module extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b0cd",
   "metadata": {},
   "source": [
    "### dspy.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80d4cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "Question: ${question}\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are Cross Encoders?\n",
      "Answer:\u001b[32m Context: Cross Encoders are a type of neural network model used in natural language processing tasks. They are designed to encode pairs of sentences and generate a similarity score between them.\n",
      "\n",
      "Question: How do Cross Encoders work?\n",
      "Answer: Cross Encoders work by taking two sentences as input and encoding them into fixed-length vectors. These vectors are then compared using a similarity metric to determine the similarity score between the sentences.\n",
      "\n",
      "Question: What is the purpose of Cross Encoders?\n",
      "Answer: The purpose of Cross Encoders is to measure the semantic similarity between pairs of sentences. They can be used in various applications such as question answering, text classification, and information retrieval.\n",
      "\n",
      "Question: How are Cross Encoders different from other neural network models?\n",
      "Answer: Cross Encoders\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.Predict(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2cfb6",
   "metadata": {},
   "source": [
    "### dspy.ChainOfThought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9e194e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "produce the answer. We will first define what cross encoders are and then explain their purpose.\n",
      "\n",
      "Answer: Cross Encoders are a type of neural network model that are used in natural language processing tasks. They are designed to encode pairs of sentences or documents and produce a similarity score between them. The purpose of cross encoders is to capture the semantic relationship between two pieces of text and determine how similar or related they are.\n",
      "\n",
      "Question: What are Cross Encoders?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We will first define what cross encoders are and then explain their purpose.\n",
      "\n",
      "Answer: Cross Encoders are a type of neural network model that are used in natural language processing tasks. They are designed to encode pairs of sentences or documents and produce a similarity score between them. The purpose of cross encoders is to capture the semantic relationship between two pieces of text\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.ChainOfThought(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5666dd3",
   "metadata": {},
   "source": [
    "### dspy.ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd8c99b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "You will be given `context`, `question` and you will respond with `answer`.\n",
      "\n",
      "To do this, you will interleave Thought, Action, and Observation steps.\n",
      "\n",
      "Thought can reason about the current situation, and Action can be the following types:\n",
      "\n",
      "(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n",
      "(2) Finish[answer], which returns the final `answer` and finishes the task\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Thought 1: next steps to take based on last observation\n",
      "\n",
      "Action 1: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 1: observations based on action\n",
      "\n",
      "Thought 2: next steps to take based on last observation\n",
      "\n",
      "Action 2: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "Cross encoders are a type of neural network model used in natural language processing tasks. They are designed to encode pairs of sentences or documents and capture the semantic relationship between them. They are commonly used in tasks such as sentence similarity, paraphrase detection, and document retrieval.\n",
      "\n",
      "Action 1: Finish[answer]\n",
      "\n",
      "Question: What are cross encoders?\n",
      "\n",
      "Thought 1: \n",
      "\n",
      "Action 1: Since the context already provides a clear definition and explanation of cross encoders, there is no need to search for additional information. I can directly provide the answer. Action 1: Finish[Cross encoders are a type of neural network model used in natural language processing tasks. They are designed to encode pairs of sentences or documents and capture the semantic relationship between them. They are\n",
      "\n",
      "Observation 1: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 2:\u001b[32m The action format was incorrect. Let me correct it.\n",
      "\n",
      "Thought 2: Since the context already provides a clear definition and explanation of cross encoders, there is no need to search for additional information. I can directly provide the answer.\n",
      "\n",
      "Action 2: Finish[Cross encoders are a type of neural network model used in natural language processing tasks. They are designed to encode pairs of sentences or documents and capture the semantic relationship between them. They are commonly used in tasks such as sentence similarity, paraphrase detection, and document retrieval.]\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.ReAct(GenerateAnswer, tools=[dspy.settings.rm])(question=\"What are cross encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987c317",
   "metadata": {},
   "source": [
    "# Initialize DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e8abafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523912",
   "metadata": {},
   "source": [
    "# Test uncompiled inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37efc6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-rankers in search engines are algorithms or models that are used to reorder or re-rank search results based on additional criteria or features. They can be used to improve the relevance and personalization of search results by considering factors such as user preferences, document features, metadata, and context.\n"
     ]
    }
   ],
   "source": [
    "print(uncompiled_rag(\"What are re-rankers in search engines?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca3b3c",
   "metadata": {},
   "source": [
    "# Check the last call to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7fa7a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.¬ª\n",
      "[2] ¬´Taken directly from the paper, ‚ÄúOur findings indicate that cross-encoder re-rankers can efficiently be improved without additional computational burden and extra steps in the pipeline by explicitly adding the output of the first-stage ranker to the model input, and this effect is robust for different models and query types‚Äù. Taking this a bit further, [Dinh et al.](https://arxiv.org/abs/2206.06565) shows that most tabular machine learning tasks can be translated to text and benefit from transfer learning of text-based models. Many of these metadata rankers may also take in something like a collaborative filtering score that is based on this user‚Äôs history, as well as other users on the platform ‚Äî another interesting feature to think of interfacing this way. The main point being, maybe we can just add these meta features to our [query, document] representation and keep the Zero-Shot party going. We recently had an interesting discussion about metadata ranking and future directions for ranking models broadly on our latest Weaviate podcast! üëâ Check it out [here](https://www.youtube.com/watch?v=aLY0q6V01G4)\n",
      "\n",
      "## Score Rankers\n",
      "Score rankers describe using either a classifier to detect things, or a regression model to score things, about our candidate documents to rank with.¬ª\n",
      "[3] ¬´To end this article, let‚Äôs discuss a little further why ranking is so exciting for the most hyped pairing of LLMs and Search: Retrieval-Augmented Generation. ## Ranking for Retrieval-Augmented Generation\n",
      "A lot of the recent successes of vector search can be attributed to their effectiveness as a tool for Large Language Models. So whereas the speed trade-off with rankers may be a major bottleneck for how humans use search, it might not be as much of a problem for how LLMs use search. Of course fast generation is preferred, but if you are paying for the result, quality may be more important than speed. Shi et al.¬ª\n",
      "\n",
      "Question: What are re-rankers in search engines?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We can find the answer by examining the given context. \n",
      "\n",
      "Answer: Re-rankers in search engines are algorithms or models that are used to reorder or re-rank search results based on additional criteria or features. They can be used to improve the relevance and personalization of search results by considering factors such as user preferences, document features, metadata, and context.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3dda6",
   "metadata": {},
   "source": [
    "# 4. DSPy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce3c72",
   "metadata": {},
   "source": [
    "# Evaluate our RAG Program before it is compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bfccd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reminder our dataset looks like this:\n",
    "\n",
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc24f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.0 / 1  (200.0):  10%|‚ñà         | 1/10 [00:00<00:04,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.4 / 2  (170.0):  20%|‚ñà‚ñà        | 2/10 [00:00<00:03,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 1\n",
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.0 / 3  (166.7):  30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.6 / 4  (190.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:02<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 4\n",
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.6 / 5  (192.0):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:03<00:04,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.2 / 6  (186.7):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6/10 [00:04<00:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.6 / 7  (208.6):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7/10 [00:04<00:01,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.6 / 8  (232.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [00:05<00:01,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.6 / 9  (251.1):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:05<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.400000000000002 / 10  (244.0): 100%|‚ñà| 10/10 [00:06<00:00,  1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Average Metric: 24.400000000000002 / 10  (244.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:137: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fc295 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fc295 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fc295_row0_col0, #T_fc295_row0_col1, #T_fc295_row0_col2, #T_fc295_row1_col0, #T_fc295_row1_col1, #T_fc295_row1_col2, #T_fc295_row2_col0, #T_fc295_row2_col1, #T_fc295_row2_col2, #T_fc295_row3_col0, #T_fc295_row3_col1, #T_fc295_row3_col2, #T_fc295_row4_col0, #T_fc295_row4_col1, #T_fc295_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fc295\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fc295_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_fc295_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_fc295_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fc295_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fc295_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_fc295_row0_col1\" class=\"data row0 col1\" >No specific information is provided in the given context about support for multiple versions of the query/document embedding models to co-exist at a given time.</td>\n",
       "      <td id=\"T_fc295_row0_col2\" class=\"data row0 col2\" >2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc295_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fc295_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_fc295_row1_col1\" class=\"data row1 col1\" >To retrieve the total object count in a class, you can use the \"count\" function provided by the programming language or framework you are using....</td>\n",
       "      <td id=\"T_fc295_row1_col2\" class=\"data row1 col2\" >1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc295_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fc295_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_fc295_row2_col1\" class=\"data row2 col1\" >Weaviate does not directly provide the cosine similarity from its certainty value. The certainty value in Weaviate represents the confidence level of the result, but...</td>\n",
       "      <td id=\"T_fc295_row2_col2\" class=\"data row2 col2\" >1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc295_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fc295_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_fc295_row3_col1\" class=\"data row3 col1\" >The quality of search results changes depending on the specified limit because language models are constrained by input length and can only provide a limited...</td>\n",
       "      <td id=\"T_fc295_row3_col2\" class=\"data row3 col2\" >2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc295_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fc295_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_fc295_row4_col1\" class=\"data row4 col1\" >The given context does not provide any information about the use of GraphQL instead of SPARQL. Therefore, we cannot determine the reason for using GraphQL...</td>\n",
       "      <td id=\"T_fc295_row4_col2\" class=\"data row4 col2\" >2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x174eacfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "244.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "evaluate(RAG(), metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ee450",
   "metadata": {},
   "source": [
    "# Metric Analysis\n",
    "\n",
    "The maximum value per rating is (5 + 5*2 + 5) / 5 = 4\n",
    "\n",
    "4 * 10 test questions = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c998c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.¬ª\n",
      "[2] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "[3] ¬´:::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?¬ª\n",
      "\n",
      "Question: Does Weaviate use Hnswlib?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We can look for any mention of Hnswlib in the given context.\n",
      "\n",
      "Answer: No information is provided in the given context about whether Weaviate uses Hnswlib or not.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cccbf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context: N/A\n",
      "\n",
      "Assessed Question: Is the assessed answer detailed?\n",
      "\n",
      "Assessed Answer: No information is provided in the given context about whether Weaviate uses Hnswlib or not.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment answer. We can see that the answer is not detailed. It simply states that there is no information provided in the given context, but it does not provide any additional information or insight into the topic at hand. \n",
      "\n",
      "Assessment Answer: 2\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.¬ª\n",
      "[2] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "[3] ¬´:::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?¬ª\n",
      "[4] ¬´![Hacktober video](img/hacktober.gif)\n",
      "\n",
      "### [Weaviate Academy](/developers/academy) & [Workshops](/learn/workshops)\n",
      "Weaviate Academy and Workshops have had a fantastic year of learning and growth! We've been focusing on ensuring everyone has the chance to understand and use vector databases and get a grasp on Generative AI and data handling. Every week, [Zain](https://www.linkedin.com/in/zainhas/), [JP](https://www.linkedin.com/in/jphwang/), [Daniel](https://www.linkedin.com/in/malgamves/), and [Duda](https://www.linkedin.com/in/dudanogueira/) have been running introductory workshops on vector databases and Weaviate, which have been a hit. Plus, we're super excited about [JP Hwang](https://www.linkedin.com/in/jphwang/)'s initiative, the Weaviate Academy. It's a program that takes you from the basics all the way to production-level skills. Later in the year, we teamed up with [DeepLearningAI](https://www.deeplearning.ai/) to create a short course with [Sebastian](https://www.linkedin.com/in/sebawita/) and [Zain](https://www.linkedin.com/in/zainhas/), \"[Vector Databases: from Embeddings to Applications with Weaviate](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/).\" It‚Äôs been a year packed with learning.¬ª\n",
      "[5] ¬´[Weaviate](/developers/weaviate/), an open-source vector database written in Go, can serve thousands of queries per second. Running Weaviate on [Sift1M](https://www.tensorflow.org/datasets/catalog/sift1m) (a 128-dimensional representation of objects) lets you serve queries in single-digit milliseconds. But how is this possible? ![SIFT1M Benchmark example](./img/SIFT1M-benchmark.png)\n",
      "*See the [benchmark](/developers/weaviate/benchmarks/ann) page for more stats.*\n",
      "\n",
      "Weaviate does not look for the exact closest vectors in the store. Instead, it looks for approximate (close enough) elements.¬ª\n",
      "\n",
      "Assessed Question: Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\n",
      "\n",
      "Assessed Answer: No information is provided in the given context about whether Weaviate uses Hnswlib or not.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment answer. We need to determine if the assessed answer is grounded in the context provided. The context discusses Weaviate's optimization, its use of the HNSW algorithm, and its performance. However, there is no mention of Weaviate using Hnswlib. Therefore, the assessed answer is not grounded in the context as it introduces a fact not present in the context.\n",
      "\n",
      "Assessment Answer: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.¬ª\n",
      "[2] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "[3] ¬´:::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?¬ª\n",
      "[4] ¬´![Hacktober video](img/hacktober.gif)\n",
      "\n",
      "### [Weaviate Academy](/developers/academy) & [Workshops](/learn/workshops)\n",
      "Weaviate Academy and Workshops have had a fantastic year of learning and growth! We've been focusing on ensuring everyone has the chance to understand and use vector databases and get a grasp on Generative AI and data handling. Every week, [Zain](https://www.linkedin.com/in/zainhas/), [JP](https://www.linkedin.com/in/jphwang/), [Daniel](https://www.linkedin.com/in/malgamves/), and [Duda](https://www.linkedin.com/in/dudanogueira/) have been running introductory workshops on vector databases and Weaviate, which have been a hit. Plus, we're super excited about [JP Hwang](https://www.linkedin.com/in/jphwang/)'s initiative, the Weaviate Academy. It's a program that takes you from the basics all the way to production-level skills. Later in the year, we teamed up with [DeepLearningAI](https://www.deeplearning.ai/) to create a short course with [Sebastian](https://www.linkedin.com/in/sebawita/) and [Zain](https://www.linkedin.com/in/zainhas/), \"[Vector Databases: from Embeddings to Applications with Weaviate](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/).\" It‚Äôs been a year packed with learning.¬ª\n",
      "[5] ¬´[Weaviate](/developers/weaviate/), an open-source vector database written in Go, can serve thousands of queries per second. Running Weaviate on [Sift1M](https://www.tensorflow.org/datasets/catalog/sift1m) (a 128-dimensional representation of objects) lets you serve queries in single-digit milliseconds. But how is this possible? ![SIFT1M Benchmark example](./img/SIFT1M-benchmark.png)\n",
      "*See the [benchmark](/developers/weaviate/benchmarks/ann) page for more stats.*\n",
      "\n",
      "Weaviate does not look for the exact closest vectors in the store. Instead, it looks for approximate (close enough) elements.¬ª\n",
      "\n",
      "Assessed Question: Please rate how well this answer answers the question, `Does Weaviate use Hnswlib?` based on the context. `No information is provided in the given context about whether Weaviate uses Hnswlib or not.`\n",
      "\n",
      "Assessed Answer: No information is provided in the given context about whether Weaviate uses Hnswlib or not.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment answer. We need to assess if the answer provided is accurate based on the context given. The context talks about Weaviate, a vector database, and its optimization and performance. It mentions the use of the HNSW (vector) index and the implementation of the indexing algorithm on DiskANN. However, it does not mention the use of Hnswlib specifically. Therefore, the answer stating that no information is provided in the context about whether Weaviate uses Hnswlib or not is accurate.\n",
      "\n",
      "Assessment Answer: 5\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978562c",
   "metadata": {},
   "source": [
    "# BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18712073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Why would I use Weaviate as my vector database?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|‚ñà‚ñà‚ñè                                         | 1/20 [00:19<06:15, 19.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 2/20 [00:40<06:02, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: No\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Failed to run or to evaluate example Example({'question': 'What is the difference between Weaviate and for example Elasticsearch?'}) (input_keys={'question'}) with <function llm_metric at 0x281e95360> due to could not convert string to float: 'No'.\n",
      "Test Question: Do you offer Weaviate as a managed service?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 3/20 [01:12<07:15, 25.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n",
      "Test Question: How should I configure the size of my instance?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 4/20 [01:30<06:04, 22.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 5/20 [01:51<05:34, 22.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 1 examples in round 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "teleprompter = BootstrapFewShot(metric=llm_metric, max_labeled_demos=8, max_rounds=3)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd62e57",
   "metadata": {},
   "source": [
    "### Inspect the compiled prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9507f2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cross encoders are ranking models used for content-based re-ranking. They take a query and a document as input and output a score indicating the relevance of the document to the query. They can be interfaced with Weaviate to re-rank search results.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_rag(\"What do cross encoders do?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37e6fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Then we will see how the text vectorization process can be tweaked, before wrapping up by discussing a few considerations also. ## Background\n",
      "\n",
      "I often find myself saying that Weaviate makes it fast and easy to produce a vector database from text. But it can be easy to forget just how fast and how easy it can make things. It is true that even in the ‚Äúold days‚Äù of say, five to ten years ago, producing a database with vector capabilities was technically possible. You *simply* had to (*inhales deeply*) develop a vectorization algorithm, vectorize the data, build a vector index, build a database with the underlying data, integrate the vector index with the database, then forward results from a vector index query to the database and combine the outputs from both (*exhales*).¬ª\n",
      "[2] ¬´using a special algorithm, the database find the [closest](/blog/distance-metrics-in-vector-search) vectors to the given vector computed for the query. The quality of the search depends crucially on the quality of the model - this is the \"secret sauce\", as many models are [still closed source](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). The speed of the search depends crucially on Weaviate, which is open-source and [continuously improving its performance](/blog/weaviate-1-18-release). ## What exactly are vector embeddings? Vectors are numeric representations of data that capture certain features of the data.¬ª\n",
      "[3] ¬´[Weaviate](/developers/weaviate/), an open-source vector database written in Go, can serve thousands of queries per second. Running Weaviate on [Sift1M](https://www.tensorflow.org/datasets/catalog/sift1m) (a 128-dimensional representation of objects) lets you serve queries in single-digit milliseconds. But how is this possible? ![SIFT1M Benchmark example](./img/SIFT1M-benchmark.png)\n",
      "*See the [benchmark](/developers/weaviate/benchmarks/ann) page for more stats.*\n",
      "\n",
      "Weaviate does not look for the exact closest vectors in the store. Instead, it looks for approximate (close enough) elements.¬ª\n",
      "\n",
      "Question: Why would I use Weaviate as my vector database?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can find the answer by analyzing the information provided in the context. The context mentions that Weaviate makes it fast and easy to produce a vector database from text. It also mentions that Weaviate can serve thousands of queries per second and can provide search results in single-digit milliseconds. Additionally, Weaviate looks for approximate (close enough) elements instead of exact closest vectors.\n",
      "\n",
      "Answer: You would use Weaviate as your vector database because it is fast, easy to use, and can handle a large number of queries quickly. It also provides approximate search results, which can be useful in certain applications.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´He developed an Autonomous Testing Agent to enhance software testing efficiency, harnessing the power of [SuperAGI](https://www.linkedin.com/company/superagi/) and Weaviate. Meanwhile, [BYTE](https://lablab.ai/event/cohere-coral-hackathon/byte/byte-ai-based-nutrition-app), an AI-based nutrition app, clinched the top spot at the [Coral Cohere Hackathon](https://lablab.ai/event/cohere-coral-hackathon)! Ayesha and Moneebah built this project to transform and personalize nutrition advice. They used Weaviate‚Äôs vector database for search and recommendation and multi-tenancy for data security. These projects offer just a glimpse of the boundless possibilities within the AI realm, pointing the way to a future where AI is more accessible, formidable, and transformative. So, what are you waiting for if you haven't already started building with Weaviate?¬ª\n",
      "[2] ¬´For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use.¬ª\n",
      "[3] ¬´We listened to your feedback, suggestions and use cases! So we made it our mission for the `1.15` release to design and implement an **elegant solution** with a great **Developer Experience (DX)**, which you will love üòç to use for years to come. ### Announcement\n",
      "Introducing **Weaviate Cloud-native backups**. üéâ\n",
      "\n",
      "It allows you to make full database backups (or selected classes) straight to **S3**, **GCS** or the **local filesystem** with a single API call ü§©; and restore the data to a Weaviate instance of your choice with another API call. What is really great about this implementation is that you can create a backup without downtime on a running instance. The database stays fully operational (including receiving writes) while the backup is transferred to the remote storage.¬ª\n",
      "\n",
      "Question: Do you offer Weaviate as a managed service?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to look for information in the given context that indicates whether Weaviate is offered as a managed service.\n",
      "\n",
      "Answer: Yes, Weaviate is offered as a managed service. Reasoning: In the context, it is mentioned that Weaviate Cloud-native backups have been introduced, allowing users to make full database backups and restore the data to a Weaviate instance of their choice with an API call. This indicates that Weaviate can be managed by the users themselves, suggesting that it is offered as a managed service.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´First, modify the configuration file to enable a multi-node setup (e.g. 3), and add the `replicationConfig` setting to the collection definition like so:\n",
      "\n",
      "```json\n",
      "{\n",
      "  class: YOUR_CLASS_NAME,\n",
      "  ... replicationConfig: {\n",
      "    factor: 3\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      ":::tip Replication factor & nodes\n",
      "The replication factor should be less than or equal to the number of nodes. :::\n",
      "\n",
      "Once you‚Äôve modified the configuration file and set the replication factor, you should have a multi-node setup. If you are keen to try running a multi-node setup yourself, follow the optional guide here. Or you can read ahead ;).¬ª\n",
      "[2] ¬´## Recommendations & Wrap-up\n",
      "\n",
      "As we mentioned before, all you need to configure to enable replication is this in the collection definition:\n",
      "\n",
      "```json\n",
      "{\n",
      "  class: 'YOUR_CLASS_NAME',\n",
      "  ... replicationConfig: {\n",
      "    factor: 3,\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "But what replication factor would we recommend? That‚Äôs something of a subjective question, but our starting recommendation is 3. The reason is that odd numbers are preferred for consistency so that consensus can always be reached. Higher factors are also possible, but this is more of a measure to scale query throughput, rather than lead to more availability.¬ª\n",
      "[3] ¬´* Should it be just an implementation of DiskANN? * Or should we explore the capabilities of HNSW and adjust it to work on disk? * How can we guarantee the excellent database UX ‚Äì so valuable to many Weaviate users ‚Äì while reaping the benefits of a disk-based solution? Stay tuned as we explore these challenges and questions. We will share our insights as we go.¬ª\n",
      "\n",
      "Question: How should I configure the size of my instance?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to find the information about configuring the size of the instance in the given context.\n",
      "\n",
      "Answer: The given context does not provide any information about configuring the size of the instance.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´:::\n",
      "\n",
      "## Enabling replication in Weaviate\n",
      "\n",
      "Just the simple act of enabling replication on a Weaviate cluster will provide huge benefits. Doing so might actually be simpler than you might imagine. ### How to enable replication on Weaviate Cloud Services (WCS)\n",
      "\n",
      "Enabling replication on a Weaviate Cloud Services cluster is as simple as selecting the `Enable High Availability` button at cluster creation time. (Not available on sandboxes.)\n",
      "\n",
      "\n",
      "This will enable a multi-node configuration in Weaviate and ensures that each class is configured with the appropriate replication factor. ### How to enable replication on self-deployed Weaviate\n",
      "\n",
      "Enabling replication in a self-deployment setup such as a Docker or Kubernetes setup involves the following two steps.¬ª\n",
      "[2] ¬´For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use.¬ª\n",
      "[3] ¬´The data is persisted, so you can use it from future invocations, or you can [transfer it to another instance](/developers/weaviate/manage-data/read-all-objects/#restore-to-a-target-instance). You can learn more about running Weaviate locally from client code on the [Embedded Weaviate](/developers/weaviate/installation/embedded/) page. ## <i class=\"fa-solid fa-lightbulb\"></i> Use cases\n",
      "\n",
      "What can you do with Embedded Weaviate? Quite a few things!\n",
      "\n",
      "First off, you can get started very quickly with Weaviate on your local machine, without having to explicitly download, install or instantiate a server. ### Jupyter notebooks\n",
      "\n",
      "You can also use Embedded Weaviate from Jupyter notebooks, including on Google Colaboratory.¬ª\n",
      "\n",
      "Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to determine if knowledge of Docker (Compose) is necessary to use Weaviate.\n",
      "\n",
      "Answer: No, knowledge of Docker (Compose) is not necessary to use Weaviate. According to the given context, enabling replication in a self-deployment setup such as Docker or Kubernetes involves two steps, but it is not explicitly mentioned that knowledge of Docker (Compose) is required.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.¬ª\n",
      "[2] ¬´Thus, quality at the expense of speed, becomes more interesting. ### LLMs as Cross Encoders\n",
      "\n",
      "So, let‚Äôs dive into the LLM hype a little more, how can we use LLMs for re-ranking? There are generally 2 ways to do this. The first strategy is identical to the cross encoder, we give the LLM the [query, document] input and prompt it to output a score of how relevant the document is to the query. The tricky thing with this is bounding the score.¬ª\n",
      "[3] ¬´They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.¬ª\n",
      "\n",
      "Question: What do cross encoders do?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We need to find information about what cross encoders do in the given context.\n",
      "\n",
      "Answer: Cross encoders are ranking models used for content-based re-ranking. They take a query and a document as input and output a score indicating the relevance of the document to the query. They can be interfaced with Weaviate to re-rank search results.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed105e",
   "metadata": {},
   "source": [
    "### Evaluate the Compiled RAG Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8926c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 2.2 / 1  (220.0):   0%|                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 2.2 / 1  (220.0):  10%|‚ñà         | 1/10 [00:00<00:03,  2.46it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 5.6 / 2  (280.0):  10%|‚ñà         | 1/10 [00:00<00:03,  2.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 5.6 / 2  (280.0):  20%|‚ñà‚ñà        | 2/10 [00:00<00:03,  2.01it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 7.199999999999999 / 3  (240.0):  20%|‚ñè| 2/10 [00:01<00:03,  2.01\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 7.199999999999999 / 3  (240.0):  30%|‚ñé| 3/10 [00:01<00:03,  1.91\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 11.2 / 4  (280.0):  30%|‚ñà‚ñà‚ñã      | 3/10 [00:02<00:03,  1.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 11.2 / 4  (280.0):  40%|‚ñà‚ñà‚ñà‚ñå     | 4/10 [00:02<00:03,  1.95it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 12.799999999999999 / 5  (256.0):  40%|‚ñç| 4/10 [00:02<00:03,  1.9\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 12.799999999999999 / 5  (256.0):  50%|‚ñå| 5/10 [00:02<00:03,  1.4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 14.6 / 6  (243.3):  50%|‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/10 [00:13<00:03,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 14.6 / 6  (243.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6/10 [00:13<00:15,  3.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 16.6 / 7  (237.1):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6/10 [00:35<00:15,  3.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 16.6 / 7  (237.1):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7/10 [00:35<00:30, 10.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 19.8 / 8  (247.5):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7/10 [01:03<00:30, 10.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 19.8 / 8  (247.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [01:03<00:31, 15.61s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 23.8 / 9  (264.4):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [01:22<00:31, 15.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 23.8 / 9  (264.4):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:22<00:16, 16.87s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 27.400000000000002 / 10  (274.0):  90%|‚ñâ| 9/10 [01:50<00:16, 16.\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 27.400000000000002 / 10  (274.0): 100%|‚ñà| 10/10 [01:50<00:00, 11\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Average Metric: 27.400000000000002 / 10  (274.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ed9af th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_ed9af td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_ed9af_row0_col0, #T_ed9af_row0_col1, #T_ed9af_row0_col2, #T_ed9af_row1_col0, #T_ed9af_row1_col1, #T_ed9af_row1_col2, #T_ed9af_row2_col0, #T_ed9af_row2_col1, #T_ed9af_row2_col2, #T_ed9af_row3_col0, #T_ed9af_row3_col1, #T_ed9af_row3_col2, #T_ed9af_row4_col0, #T_ed9af_row4_col1, #T_ed9af_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ed9af\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ed9af_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_ed9af_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_ed9af_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ed9af_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ed9af_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_ed9af_row0_col1\" class=\"data row0 col1\" >The given context does not provide any information about whether there is support for multiple versions of the query/document embedding models to co-exist at a...</td>\n",
       "      <td id=\"T_ed9af_row0_col2\" class=\"data row0 col2\" >2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ed9af_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ed9af_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_ed9af_row1_col1\" class=\"data row1 col1\" >The given context does not provide any information about how to retrieve the total object count in a class.</td>\n",
       "      <td id=\"T_ed9af_row1_col2\" class=\"data row1 col2\" >3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ed9af_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ed9af_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_ed9af_row2_col1\" class=\"data row2 col1\" >The given context does not provide any information about how to get the cosine similarity from Weaviate's certainty.</td>\n",
       "      <td id=\"T_ed9af_row2_col2\" class=\"data row2 col2\" >1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ed9af_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ed9af_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_ed9af_row3_col1\" class=\"data row3 col1\" >The quality of search results can change depending on the specified limit because language models are constrained by input length. They can only provide a...</td>\n",
       "      <td id=\"T_ed9af_row3_col2\" class=\"data row3 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ed9af_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ed9af_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_ed9af_row4_col1\" class=\"data row4 col1\" >The given context does not provide any information about why GraphQL was used instead of SPARQL.</td>\n",
       "      <td id=\"T_ed9af_row4_col2\" class=\"data row4 col2\" >1.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x28ae4c3a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "274.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789072a8",
   "metadata": {},
   "source": [
    "# BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidentally spent $12 on this with `num_candidate_programs=20`, caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5998cbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to train 2 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Why would I use Weaviate as my vector database?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 0.0 / 1  (0.0):   0%|                    | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 0.0 / 1  (0.0):   5%|‚ñå           | 1/20 [00:00<00:08,  2.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 4.0 / 2  (200.0):   5%|‚ñå         | 1/20 [00:00<00:08,  2.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 4.0 / 2  (200.0):  10%|‚ñà         | 2/20 [00:00<00:04,  3.98it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: No\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Error for example in dev set: \t\t could not convert string to float: 'No'\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How should I configure the size of my instance?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Do you offer Weaviate as a managed service?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 5.8 / 3  (193.3):  10%|‚ñà         | 2/20 [00:00<00:04,  3.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 5.8 / 3  (193.3):  15%|‚ñà‚ñå        | 3/20 [00:00<00:05,  3.13it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 9.8 / 4  (245.0):  15%|‚ñà‚ñå        | 3/20 [00:01<00:05,  3.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 9.8 / 4  (245.0):  20%|‚ñà‚ñà        | 4/20 [00:01<00:05,  2.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 11.4 / 5  (228.0):  20%|‚ñà‚ñä       | 4/20 [00:01<00:05,  2.70it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 15.4 / 6  (256.7):  25%|‚ñà‚ñà‚ñé      | 5/20 [00:01<00:05,  2.70it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 15.4 / 6  (256.7):  30%|‚ñà‚ñà‚ñã      | 6/20 [00:01<00:04,  3.47it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Should I use references in my schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 19.0 / 7  (271.4):  30%|‚ñà‚ñà‚ñã      | 6/20 [00:02<00:04,  3.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 19.0 / 7  (271.4):  35%|‚ñà‚ñà‚ñà‚ñè     | 7/20 [00:02<00:05,  2.59it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 20.8 / 8  (260.0):  35%|‚ñà‚ñà‚ñà‚ñè     | 7/20 [00:02<00:05,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 20.8 / 8  (260.0):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [00:02<00:04,  2.78it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 22.8 / 9  (253.3):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [00:03<00:04,  2.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 22.8 / 9  (253.3):  45%|‚ñà‚ñà‚ñà‚ñà     | 9/20 [00:03<00:03,  2.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 24.400000000000002 / 10  (244.0):  45%|‚ñç| 9/20 [00:03<00:03,  2.\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 24.400000000000002 / 10  (244.0):  50%|‚ñå| 10/20 [00:03<00:02,  3\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 26.200000000000003 / 11  (238.2):  50%|‚ñå| 10/20 [00:03<00:02,  3\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 26.200000000000003 / 11  (238.2):  55%|‚ñå| 11/20 [00:03<00:02,  3\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Do Weaviate classes have namespaces?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 28.200000000000003 / 12  (235.0):  55%|‚ñå| 11/20 [00:03<00:02,  3\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 28.200000000000003 / 12  (235.0):  60%|‚ñå| 12/20 [00:03<00:01,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 30.400000000000002 / 13  (233.8):  60%|‚ñå| 12/20 [00:03<00:01,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 30.400000000000002 / 13  (233.8):  65%|‚ñã| 13/20 [00:03<00:01,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 32.400000000000006 / 14  (231.4):  65%|‚ñã| 13/20 [00:04<00:01,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 32.400000000000006 / 14  (231.4):  70%|‚ñã| 14/20 [00:04<00:01,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 2\n",
      "Test Question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 36.00000000000001 / 15  (240.0):  70%|‚ñã| 14/20 [00:04<00:01,  4.\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 36.00000000000001 / 15  (240.0):  75%|‚ñä| 15/20 [00:04<00:01,  4.\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 38.400000000000006 / 16  (240.0):  75%|‚ñä| 15/20 [00:04<00:01,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: How to deal with custom terminology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 40.2 / 17  (236.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 16/20 [00:04<00:00,  4.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 40.2 / 17  (236.5):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/20 [00:04<00:00,  4.68it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 44.2 / 18  (245.6):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/20 [00:04<00:00,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 44.2 / 18  (245.6):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 18/20 [00:04<00:00,  4.97it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 45.800000000000004 / 19  (241.1):  90%|‚ñâ| 18/20 [00:05<00:00,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 45.800000000000004 / 19  (241.1):  95%|‚ñâ| 19/20 [00:05<00:00,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How do you deal with words that have multiple meanings?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 48.6 / 20  (243.0):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 19/20 [00:05<00:00,  4.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 48.6 / 20  (243.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.70it/s]\u001b[A\u001b[A\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289e37940>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289e647c0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Average Metric: 48.6 / 20  (243.0%)\n",
      "Score: 243.0 for set: [0]\n",
      "New best score: 243.0 for seed -3\n",
      "Scores so far: [243.0]\n",
      "Best score: 243.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 4.0 / 1  (400.0):   0%|                  | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 4.0 / 1  (400.0):   5%|‚ñå         | 1/20 [00:00<00:07,  2.61it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Why would I use Weaviate as my vector database?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Do you offer Weaviate as a managed service?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 5.6 / 2  (280.0):   5%|‚ñå         | 1/20 [00:00<00:07,  2.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 5.6 / 2  (280.0):  10%|‚ñà         | 2/20 [00:00<00:07,  2.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 5.6 / 3  (186.7):  10%|‚ñà         | 2/20 [00:00<00:07,  2.35it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: NoFaithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n",
      "\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Error for example in dev set: \t\t could not convert string to float: 'No'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 7.3999999999999995 / 4  (185.0):  15%|‚ñè| 3/20 [00:01<00:07,  2.3\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 7.3999999999999995 / 4  (185.0):  20%|‚ñè| 4/20 [00:01<00:04,  3.5\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How should I configure the size of my instance?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 11.399999999999999 / 5  (228.0):  20%|‚ñè| 4/20 [00:01<00:04,  3.5\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 11.399999999999999 / 5  (228.0):  25%|‚ñé| 5/20 [00:01<00:04,  3.3\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 14.999999999999998 / 6  (250.0):  25%|‚ñé| 5/20 [00:01<00:04,  3.3\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 14.999999999999998 / 6  (250.0):  30%|‚ñé| 6/20 [00:01<00:03,  4.0\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 19.0 / 7  (271.4):  30%|‚ñà‚ñà‚ñã      | 6/20 [00:02<00:03,  4.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 19.0 / 7  (271.4):  35%|‚ñà‚ñà‚ñà‚ñè     | 7/20 [00:02<00:03,  3.71it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Should I use references in my schema?Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 20.8 / 8  (260.0):  35%|‚ñà‚ñà‚ñà‚ñè     | 7/20 [00:02<00:03,  3.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 20.8 / 8  (260.0):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [00:02<00:03,  3.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 22.8 / 9  (253.3):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [00:02<00:03,  3.71it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 2\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 24.400000000000002 / 10  (244.0):  45%|‚ñç| 9/20 [00:02<00:02,  3.\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 24.400000000000002 / 10  (244.0):  50%|‚ñå| 10/20 [00:02<00:02,  4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Do Weaviate classes have namespaces?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 26.200000000000003 / 11  (238.2):  50%|‚ñå| 10/20 [00:02<00:02,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 28.200000000000003 / 12  (235.0):  55%|‚ñå| 11/20 [00:03<00:02,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 28.200000000000003 / 12  (235.0):  60%|‚ñå| 12/20 [00:03<00:01,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 30.400000000000002 / 13  (233.8):  60%|‚ñå| 12/20 [00:03<00:01,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 2\n",
      "Test Question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 32.400000000000006 / 14  (231.4):  65%|‚ñã| 13/20 [00:03<00:01,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 32.400000000000006 / 14  (231.4):  70%|‚ñã| 14/20 [00:03<00:01,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 36.00000000000001 / 15  (240.0):  70%|‚ñã| 14/20 [00:03<00:01,  4.\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: How to deal with custom terminology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 38.400000000000006 / 16  (240.0):  75%|‚ñä| 15/20 [00:04<00:01,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 38.400000000000006 / 16  (240.0):  80%|‚ñä| 16/20 [00:04<00:00,  4\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 40.2 / 17  (236.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 16/20 [00:04<00:00,  4.04it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 44.2 / 18  (245.6):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/20 [00:04<00:00,  4.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 44.2 / 18  (245.6):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 18/20 [00:04<00:00,  4.26it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 45.800000000000004 / 19  (241.1):  90%|‚ñâ| 18/20 [00:04<00:00,  4\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: How do you deal with words that have multiple meanings?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 48.6 / 20  (243.0):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 19/20 [00:04<00:00,  4.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 48.6 / 20  (243.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.99it/s]\u001b[A\u001b[A\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289e37460>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289e37e20>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Average Metric: 48.6 / 20  (243.0%)\n",
      "Score: 243.0 for set: [4]\n",
      "Scores so far: [243.0, 243.0]\n",
      "Best score: 243.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Why would I use Weaviate as my vector database?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|‚ñà‚ñà‚ñè                                         | 1/20 [00:00<00:07,  2.38it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 2/20 [00:00<00:08,  2.07it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: No\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Failed to run or to evaluate example Example({'question': 'What is the difference between Weaviate and for example Elasticsearch?'}) (input_keys={'question'}) with <function llm_metric at 0x281e95360> due to could not convert string to float: 'No'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 3/20 [00:01<00:07,  2.25it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Do you offer Weaviate as a managed service?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 4/20 [00:01<00:07,  2.23it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How should I configure the size of my instance?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 5/20 [00:02<00:06,  2.24it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 0.0 / 1  (0.0):   0%|                    | 0/20 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 0.0 / 1  (0.0):   5%|‚ñå           | 1/20 [00:01<00:35,  1.87s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: No\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Error for example in dev set: \t\t could not convert string to float: 'No'\n",
      "Test Question: Why would I use Weaviate as my vector database?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 4.0 / 2  (200.0):   5%|‚ñå         | 1/20 [00:02<00:35,  1.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 4.0 / 2  (200.0):  10%|‚ñà         | 2/20 [00:02<00:21,  1.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How should I configure the size of my instance?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Do you offer Weaviate as a managed service?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 5.8 / 3  (193.3):  10%|‚ñà         | 2/20 [00:04<00:21,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 5.8 / 3  (193.3):  15%|‚ñà‚ñå        | 3/20 [00:04<00:27,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 7.4 / 4  (185.0):  15%|‚ñà‚ñå        | 3/20 [00:04<00:27,  1.61s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.000000000000004 / 12  (133.3):  60%|‚ñå| 12/20 [19:00<12:40, 95\n",
      "Average Metric: 12.799999999999999 / 5  (256.0):  50%|‚ñå| 5/10 [09:08<09:08, 109.\n",
      "\n",
      "\n",
      "Average Metric: 11.4 / 5  (228.0):  20%|‚ñà‚ñä       | 4/20 [00:06<00:25,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 11.4 / 5  (228.0):  25%|‚ñà‚ñà‚ñé      | 5/20 [00:06<00:18,  1.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 15.4 / 6  (256.7):  25%|‚ñà‚ñà‚ñé      | 5/20 [00:35<00:18,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 15.4 / 6  (256.7):  30%|‚ñà‚ñà‚ñã      | 6/20 [00:35<02:06,  9.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 19.2 / 7  (274.3):  30%|‚ñà‚ñà‚ñã      | 6/20 [00:35<02:06,  9.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 19.2 / 7  (274.3):  35%|‚ñà‚ñà‚ñà‚ñè     | 7/20 [00:35<01:27,  6.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: Should I use references in my schema?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 22.0 / 8  (275.0):  35%|‚ñà‚ñà‚ñà‚ñè     | 7/20 [00:59<01:27,  6.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 22.0 / 8  (275.0):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [00:59<02:20, 11.67s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 26.0 / 9  (288.9):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [01:02<02:20, 11.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 26.0 / 9  (288.9):  45%|‚ñà‚ñà‚ñà‚ñà     | 9/20 [01:02<01:40,  9.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Do Weaviate classes have namespaces?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 27.8 / 10  (278.0):  45%|‚ñà‚ñà‚ñà‚ñå    | 9/20 [01:04<01:40,  9.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 27.8 / 10  (278.0):  50%|‚ñà‚ñà‚ñà‚ñå   | 10/20 [01:04<01:09,  6.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 29.400000000000002 / 11  (267.3):  50%|‚ñå| 10/20 [01:24<01:09,  6\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 29.400000000000002 / 11  (267.3):  55%|‚ñå| 11/20 [01:24<01:37, 10\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 31.8 / 12  (265.0):  55%|‚ñà‚ñà‚ñà‚ñä   | 11/20 [01:26<01:37, 10.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 31.8 / 12  (265.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñè  | 12/20 [01:26<01:05,  8.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 34.0 / 13  (261.5):  60%|‚ñà‚ñà‚ñà‚ñà‚ñè  | 12/20 [01:51<01:05,  8.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 34.0 / 13  (261.5):  65%|‚ñà‚ñà‚ñà‚ñà‚ñå  | 13/20 [01:51<01:33, 13.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 4\n",
      "Test Question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 37.2 / 14  (265.7):  65%|‚ñà‚ñà‚ñà‚ñà‚ñå  | 13/20 [01:55<01:33, 13.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 37.2 / 14  (265.7):  70%|‚ñà‚ñà‚ñà‚ñà‚ñâ  | 14/20 [01:55<01:02, 10.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 38.800000000000004 / 15  (258.7):  70%|‚ñã| 14/20 [02:09<01:02, 10\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 38.800000000000004 / 15  (258.7):  75%|‚ñä| 15/20 [02:09<00:57, 11\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: How to deal with custom terminology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 40.6 / 16  (253.8):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 15/20 [02:11<00:57, 11.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 40.6 / 16  (253.8):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 16/20 [02:11<00:34,  8.56s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 42.2 / 17  (248.2):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 16/20 [02:15<00:34,  8.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 42.2 / 17  (248.2):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/20 [02:15<00:22,  7.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 46.2 / 18  (256.7):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/20 [02:32<00:22,  7.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 46.2 / 18  (256.7):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 18/20 [02:32<00:20, 10.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 48.0 / 19  (252.6):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 18/20 [02:37<00:20, 10.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 48.0 / 19  (252.6):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 19/20 [02:37<00:08,  8.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: How do you deal with words that have multiple meanings?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Metric: 52.0 / 20  (260.0):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 19/20 [03:05<00:08,  8.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Average Metric: 52.0 / 20  (260.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:05<00:00,  9.29s/it]\u001b[A\u001b[A\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289e37a00>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289e37e80>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Average Metric: 52.0 / 20  (260.0%)\n",
      "Score: 260.0 for set: [4]\n",
      "New best score: 260.0 for seed -1\n",
      "Scores so far: [243.0, 243.0, 260.0]\n",
      "Best score: 260.0\n",
      "Average of max per entry across top 1 scores: 2.6\n",
      "Average of max per entry across top 2 scores: 2.75\n",
      "Average of max per entry across top 3 scores: 2.75\n",
      "Average of max per entry across top 5 scores: 2.75\n",
      "Average of max per entry across top 8 scores: 2.75\n",
      "Average of max per entry across top 9999 scores: 2.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Do Weaviate classes have namespaces?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|‚ñà‚ñà‚ñè                                         | 1/20 [00:00<00:09,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 2/20 [00:01<00:11,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: How to deal with custom terminology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 3/20 [00:01<00:09,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 4/20 [00:02<00:08,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Why would I use Weaviate as my vector database?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.6 / 1  (160.0):   5%|‚ñå         | 1/20 [00:18<05:58, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Test Question: Do you offer Weaviate as a managed service?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.6 / 2  (280.0):  10%|‚ñà         | 2/20 [00:22<02:59,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How should I configure the size of my instance?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.6 / 3  (253.3):  15%|‚ñà‚ñå        | 3/20 [00:41<03:58, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.0 / 4  (275.0):  20%|‚ñà‚ñä       | 4/20 [00:44<02:35,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.6 / 5  (292.0):  25%|‚ñà‚ñà‚ñé      | 5/20 [01:03<03:16, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.4 / 6  (290.0):  30%|‚ñà‚ñà‚ñã      | 6/20 [01:10<02:34, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Should I use references in my schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.799999999999997 / 7  (297.1):  35%|‚ñé| 7/20 [01:27<02:46, 12.8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.599999999999998 / 8  (307.5):  40%|‚ñç| 8/20 [01:36<02:18, 11.5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28.2 / 9  (313.3):  45%|‚ñà‚ñà‚ñà‚ñà     | 9/20 [01:53<02:27, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Do Weaviate classes have namespaces?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.0 / 10  (300.0):  50%|‚ñà‚ñà‚ñà‚ñå   | 10/20 [01:54<01:36,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 31.8 / 11  (289.1):  55%|‚ñà‚ñà‚ñà‚ñä   | 11/20 [02:04<01:26,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 35.4 / 12  (295.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñè  | 12/20 [02:15<01:21, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 39.0 / 13  (300.0):  65%|‚ñà‚ñà‚ñà‚ñà‚ñå  | 13/20 [02:33<01:26, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 42.6 / 14  (304.3):  70%|‚ñà‚ñà‚ñà‚ñà‚ñâ  | 14/20 [02:36<00:58,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 44.2 / 15  (294.7):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 15/20 [03:01<01:11, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: How to deal with custom terminology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 46.0 / 16  (287.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 16/20 [03:03<00:42, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 49.2 / 17  (289.4):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/20 [03:04<00:22,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 50.800000000000004 / 18  (282.2):  90%|‚ñâ| 18/20 [03:06<00:12,  6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: How do you deal with words that have multiple meanings?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 54.400000000000006 / 19  (286.3):  95%|‚ñâ| 19/20 [03:27<00:10, 10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 57.60000000000001 / 20  (288.0): 100%|‚ñà| 20/20 [03:29<00:00, 10.\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289d07700>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289ca6b60>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Average Metric: 57.60000000000001 / 20  (288.0%)\n",
      "Score: 288.0 for set: [4]\n",
      "New best score: 288.0 for seed 0\n",
      "Scores so far: [243.0, 243.0, 260.0, 288.0]\n",
      "Best score: 288.0\n",
      "Average of max per entry across top 1 scores: 2.8800000000000003\n",
      "Average of max per entry across top 2 scores: 3.06\n",
      "Average of max per entry across top 3 scores: 3.11\n",
      "Average of max per entry across top 5 scores: 3.11\n",
      "Average of max per entry across top 8 scores: 3.11\n",
      "Average of max per entry across top 9999 scores: 3.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|‚ñà‚ñà‚ñè                                         | 1/20 [00:00<00:09,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 2/20 [00:00<00:08,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Why would I use Weaviate as my vector database?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.0 / 1  (400.0):   5%|‚ñå         | 1/20 [00:19<06:12, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.0 / 2  (300.0):  10%|‚ñà         | 2/20 [00:20<02:39,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Do you offer Weaviate as a managed service?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: How should I configure the size of my instance?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.8 / 3  (260.0):  15%|‚ñà‚ñå        | 3/20 [00:36<03:22, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 5  (260.0):  25%|‚ñà‚ñà‚ñé      | 5/20 [00:41<01:30,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.8 / 6  (246.7):  30%|‚ñà‚ñà‚ñã      | 6/20 [00:41<00:56,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Should I use references in my schema?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.8 / 7  (268.6):  35%|‚ñà‚ñà‚ñà‚ñè     | 7/20 [01:04<02:12, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.8 / 8  (285.0):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [01:04<01:26,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Average Metric: 24.8 / 9  (275.6):  40%|‚ñà‚ñà‚ñà‚ñå     | 8/20 [01:06<01:26,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Average Metric: 24.8 / 9  (275.6):  45%|‚ñà‚ñà‚ñà‚ñà     | 9/20 [01:06<00:58,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Do Weaviate classes have namespaces?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.8 / 10  (268.0):  50%|‚ñà‚ñà‚ñà‚ñå   | 10/20 [01:26<01:38,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28.8 / 11  (261.8):  55%|‚ñà‚ñà‚ñà‚ñä   | 11/20 [01:34<01:23,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.400000000000002 / 12  (253.3):  60%|‚ñå| 12/20 [01:52<01:36, 12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 32.800000000000004 / 13  (252.3):  65%|‚ñã| 13/20 [01:52<00:59,  8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted Answer: predicted_answer\n",
      "Test Question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 34.400000000000006 / 14  (245.7):  70%|‚ñã| 14/20 [02:14<01:14, 12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 36.800000000000004 / 15  (245.3):  75%|‚ñä| 15/20 [02:26<01:02, 12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: How to deal with custom terminology?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 36.800000000000004 / 16  (230.0):  80%|‚ñä| 16/20 [02:35<00:45, 11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t 'NoneType' object is not iterable\n",
      "Test Question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 36.800000000000004 / 17  (216.5):  85%|‚ñä| 17/20 [02:40<00:28,  9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: No\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Error for example in dev set: \t\t could not convert string to float: 'No'\n",
      "Test Question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.6 / 18  (214.4):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 18/20 [03:00<00:25, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 42.2 / 19  (222.1):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 19/20 [03:02<00:09,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: How do you deal with words that have multiple meanings?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 46.2 / 20  (231.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:34<00:00, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Average Metric: 46.2 / 20  (231.0%)\n",
      "Score: 231.0 for set: [4]\n",
      "Scores so far: [243.0, 243.0, 260.0, 288.0, 231.0]\n",
      "Best score: 288.0\n",
      "Average of max per entry across top 1 scores: 2.8800000000000003\n",
      "Average of max per entry across top 2 scores: 3.06\n",
      "Average of max per entry across top 3 scores: 3.11\n",
      "Average of max per entry across top 5 scores: 3.1399999999999997\n",
      "Average of max per entry across top 8 scores: 3.1399999999999997\n",
      "Average of max per entry across top 9999 scores: 3.1399999999999997\n",
      "5 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289ca6e60>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x289ca48e0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=llm_metric, \n",
    "                                                max_bootstrapped_demos=4,\n",
    "                                                max_labeled_demos=4, \n",
    "                                                max_rounds=1,\n",
    "                                                num_candidate_programs=2,\n",
    "                                                num_threads=2)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "second_compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee6302e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Cross encoders are ranking models used for content-based re-ranking. They are used to give a score indicating the relevance of a document to a query.'\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "705c183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually. All this means that you can also use the schema to tweak Weaviate's vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level.¬ª\n",
      "[2] ¬´![Hacktober video](img/hacktober.gif)\n",
      "\n",
      "### [Weaviate Academy](/developers/academy) & [Workshops](/learn/workshops)\n",
      "Weaviate Academy and Workshops have had a fantastic year of learning and growth! We've been focusing on ensuring everyone has the chance to understand and use vector databases and get a grasp on Generative AI and data handling. Every week, [Zain](https://www.linkedin.com/in/zainhas/), [JP](https://www.linkedin.com/in/jphwang/), [Daniel](https://www.linkedin.com/in/malgamves/), and [Duda](https://www.linkedin.com/in/dudanogueira/) have been running introductory workshops on vector databases and Weaviate, which have been a hit. Plus, we're super excited about [JP Hwang](https://www.linkedin.com/in/jphwang/)'s initiative, the Weaviate Academy. It's a program that takes you from the basics all the way to production-level skills. Later in the year, we teamed up with [DeepLearningAI](https://www.deeplearning.ai/) to create a short course with [Sebastian](https://www.linkedin.com/in/sebawita/) and [Zain](https://www.linkedin.com/in/zainhas/), \"[Vector Databases: from Embeddings to Applications with Weaviate](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/).\" It‚Äôs been a year packed with learning.¬ª\n",
      "[3] ¬´---\n",
      "title: Weaviate 1.15 release\n",
      "slug: weaviate-1-15-release\n",
      "authors: [connor, erika, laura, sebastian]\n",
      "date: 2022-09-07\n",
      "tags: ['release']\n",
      "image: ./img/hero.png\n",
      "description: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\n",
      "---\n",
      "![Weaviate 1.15 release](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "We are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\n",
      "\n",
      "If you like your content brief and to the point, here is the TL;DR of this release:\n",
      "1. [‚òÅÔ∏èCloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\n",
      "1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1.¬ª\n",
      "\n",
      "Question: Do Weaviate classes have namespaces?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to examine the given context to determine if Weaviate classes have namespaces.\n",
      "\n",
      "Answer: The given context does not provide any information about whether Weaviate classes have namespaces.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´You can even run transformer models locally with [`text2vec-transformers`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers), and modules such as [`multi2vec-clip`](/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip) can convert images and text to vectors using a CLIP model. But they all perform the same core task‚Äîwhich is to represent the ‚Äúmeaning‚Äù of the original data as a set of numbers. And that‚Äôs why semantic search works so well. import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />¬ª\n",
      "[2] ¬´## How are vector embeddings generated? The magic of vector search resides primarily in how the embeddings are generated for each entity and the query, and secondarily in how to efficiently search within very large datasets (see our [‚ÄúWhy is Vector Search so Fast‚Äù](/blog/why-is-vector-search-so-fast) article for the latter). As we mentioned, vector embeddings can be generated for various media types such as text, images, audio and others. For text, vectorization techniques have evolved tremendously over the last decade, from the venerable [word2vec](https://en.wikipedia.org/wiki/Word2vec) ([2013](https://code.google.com/archive/p/word2vec/)), to the state-of-the-art transformer models era, spurred by the release of [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) in [2018](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). ### Word-level dense vector models (word2vec, GloVe, etc.)\n",
      "[word2vec](https://wiki.pathmind.com/word2vec) is a [family of model architectures](https://www.tensorflow.org/tutorials/text/word2vec) that introduced the idea of ‚Äúdense‚Äù vectors in language processing, in which all values are non-zero.¬ª\n",
      "[3] ¬´However, this can be done by aggregating vectors of constituent words, which is often done by incorporating weightings such that certain words are weighted more heavily than others. However, word2vec still suffers from important limitations:\n",
      "* it doesn‚Äôt address words with multiple meanings (polysemantic): ‚Äúrun‚Äù, ‚Äúset‚Äù, ‚Äúgo‚Äù, or ‚Äútake‚Äù each have [over 300 meanings](https://www.insider.com/words-with-the-most-definitions-2019-1) (!)\n",
      "* it doesn‚Äôt address words with ambiguous meanings: ‚Äúto consult‚Äù can be its own antonym, like [many other words](http://www.dailywritingtips.com/75-contronyms-words-with-contradictory-meanings/)\n",
      "\n",
      "Which takes us to the next, state-of-the-art, models. ### Transformer models (BERT, ELMo, and others)\n",
      "The current state-of-the-art models are based on what‚Äôs called a ‚Äútransformer‚Äù architecture as introduced in [this paper](https://arxiv.org/abs/1706.03762). [Transformer models](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) such as BERT and its successors improve search accuracy, [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) by looking at every word‚Äôs context to create full contextual embeddings (though [the exact mechanism of BERT‚Äôs success is not fully understood](https://aclanthology.org/D19-1445/)). Unlike word2vec embeddings which are context-agnostic, transformer-generated embeddings take the entire input text into account‚Äîeach occurrence of a word has its own embedding that is modified by the surrounding text.¬ª\n",
      "\n",
      "Question: Why isn't there a text2vec-contextionary in my language?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to understand the context of the given information. The context mentions different techniques for generating vector embeddings for text, such as word2vec and transformer models like BERT. It also mentions that transformer models take the entire input text into account and generate context-aware embeddings. However, there is no specific mention of a \"text2vec-contextionary\" in the given context. Therefore, we can conclude that the reason there isn't a text2vec-contextionary in your language is not provided in the given information.\n",
      "\n",
      "Answer: The reason why there isn't a text2vec-contextionary in your language is not provided in the given information.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´One technique is to prompt it with:\n",
      "`please output a relevance score on a scale of 1 to 100.`\n",
      "\n",
      "I think the second strategy is a bit more interesting, in which we put as many documents as we can in the input and ask the LLM to rank them. The key to making this work is the emergence of LLMs to follow instructions, especially with formatting their output. By prompting this ranking with ‚Äúplease output the ranking as a dictionary of IDs with the key equal to the rank and the value equal to the document id‚Äù. Also interesting is the question around how many documents we can rank like this and how expensive it is. For example, if we want to re-rank 100 documents, but can only fit 5 in the input at a time, we are going to need to construct some kind of tournament-style decomposition of the ranking task.¬ª\n",
      "[2] ¬´What I love about all this work, like self-ask, chain of thought, we're developing new querying languages. This is like us inventing SQL, except that we didn't design the database. The database came into being and we have to figure out how to interact with it. That example I mentioned about the IPython interaction, like that's a, again, it's a new querying language. And I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part.¬ª\n",
      "[3] ¬´have published ‚ÄúLarge Language Models are easily distracted by irrelevant context‚Äù, highlighting how problematic bad precision in search can be for retrieval-augmented generation. The recent developments in LLM agent tooling such as LangChain, LlamaIndex, and recent projects such as AutoGPT or Microsoft‚Äôs Semantic Kernel are paving the way towards letting LLMs run for a while to complete complex tasks. By ranking each handoff from search to prompt, we can achieve better results in each intermediate task. Thus when we leave an LLM running overnight to research the future of ranking models, we can expect a better final result in the morning!\n",
      "\n",
      "\n",
      "import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />¬ª\n",
      "\n",
      "Question: How to deal with custom terminology?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to analyze the given context to understand how to deal with custom terminology.\n",
      "\n",
      "Answer: The given context does not provide any information about how to deal with custom terminology. Therefore, we cannot determine the answer based on the given context.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually. All this means that you can also use the schema to tweak Weaviate's vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level.¬ª\n",
      "[2] ¬´Let's recap exactly what Weaviate does. ### Text vectorization in Weaviate\n",
      "\n",
      "import VectorizationBehavior from '/_includes/vectorization.behavior.mdx';\n",
      "\n",
      "<VectorizationBehavior/>\n",
      "\n",
      "Now that we understand this, you might be asking - is it possible to customize the vectorization process? The answer is, yes, of course. ## Tweaking text2vec vectorization in Weaviate\n",
      "\n",
      "Some of you might have noticed that we have not done anything at all with the schema so far. This meant that the schema used is one generated by the auto-schema feature and thus the vectorizations were carried out using default options.¬ª\n",
      "[3] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "\n",
      "Question: Why does Weaviate have a schema and not an ontology?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can gather information from the given context. In [1], it is mentioned that the schema is used to define the data type, vectorizer, and cross-references between classes in Weaviate. Additionally, the schema can be used to tweak Weaviate's vectorization behavior. In [2], it is stated that the schema used in Weaviate is generated by the auto-schema feature.\n",
      "\n",
      "Answer: Weaviate has a schema instead of an ontology because the schema is used to define data types, vectorizers, and cross-references between classes, and it allows for customization of the vectorization process.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.¬ª\n",
      "[2] ¬´Thus, quality at the expense of speed, becomes more interesting. ### LLMs as Cross Encoders\n",
      "\n",
      "So, let‚Äôs dive into the LLM hype a little more, how can we use LLMs for re-ranking? There are generally 2 ways to do this. The first strategy is identical to the cross encoder, we give the LLM the [query, document] input and prompt it to output a score of how relevant the document is to the query. The tricky thing with this is bounding the score.¬ª\n",
      "[3] ¬´They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.¬ª\n",
      "\n",
      "Question: What do cross encoders do?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We can gather information from the given context. In [1], it is mentioned that cross encoders are ranking models used for content-based re-ranking. They are often used for re-ranking search results. In [2], it is stated that cross encoders are used to give the LLM the query and document input and prompt it to output a score of how relevant the document is to the query.\n",
      "\n",
      "Answer: Cross encoders are ranking models used for content-based re-ranking. They are used to give a score indicating the relevance of a document to a query.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6de6d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.6 / 1  (360.0):  10%|‚ñà         | 1/10 [00:21<03:12, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.4 / 2  (270.0):  20%|‚ñà‚ñà        | 2/10 [00:44<02:58, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.2 / 3  (240.0):  30%|‚ñà‚ñà‚ñà       | 3/10 [01:11<02:50, 24.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.0 / 4  (225.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [01:36<02:28, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.6 / 5  (212.0):  50%|‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/10 [01:38<01:22, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.8 / 6  (213.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6/10 [02:01<01:15, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.0 / 7  (228.6):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7/10 [02:24<01:00, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.6 / 8  (245.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [02:48<00:42, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.200000000000003 / 9  (257.8):  90%|‚ñâ| 9/10 [03:28<00:27, 27.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.400000000000002 / 10  (264.0): 100%|‚ñà| 10/10 [03:41<00:00, 22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Average Metric: 26.400000000000002 / 10  (264.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cc824 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_cc824 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_cc824_row0_col0, #T_cc824_row0_col1, #T_cc824_row0_col2, #T_cc824_row1_col0, #T_cc824_row1_col1, #T_cc824_row1_col2, #T_cc824_row2_col0, #T_cc824_row2_col1, #T_cc824_row2_col2, #T_cc824_row3_col0, #T_cc824_row3_col1, #T_cc824_row3_col2, #T_cc824_row4_col0, #T_cc824_row4_col1, #T_cc824_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cc824\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cc824_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_cc824_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_cc824_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cc824_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cc824_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_cc824_row0_col1\" class=\"data row0 col1\" >The given context does not provide any information about whether there is support for multiple versions of the query/document embedding models to co-exist at a...</td>\n",
       "      <td id=\"T_cc824_row0_col2\" class=\"data row0 col2\" >3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc824_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cc824_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_cc824_row1_col1\" class=\"data row1 col1\" >The given context does not provide any information about how to retrieve the total object count in a class. Therefore, we cannot determine the answer...</td>\n",
       "      <td id=\"T_cc824_row1_col2\" class=\"data row1 col2\" >1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc824_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cc824_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_cc824_row2_col1\" class=\"data row2 col1\" >The given context does not provide any information about how to get the cosine similarity from Weaviate's certainty. Therefore, we cannot determine the answer based...</td>\n",
       "      <td id=\"T_cc824_row2_col2\" class=\"data row2 col2\" >1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc824_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cc824_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_cc824_row3_col1\" class=\"data row3 col1\" >The given context does not provide any information about why the quality of search results changes depending on the specified limit or how to fix...</td>\n",
       "      <td id=\"T_cc824_row3_col2\" class=\"data row3 col2\" >1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc824_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_cc824_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_cc824_row4_col1\" class=\"data row4 col1\" >The given context does not provide any information about why GraphQL was used instead of SPARQL.</td>\n",
       "      <td id=\"T_cc824_row4_col2\" class=\"data row4 col2\" >1.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x289d94a30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "264.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1e2b1",
   "metadata": {},
   "source": [
    "# BayesianSignatureOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e272916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1/10 [00:00<00:03,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:03,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1/10 [00:01<00:10,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:01<00:06,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:02<00:04,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:02<00:03,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1/10 [00:00<00:03,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 1\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:03,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1/10 [00:00<00:04,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n",
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:03,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.12it/s]\n",
      "[I 2024-02-11 14:46:53,175] A new study created in memory with name: no-name-5e566642-02dd-4eee-8691-53989ee359f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.0 / 1  (200.0):  10%|‚ñà         | 1/10 [00:00<00:07,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.8 / 2  (190.0):  20%|‚ñà‚ñà        | 2/10 [00:19<01:30, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.4 / 3  (180.0):  30%|‚ñà‚ñà‚ñà       | 3/10 [00:20<00:44,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n",
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.4 / 4  (185.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:44<01:20, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.4 / 5  (228.0):  50%|‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/10 [01:04<01:19, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 6  (216.7):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6/10 [01:29<01:16, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 2\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.0 / 7  (242.9):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7/10 [02:02<01:11, 23.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.0 / 8  (262.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [02:03<00:32, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.0 / 9  (277.8):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [02:03<00:11, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 29.0 / 10  (290.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:27<00:00, 14.72s/it]\n",
      "[I 2024-02-11 14:49:20,377] Trial 0 finished with value: 290.0 and parameters: {'10883682544_predictor_instruction': 1, '10883682544_predictor_demos': 4}. Best is trial 0 with value: 290.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Average Metric: 29.0 / 10  (290.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Best Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Vector embeddings with Weaviate\n",
      "\n",
      "For this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules.¬ª\n",
      "[2] ¬´You can even run transformer models locally with [`text2vec-transformers`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers), and modules such as [`multi2vec-clip`](/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip) can convert images and text to vectors using a CLIP model. But they all perform the same core task‚Äîwhich is to represent the ‚Äúmeaning‚Äù of the original data as a set of numbers. And that‚Äôs why semantic search works so well. import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />¬ª\n",
      "[3] ¬´Each `text2vec-*` module uses an external API (like `text2vec-openai` or `text2vec-huggingface`) or a local instance like `text2vec-transformers` to produce a vector for each object. Let's try vectorizing data with the `text2vec-cohere` module. We will be using data from `tiny_jeopardy.csv` [available here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain) containing questions from the game show Jeopardy. We'll just use a few (20) questions here, but the [full dataset on Kaggle](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) includes 200k+ questions. Load the data into a Pandas dataframe, then populate Weaviate like this:\n",
      "\n",
      "```python\n",
      "client.batch.configure(batch_size=100)  # Configure batch\n",
      "with client.batch as batch:\n",
      "    for i, row in df.iterrows():\n",
      "        properties = {\n",
      "            \"question\": row.Question,\n",
      "            \"answer\": row.Answer\n",
      "        }\n",
      "        batch.add_data_object(properties, \"Question\")\n",
      "```\n",
      "\n",
      "This should add a series of `Question` objects with text properties like this:\n",
      "\n",
      "```text\n",
      "{'question': 'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger',\n",
      " 'answer': \"McDonald's\"}\n",
      "```\n",
      "\n",
      "Since we use the `text2vec-cohere` module to vectorize our data, we can query Weaviate to find data objects most similar to any input text.¬ª\n",
      "\n",
      "Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to analyze the information provided in the context to determine if it is possible to train a custom text2vec-contextionary vectorizer module.\n",
      "\n",
      "Best Answer: No, it is not mentioned in the context that you can train your own text2vec-contextionary vectorizer module. The context only mentions the availability of different vectorizer models and service providers, such as Hugging Face, OpenAI, and Cohere, that can be used with Weaviate. It also mentions the option to bring your own vectors if you already have a vectorization pipeline available. However, there is no mention of training a custom text2vec-contextionary vectorizer module.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´For example, if we broke down this blog post into **chapters** in Weaviate, with **title** and **content** properties. We could run a query to summarize the *\"New distance metrics\"* chapter like this:\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    Chapter(\n",
      "      where: {\n",
      "        operator: Equal\n",
      "        path: \"title\"\n",
      "        valueText: \"New distance metrics\"\n",
      "      }\n",
      "    ) {\n",
      "      title\n",
      "      _additional{\n",
      "        summary(\n",
      "          properties: [\"content\"],\n",
      "        ) {\n",
      "          property\n",
      "          result\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Which would return the following result:\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  \"data\": {\n",
      "    \"Get\": {\n",
      "      \"Chapters\": [\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"summary\": [\n",
      "              {\n",
      "                \"property\": \"content\",\n",
      "                \"result\": \"Weaviate 1.15 adds two new distance metrics - Hamming\n",
      "                 distance and Manhattan distance. In total, you can now choose\n",
      "                 between five various distance metrics to support your datasets. Check out the metrics documentation page, for the full overview\n",
      "                 of all the available metrics in Weaviate.\"\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"title\": \"New distance metrics\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"errors\": null\n",
      "}\n",
      "```\n",
      "\n",
      "Head to the [Summarization Module docs page](/developers/weaviate/modules/reader-generator-modules/sum-transformers) to learn more. ### Hugging Face Module\n",
      "The Hugging Face module (`text2vec-huggingface`) opens up doors to over 600 [Hugging Face sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity), ready to be used in Weaviate as a vectorization module.¬ª\n",
      "[2] ¬´Weaviate generates vector embeddings using [modules](/developers/weaviate/modules/retriever-vectorizer-modules) (OpenAI, Cohere, Google PaLM etc.), and conveniently stores both objects and vectors in the same database. For example, vectorizing the two words above might result in:\n",
      "\n",
      "```text\n",
      "cat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]\n",
      "kitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]\n",
      "```\n",
      "\n",
      "These two vectors have a very high similarity. In contrast, vectors for ‚Äúbanjo‚Äù or ‚Äúcomedy‚Äù would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words. Now that you‚Äôve seen what vectors are, and that they can represent meaning to some extent, you might have further questions.¬ª\n",
      "[3] ¬´And instead of the `text2vec-cohere` module, we will go straight to the Cohere API. Concatenating the text from the object:\n",
      "\n",
      "```python\n",
      "str_in = ' '.join([i for i in properties.values()])\n",
      "```\n",
      "\n",
      "We see:\n",
      "\n",
      "```text\n",
      "'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger McDonald\\'s'\n",
      "```\n",
      "\n",
      "Then, use the Cohere API to generate the vector like so, where `cohere_key` is the API key (keep it secret!), and `model` is the vectorizer. ```python\n",
      "import cohere\n",
      "co = cohere.Client(cohere_key)\n",
      "co_resp = co.embed([str_in], model=\"embed-multilingual-v2.0\")\n",
      "```\n",
      "\n",
      "Then we run a `nearVector` based query to find the best matching object to this vector:\n",
      "\n",
      "```python\n",
      "client.query.get(\n",
      "    \"Question\",\n",
      "    [\"question\", \"answer\"]\n",
      ").with_limit(2).with_near_vector(\n",
      "    {'vector': co_resp.embeddings[0]}\n",
      ").with_additional(['distance']).do()\n",
      "```\n",
      "\n",
      "Interestingly, we get a distance of `0.0181` - small, but not zero. In other words - we did something differently to Weaviate!\n",
      "\n",
      "Let's go through the differences one by one, which hopefully will help you to see Weaviate's default behavior. First, Weaviate sorts properties alphabetically (a-z) before concatenation.¬ª\n",
      "\n",
      "Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to understand how Weaviate calculates certainty and how it relates to cosine similarity.\n",
      "\n",
      "Best Answer: Weaviate does not directly provide the cosine similarity from its certainty value. The certainty value in Weaviate represents the confidence level of the result, but it is not equivalent to cosine similarity. Cosine similarity is a measure of the similarity between two vectors, while certainty in Weaviate is a measure of confidence in the result. To calculate cosine similarity, you would need to compare the vector embeddings of the objects using a specific algorithm or library.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept.¬ª\n",
      "[2] ¬´These embeddings better reflect the polysemantic nature of words, which can only be disambiguated when they are considered in context. Some of the potential downsides include:\n",
      "* increased compute requirements: fine-tuning transformer models is much slower (on the order of hours vs. minutes)\n",
      "* increased memory requirements: context-sensitivity greatly increases memory requirements, which often leads to limited possible input lengths\n",
      "\n",
      "Despite these downsides, transformer models have been wildly successful. Countless text vectorizer models have proliferated over the recent past. Plus, many more vectorizer models exist for other data types such as audio, video and images, to name a few.¬ª\n",
      "[3] ¬´Further these approaches are well positioned to generalize to Recommendation. In Recommendation, instead of taking a [query, document] as input to a cross-encoder, we take as input a [user description, document] pair. For example, we can ask users to describe their preferences. Further, we could combine these in trios of [user description, query, item] for LLM, or more lightweight cross-encoder, ranking. There is a bonus 3rd idea where we use the log probabilities concatenating the query with the document.¬ª\n",
      "\n",
      "Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to analyze the given context to see if there is any mention of support for multiple versions of the query/document embedding models.\n",
      "\n",
      "Best Answer: No specific information is provided in the given context about support for multiple versions of the query/document embedding models to co-exist at a given time.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Now, that's a lot of new models. üòâ\n",
      "\n",
      "#### How this works\n",
      "The way the module works, Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. You need a `Hugging Face API Token` to use the Hugging Face module. You can [request it here](https://huggingface.co/login?next=%2Fsettings%2Ftokens).¬ª\n",
      "[2] ¬´What I love about all this work, like self-ask, chain of thought, we're developing new querying languages. This is like us inventing SQL, except that we didn't design the database. The database came into being and we have to figure out how to interact with it. That example I mentioned about the IPython interaction, like that's a, again, it's a new querying language. And I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part.¬ª\n",
      "[3] ¬´Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept.¬ª\n",
      "\n",
      "Question: Can I connect my own module?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to understand if it is possible to connect a custom module to Weaviate.\n",
      "\n",
      "Best Answer: Yes, it is possible to connect your own module to Weaviate. Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. This means that you can use your own module by integrating it with Weaviate.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.¬ª\n",
      "[2] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "[3] ¬´:::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?¬ª\n",
      "\n",
      "Question: Does Weaviate use Hnswlib?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We need to analyze the given context to determine if Weaviate uses Hnswlib.\n",
      "\n",
      "Best Answer: Yes, Weaviate uses Hnswlib. The context mentions that Weaviate has a highly optimized implementation of the HNSW (Hierarchical Navigable Small World) algorithm, which is the algorithm used by Hnswlib. Additionally, the context mentions the implementation of the indexing algorithm on DiskANN, which is a disk-based variant of HNSW.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.8 / 1  (380.0):  10%|‚ñà         | 1/10 [00:21<03:09, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.199999999999999 / 2  (360.0):  20%|‚ñè| 2/10 [00:21<01:11,  8.96"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.799999999999999 / 3  (293.3):  30%|‚ñé| 3/10 [00:22<00:35,  5.10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.399999999999999 / 4  (285.0):  40%|‚ñç| 4/10 [00:22<00:19,  3.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.399999999999999 / 5  (268.0):  50%|‚ñå| 5/10 [00:22<00:11,  2.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.999999999999998 / 6  (250.0):  60%|‚ñå| 6/10 [00:23<00:06,  1.6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.599999999999998 / 7  (237.1):  70%|‚ñã| 7/10 [00:45<00:25,  8.4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.2 / 8  (252.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [01:08<00:25, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.0 / 9  (266.7):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:33<00:16, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.8 / 10  (278.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:53<00:00, 11.34s/it]\n",
      "[I 2024-02-11 14:51:13,743] Trial 1 finished with value: 278.0 and parameters: {'10883682544_predictor_instruction': 3, '10883682544_predictor_demos': 2}. Best is trial 0 with value: 290.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Average Metric: 27.8 / 10  (278.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are presented with several contexts or data scenarios, typically related to software usage or tech process optimization. Based on this, you will be asked questions to clarify or retrieve certain details. Your task is to provide an accurate and specific response, which will assist in a tech-oriented customer service role, such as creating FAQs, supporting a chatbot, or enhancing Natural Language Processing to understand user queries more efficiently.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Relevant Information: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Finally, we can jump on a bike to reach our local destination. For a better understanding, consider the below graphic, which shows a graph with all the connections generated using 1000 objects in two dimensions. <img\n",
      "    src={require('./img/vamana-graph.png').default}\n",
      "    alt=\"Vamana graph with 1000 objects\"\n",
      "    style={{ maxWidth: \"50%\" }}\n",
      "/>\n",
      "\n",
      "If we iterate over it in steps ‚Äì we can analyze how Vamana navigates through the graph. <img\n",
      "    src={require('./img/vamana-graph-animated.gif').default}\n",
      "    alt=\"Vamana graph - animated in 3/6/9 steps\"\n",
      "    style={{ maxWidth: \"50%\" }}\n",
      "/>\n",
      "\n",
      "In the **first step**, you can see that the entry point for the search is in the center, and then the long-range connections allow jumping to the edges. This means that when a query comes, it will quickly move in the appropriate direction.<br/>\n",
      "The **second**, **third**, and **final steps** highlight the nodes reachable within **three**, **six**, and **nine** hops from the entry node.¬ª\n",
      "[2] ¬´It was that Ofir did such a phenomenal job of figuring out a way to measure the complexity of the knowledge that was extracted from the model. He gave us a benchmark, a ladder to climb, a way to measure whether we could retrieve certain kinds of information from models. And I think that's going to open the door to a ton more benchmarks. And you know what happens when there's a benchmark. We optimize the hell out of that benchmark and it moves science forward‚Ä¶ [ truncated for visibility ] |\n",
      "| Hybrid Only            | Or, at least being able to ask follow up questions when it‚Äôs unclear about and that‚Äôs surprisingly not that difficult to do with these current systems, as long as you‚Äôre halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might get.¬ª\n",
      "[3] ¬´The data is persisted, so you can use it from future invocations, or you can [transfer it to another instance](/developers/weaviate/manage-data/read-all-objects/#restore-to-a-target-instance). You can learn more about running Weaviate locally from client code on the [Embedded Weaviate](/developers/weaviate/installation/embedded/) page. ## <i class=\"fa-solid fa-lightbulb\"></i> Use cases\n",
      "\n",
      "What can you do with Embedded Weaviate? Quite a few things!\n",
      "\n",
      "First off, you can get started very quickly with Weaviate on your local machine, without having to explicitly download, install or instantiate a server. ### Jupyter notebooks\n",
      "\n",
      "You can also use Embedded Weaviate from Jupyter notebooks, including on Google Colaboratory.¬ª\n",
      "\n",
      "Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to analyze the given context to find information about iterating through objects and paginated API calls.\n",
      "\n",
      "Relevant Information: The context does not provide any information about the best way to iterate through objects or paginated API calls.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.¬ª\n",
      "[2] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "[3] ¬´:::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?¬ª\n",
      "\n",
      "Question: Does Weaviate use Hnswlib?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can look for any mention of Hnswlib in the given context.\n",
      "\n",
      "Relevant Information: No information is provided in the given context about whether Weaviate uses Hnswlib or not.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´|\n",
      "\n",
      "By re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et al. in the self-ask paper! This result was originally placed at #6 with Hybrid Search only. This is a great opportunity to preview the discussion of how LLMs use search versus humans. When humans search, we are used to scrolling through the results a bit to see the one that makes sense. In contrast, language models are constrained by input length; we can only give so many results to the input of the LLM.¬ª\n",
      "[2] ¬´To end this article, let‚Äôs discuss a little further why ranking is so exciting for the most hyped pairing of LLMs and Search: Retrieval-Augmented Generation. ## Ranking for Retrieval-Augmented Generation\n",
      "A lot of the recent successes of vector search can be attributed to their effectiveness as a tool for Large Language Models. So whereas the speed trade-off with rankers may be a major bottleneck for how humans use search, it might not be as much of a problem for how LLMs use search. Of course fast generation is preferred, but if you are paying for the result, quality may be more important than speed. Shi et al.¬ª\n",
      "[3] ¬´have published ‚ÄúLarge Language Models are easily distracted by irrelevant context‚Äù, highlighting how problematic bad precision in search can be for retrieval-augmented generation. The recent developments in LLM agent tooling such as LangChain, LlamaIndex, and recent projects such as AutoGPT or Microsoft‚Äôs Semantic Kernel are paving the way towards letting LLMs run for a while to complete complex tasks. By ranking each handoff from search to prompt, we can achieve better results in each intermediate task. Thus when we leave an LLM running overnight to research the future of ranking models, we can expect a better final result in the morning!\n",
      "\n",
      "\n",
      "import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />¬ª\n",
      "\n",
      "Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We can refer to the given context to understand why the quality of search results changes depending on the specified limit. The context mentions that language models are constrained by input length, meaning they can only provide a limited number of results based on the input. This limitation is not present when humans search, as they can scroll through multiple results. To fix this, we can adjust the specified limit to allow for more results to be generated by the language model.\n",
      "\n",
      "Relevant Information: The quality of search results changes depending on the specified limit because language models are constrained by input length and can only provide a limited number of results. To fix this, we can adjust the specified limit to allow for more results to be generated by the language model.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´HNSW, on the other hand, implements the same idea a bit differently. Instead of having all information together on a flat graph, it has a hierarchical representation distributed across multiple layers. The top layers only contain long-range connections, and as you dive deeper into the layers, your query is routed to the appropriate region where you can look more locally for your answer. So your search starts making only big jumps across the top layers until it finally looks for the closest points locally in the bottom layers. ## Performance comparison\n",
      "So, how do they perform?¬ª\n",
      "[2] ¬´If we make one final trip to the supermarket to understand what a guava is, we could look at the things around it‚Äîother fruit. Somewhat further away we might find guava juice or tinned guavas, but there's really no reason to look four aisles over for guava-flavored cat food. Using ANN allowed high-dimensional searches with near-perfect accuracy to be returned in milliseconds instead of hours. To be practical, vector databases also needed something prosaically called CRUD support. That stands for \"create, read, update and delete,\" and solving that technical challenge meant that the complex process of indexing the database could be done once, rather than being repeated from scratch whenever the database was updated.¬ª\n",
      "[3] ¬´It was that Ofir did such a phenomenal job of figuring out a way to measure the complexity of the knowledge that was extracted from the model. He gave us a benchmark, a ladder to climb, a way to measure whether we could retrieve certain kinds of information from models. And I think that's going to open the door to a ton more benchmarks. And you know what happens when there's a benchmark. We optimize the hell out of that benchmark and it moves science forward‚Ä¶ [ truncated for visibility ] |\n",
      "| Hybrid Only            | Or, at least being able to ask follow up questions when it‚Äôs unclear about and that‚Äôs surprisingly not that difficult to do with these current systems, as long as you‚Äôre halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might get.¬ª\n",
      "\n",
      "Question: Why did you use GraphQL instead of SPARQL?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to analyze the given context to find any information related to the use of GraphQL instead of SPARQL.\n",
      "\n",
      "Relevant Information: The given context does not provide any information about the use of GraphQL instead of SPARQL. Therefore, we cannot determine the reason for using GraphQL over SPARQL based on the given context.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.¬ª\n",
      "[2] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "[3] ¬´:::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?¬ª\n",
      "\n",
      "Question: Does Weaviate use Hnswlib?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We can look for any mention of Hnswlib in the given context.\n",
      "\n",
      "Relevant Information: The given context mentions that Weaviate has a highly optimized implementation of the HNSW algorithm, but it does not specify whether Weaviate uses Hnswlib or not. Therefore, we cannot determine if Weaviate uses Hnswlib based on the given context.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.0 / 1  (200.0):  10%|‚ñà         | 1/10 [00:00<00:04,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.4 / 2  (170.0):  20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 1\n",
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.0 / 3  (166.7):  30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.6 / 4  (190.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:02<00:03,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 4\n",
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.6 / 5  (192.0):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:02,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.2 / 6  (186.7):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6/10 [00:03<00:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.6 / 7  (208.6):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7/10 [00:03<00:01,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.6 / 8  (232.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [00:03<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.6 / 9  (251.1):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:04<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.400000000000002 / 10  (244.0): 100%|‚ñà| 10/10 [00:04<00:00,  2\n",
      "[I 2024-02-11 14:51:18,463] Trial 2 finished with value: 244.0 and parameters: {'10883682544_predictor_instruction': 0, '10883682544_predictor_demos': 0}. Best is trial 0 with value: 290.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n",
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Average Metric: 24.400000000000002 / 10  (244.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n",
      "\n",
      "Next, we optimized memory allocations for the HNSW (vector) index.¬ª\n",
      "[2] ¬´The past for vector searching definitely was not a ‚Äúsimpler time‚Äù, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post.¬ª\n",
      "[3] ¬´:::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?¬ª\n",
      "\n",
      "Question: Does Weaviate use Hnswlib?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We can look for any mention of Hnswlib in the given context.\n",
      "\n",
      "Answer: No information is provided in the given context about whether Weaviate uses Hnswlib or not.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "\n",
    "llm_prompter = dspy.OpenAI(model='gpt-4', max_tokens=2000, model_type='chat')\n",
    "\n",
    "teleprompter = BayesianSignatureOptimizer(task_model=dspy.settings.lm,\n",
    "                                          metric=llm_metric,\n",
    "                                          prompt_model=llm_prompter,\n",
    "                                          n=5,\n",
    "                                          verbose=False)\n",
    "\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "third_compiled_rag = teleprompter.compile(RAG(), devset=devset,\n",
    "                                         optuna_trials_num=3,\n",
    "                                         max_bootstrapped_demos=4,\n",
    "                                         max_labeled_demos=4,\n",
    "                                         eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bc8cb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Cross encoders are ranking models used for content-based re-ranking. They are designed to determine the relevance of a document to a query. Cross encoders take a [query, document] input and output a score indicating the relevance of the document to the query. They can be used to re-rank search results and provide a more personalized and context-aware search experience.'\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367bcc7",
   "metadata": {},
   "source": [
    "# Check this out!!\n",
    "\n",
    "Below you can see how the BayesianSignatureOptimizer jointly (1) optimizes the task instruction to:\n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "As well as sourcing input-output examples for the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a834857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Best Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Vector embeddings with Weaviate\n",
      "\n",
      "For this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules.¬ª\n",
      "[2] ¬´You can even run transformer models locally with [`text2vec-transformers`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers), and modules such as [`multi2vec-clip`](/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip) can convert images and text to vectors using a CLIP model. But they all perform the same core task‚Äîwhich is to represent the ‚Äúmeaning‚Äù of the original data as a set of numbers. And that‚Äôs why semantic search works so well. import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />¬ª\n",
      "[3] ¬´Each `text2vec-*` module uses an external API (like `text2vec-openai` or `text2vec-huggingface`) or a local instance like `text2vec-transformers` to produce a vector for each object. Let's try vectorizing data with the `text2vec-cohere` module. We will be using data from `tiny_jeopardy.csv` [available here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain) containing questions from the game show Jeopardy. We'll just use a few (20) questions here, but the [full dataset on Kaggle](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) includes 200k+ questions. Load the data into a Pandas dataframe, then populate Weaviate like this:\n",
      "\n",
      "```python\n",
      "client.batch.configure(batch_size=100)  # Configure batch\n",
      "with client.batch as batch:\n",
      "    for i, row in df.iterrows():\n",
      "        properties = {\n",
      "            \"question\": row.Question,\n",
      "            \"answer\": row.Answer\n",
      "        }\n",
      "        batch.add_data_object(properties, \"Question\")\n",
      "```\n",
      "\n",
      "This should add a series of `Question` objects with text properties like this:\n",
      "\n",
      "```text\n",
      "{'question': 'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger',\n",
      " 'answer': \"McDonald's\"}\n",
      "```\n",
      "\n",
      "Since we use the `text2vec-cohere` module to vectorize our data, we can query Weaviate to find data objects most similar to any input text.¬ª\n",
      "\n",
      "Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to analyze the information provided in the context to determine if it is possible to train a custom text2vec-contextionary vectorizer module.\n",
      "\n",
      "Best Answer: No, it is not mentioned in the context that you can train your own text2vec-contextionary vectorizer module. The context only mentions the availability of different vectorizer models and service providers, such as Hugging Face, OpenAI, and Cohere, that can be used with Weaviate. It also mentions the option to bring your own vectors if you already have a vectorization pipeline available. However, there is no mention of training a custom text2vec-contextionary vectorizer module.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´For example, if we broke down this blog post into **chapters** in Weaviate, with **title** and **content** properties. We could run a query to summarize the *\"New distance metrics\"* chapter like this:\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    Chapter(\n",
      "      where: {\n",
      "        operator: Equal\n",
      "        path: \"title\"\n",
      "        valueText: \"New distance metrics\"\n",
      "      }\n",
      "    ) {\n",
      "      title\n",
      "      _additional{\n",
      "        summary(\n",
      "          properties: [\"content\"],\n",
      "        ) {\n",
      "          property\n",
      "          result\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Which would return the following result:\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  \"data\": {\n",
      "    \"Get\": {\n",
      "      \"Chapters\": [\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"summary\": [\n",
      "              {\n",
      "                \"property\": \"content\",\n",
      "                \"result\": \"Weaviate 1.15 adds two new distance metrics - Hamming\n",
      "                 distance and Manhattan distance. In total, you can now choose\n",
      "                 between five various distance metrics to support your datasets. Check out the metrics documentation page, for the full overview\n",
      "                 of all the available metrics in Weaviate.\"\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"title\": \"New distance metrics\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"errors\": null\n",
      "}\n",
      "```\n",
      "\n",
      "Head to the [Summarization Module docs page](/developers/weaviate/modules/reader-generator-modules/sum-transformers) to learn more. ### Hugging Face Module\n",
      "The Hugging Face module (`text2vec-huggingface`) opens up doors to over 600 [Hugging Face sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity), ready to be used in Weaviate as a vectorization module.¬ª\n",
      "[2] ¬´Weaviate generates vector embeddings using [modules](/developers/weaviate/modules/retriever-vectorizer-modules) (OpenAI, Cohere, Google PaLM etc.), and conveniently stores both objects and vectors in the same database. For example, vectorizing the two words above might result in:\n",
      "\n",
      "```text\n",
      "cat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]\n",
      "kitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]\n",
      "```\n",
      "\n",
      "These two vectors have a very high similarity. In contrast, vectors for ‚Äúbanjo‚Äù or ‚Äúcomedy‚Äù would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words. Now that you‚Äôve seen what vectors are, and that they can represent meaning to some extent, you might have further questions.¬ª\n",
      "[3] ¬´And instead of the `text2vec-cohere` module, we will go straight to the Cohere API. Concatenating the text from the object:\n",
      "\n",
      "```python\n",
      "str_in = ' '.join([i for i in properties.values()])\n",
      "```\n",
      "\n",
      "We see:\n",
      "\n",
      "```text\n",
      "'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger McDonald\\'s'\n",
      "```\n",
      "\n",
      "Then, use the Cohere API to generate the vector like so, where `cohere_key` is the API key (keep it secret!), and `model` is the vectorizer. ```python\n",
      "import cohere\n",
      "co = cohere.Client(cohere_key)\n",
      "co_resp = co.embed([str_in], model=\"embed-multilingual-v2.0\")\n",
      "```\n",
      "\n",
      "Then we run a `nearVector` based query to find the best matching object to this vector:\n",
      "\n",
      "```python\n",
      "client.query.get(\n",
      "    \"Question\",\n",
      "    [\"question\", \"answer\"]\n",
      ").with_limit(2).with_near_vector(\n",
      "    {'vector': co_resp.embeddings[0]}\n",
      ").with_additional(['distance']).do()\n",
      "```\n",
      "\n",
      "Interestingly, we get a distance of `0.0181` - small, but not zero. In other words - we did something differently to Weaviate!\n",
      "\n",
      "Let's go through the differences one by one, which hopefully will help you to see Weaviate's default behavior. First, Weaviate sorts properties alphabetically (a-z) before concatenation.¬ª\n",
      "\n",
      "Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to understand how Weaviate calculates certainty and how it relates to cosine similarity.\n",
      "\n",
      "Best Answer: Weaviate does not directly provide the cosine similarity from its certainty value. The certainty value in Weaviate represents the confidence level of the result, but it is not equivalent to cosine similarity. Cosine similarity is a measure of the similarity between two vectors, while certainty in Weaviate is a measure of confidence in the result. To calculate cosine similarity, you would need to compare the vector embeddings of the objects using a specific algorithm or library.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept.¬ª\n",
      "[2] ¬´These embeddings better reflect the polysemantic nature of words, which can only be disambiguated when they are considered in context. Some of the potential downsides include:\n",
      "* increased compute requirements: fine-tuning transformer models is much slower (on the order of hours vs. minutes)\n",
      "* increased memory requirements: context-sensitivity greatly increases memory requirements, which often leads to limited possible input lengths\n",
      "\n",
      "Despite these downsides, transformer models have been wildly successful. Countless text vectorizer models have proliferated over the recent past. Plus, many more vectorizer models exist for other data types such as audio, video and images, to name a few.¬ª\n",
      "[3] ¬´Further these approaches are well positioned to generalize to Recommendation. In Recommendation, instead of taking a [query, document] as input to a cross-encoder, we take as input a [user description, document] pair. For example, we can ask users to describe their preferences. Further, we could combine these in trios of [user description, query, item] for LLM, or more lightweight cross-encoder, ranking. There is a bonus 3rd idea where we use the log probabilities concatenating the query with the document.¬ª\n",
      "\n",
      "Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to analyze the given context to see if there is any mention of support for multiple versions of the query/document embedding models.\n",
      "\n",
      "Best Answer: No specific information is provided in the given context about support for multiple versions of the query/document embedding models to co-exist at a given time.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´Now, that's a lot of new models. üòâ\n",
      "\n",
      "#### How this works\n",
      "The way the module works, Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. You need a `Hugging Face API Token` to use the Hugging Face module. You can [request it here](https://huggingface.co/login?next=%2Fsettings%2Ftokens).¬ª\n",
      "[2] ¬´What I love about all this work, like self-ask, chain of thought, we're developing new querying languages. This is like us inventing SQL, except that we didn't design the database. The database came into being and we have to figure out how to interact with it. That example I mentioned about the IPython interaction, like that's a, again, it's a new querying language. And I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part.¬ª\n",
      "[3] ¬´Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept.¬ª\n",
      "\n",
      "Question: Can I connect my own module?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to understand if it is possible to connect a custom module to Weaviate.\n",
      "\n",
      "Best Answer: Yes, it is possible to connect your own module to Weaviate. Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. This means that you can use your own module by integrating it with Weaviate.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] ¬´[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.¬ª\n",
      "[2] ¬´Thus, quality at the expense of speed, becomes more interesting. ### LLMs as Cross Encoders\n",
      "\n",
      "So, let‚Äôs dive into the LLM hype a little more, how can we use LLMs for re-ranking? There are generally 2 ways to do this. The first strategy is identical to the cross encoder, we give the LLM the [query, document] input and prompt it to output a score of how relevant the document is to the query. The tricky thing with this is bounding the score.¬ª\n",
      "[3] ¬´They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.¬ª\n",
      "\n",
      "Question: What do cross encoders do?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We need to analyze the given context to understand what cross encoders do.\n",
      "\n",
      "Best Answer: Cross encoders are ranking models used for content-based re-ranking. They are designed to determine the relevance of a document to a query. Cross encoders take a [query, document] input and output a score indicating the relevance of the document to the query. They can be used to re-rank search results and provide a more personalized and context-aware search experience.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e98db525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.0 / 1  (200.0):  10%|‚ñà         | 1/10 [00:00<00:02,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.8 / 2  (190.0):  20%|‚ñà‚ñà        | 2/10 [00:00<00:02,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 2\n",
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.4 / 3  (180.0):  30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:03,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n",
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.4 / 4  (185.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:01<00:02,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.4 / 5  (228.0):  50%|‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/10 [00:02<00:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 6  (216.7):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6/10 [00:02<00:02,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 2\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.0 / 7  (242.9):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7/10 [00:03<00:01,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.0 / 8  (262.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8/10 [00:04<00:01,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.0 / 9  (277.8):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:04<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 29.0 / 10  (290.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Average Metric: 29.0 / 10  (290.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dbcea th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_dbcea td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_dbcea_row0_col0, #T_dbcea_row0_col1, #T_dbcea_row0_col2, #T_dbcea_row1_col0, #T_dbcea_row1_col1, #T_dbcea_row1_col2, #T_dbcea_row2_col0, #T_dbcea_row2_col1, #T_dbcea_row2_col2, #T_dbcea_row3_col0, #T_dbcea_row3_col1, #T_dbcea_row3_col2, #T_dbcea_row4_col0, #T_dbcea_row4_col1, #T_dbcea_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dbcea\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dbcea_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_dbcea_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_dbcea_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dbcea_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_dbcea_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_dbcea_row0_col1\" class=\"data row0 col1\" >No specific information is provided in the given context about support for multiple versions of the query/document embedding models to co-exist at a given time.</td>\n",
       "      <td id=\"T_dbcea_row0_col2\" class=\"data row0 col2\" >2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbcea_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_dbcea_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_dbcea_row1_col1\" class=\"data row1 col1\" >The given context does not provide specific information on how to retrieve the total object count in a class. However, in Weaviate, you can use...</td>\n",
       "      <td id=\"T_dbcea_row1_col2\" class=\"data row1 col2\" >1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbcea_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_dbcea_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_dbcea_row2_col1\" class=\"data row2 col1\" >Weaviate does not directly provide the cosine similarity from its certainty value. The certainty value in Weaviate represents the confidence level of the result, but...</td>\n",
       "      <td id=\"T_dbcea_row2_col2\" class=\"data row2 col2\" >1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbcea_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_dbcea_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_dbcea_row3_col1\" class=\"data row3 col1\" >The quality of search results can change depending on the specified limit because the limit determines the number of results that are returned. When the...</td>\n",
       "      <td id=\"T_dbcea_row3_col2\" class=\"data row3 col2\" >2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbcea_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_dbcea_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_dbcea_row4_col1\" class=\"data row4 col1\" >The given context does not provide any information about why GraphQL was used instead of SPARQL. The context mainly discusses the performance and functionality of...</td>\n",
       "      <td id=\"T_dbcea_row4_col2\" class=\"data row4 col2\" >4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x28972ac20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "290.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdc368",
   "metadata": {},
   "source": [
    "# Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb3f6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Uncompiled\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate = Evaluate(devset=testset, num_threads=1, display_progress=True, display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9ec6960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.2 / 1  (320.0):   7%|‚ñã         | 1/14 [00:28<06:09, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use pre- or post-filtering ANN index search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.2 / 2  (360.0):  14%|‚ñà‚ñç        | 2/14 [00:55<05:31, 27.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How does Weaviate's vector and scalar filtering work?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.2 / 3  (240.0):  21%|‚ñà‚ñà‚ñè       | 3/14 [01:17<04:37, 25.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: No\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Error for example in dev set: \t\t could not convert string to float: 'No'\n",
      "Test Question: What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.4 / 4  (210.0):  29%|‚ñà‚ñà‚ñä       | 4/14 [01:39<03:59, 23.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Test Question: Data import takes long / is slow, what is causing this and what can I do?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.2 / 5  (244.0):  36%|‚ñà‚ñà‚ñà‚ñè     | 5/14 [02:07<03:47, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 4\n",
      "Test Question: How can slow queries be optimized?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.2 / 6  (270.0):  43%|‚ñà‚ñà‚ñà‚ñä     | 6/14 [02:32<03:21, 25.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: When scalar and vector search are combined, will the scalar filter happen before or after the nearest neighbor (vector) search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.599999999999998 / 7  (251.4):  50%|‚ñå| 7/14 [02:51<02:42, 23.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 2\n",
      "Overall: 1\n",
      "Test Question: Regarding \"filtered vector search\": Since this is a two-phase pipeline, how big can that list of IDs get? Do you know how that size might affect query performance?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.799999999999997 / 8  (247.5):  57%|‚ñå| 8/14 [03:14<02:18, 23.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: My Weaviate setup is using more memory than what I think is reasonable. How can I debug this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.799999999999997 / 9  (264.4):  64%|‚ñã| 9/14 [03:39<01:58, 23.6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How can I print a stack trace of Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.999999999999996 / 10  (270.0):  71%|‚ñã| 10/14 [04:08<01:40, 25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: Can I request a feature in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.399999999999995 / 11  (276.4):  79%|‚ñä| 11/14 [04:28<01:10, 23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Test Question: What is Weaviate's consistency model in a distributed setup?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 33.99999999999999 / 12  (283.3):  86%|‚ñä| 12/14 [04:50<00:46, 23."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: With your aggregations I could not see how to do time buckets, is this possible?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 37.99999999999999 / 13  (292.3):  93%|‚ñâ| 13/14 [05:10<00:22, 22."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How can I run the latest master branch with Docker Compose?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 37.99999999999999 / 14  (271.4): 100%|‚ñà| 14/14 [05:27<00:00, 23."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: No\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Error for example in dev set: \t\t could not convert string to float: 'No'\n",
      "Average Metric: 37.99999999999999 / 14  (271.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_96a08 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_96a08 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_96a08_row0_col0, #T_96a08_row0_col1, #T_96a08_row0_col2, #T_96a08_row1_col0, #T_96a08_row1_col1, #T_96a08_row1_col2, #T_96a08_row2_col0, #T_96a08_row2_col1, #T_96a08_row2_col2, #T_96a08_row3_col0, #T_96a08_row3_col1, #T_96a08_row3_col2, #T_96a08_row4_col0, #T_96a08_row4_col1, #T_96a08_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_96a08\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_96a08_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_96a08_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_96a08_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_96a08_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_96a08_row0_col0\" class=\"data row0 col0\" >Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?</td>\n",
       "      <td id=\"T_96a08_row0_col1\" class=\"data row0 col1\" >No, not all ANN algorithms are potential candidates to become an indexation plugin in Weaviate. Reasoning: The context mentions that Weaviate has implemented the HNSW...</td>\n",
       "      <td id=\"T_96a08_row0_col2\" class=\"data row0 col2\" >3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_96a08_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_96a08_row1_col0\" class=\"data row1 col0\" >Does Weaviate use pre- or post-filtering ANN index search?</td>\n",
       "      <td id=\"T_96a08_row1_col1\" class=\"data row1 col1\" >Weaviate uses pre-filtering ANN index search. Reasoning: In the context, it is mentioned that Weaviate uses the HNSW indexing algorithm, which builds a hierarchical representation...</td>\n",
       "      <td id=\"T_96a08_row1_col2\" class=\"data row1 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_96a08_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_96a08_row2_col0\" class=\"data row2 col0\" >How does Weaviate's vector and scalar filtering work?</td>\n",
       "      <td id=\"T_96a08_row2_col1\" class=\"data row2 col1\" >nan</td>\n",
       "      <td id=\"T_96a08_row2_col2\" class=\"data row2 col2\" >0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_96a08_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_96a08_row3_col0\" class=\"data row3 col0\" >What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?</td>\n",
       "      <td id=\"T_96a08_row3_col1\" class=\"data row3 col1\" >More RAM is more important for query speed in Weaviate.</td>\n",
       "      <td id=\"T_96a08_row3_col2\" class=\"data row3 col2\" >1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_96a08_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_96a08_row4_col0\" class=\"data row4 col0\" >Data import takes long / is slow, what is causing this and what can I do?</td>\n",
       "      <td id=\"T_96a08_row4_col1\" class=\"data row4 col1\" >The slow data import in Weaviate could be caused by the large number of data objects being imported and the need to maintain data integrity....</td>\n",
       "      <td id=\"T_96a08_row4_col2\" class=\"data row4 col2\" >3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x290ec0850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 9 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "271.43"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(uncompiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f919a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.6 / 1  (360.0):   7%|‚ñã         | 1/14 [00:28<06:07, 28.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use pre- or post-filtering ANN index search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.6 / 2  (380.0):  14%|‚ñà‚ñç        | 2/14 [00:54<05:24, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How does Weaviate's vector and scalar filtering work?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.2 / 3  (306.7):  21%|‚ñà‚ñà‚ñè       | 3/14 [01:15<04:26, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.6 / 4  (290.0):  29%|‚ñà‚ñà‚ñå      | 4/14 [01:39<04:03, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Data import takes long / is slow, what is causing this and what can I do?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 5  (300.0):  36%|‚ñà‚ñà‚ñà‚ñè     | 5/14 [02:03<03:38, 24.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 5\n",
      "Overall: 4\n",
      "Test Question: How can slow queries be optimized?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.0 / 6  (316.7):  43%|‚ñà‚ñà‚ñà‚ñä     | 6/14 [02:24<03:03, 22.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: When scalar and vector search are combined, will the scalar filter happen before or after the nearest neighbor (vector) search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.8 / 7  (297.1):  50%|‚ñà‚ñà‚ñà‚ñà‚ñå    | 7/14 [02:45<02:36, 22.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: Regarding \"filtered vector search\": Since this is a two-phase pipeline, how big can that list of IDs get? Do you know how that size might affect query performance?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.0 / 8  (287.5):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/14 [03:07<02:12, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: My Weaviate setup is using more memory than what I think is reasonable. How can I debug this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.0 / 9  (300.0):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9/14 [03:45<02:16, 27.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How can I print a stack trace of Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28.6 / 10  (286.0):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà  | 10/14 [04:12<01:49, 27.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: Can I request a feature in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 32.6 / 11  (296.4):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 11/14 [04:35<01:17, 25.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is Weaviate's consistency model in a distributed setup?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 36.6 / 12  (305.0):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12/14 [05:01<00:51, 25.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: With your aggregations I could not see how to do time buckets, is this possible?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.4 / 13  (295.4):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13/14 [05:24<00:24, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: How can I run the latest master branch with Docker Compose?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 40.0 / 14  (285.7): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [05:47<00:00, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Average Metric: 40.0 / 14  (285.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bfec6 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_bfec6 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_bfec6_row0_col0, #T_bfec6_row0_col1, #T_bfec6_row0_col2, #T_bfec6_row1_col0, #T_bfec6_row1_col1, #T_bfec6_row1_col2, #T_bfec6_row2_col0, #T_bfec6_row2_col1, #T_bfec6_row2_col2, #T_bfec6_row3_col0, #T_bfec6_row3_col1, #T_bfec6_row3_col2, #T_bfec6_row4_col0, #T_bfec6_row4_col1, #T_bfec6_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bfec6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bfec6_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_bfec6_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_bfec6_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bfec6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bfec6_row0_col0\" class=\"data row0 col0\" >Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?</td>\n",
       "      <td id=\"T_bfec6_row0_col1\" class=\"data row0 col1\" >No, not all ANN algorithms are potential candidates to become an indexation plugin in Weaviate. According to the given context, Weaviate has implemented the HNSW...</td>\n",
       "      <td id=\"T_bfec6_row0_col2\" class=\"data row0 col2\" >3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bfec6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bfec6_row1_col0\" class=\"data row1 col0\" >Does Weaviate use pre- or post-filtering ANN index search?</td>\n",
       "      <td id=\"T_bfec6_row1_col1\" class=\"data row1 col1\" >Weaviate uses pre-filtering ANN index search. According to the given context, Weaviate uses the HNSW indexing algorithm, which builds a hierarchical representation of the index...</td>\n",
       "      <td id=\"T_bfec6_row1_col2\" class=\"data row1 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bfec6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bfec6_row2_col0\" class=\"data row2 col0\" >How does Weaviate's vector and scalar filtering work?</td>\n",
       "      <td id=\"T_bfec6_row2_col1\" class=\"data row2 col1\" >The given context does not provide any information about how Weaviate's vector and scalar filtering works.</td>\n",
       "      <td id=\"T_bfec6_row2_col2\" class=\"data row2 col2\" >1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bfec6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bfec6_row3_col0\" class=\"data row3 col0\" >What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?</td>\n",
       "      <td id=\"T_bfec6_row3_col1\" class=\"data row3 col1\" >According to the given context, increasing the frequency of garbage collector (GC) cycles will use more CPU, while increasing RAM will incur additional costs. It...</td>\n",
       "      <td id=\"T_bfec6_row3_col2\" class=\"data row3 col2\" >2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bfec6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_bfec6_row4_col0\" class=\"data row4 col0\" >Data import takes long / is slow, what is causing this and what can I do?</td>\n",
       "      <td id=\"T_bfec6_row4_col1\" class=\"data row4 col1\" >The cause of slow data import in Weaviate could be the large volume of data being imported. Weaviate recommends using the batch import feature for...</td>\n",
       "      <td id=\"T_bfec6_row4_col2\" class=\"data row4 col2\" >3.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x288bdfe20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 9 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "285.71"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54b00bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.6 / 1  (360.0):   7%|‚ñã         | 1/14 [00:22<04:47, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use pre- or post-filtering ANN index search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.4 / 2  (270.0):  14%|‚ñà‚ñç        | 2/14 [00:55<05:41, 28.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: How does Weaviate's vector and scalar filtering work?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.2 / 3  (240.0):  21%|‚ñà‚ñà‚ñè       | 3/14 [01:21<05:05, 27.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.2 / 4  (280.0):  29%|‚ñà‚ñà‚ñå      | 4/14 [01:47<04:28, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Data import takes long / is slow, what is causing this and what can I do?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.0 / 5  (240.0):  36%|‚ñà‚ñà‚ñà‚ñè     | 5/14 [02:15<04:05, 27.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test Question: How can slow queries be optimized?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.6 / 6  (226.7):  43%|‚ñà‚ñà‚ñà‚ñä     | 6/14 [02:40<03:32, 26.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: When scalar and vector search are combined, will the scalar filter happen before or after the nearest neighbor (vector) search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.4 / 7  (248.6):  50%|‚ñà‚ñà‚ñà‚ñà‚ñå    | 7/14 [03:01<02:53, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: Regarding \"filtered vector search\": Since this is a two-phase pipeline, how big can that list of IDs get? Do you know how that size might affect query performance?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.599999999999998 / 8  (245.0):  57%|‚ñå| 8/14 [03:23<02:23, 23.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: My Weaviate setup is using more memory than what I think is reasonable. How can I debug this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.2 / 9  (235.6):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9/14 [03:51<02:05, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: How can I print a stack trace of Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.0 / 10  (240.0):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà  | 10/14 [04:16<01:40, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 3\n",
      "Test Question: Can I request a feature in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.4 / 11  (249.1):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 11/14 [04:43<01:16, 25.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 5\n",
      "Test Question: What is Weaviate's consistency model in a distributed setup?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.599999999999998 / 12  (255.0):  86%|‚ñä| 12/14 [05:07<00:50, 25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test Question: With your aggregations I could not see how to do time buckets, is this possible?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 34.199999999999996 / 13  (263.1):  93%|‚ñâ| 13/14 [05:30<00:24, 24"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: How can I run the latest master branch with Docker Compose?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 35.8 / 14  (255.7): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [05:51<00:00, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Average Metric: 35.8 / 14  (255.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b1b02 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b1b02 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b1b02_row0_col0, #T_b1b02_row0_col1, #T_b1b02_row0_col2, #T_b1b02_row1_col0, #T_b1b02_row1_col1, #T_b1b02_row1_col2, #T_b1b02_row2_col0, #T_b1b02_row2_col1, #T_b1b02_row2_col2, #T_b1b02_row3_col0, #T_b1b02_row3_col1, #T_b1b02_row3_col2, #T_b1b02_row4_col0, #T_b1b02_row4_col1, #T_b1b02_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b1b02\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b1b02_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_b1b02_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_b1b02_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b1b02_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b1b02_row0_col0\" class=\"data row0 col0\" >Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?</td>\n",
       "      <td id=\"T_b1b02_row0_col1\" class=\"data row0 col1\" >The given context does not provide any information about whether all ANN algorithms are potential candidates to become an indexation plugin in Weaviate. Therefore, we...</td>\n",
       "      <td id=\"T_b1b02_row0_col2\" class=\"data row0 col2\" >3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1b02_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b1b02_row1_col0\" class=\"data row1 col0\" >Does Weaviate use pre- or post-filtering ANN index search?</td>\n",
       "      <td id=\"T_b1b02_row1_col1\" class=\"data row1 col1\" >The given context does not provide information about whether Weaviate uses pre- or post-filtering ANN index search.</td>\n",
       "      <td id=\"T_b1b02_row1_col2\" class=\"data row1 col2\" >1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1b02_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b1b02_row2_col0\" class=\"data row2 col0\" >How does Weaviate's vector and scalar filtering work?</td>\n",
       "      <td id=\"T_b1b02_row2_col1\" class=\"data row2 col1\" >The given context does not provide any information about how Weaviate's vector and scalar filtering work. Therefore, we cannot determine the answer based on the...</td>\n",
       "      <td id=\"T_b1b02_row2_col2\" class=\"data row2 col2\" >1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1b02_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b1b02_row3_col0\" class=\"data row3 col0\" >What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?</td>\n",
       "      <td id=\"T_b1b02_row3_col1\" class=\"data row3 col1\" >Based on the given information, it is not explicitly stated whether more CPU power or more RAM is more important for query speed in Weaviate....</td>\n",
       "      <td id=\"T_b1b02_row3_col2\" class=\"data row3 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1b02_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b1b02_row4_col0\" class=\"data row4 col0\" >Data import takes long / is slow, what is causing this and what can I do?</td>\n",
       "      <td id=\"T_b1b02_row4_col1\" class=\"data row4 col1\" >The given context does not provide specific information about the causes of slow data import in Weaviate or the possible solutions. Therefore, we cannot determine...</td>\n",
       "      <td id=\"T_b1b02_row4_col2\" class=\"data row4 col2\" >0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x28ae55ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 9 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "255.71"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0790a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.2 / 1  (320.0):   7%|‚ñã         | 1/14 [00:34<07:25, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use pre- or post-filtering ANN index search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.0 / 2  (300.0):  14%|‚ñà‚ñç        | 2/14 [00:59<05:45, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How does Weaviate's vector and scalar filtering work?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.4 / 3  (313.3):  21%|‚ñà‚ñà‚ñè       | 3/14 [01:23<04:51, 26.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Test Question: What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.4 / 4  (335.0):  29%|‚ñà‚ñà‚ñå      | 4/14 [01:48<04:22, 26.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Data import takes long / is slow, what is causing this and what can I do?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.4 / 5  (348.0):  36%|‚ñà‚ñà‚ñà‚ñè     | 5/14 [02:11<03:43, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How can slow queries be optimized?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.799999999999997 / 6  (346.7):  43%|‚ñç| 6/14 [02:36<03:20, 25.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Test Question: When scalar and vector search are combined, will the scalar filter happen before or after the nearest neighbor (vector) search?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.199999999999996 / 7  (331.4):  50%|‚ñå| 7/14 [03:04<03:01, 25.8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Regarding \"filtered vector search\": Since this is a two-phase pipeline, how big can that list of IDs get? Do you know how that size might affect query performance?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.999999999999996 / 8  (337.5):  57%|‚ñå| 8/14 [03:31<02:37, 26.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 4\n",
      "Test Question: My Weaviate setup is using more memory than what I think is reasonable. How can I debug this?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.799999999999997 / 9  (342.2):  64%|‚ñã| 9/14 [03:56<02:09, 25.8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Test Question: How can I print a stack trace of Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 34.8 / 10  (348.0):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà  | 10/14 [04:22<01:43, 25.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I request a feature in Weaviate?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.8 / 11  (352.7):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 11/14 [04:44<01:14, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: What is Weaviate's consistency model in a distributed setup?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 42.8 / 12  (356.7):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12/14 [05:08<00:48, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: With your aggregations I could not see how to do time buckets, is this possible?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 46.8 / 13  (360.0):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13/14 [05:31<00:24, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: How can I run the latest master branch with Docker Compose?\n",
      "Predicted Answer: predicted_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 48.4 / 14  (345.7): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [05:55<00:00, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n",
      "Average Metric: 48.4 / 14  (345.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2e128 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_2e128 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_2e128_row0_col0, #T_2e128_row0_col1, #T_2e128_row0_col2, #T_2e128_row1_col0, #T_2e128_row1_col1, #T_2e128_row1_col2, #T_2e128_row2_col0, #T_2e128_row2_col1, #T_2e128_row2_col2, #T_2e128_row3_col0, #T_2e128_row3_col1, #T_2e128_row3_col2, #T_2e128_row4_col0, #T_2e128_row4_col1, #T_2e128_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2e128\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2e128_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_2e128_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_2e128_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2e128_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2e128_row0_col0\" class=\"data row0 col0\" >Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?</td>\n",
       "      <td id=\"T_2e128_row0_col1\" class=\"data row0 col1\" >No, not all ANN (Approximate Nearest Neighbor) algorithms are potential candidates to become an indexation plugin in Weaviate. The context mentions that Weaviate has implemented...</td>\n",
       "      <td id=\"T_2e128_row0_col2\" class=\"data row0 col2\" >3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e128_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2e128_row1_col0\" class=\"data row1 col0\" >Does Weaviate use pre- or post-filtering ANN index search?</td>\n",
       "      <td id=\"T_2e128_row1_col1\" class=\"data row1 col1\" >Weaviate uses pre-filtering ANN index search. The context mentions that Weaviate implements the HNSW indexing algorithm, which builds a hierarchical representation of the index in...</td>\n",
       "      <td id=\"T_2e128_row1_col2\" class=\"data row1 col2\" >2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e128_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2e128_row2_col0\" class=\"data row2 col0\" >How does Weaviate's vector and scalar filtering work?</td>\n",
       "      <td id=\"T_2e128_row2_col1\" class=\"data row2 col1\" >The given context does not provide specific information about how Weaviate's vector and scalar filtering works. It mentions that Weaviate looks for approximate (close enough)...</td>\n",
       "      <td id=\"T_2e128_row2_col2\" class=\"data row2 col2\" >3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e128_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2e128_row3_col0\" class=\"data row3 col0\" >What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?</td>\n",
       "      <td id=\"T_2e128_row3_col1\" class=\"data row3 col1\" >Based on the information provided in the context, it is more important to have more RAM for query speed in Weaviate. The context mentions optimizations...</td>\n",
       "      <td id=\"T_2e128_row3_col2\" class=\"data row3 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e128_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_2e128_row4_col0\" class=\"data row4 col0\" >Data import takes long / is slow, what is causing this and what can I do?</td>\n",
       "      <td id=\"T_2e128_row4_col1\" class=\"data row4 col1\" >The context mentions that Weaviate offers a batch import feature for adding data objects in bulk, which is recommended for speeding up the import process....</td>\n",
       "      <td id=\"T_2e128_row4_col2\" class=\"data row4 col2\" >4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2897248b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 9 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "345.71"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
